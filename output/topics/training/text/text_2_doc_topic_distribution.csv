Link to paper,Score,Text,Cleaned,unigrams,bigrams,trigrams,topic1,topic2,topic3,topic4,topic5,topic6,topic7,topic8
https://link.springer.com/content/pdf/10.1140/epjds/s13688-017-0110-z.pdf,0,Several different types of information were extracted from the collected Instagram data. We used total posts per user per day as a measure of user activity. We gauged community reaction by counting the number of comments and ‘likes’ each posted photograph received. Face detection software was used to determine whether or not a photograph contained a human face as well as count the total number of faces in each photo as a proxy measure for participants’ social activity levels. Pixel-level averages were computed for Hue Saturation and Value (HSV) three color properties commonly used in image analysis. Hue describes an image’s coloring on the light spectrum (ranging from red to blue/purple). Lower hue values indicate more red and higher hue values indicate more blue. Saturation refers to the vividness of an image. Low saturation makes an image appear grey and faded. Value refers to image brightness. Lower brightness scores indicate a darker image. See Figure  for a comparison of high and low HSV values. We also checked metadata to assess whether an Instagram-provided filter was applied to alter the appearance of a photograph. Collectively these measures served as the feature set in our primary model. For the separate model fit on ratings data we used only the four ratings categories (happy sad likable interesting) as predictors.,several different type of information were extracted from the collected instagram data we used total post per user per day a a measure of user activity we gauged community reaction by counting the number of comment and like each posted photograph received face detection software wa used to determine whether or not a photograph contained a human face a well a count the total number of face in each photo a a proxy measure for participant social activity level pixellevel average were computed for hue saturation and value hsv three color property commonly used in image analysis hue describes an image coloring on the light spectrum ranging from red to bluepurple lower hue value indicate more red and higher hue value indicate more blue saturation refers to the vividness of an image low saturation make an image appear grey and faded value refers to image brightness lower brightness score indicate a darker image see figure for a comparison of high and low hsv value we also checked metadata to ass whether an instagramprovided filter wa applied to alter the appearance of a photograph collectively these measure served a the feature set in our primary model for the separate model fit on rating data we used only the four rating category happy sad likable interesting a predictor,"['several', 'different', 'type', 'information', 'extracted', 'collected', 'instagram', 'data', 'used', 'total', 'post', 'per', 'user', 'per', 'day', 'measure', 'user', 'activity', 'gauged', 'community', 'reaction', 'counting', 'number', 'comment', 'like', 'posted', 'photograph', 'received', 'face', 'detection', 'software', 'wa', 'used', 'determine', 'whether', 'photograph', 'contained', 'human', 'face', 'well', 'count', 'total', 'number', 'face', 'photo', 'proxy', 'measure', 'participant', 'social', 'activity', 'level', 'pixellevel', 'average', 'computed', 'hue', 'saturation', 'value', 'hsv', 'three', 'color', 'property', 'commonly', 'used', 'image', 'analysis', 'hue', 'describes', 'image', 'coloring', 'light', 'spectrum', 'ranging', 'red', 'bluepurple', 'lower', 'hue', 'value', 'indicate', 'red', 'higher', 'hue', 'value', 'indicate', 'blue', 'saturation', 'refers', 'vividness', 'image', 'low', 'saturation', 'make', 'image', 'appear', 'grey', 'faded', 'value', 'refers', 'image', 'brightness', 'lower', 'brightness', 'score', 'indicate', 'darker', 'image', 'see', 'figure', 'comparison', 'high', 'low', 'hsv', 'value', 'also', 'checked', 'metadata', 'ass', 'whether', 'instagramprovided', 'filter', 'wa', 'applied', 'alter', 'appearance', 'photograph', 'collectively', 'measure', 'served', 'feature', 'set', 'primary', 'model', 'separate', 'model', 'fit', 'rating', 'data', 'used', 'four', 'rating', 'category', 'happy', 'sad', 'likable', 'interesting', 'predictor']","['several different', 'different type', 'type information', 'information extracted', 'extracted collected', 'collected instagram', 'instagram data', 'data used', 'used total', 'total post', 'post per', 'per user', 'user per', 'per day', 'day measure', 'measure user', 'user activity', 'activity gauged', 'gauged community', 'community reaction', 'reaction counting', 'counting number', 'number comment', 'comment like', 'like posted', 'posted photograph', 'photograph received', 'received face', 'face detection', 'detection software', 'software wa', 'wa used', 'used determine', 'determine whether', 'whether photograph', 'photograph contained', 'contained human', 'human face', 'face well', 'well count', 'count total', 'total number', 'number face', 'face photo', 'photo proxy', 'proxy measure', 'measure participant', 'participant social', 'social activity', 'activity level', 'level pixellevel', 'pixellevel average', 'average computed', 'computed hue', 'hue saturation', 'saturation value', 'value hsv', 'hsv three', 'three color', 'color property', 'property commonly', 'commonly used', 'used image', 'image analysis', 'analysis hue', 'hue describes', 'describes image', 'image coloring', 'coloring light', 'light spectrum', 'spectrum ranging', 'ranging red', 'red bluepurple', 'bluepurple lower', 'lower hue', 'hue value', 'value indicate', 'indicate red', 'red higher', 'higher hue', 'hue value', 'value indicate', 'indicate blue', 'blue saturation', 'saturation refers', 'refers vividness', 'vividness image', 'image low', 'low saturation', 'saturation make', 'make image', 'image appear', 'appear grey', 'grey faded', 'faded value', 'value refers', 'refers image', 'image brightness', 'brightness lower', 'lower brightness', 'brightness score', 'score indicate', 'indicate darker', 'darker image', 'image see', 'see figure', 'figure comparison', 'comparison high', 'high low', 'low hsv', 'hsv value', 'value also', 'also checked', 'checked metadata', 'metadata ass', 'ass whether', 'whether instagramprovided', 'instagramprovided filter', 'filter wa', 'wa applied', 'applied alter', 'alter appearance', 'appearance photograph', 'photograph collectively', 'collectively measure', 'measure served', 'served feature', 'feature set', 'set primary', 'primary model', 'model separate', 'separate model', 'model fit', 'fit rating', 'rating data', 'data used', 'used four', 'four rating', 'rating category', 'category happy', 'happy sad', 'sad likable', 'likable interesting', 'interesting predictor']","['several different type', 'different type information', 'type information extracted', 'information extracted collected', 'extracted collected instagram', 'collected instagram data', 'instagram data used', 'data used total', 'used total post', 'total post per', 'post per user', 'per user per', 'user per day', 'per day measure', 'day measure user', 'measure user activity', 'user activity gauged', 'activity gauged community', 'gauged community reaction', 'community reaction counting', 'reaction counting number', 'counting number comment', 'number comment like', 'comment like posted', 'like posted photograph', 'posted photograph received', 'photograph received face', 'received face detection', 'face detection software', 'detection software wa', 'software wa used', 'wa used determine', 'used determine whether', 'determine whether photograph', 'whether photograph contained', 'photograph contained human', 'contained human face', 'human face well', 'face well count', 'well count total', 'count total number', 'total number face', 'number face photo', 'face photo proxy', 'photo proxy measure', 'proxy measure participant', 'measure participant social', 'participant social activity', 'social activity level', 'activity level pixellevel', 'level pixellevel average', 'pixellevel average computed', 'average computed hue', 'computed hue saturation', 'hue saturation value', 'saturation value hsv', 'value hsv three', 'hsv three color', 'three color property', 'color property commonly', 'property commonly used', 'commonly used image', 'used image analysis', 'image analysis hue', 'analysis hue describes', 'hue describes image', 'describes image coloring', 'image coloring light', 'coloring light spectrum', 'light spectrum ranging', 'spectrum ranging red', 'ranging red bluepurple', 'red bluepurple lower', 'bluepurple lower hue', 'lower hue value', 'hue value indicate', 'value indicate red', 'indicate red higher', 'red higher hue', 'higher hue value', 'hue value indicate', 'value indicate blue', 'indicate blue saturation', 'blue saturation refers', 'saturation refers vividness', 'refers vividness image', 'vividness image low', 'image low saturation', 'low saturation make', 'saturation make image', 'make image appear', 'image appear grey', 'appear grey faded', 'grey faded value', 'faded value refers', 'value refers image', 'refers image brightness', 'image brightness lower', 'brightness lower brightness', 'lower brightness score', 'brightness score indicate', 'score indicate darker', 'indicate darker image', 'darker image see', 'image see figure', 'see figure comparison', 'figure comparison high', 'comparison high low', 'high low hsv', 'low hsv value', 'hsv value also', 'value also checked', 'also checked metadata', 'checked metadata ass', 'metadata ass whether', 'ass whether instagramprovided', 'whether instagramprovided filter', 'instagramprovided filter wa', 'filter wa applied', 'wa applied alter', 'applied alter appearance', 'alter appearance photograph', 'appearance photograph collectively', 'photograph collectively measure', 'collectively measure served', 'measure served feature', 'served feature set', 'feature set primary', 'set primary model', 'primary model separate', 'model separate model', 'separate model fit', 'model fit rating', 'fit rating data', 'rating data used', 'data used four', 'used four rating', 'four rating category', 'rating category happy', 'category happy sad', 'happy sad likable', 'sad likable interesting', 'likable interesting predictor']",,,,,,,,
https://dl.acm.org/doi/abs/10.1145/2858036.2858207 ,0,Our first set of methods include developing three sets of measures spanning: linguistic structure interpersonal awareness and interaction. The choice of these measures is motivated by literature that examines associations between the behavioral expression of individuals and their responses to crises including vulnerability due to mental illness [13 23]. Each of these measure categories consists of the following variables: Linguistic Structure. For this measure we compute the fraction of nouns verbs2  and adverbs in posts and comments; automated readability index a measure to gauge the understandability of text [62]; and linguistic accommodation a process by which individuals in a conversation adjust their language style according to that of others [20]3 . Together these variables characterize the text shared by the user classes beyond their informational content. Per literature in psycholinguistics such structure is known to relate to an individual’s underlying psychological and cognitive state and can reveal cues about their social coordination [54]. Interpersonal Awareness. This measure category includes: proportion of first person singular (indicating pre-occupation with self) first person plural (indicating collective attention) second person and third person pronouns (indicating social interactivity and reference to people or objects in the environment). Literature has indicated that pronoun use can quantify an individual’s self and social awareness and can reveal mental well-being including that manifested in social media [21]. Interaction. Variables corresponding to this measure category include: volume of posts and comments authored post length length of comments authored volume of comments received on shared posts length of comments received mean vote difference (difference between upvotes and downvotes on posts authored) and response velocity (in minutes) given by the time elapsed between the first comment and the time the corresponding post was shared,our first set of method include developing three set of measure spanning linguistic structure interpersonal awareness and interaction the choice of these measure is motivated by literature that examines association between the behavioral expression of individual and their response to crisis including vulnerability due to mental illness each of these measure category consists of the following variable linguistic structure for this measure we compute the fraction of noun verb and adverb in post and comment automated readability index a measure to gauge the understandability of text and linguistic accommodation a process by which individual in a conversation adjust their language style according to that of others together these variable characterize the text shared by the user class beyond their informational content per literature in psycholinguistics such structure is known to relate to an individual underlying psychological and cognitive state and can reveal cue about their social coordination interpersonal awareness this measure category includes proportion of first person singular indicating preoccupation with self first person plural indicating collective attention second person and third person pronoun indicating social interactivity and reference to people or object in the environment literature ha indicated that pronoun use can quantify an individual self and social awareness and can reveal mental wellbeing including that manifested in social medium interaction variable corresponding to this measure category include volume of post and comment authored post length length of comment authored volume of comment received on shared post length of comment received mean vote difference difference between upvotes and downvotes on post authored and response velocity in minute given by the time elapsed between the first comment and the time the corresponding post wa shared,"['first', 'set', 'method', 'include', 'developing', 'three', 'set', 'measure', 'spanning', 'linguistic', 'structure', 'interpersonal', 'awareness', 'interaction', 'choice', 'measure', 'motivated', 'literature', 'examines', 'association', 'behavioral', 'expression', 'individual', 'response', 'crisis', 'including', 'vulnerability', 'due', 'mental', 'illness', 'measure', 'category', 'consists', 'following', 'variable', 'linguistic', 'structure', 'measure', 'compute', 'fraction', 'noun', 'verb', 'adverb', 'post', 'comment', 'automated', 'readability', 'index', 'measure', 'gauge', 'understandability', 'text', 'linguistic', 'accommodation', 'process', 'individual', 'conversation', 'adjust', 'language', 'style', 'according', 'others', 'together', 'variable', 'characterize', 'text', 'shared', 'user', 'class', 'beyond', 'informational', 'content', 'per', 'literature', 'psycholinguistics', 'structure', 'known', 'relate', 'individual', 'underlying', 'psychological', 'cognitive', 'state', 'reveal', 'cue', 'social', 'coordination', 'interpersonal', 'awareness', 'measure', 'category', 'includes', 'proportion', 'first', 'person', 'singular', 'indicating', 'preoccupation', 'self', 'first', 'person', 'plural', 'indicating', 'collective', 'attention', 'second', 'person', 'third', 'person', 'pronoun', 'indicating', 'social', 'interactivity', 'reference', 'people', 'object', 'environment', 'literature', 'ha', 'indicated', 'pronoun', 'use', 'quantify', 'individual', 'self', 'social', 'awareness', 'reveal', 'mental', 'wellbeing', 'including', 'manifested', 'social', 'medium', 'interaction', 'variable', 'corresponding', 'measure', 'category', 'include', 'volume', 'post', 'comment', 'authored', 'post', 'length', 'length', 'comment', 'authored', 'volume', 'comment', 'received', 'shared', 'post', 'length', 'comment', 'received', 'mean', 'vote', 'difference', 'difference', 'upvotes', 'downvotes', 'post', 'authored', 'response', 'velocity', 'minute', 'given', 'time', 'elapsed', 'first', 'comment', 'time', 'corresponding', 'post', 'wa', 'shared']","['first set', 'set method', 'method include', 'include developing', 'developing three', 'three set', 'set measure', 'measure spanning', 'spanning linguistic', 'linguistic structure', 'structure interpersonal', 'interpersonal awareness', 'awareness interaction', 'interaction choice', 'choice measure', 'measure motivated', 'motivated literature', 'literature examines', 'examines association', 'association behavioral', 'behavioral expression', 'expression individual', 'individual response', 'response crisis', 'crisis including', 'including vulnerability', 'vulnerability due', 'due mental', 'mental illness', 'illness measure', 'measure category', 'category consists', 'consists following', 'following variable', 'variable linguistic', 'linguistic structure', 'structure measure', 'measure compute', 'compute fraction', 'fraction noun', 'noun verb', 'verb adverb', 'adverb post', 'post comment', 'comment automated', 'automated readability', 'readability index', 'index measure', 'measure gauge', 'gauge understandability', 'understandability text', 'text linguistic', 'linguistic accommodation', 'accommodation process', 'process individual', 'individual conversation', 'conversation adjust', 'adjust language', 'language style', 'style according', 'according others', 'others together', 'together variable', 'variable characterize', 'characterize text', 'text shared', 'shared user', 'user class', 'class beyond', 'beyond informational', 'informational content', 'content per', 'per literature', 'literature psycholinguistics', 'psycholinguistics structure', 'structure known', 'known relate', 'relate individual', 'individual underlying', 'underlying psychological', 'psychological cognitive', 'cognitive state', 'state reveal', 'reveal cue', 'cue social', 'social coordination', 'coordination interpersonal', 'interpersonal awareness', 'awareness measure', 'measure category', 'category includes', 'includes proportion', 'proportion first', 'first person', 'person singular', 'singular indicating', 'indicating preoccupation', 'preoccupation self', 'self first', 'first person', 'person plural', 'plural indicating', 'indicating collective', 'collective attention', 'attention second', 'second person', 'person third', 'third person', 'person pronoun', 'pronoun indicating', 'indicating social', 'social interactivity', 'interactivity reference', 'reference people', 'people object', 'object environment', 'environment literature', 'literature ha', 'ha indicated', 'indicated pronoun', 'pronoun use', 'use quantify', 'quantify individual', 'individual self', 'self social', 'social awareness', 'awareness reveal', 'reveal mental', 'mental wellbeing', 'wellbeing including', 'including manifested', 'manifested social', 'social medium', 'medium interaction', 'interaction variable', 'variable corresponding', 'corresponding measure', 'measure category', 'category include', 'include volume', 'volume post', 'post comment', 'comment authored', 'authored post', 'post length', 'length length', 'length comment', 'comment authored', 'authored volume', 'volume comment', 'comment received', 'received shared', 'shared post', 'post length', 'length comment', 'comment received', 'received mean', 'mean vote', 'vote difference', 'difference difference', 'difference upvotes', 'upvotes downvotes', 'downvotes post', 'post authored', 'authored response', 'response velocity', 'velocity minute', 'minute given', 'given time', 'time elapsed', 'elapsed first', 'first comment', 'comment time', 'time corresponding', 'corresponding post', 'post wa', 'wa shared']","['first set method', 'set method include', 'method include developing', 'include developing three', 'developing three set', 'three set measure', 'set measure spanning', 'measure spanning linguistic', 'spanning linguistic structure', 'linguistic structure interpersonal', 'structure interpersonal awareness', 'interpersonal awareness interaction', 'awareness interaction choice', 'interaction choice measure', 'choice measure motivated', 'measure motivated literature', 'motivated literature examines', 'literature examines association', 'examines association behavioral', 'association behavioral expression', 'behavioral expression individual', 'expression individual response', 'individual response crisis', 'response crisis including', 'crisis including vulnerability', 'including vulnerability due', 'vulnerability due mental', 'due mental illness', 'mental illness measure', 'illness measure category', 'measure category consists', 'category consists following', 'consists following variable', 'following variable linguistic', 'variable linguistic structure', 'linguistic structure measure', 'structure measure compute', 'measure compute fraction', 'compute fraction noun', 'fraction noun verb', 'noun verb adverb', 'verb adverb post', 'adverb post comment', 'post comment automated', 'comment automated readability', 'automated readability index', 'readability index measure', 'index measure gauge', 'measure gauge understandability', 'gauge understandability text', 'understandability text linguistic', 'text linguistic accommodation', 'linguistic accommodation process', 'accommodation process individual', 'process individual conversation', 'individual conversation adjust', 'conversation adjust language', 'adjust language style', 'language style according', 'style according others', 'according others together', 'others together variable', 'together variable characterize', 'variable characterize text', 'characterize text shared', 'text shared user', 'shared user class', 'user class beyond', 'class beyond informational', 'beyond informational content', 'informational content per', 'content per literature', 'per literature psycholinguistics', 'literature psycholinguistics structure', 'psycholinguistics structure known', 'structure known relate', 'known relate individual', 'relate individual underlying', 'individual underlying psychological', 'underlying psychological cognitive', 'psychological cognitive state', 'cognitive state reveal', 'state reveal cue', 'reveal cue social', 'cue social coordination', 'social coordination interpersonal', 'coordination interpersonal awareness', 'interpersonal awareness measure', 'awareness measure category', 'measure category includes', 'category includes proportion', 'includes proportion first', 'proportion first person', 'first person singular', 'person singular indicating', 'singular indicating preoccupation', 'indicating preoccupation self', 'preoccupation self first', 'self first person', 'first person plural', 'person plural indicating', 'plural indicating collective', 'indicating collective attention', 'collective attention second', 'attention second person', 'second person third', 'person third person', 'third person pronoun', 'person pronoun indicating', 'pronoun indicating social', 'indicating social interactivity', 'social interactivity reference', 'interactivity reference people', 'reference people object', 'people object environment', 'object environment literature', 'environment literature ha', 'literature ha indicated', 'ha indicated pronoun', 'indicated pronoun use', 'pronoun use quantify', 'use quantify individual', 'quantify individual self', 'individual self social', 'self social awareness', 'social awareness reveal', 'awareness reveal mental', 'reveal mental wellbeing', 'mental wellbeing including', 'wellbeing including manifested', 'including manifested social', 'manifested social medium', 'social medium interaction', 'medium interaction variable', 'interaction variable corresponding', 'variable corresponding measure', 'corresponding measure category', 'measure category include', 'category include volume', 'include volume post', 'volume post comment', 'post comment authored', 'comment authored post', 'authored post length', 'post length length', 'length length comment', 'length comment authored', 'comment authored volume', 'authored volume comment', 'volume comment received', 'comment received shared', 'received shared post', 'shared post length', 'post length comment', 'length comment received', 'comment received mean', 'received mean vote', 'mean vote difference', 'vote difference difference', 'difference difference upvotes', 'difference upvotes downvotes', 'upvotes downvotes post', 'downvotes post authored', 'post authored response', 'authored response velocity', 'response velocity minute', 'velocity minute given', 'minute given time', 'given time elapsed', 'time elapsed first', 'elapsed first comment', 'first comment time', 'comment time corresponding', 'time corresponding post', 'corresponding post wa', 'post wa shared']",,,,,,,,
https://dl.acm.org/doi/abs/10.1145/2702613.2732733,1,Based on the training data thus created we pursued the use of supervised learning to develop a classifier which would indicate whether a post is of high low or no self-disclosure. We tested a variety of different classification techniques (decision trees k Nearest Neighbor naive Bayes). The best performing classifier was found to be a perceptron classifier with adaptive boosting used to amplify performance [17] whose results will be used in the remainder of this paper. We used the following feature generation rules: First we eliminated stopwords from each post based on standard list provided by Python’s NLTK library. Next we performed stemming using Porter Stemmer. We extracted uni- bi- and tri-grams from each post and considered those with five or more occurrences. We also computed two additional features – length of each post and whether the author of the post is an exclusive poster on mental health forums or is observed in our dataset to post on other forums as well. Thus each post was characterized by 1070 features. We used standard 10-fold cross validation (CV) to evaluate the classifier and ran our model over 100 random 10-fold CV assignments for generalizability of the results. We report the average accuracy precision recall F1 specificity as metrics of performance. We find that our classifier based on the perception model yields an average accuracy of 78.4% in detecting high or low self-disclosure with .74 precision and .86 recall (see Table 4 for details). Other methods like k-NN (k=5) give higher precision but at the expense of very low recall. Figure 1 gives the ROC (receiver operating characteristic) curves for all the models. Per the ROC curve corresponding to the perceptron model we find it to yield the maximum area under curve (.81) hence best performance. We further identify in Table 5 the n-grams (or features) with the highest weights given by the perceptron – it implies these features were the most significant in the classification task. We provide some brief qualitative examinations of these n-grams in the light of prior psychology literature on selfdisclosure and mental health [10 11]. We find that the ngrams primarily are associated with vulnerable and selfloathing thoughts (e.g. thoughts of suicide) bear a negative tone or depict confessional experiences. Based on prior research [10 11 12] and our own work on mental health discourse on Reddit [4] we find that these are the topical dimensions along which high self-disclosure and low/no selfdisclosure posts vary. In essence high self-disclosure posts share extensively their personal beliefs and fear for instance their vital constructs and private sensitive informational attributes. The post excerpts below have been classified to be of high self-disclosure and through them we demonstrate the use of some of the n-grams in Table 7: “I don’t want to kill myself I haven’t felt suicidal in a long time but I just want to stop life for a while you know?” “My dad would beat the living shit out of me […]. I’ve been to the hospital so many times I’ve lost track” “I hate this. I hate myself. I don't want to f****** be this person anymore. I'm unmotivated unfocused immature.”,based on the training data thus created we pursued the use of supervised learning to develop a classifier which would indicate whether a post is of high low or no selfdisclosure we tested a variety of different classification technique decision tree k nearest neighbor naive bayes the best performing classifier wa found to be a perceptron classifier with adaptive boosting used to amplify performance whose result will be used in the remainder of this paper we used the following feature generation rule first we eliminated stopwords from each post based on standard list provided by python nltk library next we performed stemming using porter stemmer we extracted uni bi and trigram from each post and considered those with five or more occurrence we also computed two additional feature length of each post and whether the author of the post is an exclusive poster on mental health forum or is observed in our dataset to post on other forum a well thus each post wa characterized by feature we used standard fold cross validation cv to evaluate the classifier and ran our model over random fold cv assignment for generalizability of the result we report the average accuracy precision recall f specificity a metric of performance we find that our classifier based on the perception model yield an average accuracy of in detecting high or low selfdisclosure with precision and recall see table for detail other method like knn k give higher precision but at the expense of very low recall figure give the roc receiver operating characteristic curve for all the model per the roc curve corresponding to the perceptron model we find it to yield the maximum area under curve hence best performance we further identify in table the ngrams or feature with the highest weight given by the perceptron it implies these feature were the most significant in the classification task we provide some brief qualitative examination of these ngrams in the light of prior psychology literature on selfdisclosure and mental health we find that the ngrams primarily are associated with vulnerable and selfloathing thought eg thought of suicide bear a negative tone or depict confessional experience based on prior research and our own work on mental health discourse on reddit we find that these are the topical dimension along which high selfdisclosure and lowno selfdisclosure post vary in essence high selfdisclosure post share extensively their personal belief and fear for instance their vital construct and private sensitive informational attribute the post excerpt below have been classified to be of high selfdisclosure and through them we demonstrate the use of some of the ngrams in table i dont want to kill myself i havent felt suicidal in a long time but i just want to stop life for a while you know my dad would beat the living shit out of me ive been to the hospital so many time ive lost track i hate this i hate myself i dont want to f be this person anymore im unmotivated unfocused immature,"['based', 'training', 'data', 'thus', 'created', 'pursued', 'use', 'supervised', 'learning', 'develop', 'classifier', 'would', 'indicate', 'whether', 'post', 'high', 'low', 'selfdisclosure', 'tested', 'variety', 'different', 'classification', 'technique', 'decision', 'tree', 'k', 'nearest', 'neighbor', 'naive', 'bayes', 'best', 'performing', 'classifier', 'wa', 'found', 'perceptron', 'classifier', 'adaptive', 'boosting', 'used', 'amplify', 'performance', 'whose', 'result', 'used', 'remainder', 'paper', 'used', 'following', 'feature', 'generation', 'rule', 'first', 'eliminated', 'stopwords', 'post', 'based', 'standard', 'list', 'provided', 'python', 'nltk', 'library', 'next', 'performed', 'stemming', 'using', 'porter', 'stemmer', 'extracted', 'uni', 'bi', 'trigram', 'post', 'considered', 'five', 'occurrence', 'also', 'computed', 'two', 'additional', 'feature', 'length', 'post', 'whether', 'author', 'post', 'exclusive', 'poster', 'mental', 'health', 'forum', 'observed', 'dataset', 'post', 'forum', 'well', 'thus', 'post', 'wa', 'characterized', 'feature', 'used', 'standard', 'fold', 'cross', 'validation', 'cv', 'evaluate', 'classifier', 'ran', 'model', 'random', 'fold', 'cv', 'assignment', 'generalizability', 'result', 'report', 'average', 'accuracy', 'precision', 'recall', 'f', 'specificity', 'metric', 'performance', 'find', 'classifier', 'based', 'perception', 'model', 'yield', 'average', 'accuracy', 'detecting', 'high', 'low', 'selfdisclosure', 'precision', 'recall', 'see', 'table', 'detail', 'method', 'like', 'knn', 'k', 'give', 'higher', 'precision', 'expense', 'low', 'recall', 'figure', 'give', 'roc', 'receiver', 'operating', 'characteristic', 'curve', 'model', 'per', 'roc', 'curve', 'corresponding', 'perceptron', 'model', 'find', 'yield', 'maximum', 'area', 'curve', 'hence', 'best', 'performance', 'identify', 'table', 'ngrams', 'feature', 'highest', 'weight', 'given', 'perceptron', 'implies', 'feature', 'significant', 'classification', 'task', 'provide', 'brief', 'qualitative', 'examination', 'ngrams', 'light', 'prior', 'psychology', 'literature', 'selfdisclosure', 'mental', 'health', 'find', 'ngrams', 'primarily', 'associated', 'vulnerable', 'selfloathing', 'thought', 'eg', 'thought', 'suicide', 'bear', 'negative', 'tone', 'depict', 'confessional', 'experience', 'based', 'prior', 'research', 'work', 'mental', 'health', 'discourse', 'reddit', 'find', 'topical', 'dimension', 'along', 'high', 'selfdisclosure', 'lowno', 'selfdisclosure', 'post', 'vary', 'essence', 'high', 'selfdisclosure', 'post', 'share', 'extensively', 'personal', 'belief', 'fear', 'instance', 'vital', 'construct', 'private', 'sensitive', 'informational', 'attribute', 'post', 'excerpt', 'classified', 'high', 'selfdisclosure', 'demonstrate', 'use', 'ngrams', 'table', 'dont', 'want', 'kill', 'havent', 'felt', 'suicidal', 'long', 'time', 'want', 'stop', 'life', 'know', 'dad', 'would', 'beat', 'living', 'shit', 'ive', 'hospital', 'many', 'time', 'ive', 'lost', 'track', 'hate', 'hate', 'dont', 'want', 'f', 'person', 'anymore', 'im', 'unmotivated', 'unfocused', 'immature']","['based training', 'training data', 'data thus', 'thus created', 'created pursued', 'pursued use', 'use supervised', 'supervised learning', 'learning develop', 'develop classifier', 'classifier would', 'would indicate', 'indicate whether', 'whether post', 'post high', 'high low', 'low selfdisclosure', 'selfdisclosure tested', 'tested variety', 'variety different', 'different classification', 'classification technique', 'technique decision', 'decision tree', 'tree k', 'k nearest', 'nearest neighbor', 'neighbor naive', 'naive bayes', 'bayes best', 'best performing', 'performing classifier', 'classifier wa', 'wa found', 'found perceptron', 'perceptron classifier', 'classifier adaptive', 'adaptive boosting', 'boosting used', 'used amplify', 'amplify performance', 'performance whose', 'whose result', 'result used', 'used remainder', 'remainder paper', 'paper used', 'used following', 'following feature', 'feature generation', 'generation rule', 'rule first', 'first eliminated', 'eliminated stopwords', 'stopwords post', 'post based', 'based standard', 'standard list', 'list provided', 'provided python', 'python nltk', 'nltk library', 'library next', 'next performed', 'performed stemming', 'stemming using', 'using porter', 'porter stemmer', 'stemmer extracted', 'extracted uni', 'uni bi', 'bi trigram', 'trigram post', 'post considered', 'considered five', 'five occurrence', 'occurrence also', 'also computed', 'computed two', 'two additional', 'additional feature', 'feature length', 'length post', 'post whether', 'whether author', 'author post', 'post exclusive', 'exclusive poster', 'poster mental', 'mental health', 'health forum', 'forum observed', 'observed dataset', 'dataset post', 'post forum', 'forum well', 'well thus', 'thus post', 'post wa', 'wa characterized', 'characterized feature', 'feature used', 'used standard', 'standard fold', 'fold cross', 'cross validation', 'validation cv', 'cv evaluate', 'evaluate classifier', 'classifier ran', 'ran model', 'model random', 'random fold', 'fold cv', 'cv assignment', 'assignment generalizability', 'generalizability result', 'result report', 'report average', 'average accuracy', 'accuracy precision', 'precision recall', 'recall f', 'f specificity', 'specificity metric', 'metric performance', 'performance find', 'find classifier', 'classifier based', 'based perception', 'perception model', 'model yield', 'yield average', 'average accuracy', 'accuracy detecting', 'detecting high', 'high low', 'low selfdisclosure', 'selfdisclosure precision', 'precision recall', 'recall see', 'see table', 'table detail', 'detail method', 'method like', 'like knn', 'knn k', 'k give', 'give higher', 'higher precision', 'precision expense', 'expense low', 'low recall', 'recall figure', 'figure give', 'give roc', 'roc receiver', 'receiver operating', 'operating characteristic', 'characteristic curve', 'curve model', 'model per', 'per roc', 'roc curve', 'curve corresponding', 'corresponding perceptron', 'perceptron model', 'model find', 'find yield', 'yield maximum', 'maximum area', 'area curve', 'curve hence', 'hence best', 'best performance', 'performance identify', 'identify table', 'table ngrams', 'ngrams feature', 'feature highest', 'highest weight', 'weight given', 'given perceptron', 'perceptron implies', 'implies feature', 'feature significant', 'significant classification', 'classification task', 'task provide', 'provide brief', 'brief qualitative', 'qualitative examination', 'examination ngrams', 'ngrams light', 'light prior', 'prior psychology', 'psychology literature', 'literature selfdisclosure', 'selfdisclosure mental', 'mental health', 'health find', 'find ngrams', 'ngrams primarily', 'primarily associated', 'associated vulnerable', 'vulnerable selfloathing', 'selfloathing thought', 'thought eg', 'eg thought', 'thought suicide', 'suicide bear', 'bear negative', 'negative tone', 'tone depict', 'depict confessional', 'confessional experience', 'experience based', 'based prior', 'prior research', 'research work', 'work mental', 'mental health', 'health discourse', 'discourse reddit', 'reddit find', 'find topical', 'topical dimension', 'dimension along', 'along high', 'high selfdisclosure', 'selfdisclosure lowno', 'lowno selfdisclosure', 'selfdisclosure post', 'post vary', 'vary essence', 'essence high', 'high selfdisclosure', 'selfdisclosure post', 'post share', 'share extensively', 'extensively personal', 'personal belief', 'belief fear', 'fear instance', 'instance vital', 'vital construct', 'construct private', 'private sensitive', 'sensitive informational', 'informational attribute', 'attribute post', 'post excerpt', 'excerpt classified', 'classified high', 'high selfdisclosure', 'selfdisclosure demonstrate', 'demonstrate use', 'use ngrams', 'ngrams table', 'table dont', 'dont want', 'want kill', 'kill havent', 'havent felt', 'felt suicidal', 'suicidal long', 'long time', 'time want', 'want stop', 'stop life', 'life know', 'know dad', 'dad would', 'would beat', 'beat living', 'living shit', 'shit ive', 'ive hospital', 'hospital many', 'many time', 'time ive', 'ive lost', 'lost track', 'track hate', 'hate hate', 'hate dont', 'dont want', 'want f', 'f person', 'person anymore', 'anymore im', 'im unmotivated', 'unmotivated unfocused', 'unfocused immature']","['based training data', 'training data thus', 'data thus created', 'thus created pursued', 'created pursued use', 'pursued use supervised', 'use supervised learning', 'supervised learning develop', 'learning develop classifier', 'develop classifier would', 'classifier would indicate', 'would indicate whether', 'indicate whether post', 'whether post high', 'post high low', 'high low selfdisclosure', 'low selfdisclosure tested', 'selfdisclosure tested variety', 'tested variety different', 'variety different classification', 'different classification technique', 'classification technique decision', 'technique decision tree', 'decision tree k', 'tree k nearest', 'k nearest neighbor', 'nearest neighbor naive', 'neighbor naive bayes', 'naive bayes best', 'bayes best performing', 'best performing classifier', 'performing classifier wa', 'classifier wa found', 'wa found perceptron', 'found perceptron classifier', 'perceptron classifier adaptive', 'classifier adaptive boosting', 'adaptive boosting used', 'boosting used amplify', 'used amplify performance', 'amplify performance whose', 'performance whose result', 'whose result used', 'result used remainder', 'used remainder paper', 'remainder paper used', 'paper used following', 'used following feature', 'following feature generation', 'feature generation rule', 'generation rule first', 'rule first eliminated', 'first eliminated stopwords', 'eliminated stopwords post', 'stopwords post based', 'post based standard', 'based standard list', 'standard list provided', 'list provided python', 'provided python nltk', 'python nltk library', 'nltk library next', 'library next performed', 'next performed stemming', 'performed stemming using', 'stemming using porter', 'using porter stemmer', 'porter stemmer extracted', 'stemmer extracted uni', 'extracted uni bi', 'uni bi trigram', 'bi trigram post', 'trigram post considered', 'post considered five', 'considered five occurrence', 'five occurrence also', 'occurrence also computed', 'also computed two', 'computed two additional', 'two additional feature', 'additional feature length', 'feature length post', 'length post whether', 'post whether author', 'whether author post', 'author post exclusive', 'post exclusive poster', 'exclusive poster mental', 'poster mental health', 'mental health forum', 'health forum observed', 'forum observed dataset', 'observed dataset post', 'dataset post forum', 'post forum well', 'forum well thus', 'well thus post', 'thus post wa', 'post wa characterized', 'wa characterized feature', 'characterized feature used', 'feature used standard', 'used standard fold', 'standard fold cross', 'fold cross validation', 'cross validation cv', 'validation cv evaluate', 'cv evaluate classifier', 'evaluate classifier ran', 'classifier ran model', 'ran model random', 'model random fold', 'random fold cv', 'fold cv assignment', 'cv assignment generalizability', 'assignment generalizability result', 'generalizability result report', 'result report average', 'report average accuracy', 'average accuracy precision', 'accuracy precision recall', 'precision recall f', 'recall f specificity', 'f specificity metric', 'specificity metric performance', 'metric performance find', 'performance find classifier', 'find classifier based', 'classifier based perception', 'based perception model', 'perception model yield', 'model yield average', 'yield average accuracy', 'average accuracy detecting', 'accuracy detecting high', 'detecting high low', 'high low selfdisclosure', 'low selfdisclosure precision', 'selfdisclosure precision recall', 'precision recall see', 'recall see table', 'see table detail', 'table detail method', 'detail method like', 'method like knn', 'like knn k', 'knn k give', 'k give higher', 'give higher precision', 'higher precision expense', 'precision expense low', 'expense low recall', 'low recall figure', 'recall figure give', 'figure give roc', 'give roc receiver', 'roc receiver operating', 'receiver operating characteristic', 'operating characteristic curve', 'characteristic curve model', 'curve model per', 'model per roc', 'per roc curve', 'roc curve corresponding', 'curve corresponding perceptron', 'corresponding perceptron model', 'perceptron model find', 'model find yield', 'find yield maximum', 'yield maximum area', 'maximum area curve', 'area curve hence', 'curve hence best', 'hence best performance', 'best performance identify', 'performance identify table', 'identify table ngrams', 'table ngrams feature', 'ngrams feature highest', 'feature highest weight', 'highest weight given', 'weight given perceptron', 'given perceptron implies', 'perceptron implies feature', 'implies feature significant', 'feature significant classification', 'significant classification task', 'classification task provide', 'task provide brief', 'provide brief qualitative', 'brief qualitative examination', 'qualitative examination ngrams', 'examination ngrams light', 'ngrams light prior', 'light prior psychology', 'prior psychology literature', 'psychology literature selfdisclosure', 'literature selfdisclosure mental', 'selfdisclosure mental health', 'mental health find', 'health find ngrams', 'find ngrams primarily', 'ngrams primarily associated', 'primarily associated vulnerable', 'associated vulnerable selfloathing', 'vulnerable selfloathing thought', 'selfloathing thought eg', 'thought eg thought', 'eg thought suicide', 'thought suicide bear', 'suicide bear negative', 'bear negative tone', 'negative tone depict', 'tone depict confessional', 'depict confessional experience', 'confessional experience based', 'experience based prior', 'based prior research', 'prior research work', 'research work mental', 'work mental health', 'mental health discourse', 'health discourse reddit', 'discourse reddit find', 'reddit find topical', 'find topical dimension', 'topical dimension along', 'dimension along high', 'along high selfdisclosure', 'high selfdisclosure lowno', 'selfdisclosure lowno selfdisclosure', 'lowno selfdisclosure post', 'selfdisclosure post vary', 'post vary essence', 'vary essence high', 'essence high selfdisclosure', 'high selfdisclosure post', 'selfdisclosure post share', 'post share extensively', 'share extensively personal', 'extensively personal belief', 'personal belief fear', 'belief fear instance', 'fear instance vital', 'instance vital construct', 'vital construct private', 'construct private sensitive', 'private sensitive informational', 'sensitive informational attribute', 'informational attribute post', 'attribute post excerpt', 'post excerpt classified', 'excerpt classified high', 'classified high selfdisclosure', 'high selfdisclosure demonstrate', 'selfdisclosure demonstrate use', 'demonstrate use ngrams', 'use ngrams table', 'ngrams table dont', 'table dont want', 'dont want kill', 'want kill havent', 'kill havent felt', 'havent felt suicidal', 'felt suicidal long', 'suicidal long time', 'long time want', 'time want stop', 'want stop life', 'stop life know', 'life know dad', 'know dad would', 'dad would beat', 'would beat living', 'beat living shit', 'living shit ive', 'shit ive hospital', 'ive hospital many', 'hospital many time', 'many time ive', 'time ive lost', 'ive lost track', 'lost track hate', 'track hate hate', 'hate hate dont', 'hate dont want', 'dont want f', 'want f person', 'f person anymore', 'person anymore im', 'anymore im unmotivated', 'im unmotivated unfocused', 'unmotivated unfocused immature']",,,,,,,,
https://dl.acm.org/doi/abs/10.1145/2702123.2702280,1,It is possible to extract various features from the activity histories of Twitter users. This section explains what kinds of features are used to estimate degree of depression and the way in which these quantities are extracted. Table 2 shows the features used in this study. A detailed explanation of each feature follows. The frequencies of words in a tweet (i.e. its bag of words) are used as a basic feature relating to the content of the tweet. Tsugawa et al. showed that the word frequencies are useful for identifying depression [37]. MeCab [20] was used to for morphological stemming and categorization of the Japanese tweet text to obtain accurate word frequencies. Particles auxiliary verbs adnominal adjectives and visual symbols were excluded for extracting content words. Words used by only one participant were also excluded resulting in a total of 84255 distinct words. However most of these words were rarely used and the distribution of word frequencies is extremely biased (see Fig. 4). Because words with a low rate of use were regarded as unlikely to be associated with depression for most users the frequencies of only the 20000 words with the highest rate of use (corresponding to 25 or more uses across all participants) were used as a feature in this study. Furthermore because the number and length of tweets differed by participant the word frequencies were normalized by the total number of words in the tweets. The topics of the tweets of each user as estimated by using a representative topic model LDA [5] were used as a second feature relating to the content of the tweets. With LDA the distribution of topics in each document is estimated from the word frequencies in each text through unsupervised learning on the assumption that the text and the words in it are generated according to a particular topic [5]. In LDA the number of topics to identify and a set of documents (as bags of words) are used as input and a topic distribution is output for each document. As mentioned in Related Work Section the topics of essays written by university students were estimated by using LDA and found to be useful in evaluating degree of depression [31]. From that study topics are expected to be a useful feature. A set of all tweets of each user was used as the user document for input in LDA and the 20000 words selected as described above were used as the words. We used LDA with collapsed Gibbs sampling [16]. As the parameters of LDA we used α = 50/K and β = 0.1 where K is the number of topics [16]. All extracted topics were used as the features. The ratio of positive words and the ratio of negative words used in the tweet text are used as the final features relating to tweet content. Users with depression are intuitively expected to use negative words more frequently than users without depression do. To categorize words a dictionary of affective words [19] which is compiled by manual evaluation of a dictionary of positive and negative words extracted according to a technique proposed in the literature [35] is used. The dictionary contains 760 positive words and 862 negative words. The user’s timing of tweets frequency of tweets average number of words retweet rate (rate of republishing other users’ tweets) mention rate (rate of directly referencing at least one other user) ratio of tweets containing a uniform resource locator (URL) number of users being followed and number of users following are used as features independent of the content of the tweet. The relative ratios of tweets posted during each hour of the day were used to characterize the timing of tweets; the number of posts per day was used as the posting frequency; and the ratio of qualifying tweets to all tweets were used for the retweet ratio mention ratio and ratio of tweets containing a URL. These features are used in prior research [14].,it is possible to extract various feature from the activity history of twitter user this section explains what kind of feature are used to estimate degree of depression and the way in which these quantity are extracted table show the feature used in this study a detailed explanation of each feature follows the frequency of word in a tweet ie it bag of word are used a a basic feature relating to the content of the tweet tsugawa et al showed that the word frequency are useful for identifying depression mecab wa used to for morphological stemming and categorization of the japanese tweet text to obtain accurate word frequency particle auxiliary verb adnominal adjective and visual symbol were excluded for extracting content word word used by only one participant were also excluded resulting in a total of distinct word however most of these word were rarely used and the distribution of word frequency is extremely biased see fig because word with a low rate of use were regarded a unlikely to be associated with depression for most user the frequency of only the word with the highest rate of use corresponding to or more us across all participant were used a a feature in this study furthermore because the number and length of tweet differed by participant the word frequency were normalized by the total number of word in the tweet the topic of the tweet of each user a estimated by using a representative topic model lda were used a a second feature relating to the content of the tweet with lda the distribution of topic in each document is estimated from the word frequency in each text through unsupervised learning on the assumption that the text and the word in it are generated according to a particular topic in lda the number of topic to identify and a set of document a bag of word are used a input and a topic distribution is output for each document a mentioned in related work section the topic of essay written by university student were estimated by using lda and found to be useful in evaluating degree of depression from that study topic are expected to be a useful feature a set of all tweet of each user wa used a the user document for input in lda and the word selected a described above were used a the word we used lda with collapsed gibbs sampling a the parameter of lda we used k and where k is the number of topic all extracted topic were used a the feature the ratio of positive word and the ratio of negative word used in the tweet text are used a the final feature relating to tweet content user with depression are intuitively expected to use negative word more frequently than user without depression do to categorize word a dictionary of affective word which is compiled by manual evaluation of a dictionary of positive and negative word extracted according to a technique proposed in the literature is used the dictionary contains positive word and negative word the user timing of tweet frequency of tweet average number of word retweet rate rate of republishing other user tweet mention rate rate of directly referencing at least one other user ratio of tweet containing a uniform resource locator url number of user being followed and number of user following are used a feature independent of the content of the tweet the relative ratio of tweet posted during each hour of the day were used to characterize the timing of tweet the number of post per day wa used a the posting frequency and the ratio of qualifying tweet to all tweet were used for the retweet ratio mention ratio and ratio of tweet containing a url these feature are used in prior research,"['possible', 'extract', 'various', 'feature', 'activity', 'history', 'twitter', 'user', 'section', 'explains', 'kind', 'feature', 'used', 'estimate', 'degree', 'depression', 'way', 'quantity', 'extracted', 'table', 'show', 'feature', 'used', 'study', 'detailed', 'explanation', 'feature', 'follows', 'frequency', 'word', 'tweet', 'ie', 'bag', 'word', 'used', 'basic', 'feature', 'relating', 'content', 'tweet', 'tsugawa', 'et', 'al', 'showed', 'word', 'frequency', 'useful', 'identifying', 'depression', 'mecab', 'wa', 'used', 'morphological', 'stemming', 'categorization', 'japanese', 'tweet', 'text', 'obtain', 'accurate', 'word', 'frequency', 'particle', 'auxiliary', 'verb', 'adnominal', 'adjective', 'visual', 'symbol', 'excluded', 'extracting', 'content', 'word', 'word', 'used', 'one', 'participant', 'also', 'excluded', 'resulting', 'total', 'distinct', 'word', 'however', 'word', 'rarely', 'used', 'distribution', 'word', 'frequency', 'extremely', 'biased', 'see', 'fig', 'word', 'low', 'rate', 'use', 'regarded', 'unlikely', 'associated', 'depression', 'user', 'frequency', 'word', 'highest', 'rate', 'use', 'corresponding', 'us', 'across', 'participant', 'used', 'feature', 'study', 'furthermore', 'number', 'length', 'tweet', 'differed', 'participant', 'word', 'frequency', 'normalized', 'total', 'number', 'word', 'tweet', 'topic', 'tweet', 'user', 'estimated', 'using', 'representative', 'topic', 'model', 'lda', 'used', 'second', 'feature', 'relating', 'content', 'tweet', 'lda', 'distribution', 'topic', 'document', 'estimated', 'word', 'frequency', 'text', 'unsupervised', 'learning', 'assumption', 'text', 'word', 'generated', 'according', 'particular', 'topic', 'lda', 'number', 'topic', 'identify', 'set', 'document', 'bag', 'word', 'used', 'input', 'topic', 'distribution', 'output', 'document', 'mentioned', 'related', 'work', 'section', 'topic', 'essay', 'written', 'university', 'student', 'estimated', 'using', 'lda', 'found', 'useful', 'evaluating', 'degree', 'depression', 'study', 'topic', 'expected', 'useful', 'feature', 'set', 'tweet', 'user', 'wa', 'used', 'user', 'document', 'input', 'lda', 'word', 'selected', 'described', 'used', 'word', 'used', 'lda', 'collapsed', 'gibbs', 'sampling', 'parameter', 'lda', 'used', 'k', 'k', 'number', 'topic', 'extracted', 'topic', 'used', 'feature', 'ratio', 'positive', 'word', 'ratio', 'negative', 'word', 'used', 'tweet', 'text', 'used', 'final', 'feature', 'relating', 'tweet', 'content', 'user', 'depression', 'intuitively', 'expected', 'use', 'negative', 'word', 'frequently', 'user', 'without', 'depression', 'categorize', 'word', 'dictionary', 'affective', 'word', 'compiled', 'manual', 'evaluation', 'dictionary', 'positive', 'negative', 'word', 'extracted', 'according', 'technique', 'proposed', 'literature', 'used', 'dictionary', 'contains', 'positive', 'word', 'negative', 'word', 'user', 'timing', 'tweet', 'frequency', 'tweet', 'average', 'number', 'word', 'retweet', 'rate', 'rate', 'republishing', 'user', 'tweet', 'mention', 'rate', 'rate', 'directly', 'referencing', 'least', 'one', 'user', 'ratio', 'tweet', 'containing', 'uniform', 'resource', 'locator', 'url', 'number', 'user', 'followed', 'number', 'user', 'following', 'used', 'feature', 'independent', 'content', 'tweet', 'relative', 'ratio', 'tweet', 'posted', 'hour', 'day', 'used', 'characterize', 'timing', 'tweet', 'number', 'post', 'per', 'day', 'wa', 'used', 'posting', 'frequency', 'ratio', 'qualifying', 'tweet', 'tweet', 'used', 'retweet', 'ratio', 'mention', 'ratio', 'ratio', 'tweet', 'containing', 'url', 'feature', 'used', 'prior', 'research']","['possible extract', 'extract various', 'various feature', 'feature activity', 'activity history', 'history twitter', 'twitter user', 'user section', 'section explains', 'explains kind', 'kind feature', 'feature used', 'used estimate', 'estimate degree', 'degree depression', 'depression way', 'way quantity', 'quantity extracted', 'extracted table', 'table show', 'show feature', 'feature used', 'used study', 'study detailed', 'detailed explanation', 'explanation feature', 'feature follows', 'follows frequency', 'frequency word', 'word tweet', 'tweet ie', 'ie bag', 'bag word', 'word used', 'used basic', 'basic feature', 'feature relating', 'relating content', 'content tweet', 'tweet tsugawa', 'tsugawa et', 'et al', 'al showed', 'showed word', 'word frequency', 'frequency useful', 'useful identifying', 'identifying depression', 'depression mecab', 'mecab wa', 'wa used', 'used morphological', 'morphological stemming', 'stemming categorization', 'categorization japanese', 'japanese tweet', 'tweet text', 'text obtain', 'obtain accurate', 'accurate word', 'word frequency', 'frequency particle', 'particle auxiliary', 'auxiliary verb', 'verb adnominal', 'adnominal adjective', 'adjective visual', 'visual symbol', 'symbol excluded', 'excluded extracting', 'extracting content', 'content word', 'word word', 'word used', 'used one', 'one participant', 'participant also', 'also excluded', 'excluded resulting', 'resulting total', 'total distinct', 'distinct word', 'word however', 'however word', 'word rarely', 'rarely used', 'used distribution', 'distribution word', 'word frequency', 'frequency extremely', 'extremely biased', 'biased see', 'see fig', 'fig word', 'word low', 'low rate', 'rate use', 'use regarded', 'regarded unlikely', 'unlikely associated', 'associated depression', 'depression user', 'user frequency', 'frequency word', 'word highest', 'highest rate', 'rate use', 'use corresponding', 'corresponding us', 'us across', 'across participant', 'participant used', 'used feature', 'feature study', 'study furthermore', 'furthermore number', 'number length', 'length tweet', 'tweet differed', 'differed participant', 'participant word', 'word frequency', 'frequency normalized', 'normalized total', 'total number', 'number word', 'word tweet', 'tweet topic', 'topic tweet', 'tweet user', 'user estimated', 'estimated using', 'using representative', 'representative topic', 'topic model', 'model lda', 'lda used', 'used second', 'second feature', 'feature relating', 'relating content', 'content tweet', 'tweet lda', 'lda distribution', 'distribution topic', 'topic document', 'document estimated', 'estimated word', 'word frequency', 'frequency text', 'text unsupervised', 'unsupervised learning', 'learning assumption', 'assumption text', 'text word', 'word generated', 'generated according', 'according particular', 'particular topic', 'topic lda', 'lda number', 'number topic', 'topic identify', 'identify set', 'set document', 'document bag', 'bag word', 'word used', 'used input', 'input topic', 'topic distribution', 'distribution output', 'output document', 'document mentioned', 'mentioned related', 'related work', 'work section', 'section topic', 'topic essay', 'essay written', 'written university', 'university student', 'student estimated', 'estimated using', 'using lda', 'lda found', 'found useful', 'useful evaluating', 'evaluating degree', 'degree depression', 'depression study', 'study topic', 'topic expected', 'expected useful', 'useful feature', 'feature set', 'set tweet', 'tweet user', 'user wa', 'wa used', 'used user', 'user document', 'document input', 'input lda', 'lda word', 'word selected', 'selected described', 'described used', 'used word', 'word used', 'used lda', 'lda collapsed', 'collapsed gibbs', 'gibbs sampling', 'sampling parameter', 'parameter lda', 'lda used', 'used k', 'k k', 'k number', 'number topic', 'topic extracted', 'extracted topic', 'topic used', 'used feature', 'feature ratio', 'ratio positive', 'positive word', 'word ratio', 'ratio negative', 'negative word', 'word used', 'used tweet', 'tweet text', 'text used', 'used final', 'final feature', 'feature relating', 'relating tweet', 'tweet content', 'content user', 'user depression', 'depression intuitively', 'intuitively expected', 'expected use', 'use negative', 'negative word', 'word frequently', 'frequently user', 'user without', 'without depression', 'depression categorize', 'categorize word', 'word dictionary', 'dictionary affective', 'affective word', 'word compiled', 'compiled manual', 'manual evaluation', 'evaluation dictionary', 'dictionary positive', 'positive negative', 'negative word', 'word extracted', 'extracted according', 'according technique', 'technique proposed', 'proposed literature', 'literature used', 'used dictionary', 'dictionary contains', 'contains positive', 'positive word', 'word negative', 'negative word', 'word user', 'user timing', 'timing tweet', 'tweet frequency', 'frequency tweet', 'tweet average', 'average number', 'number word', 'word retweet', 'retweet rate', 'rate rate', 'rate republishing', 'republishing user', 'user tweet', 'tweet mention', 'mention rate', 'rate rate', 'rate directly', 'directly referencing', 'referencing least', 'least one', 'one user', 'user ratio', 'ratio tweet', 'tweet containing', 'containing uniform', 'uniform resource', 'resource locator', 'locator url', 'url number', 'number user', 'user followed', 'followed number', 'number user', 'user following', 'following used', 'used feature', 'feature independent', 'independent content', 'content tweet', 'tweet relative', 'relative ratio', 'ratio tweet', 'tweet posted', 'posted hour', 'hour day', 'day used', 'used characterize', 'characterize timing', 'timing tweet', 'tweet number', 'number post', 'post per', 'per day', 'day wa', 'wa used', 'used posting', 'posting frequency', 'frequency ratio', 'ratio qualifying', 'qualifying tweet', 'tweet tweet', 'tweet used', 'used retweet', 'retweet ratio', 'ratio mention', 'mention ratio', 'ratio ratio', 'ratio tweet', 'tweet containing', 'containing url', 'url feature', 'feature used', 'used prior', 'prior research']","['possible extract various', 'extract various feature', 'various feature activity', 'feature activity history', 'activity history twitter', 'history twitter user', 'twitter user section', 'user section explains', 'section explains kind', 'explains kind feature', 'kind feature used', 'feature used estimate', 'used estimate degree', 'estimate degree depression', 'degree depression way', 'depression way quantity', 'way quantity extracted', 'quantity extracted table', 'extracted table show', 'table show feature', 'show feature used', 'feature used study', 'used study detailed', 'study detailed explanation', 'detailed explanation feature', 'explanation feature follows', 'feature follows frequency', 'follows frequency word', 'frequency word tweet', 'word tweet ie', 'tweet ie bag', 'ie bag word', 'bag word used', 'word used basic', 'used basic feature', 'basic feature relating', 'feature relating content', 'relating content tweet', 'content tweet tsugawa', 'tweet tsugawa et', 'tsugawa et al', 'et al showed', 'al showed word', 'showed word frequency', 'word frequency useful', 'frequency useful identifying', 'useful identifying depression', 'identifying depression mecab', 'depression mecab wa', 'mecab wa used', 'wa used morphological', 'used morphological stemming', 'morphological stemming categorization', 'stemming categorization japanese', 'categorization japanese tweet', 'japanese tweet text', 'tweet text obtain', 'text obtain accurate', 'obtain accurate word', 'accurate word frequency', 'word frequency particle', 'frequency particle auxiliary', 'particle auxiliary verb', 'auxiliary verb adnominal', 'verb adnominal adjective', 'adnominal adjective visual', 'adjective visual symbol', 'visual symbol excluded', 'symbol excluded extracting', 'excluded extracting content', 'extracting content word', 'content word word', 'word word used', 'word used one', 'used one participant', 'one participant also', 'participant also excluded', 'also excluded resulting', 'excluded resulting total', 'resulting total distinct', 'total distinct word', 'distinct word however', 'word however word', 'however word rarely', 'word rarely used', 'rarely used distribution', 'used distribution word', 'distribution word frequency', 'word frequency extremely', 'frequency extremely biased', 'extremely biased see', 'biased see fig', 'see fig word', 'fig word low', 'word low rate', 'low rate use', 'rate use regarded', 'use regarded unlikely', 'regarded unlikely associated', 'unlikely associated depression', 'associated depression user', 'depression user frequency', 'user frequency word', 'frequency word highest', 'word highest rate', 'highest rate use', 'rate use corresponding', 'use corresponding us', 'corresponding us across', 'us across participant', 'across participant used', 'participant used feature', 'used feature study', 'feature study furthermore', 'study furthermore number', 'furthermore number length', 'number length tweet', 'length tweet differed', 'tweet differed participant', 'differed participant word', 'participant word frequency', 'word frequency normalized', 'frequency normalized total', 'normalized total number', 'total number word', 'number word tweet', 'word tweet topic', 'tweet topic tweet', 'topic tweet user', 'tweet user estimated', 'user estimated using', 'estimated using representative', 'using representative topic', 'representative topic model', 'topic model lda', 'model lda used', 'lda used second', 'used second feature', 'second feature relating', 'feature relating content', 'relating content tweet', 'content tweet lda', 'tweet lda distribution', 'lda distribution topic', 'distribution topic document', 'topic document estimated', 'document estimated word', 'estimated word frequency', 'word frequency text', 'frequency text unsupervised', 'text unsupervised learning', 'unsupervised learning assumption', 'learning assumption text', 'assumption text word', 'text word generated', 'word generated according', 'generated according particular', 'according particular topic', 'particular topic lda', 'topic lda number', 'lda number topic', 'number topic identify', 'topic identify set', 'identify set document', 'set document bag', 'document bag word', 'bag word used', 'word used input', 'used input topic', 'input topic distribution', 'topic distribution output', 'distribution output document', 'output document mentioned', 'document mentioned related', 'mentioned related work', 'related work section', 'work section topic', 'section topic essay', 'topic essay written', 'essay written university', 'written university student', 'university student estimated', 'student estimated using', 'estimated using lda', 'using lda found', 'lda found useful', 'found useful evaluating', 'useful evaluating degree', 'evaluating degree depression', 'degree depression study', 'depression study topic', 'study topic expected', 'topic expected useful', 'expected useful feature', 'useful feature set', 'feature set tweet', 'set tweet user', 'tweet user wa', 'user wa used', 'wa used user', 'used user document', 'user document input', 'document input lda', 'input lda word', 'lda word selected', 'word selected described', 'selected described used', 'described used word', 'used word used', 'word used lda', 'used lda collapsed', 'lda collapsed gibbs', 'collapsed gibbs sampling', 'gibbs sampling parameter', 'sampling parameter lda', 'parameter lda used', 'lda used k', 'used k k', 'k k number', 'k number topic', 'number topic extracted', 'topic extracted topic', 'extracted topic used', 'topic used feature', 'used feature ratio', 'feature ratio positive', 'ratio positive word', 'positive word ratio', 'word ratio negative', 'ratio negative word', 'negative word used', 'word used tweet', 'used tweet text', 'tweet text used', 'text used final', 'used final feature', 'final feature relating', 'feature relating tweet', 'relating tweet content', 'tweet content user', 'content user depression', 'user depression intuitively', 'depression intuitively expected', 'intuitively expected use', 'expected use negative', 'use negative word', 'negative word frequently', 'word frequently user', 'frequently user without', 'user without depression', 'without depression categorize', 'depression categorize word', 'categorize word dictionary', 'word dictionary affective', 'dictionary affective word', 'affective word compiled', 'word compiled manual', 'compiled manual evaluation', 'manual evaluation dictionary', 'evaluation dictionary positive', 'dictionary positive negative', 'positive negative word', 'negative word extracted', 'word extracted according', 'extracted according technique', 'according technique proposed', 'technique proposed literature', 'proposed literature used', 'literature used dictionary', 'used dictionary contains', 'dictionary contains positive', 'contains positive word', 'positive word negative', 'word negative word', 'negative word user', 'word user timing', 'user timing tweet', 'timing tweet frequency', 'tweet frequency tweet', 'frequency tweet average', 'tweet average number', 'average number word', 'number word retweet', 'word retweet rate', 'retweet rate rate', 'rate rate republishing', 'rate republishing user', 'republishing user tweet', 'user tweet mention', 'tweet mention rate', 'mention rate rate', 'rate rate directly', 'rate directly referencing', 'directly referencing least', 'referencing least one', 'least one user', 'one user ratio', 'user ratio tweet', 'ratio tweet containing', 'tweet containing uniform', 'containing uniform resource', 'uniform resource locator', 'resource locator url', 'locator url number', 'url number user', 'number user followed', 'user followed number', 'followed number user', 'number user following', 'user following used', 'following used feature', 'used feature independent', 'feature independent content', 'independent content tweet', 'content tweet relative', 'tweet relative ratio', 'relative ratio tweet', 'ratio tweet posted', 'tweet posted hour', 'posted hour day', 'hour day used', 'day used characterize', 'used characterize timing', 'characterize timing tweet', 'timing tweet number', 'tweet number post', 'number post per', 'post per day', 'per day wa', 'day wa used', 'wa used posting', 'used posting frequency', 'posting frequency ratio', 'frequency ratio qualifying', 'ratio qualifying tweet', 'qualifying tweet tweet', 'tweet tweet used', 'tweet used retweet', 'used retweet ratio', 'retweet ratio mention', 'ratio mention ratio', 'mention ratio ratio', 'ratio ratio tweet', 'ratio tweet containing', 'tweet containing url', 'containing url feature', 'url feature used', 'feature used prior', 'used prior research']",,,,,,,,
https://www.nature.com/articles/s41598-017-12961-9,0,In an effort to minimize noisy and unreliable data we applied several quality assurance measures in our data collection process. MTurk workers who have completed at least 100 tasks with a minimum 95% approval rating have been found to provide reliable valid survey responses33. We restricted survey visibility only to workers with these qualifications. Survey access was also restricted to U.S. IP addresses as MTurk data collected from outside the United States are generally of poorer quality34. All participants were only permitted to take the survey once. We excluded participants with a total of fewer than five Twitter posts. We also excluded participants with CES-D scores of 21 or lower (depression) or TSQ scores of 5 or lower (PTSD). Studies have indicated that a CES-D score of 22 represents an optimal cutoff for identifying clinically relevant depression3536; an equivalent TSQ cutoff of 6 has been found to be optimal in the case of PTSD32. We note here that in the study that inspired the present work De Choudhury et al.8 used two depression scales (CES-D and BDI) and filtered individuals whose depression score did not correlate across the both scales. This additional criteria is a methodological strength of De Choudhury et al.8 with respect to the present work.,in an effort to minimize noisy and unreliable data we applied several quality assurance measure in our data collection process mturk worker who have completed at least task with a minimum approval rating have been found to provide reliable valid survey response we restricted survey visibility only to worker with these qualification survey access wa also restricted to u ip address a mturk data collected from outside the united state are generally of poorer quality all participant were only permitted to take the survey once we excluded participant with a total of fewer than five twitter post we also excluded participant with cesd score of or lower depression or tsq score of or lower ptsd study have indicated that a cesd score of represents an optimal cutoff for identifying clinically relevant depression an equivalent tsq cutoff of ha been found to be optimal in the case of ptsd we note here that in the study that inspired the present work de choudhury et al used two depression scale cesd and bdi and filtered individual whose depression score did not correlate across the both scale this additional criterion is a methodological strength of de choudhury et al with respect to the present work,"['effort', 'minimize', 'noisy', 'unreliable', 'data', 'applied', 'several', 'quality', 'assurance', 'measure', 'data', 'collection', 'process', 'mturk', 'worker', 'completed', 'least', 'task', 'minimum', 'approval', 'rating', 'found', 'provide', 'reliable', 'valid', 'survey', 'response', 'restricted', 'survey', 'visibility', 'worker', 'qualification', 'survey', 'access', 'wa', 'also', 'restricted', 'u', 'ip', 'address', 'mturk', 'data', 'collected', 'outside', 'united', 'state', 'generally', 'poorer', 'quality', 'participant', 'permitted', 'take', 'survey', 'excluded', 'participant', 'total', 'fewer', 'five', 'twitter', 'post', 'also', 'excluded', 'participant', 'cesd', 'score', 'lower', 'depression', 'tsq', 'score', 'lower', 'ptsd', 'study', 'indicated', 'cesd', 'score', 'represents', 'optimal', 'cutoff', 'identifying', 'clinically', 'relevant', 'depression', 'equivalent', 'tsq', 'cutoff', 'ha', 'found', 'optimal', 'case', 'ptsd', 'note', 'study', 'inspired', 'present', 'work', 'de', 'choudhury', 'et', 'al', 'used', 'two', 'depression', 'scale', 'cesd', 'bdi', 'filtered', 'individual', 'whose', 'depression', 'score', 'correlate', 'across', 'scale', 'additional', 'criterion', 'methodological', 'strength', 'de', 'choudhury', 'et', 'al', 'respect', 'present', 'work']","['effort minimize', 'minimize noisy', 'noisy unreliable', 'unreliable data', 'data applied', 'applied several', 'several quality', 'quality assurance', 'assurance measure', 'measure data', 'data collection', 'collection process', 'process mturk', 'mturk worker', 'worker completed', 'completed least', 'least task', 'task minimum', 'minimum approval', 'approval rating', 'rating found', 'found provide', 'provide reliable', 'reliable valid', 'valid survey', 'survey response', 'response restricted', 'restricted survey', 'survey visibility', 'visibility worker', 'worker qualification', 'qualification survey', 'survey access', 'access wa', 'wa also', 'also restricted', 'restricted u', 'u ip', 'ip address', 'address mturk', 'mturk data', 'data collected', 'collected outside', 'outside united', 'united state', 'state generally', 'generally poorer', 'poorer quality', 'quality participant', 'participant permitted', 'permitted take', 'take survey', 'survey excluded', 'excluded participant', 'participant total', 'total fewer', 'fewer five', 'five twitter', 'twitter post', 'post also', 'also excluded', 'excluded participant', 'participant cesd', 'cesd score', 'score lower', 'lower depression', 'depression tsq', 'tsq score', 'score lower', 'lower ptsd', 'ptsd study', 'study indicated', 'indicated cesd', 'cesd score', 'score represents', 'represents optimal', 'optimal cutoff', 'cutoff identifying', 'identifying clinically', 'clinically relevant', 'relevant depression', 'depression equivalent', 'equivalent tsq', 'tsq cutoff', 'cutoff ha', 'ha found', 'found optimal', 'optimal case', 'case ptsd', 'ptsd note', 'note study', 'study inspired', 'inspired present', 'present work', 'work de', 'de choudhury', 'choudhury et', 'et al', 'al used', 'used two', 'two depression', 'depression scale', 'scale cesd', 'cesd bdi', 'bdi filtered', 'filtered individual', 'individual whose', 'whose depression', 'depression score', 'score correlate', 'correlate across', 'across scale', 'scale additional', 'additional criterion', 'criterion methodological', 'methodological strength', 'strength de', 'de choudhury', 'choudhury et', 'et al', 'al respect', 'respect present', 'present work']","['effort minimize noisy', 'minimize noisy unreliable', 'noisy unreliable data', 'unreliable data applied', 'data applied several', 'applied several quality', 'several quality assurance', 'quality assurance measure', 'assurance measure data', 'measure data collection', 'data collection process', 'collection process mturk', 'process mturk worker', 'mturk worker completed', 'worker completed least', 'completed least task', 'least task minimum', 'task minimum approval', 'minimum approval rating', 'approval rating found', 'rating found provide', 'found provide reliable', 'provide reliable valid', 'reliable valid survey', 'valid survey response', 'survey response restricted', 'response restricted survey', 'restricted survey visibility', 'survey visibility worker', 'visibility worker qualification', 'worker qualification survey', 'qualification survey access', 'survey access wa', 'access wa also', 'wa also restricted', 'also restricted u', 'restricted u ip', 'u ip address', 'ip address mturk', 'address mturk data', 'mturk data collected', 'data collected outside', 'collected outside united', 'outside united state', 'united state generally', 'state generally poorer', 'generally poorer quality', 'poorer quality participant', 'quality participant permitted', 'participant permitted take', 'permitted take survey', 'take survey excluded', 'survey excluded participant', 'excluded participant total', 'participant total fewer', 'total fewer five', 'fewer five twitter', 'five twitter post', 'twitter post also', 'post also excluded', 'also excluded participant', 'excluded participant cesd', 'participant cesd score', 'cesd score lower', 'score lower depression', 'lower depression tsq', 'depression tsq score', 'tsq score lower', 'score lower ptsd', 'lower ptsd study', 'ptsd study indicated', 'study indicated cesd', 'indicated cesd score', 'cesd score represents', 'score represents optimal', 'represents optimal cutoff', 'optimal cutoff identifying', 'cutoff identifying clinically', 'identifying clinically relevant', 'clinically relevant depression', 'relevant depression equivalent', 'depression equivalent tsq', 'equivalent tsq cutoff', 'tsq cutoff ha', 'cutoff ha found', 'ha found optimal', 'found optimal case', 'optimal case ptsd', 'case ptsd note', 'ptsd note study', 'note study inspired', 'study inspired present', 'inspired present work', 'present work de', 'work de choudhury', 'de choudhury et', 'choudhury et al', 'et al used', 'al used two', 'used two depression', 'two depression scale', 'depression scale cesd', 'scale cesd bdi', 'cesd bdi filtered', 'bdi filtered individual', 'filtered individual whose', 'individual whose depression', 'whose depression score', 'depression score correlate', 'score correlate across', 'correlate across scale', 'across scale additional', 'scale additional criterion', 'additional criterion methodological', 'criterion methodological strength', 'methodological strength de', 'strength de choudhury', 'de choudhury et', 'choudhury et al', 'et al respect', 'al respect present', 'respect present work']",,,,,,,,
https://aclanthology.org/W14-3214.pdf,1,Ngrams of order to 1 to 3 found via HappierFunTokenizer and restricted to those used by at least 5% of users (resulting in 10450 ngrams). The features were encoded as relative frequency of mentioning each ngram (ng):,ngrams of order to to found via happierfuntokenizer and restricted to those used by at least of user resulting in ngrams the feature were encoded a relative frequency of mentioning each ngram ng,"['ngrams', 'order', 'found', 'via', 'happierfuntokenizer', 'restricted', 'used', 'least', 'user', 'resulting', 'ngrams', 'feature', 'encoded', 'relative', 'frequency', 'mentioning', 'ngram', 'ng']","['ngrams order', 'order found', 'found via', 'via happierfuntokenizer', 'happierfuntokenizer restricted', 'restricted used', 'used least', 'least user', 'user resulting', 'resulting ngrams', 'ngrams feature', 'feature encoded', 'encoded relative', 'relative frequency', 'frequency mentioning', 'mentioning ngram', 'ngram ng']","['ngrams order found', 'order found via', 'found via happierfuntokenizer', 'via happierfuntokenizer restricted', 'happierfuntokenizer restricted used', 'restricted used least', 'used least user', 'least user resulting', 'user resulting ngrams', 'resulting ngrams feature', 'ngrams feature encoded', 'feature encoded relative', 'encoded relative frequency', 'relative frequency mentioning', 'frequency mentioning ngram', 'mentioning ngram ng']",,,,,,,,
https://ieeexplore.ieee.org/abstract/document/6784326,0,To characterize the difference between CLINICAL and CONTROL communities a variety of features are extracted: Affective features: We use the lexicon—Affective Norms for English Words (ANEW) [5]—to extract the sentiment conveyed in the content. This lexicon consists of 1034 words rated in terms of valence and arousal and is thus suitable for a quantitative estimation. The valence of ANEW words is on a scale of 1 (very unpleasant) to 9 (very pleasant). The arousal is measured on the same scale—1 (least active) to 9 (most active). A cloud visualization of ANEW words used in the blog posts made by CLINICAL and CONTROL groups is illustrated in Fig. 1. Mood tags: LiveJournal provides a mechanism for users to tag their posts from a list of 132 pre-defined mood labels.4 Thus in addition to the emotion expressed in the text of posts the mood tag produced allows us direct access to the user sentiment. A cloud visualization of moods tagged on blog posts made by CLINICAL and CONTROL communities is llustrated in Fig. 2. LIWC features: We examine the proportions of words in psycholinguistic categories as defined in the LIWC package [27]: linguistic social affective cognitive perceptual biological relativity personal concerns and spoken.5 Table 3 presents the mean of these LIWC psycholinguistic processes for the CLINICAL and CONTROL communities. Whilst similar in the use words with positive emotion people in the CLINICAL communities tend to use words with more negative emotion—as examples anxiety anger and sadness. Further they discuss more issues about health and death in comparison with the CONTROL group. On the other hand the users in the CONTROL group discuss more neutral life related topics—ingestion home and leisure words. Topics: For extracting topics latent Dirichlet allocation (LDA) [4] is used as a Bayesian probabilistic modelling framework. LDA extracts the probabilities —that is words in a topic and then assigns a topic to each word in a document. For the inference part we implemented Gibbs inference detailed in [10]. We set the number of topics to 50 run the Gibbs for 5000 samples and use the last Gibbs sample to interpret the results.,to characterize the difference between clinical and control community a variety of feature are extracted affective feature we use the lexiconaffective norm for english word anew to extract the sentiment conveyed in the content this lexicon consists of word rated in term of valence and arousal and is thus suitable for a quantitative estimation the valence of anew word is on a scale of very unpleasant to very pleasant the arousal is measured on the same scale least active to most active a cloud visualization of anew word used in the blog post made by clinical and control group is illustrated in fig mood tag livejournal provides a mechanism for user to tag their post from a list of predefined mood label thus in addition to the emotion expressed in the text of post the mood tag produced allows u direct access to the user sentiment a cloud visualization of mood tagged on blog post made by clinical and control community is llustrated in fig liwc feature we examine the proportion of word in psycholinguistic category a defined in the liwc package linguistic social affective cognitive perceptual biological relativity personal concern and spoken table present the mean of these liwc psycholinguistic process for the clinical and control community whilst similar in the use word with positive emotion people in the clinical community tend to use word with more negative emotionas example anxiety anger and sadness further they discus more issue about health and death in comparison with the control group on the other hand the user in the control group discus more neutral life related topicsingestion home and leisure word topic for extracting topic latent dirichlet allocation lda is used a a bayesian probabilistic modelling framework lda extract the probability that is word in a topic and then assigns a topic to each word in a document for the inference part we implemented gibbs inference detailed in we set the number of topic to run the gibbs for sample and use the last gibbs sample to interpret the result,"['characterize', 'difference', 'clinical', 'control', 'community', 'variety', 'feature', 'extracted', 'affective', 'feature', 'use', 'lexiconaffective', 'norm', 'english', 'word', 'anew', 'extract', 'sentiment', 'conveyed', 'content', 'lexicon', 'consists', 'word', 'rated', 'term', 'valence', 'arousal', 'thus', 'suitable', 'quantitative', 'estimation', 'valence', 'anew', 'word', 'scale', 'unpleasant', 'pleasant', 'arousal', 'measured', 'scale', 'least', 'active', 'active', 'cloud', 'visualization', 'anew', 'word', 'used', 'blog', 'post', 'made', 'clinical', 'control', 'group', 'illustrated', 'fig', 'mood', 'tag', 'livejournal', 'provides', 'mechanism', 'user', 'tag', 'post', 'list', 'predefined', 'mood', 'label', 'thus', 'addition', 'emotion', 'expressed', 'text', 'post', 'mood', 'tag', 'produced', 'allows', 'u', 'direct', 'access', 'user', 'sentiment', 'cloud', 'visualization', 'mood', 'tagged', 'blog', 'post', 'made', 'clinical', 'control', 'community', 'llustrated', 'fig', 'liwc', 'feature', 'examine', 'proportion', 'word', 'psycholinguistic', 'category', 'defined', 'liwc', 'package', 'linguistic', 'social', 'affective', 'cognitive', 'perceptual', 'biological', 'relativity', 'personal', 'concern', 'spoken', 'table', 'present', 'mean', 'liwc', 'psycholinguistic', 'process', 'clinical', 'control', 'community', 'whilst', 'similar', 'use', 'word', 'positive', 'emotion', 'people', 'clinical', 'community', 'tend', 'use', 'word', 'negative', 'emotionas', 'example', 'anxiety', 'anger', 'sadness', 'discus', 'issue', 'health', 'death', 'comparison', 'control', 'group', 'hand', 'user', 'control', 'group', 'discus', 'neutral', 'life', 'related', 'topicsingestion', 'home', 'leisure', 'word', 'topic', 'extracting', 'topic', 'latent', 'dirichlet', 'allocation', 'lda', 'used', 'bayesian', 'probabilistic', 'modelling', 'framework', 'lda', 'extract', 'probability', 'word', 'topic', 'assigns', 'topic', 'word', 'document', 'inference', 'part', 'implemented', 'gibbs', 'inference', 'detailed', 'set', 'number', 'topic', 'run', 'gibbs', 'sample', 'use', 'last', 'gibbs', 'sample', 'interpret', 'result']","['characterize difference', 'difference clinical', 'clinical control', 'control community', 'community variety', 'variety feature', 'feature extracted', 'extracted affective', 'affective feature', 'feature use', 'use lexiconaffective', 'lexiconaffective norm', 'norm english', 'english word', 'word anew', 'anew extract', 'extract sentiment', 'sentiment conveyed', 'conveyed content', 'content lexicon', 'lexicon consists', 'consists word', 'word rated', 'rated term', 'term valence', 'valence arousal', 'arousal thus', 'thus suitable', 'suitable quantitative', 'quantitative estimation', 'estimation valence', 'valence anew', 'anew word', 'word scale', 'scale unpleasant', 'unpleasant pleasant', 'pleasant arousal', 'arousal measured', 'measured scale', 'scale least', 'least active', 'active active', 'active cloud', 'cloud visualization', 'visualization anew', 'anew word', 'word used', 'used blog', 'blog post', 'post made', 'made clinical', 'clinical control', 'control group', 'group illustrated', 'illustrated fig', 'fig mood', 'mood tag', 'tag livejournal', 'livejournal provides', 'provides mechanism', 'mechanism user', 'user tag', 'tag post', 'post list', 'list predefined', 'predefined mood', 'mood label', 'label thus', 'thus addition', 'addition emotion', 'emotion expressed', 'expressed text', 'text post', 'post mood', 'mood tag', 'tag produced', 'produced allows', 'allows u', 'u direct', 'direct access', 'access user', 'user sentiment', 'sentiment cloud', 'cloud visualization', 'visualization mood', 'mood tagged', 'tagged blog', 'blog post', 'post made', 'made clinical', 'clinical control', 'control community', 'community llustrated', 'llustrated fig', 'fig liwc', 'liwc feature', 'feature examine', 'examine proportion', 'proportion word', 'word psycholinguistic', 'psycholinguistic category', 'category defined', 'defined liwc', 'liwc package', 'package linguistic', 'linguistic social', 'social affective', 'affective cognitive', 'cognitive perceptual', 'perceptual biological', 'biological relativity', 'relativity personal', 'personal concern', 'concern spoken', 'spoken table', 'table present', 'present mean', 'mean liwc', 'liwc psycholinguistic', 'psycholinguistic process', 'process clinical', 'clinical control', 'control community', 'community whilst', 'whilst similar', 'similar use', 'use word', 'word positive', 'positive emotion', 'emotion people', 'people clinical', 'clinical community', 'community tend', 'tend use', 'use word', 'word negative', 'negative emotionas', 'emotionas example', 'example anxiety', 'anxiety anger', 'anger sadness', 'sadness discus', 'discus issue', 'issue health', 'health death', 'death comparison', 'comparison control', 'control group', 'group hand', 'hand user', 'user control', 'control group', 'group discus', 'discus neutral', 'neutral life', 'life related', 'related topicsingestion', 'topicsingestion home', 'home leisure', 'leisure word', 'word topic', 'topic extracting', 'extracting topic', 'topic latent', 'latent dirichlet', 'dirichlet allocation', 'allocation lda', 'lda used', 'used bayesian', 'bayesian probabilistic', 'probabilistic modelling', 'modelling framework', 'framework lda', 'lda extract', 'extract probability', 'probability word', 'word topic', 'topic assigns', 'assigns topic', 'topic word', 'word document', 'document inference', 'inference part', 'part implemented', 'implemented gibbs', 'gibbs inference', 'inference detailed', 'detailed set', 'set number', 'number topic', 'topic run', 'run gibbs', 'gibbs sample', 'sample use', 'use last', 'last gibbs', 'gibbs sample', 'sample interpret', 'interpret result']","['characterize difference clinical', 'difference clinical control', 'clinical control community', 'control community variety', 'community variety feature', 'variety feature extracted', 'feature extracted affective', 'extracted affective feature', 'affective feature use', 'feature use lexiconaffective', 'use lexiconaffective norm', 'lexiconaffective norm english', 'norm english word', 'english word anew', 'word anew extract', 'anew extract sentiment', 'extract sentiment conveyed', 'sentiment conveyed content', 'conveyed content lexicon', 'content lexicon consists', 'lexicon consists word', 'consists word rated', 'word rated term', 'rated term valence', 'term valence arousal', 'valence arousal thus', 'arousal thus suitable', 'thus suitable quantitative', 'suitable quantitative estimation', 'quantitative estimation valence', 'estimation valence anew', 'valence anew word', 'anew word scale', 'word scale unpleasant', 'scale unpleasant pleasant', 'unpleasant pleasant arousal', 'pleasant arousal measured', 'arousal measured scale', 'measured scale least', 'scale least active', 'least active active', 'active active cloud', 'active cloud visualization', 'cloud visualization anew', 'visualization anew word', 'anew word used', 'word used blog', 'used blog post', 'blog post made', 'post made clinical', 'made clinical control', 'clinical control group', 'control group illustrated', 'group illustrated fig', 'illustrated fig mood', 'fig mood tag', 'mood tag livejournal', 'tag livejournal provides', 'livejournal provides mechanism', 'provides mechanism user', 'mechanism user tag', 'user tag post', 'tag post list', 'post list predefined', 'list predefined mood', 'predefined mood label', 'mood label thus', 'label thus addition', 'thus addition emotion', 'addition emotion expressed', 'emotion expressed text', 'expressed text post', 'text post mood', 'post mood tag', 'mood tag produced', 'tag produced allows', 'produced allows u', 'allows u direct', 'u direct access', 'direct access user', 'access user sentiment', 'user sentiment cloud', 'sentiment cloud visualization', 'cloud visualization mood', 'visualization mood tagged', 'mood tagged blog', 'tagged blog post', 'blog post made', 'post made clinical', 'made clinical control', 'clinical control community', 'control community llustrated', 'community llustrated fig', 'llustrated fig liwc', 'fig liwc feature', 'liwc feature examine', 'feature examine proportion', 'examine proportion word', 'proportion word psycholinguistic', 'word psycholinguistic category', 'psycholinguistic category defined', 'category defined liwc', 'defined liwc package', 'liwc package linguistic', 'package linguistic social', 'linguistic social affective', 'social affective cognitive', 'affective cognitive perceptual', 'cognitive perceptual biological', 'perceptual biological relativity', 'biological relativity personal', 'relativity personal concern', 'personal concern spoken', 'concern spoken table', 'spoken table present', 'table present mean', 'present mean liwc', 'mean liwc psycholinguistic', 'liwc psycholinguistic process', 'psycholinguistic process clinical', 'process clinical control', 'clinical control community', 'control community whilst', 'community whilst similar', 'whilst similar use', 'similar use word', 'use word positive', 'word positive emotion', 'positive emotion people', 'emotion people clinical', 'people clinical community', 'clinical community tend', 'community tend use', 'tend use word', 'use word negative', 'word negative emotionas', 'negative emotionas example', 'emotionas example anxiety', 'example anxiety anger', 'anxiety anger sadness', 'anger sadness discus', 'sadness discus issue', 'discus issue health', 'issue health death', 'health death comparison', 'death comparison control', 'comparison control group', 'control group hand', 'group hand user', 'hand user control', 'user control group', 'control group discus', 'group discus neutral', 'discus neutral life', 'neutral life related', 'life related topicsingestion', 'related topicsingestion home', 'topicsingestion home leisure', 'home leisure word', 'leisure word topic', 'word topic extracting', 'topic extracting topic', 'extracting topic latent', 'topic latent dirichlet', 'latent dirichlet allocation', 'dirichlet allocation lda', 'allocation lda used', 'lda used bayesian', 'used bayesian probabilistic', 'bayesian probabilistic modelling', 'probabilistic modelling framework', 'modelling framework lda', 'framework lda extract', 'lda extract probability', 'extract probability word', 'probability word topic', 'word topic assigns', 'topic assigns topic', 'assigns topic word', 'topic word document', 'word document inference', 'document inference part', 'inference part implemented', 'part implemented gibbs', 'implemented gibbs inference', 'gibbs inference detailed', 'inference detailed set', 'detailed set number', 'set number topic', 'number topic run', 'topic run gibbs', 'run gibbs sample', 'gibbs sample use', 'sample use last', 'use last gibbs', 'last gibbs sample', 'gibbs sample interpret', 'sample interpret result']",,,,,,,,
https://www.jmir.org/2017/7/e243/,0,Weibo posts were segmented using the Stanford word segmenter [39] that resulted in 349374 words and phrases. Thereafter the SC-LIWC [33] dictionary was applied to count the appearance of each category of words in every respondents’ Weibo posts. The SC-LIWC dictionary includes 7450 words that are grouped into 71 categories including 7 main linguistic or psychological categories and 64 subcategories. In addition the total number of words or phrases that each respondent published in the 12 months was counted as the 72nd category. Scores of the SC-LIWC categories were counted as percentages of the total number of words.,weibo post were segmented using the stanford word segmenter that resulted in word and phrase thereafter the scliwc dictionary wa applied to count the appearance of each category of word in every respondent weibo post the scliwc dictionary includes word that are grouped into category including main linguistic or psychological category and subcategories in addition the total number of word or phrase that each respondent published in the month wa counted a the nd category score of the scliwc category were counted a percentage of the total number of word,"['weibo', 'post', 'segmented', 'using', 'stanford', 'word', 'segmenter', 'resulted', 'word', 'phrase', 'thereafter', 'scliwc', 'dictionary', 'wa', 'applied', 'count', 'appearance', 'category', 'word', 'every', 'respondent', 'weibo', 'post', 'scliwc', 'dictionary', 'includes', 'word', 'grouped', 'category', 'including', 'main', 'linguistic', 'psychological', 'category', 'subcategories', 'addition', 'total', 'number', 'word', 'phrase', 'respondent', 'published', 'month', 'wa', 'counted', 'nd', 'category', 'score', 'scliwc', 'category', 'counted', 'percentage', 'total', 'number', 'word']","['weibo post', 'post segmented', 'segmented using', 'using stanford', 'stanford word', 'word segmenter', 'segmenter resulted', 'resulted word', 'word phrase', 'phrase thereafter', 'thereafter scliwc', 'scliwc dictionary', 'dictionary wa', 'wa applied', 'applied count', 'count appearance', 'appearance category', 'category word', 'word every', 'every respondent', 'respondent weibo', 'weibo post', 'post scliwc', 'scliwc dictionary', 'dictionary includes', 'includes word', 'word grouped', 'grouped category', 'category including', 'including main', 'main linguistic', 'linguistic psychological', 'psychological category', 'category subcategories', 'subcategories addition', 'addition total', 'total number', 'number word', 'word phrase', 'phrase respondent', 'respondent published', 'published month', 'month wa', 'wa counted', 'counted nd', 'nd category', 'category score', 'score scliwc', 'scliwc category', 'category counted', 'counted percentage', 'percentage total', 'total number', 'number word']","['weibo post segmented', 'post segmented using', 'segmented using stanford', 'using stanford word', 'stanford word segmenter', 'word segmenter resulted', 'segmenter resulted word', 'resulted word phrase', 'word phrase thereafter', 'phrase thereafter scliwc', 'thereafter scliwc dictionary', 'scliwc dictionary wa', 'dictionary wa applied', 'wa applied count', 'applied count appearance', 'count appearance category', 'appearance category word', 'category word every', 'word every respondent', 'every respondent weibo', 'respondent weibo post', 'weibo post scliwc', 'post scliwc dictionary', 'scliwc dictionary includes', 'dictionary includes word', 'includes word grouped', 'word grouped category', 'grouped category including', 'category including main', 'including main linguistic', 'main linguistic psychological', 'linguistic psychological category', 'psychological category subcategories', 'category subcategories addition', 'subcategories addition total', 'addition total number', 'total number word', 'number word phrase', 'word phrase respondent', 'phrase respondent published', 'respondent published month', 'published month wa', 'month wa counted', 'wa counted nd', 'counted nd category', 'nd category score', 'category score scliwc', 'score scliwc category', 'scliwc category counted', 'category counted percentage', 'counted percentage total', 'percentage total number', 'total number word']",,,,,,,,
https://dl.acm.org/doi/abs/10.1145/3025453.3025909,0,We now present a methodology of identifying posts shared in university subreddits that that are likely to be mental health expressions. Note that our Reddit data does not contain any gold standard information around whether a post shared in a university subreddit is about one’s mental health experience or condition. Our proposed method overcomes this challenge by employing an inductive transfer learning approach [16]. First we include (as ground truth data) Reddit posts made on various mental health support communities. Prior work has established that in these communities individuals selfdisclose a variety of mental health challenges explicitly [50]. Parallelly we utilize another set of Reddit posts made on generic subreddits unrelated to mental health to be a control. Next we build a machine learning classifier to distinguish between these two types of posts. Then we learn features that could detect whether an post shared in a university subreddit could be an expression of some mental health concern. We discuss these steps in detail in the following subsections.,we now present a methodology of identifying post shared in university subreddits that that are likely to be mental health expression note that our reddit data doe not contain any gold standard information around whether a post shared in a university subreddit is about one mental health experience or condition our proposed method overcomes this challenge by employing an inductive transfer learning approach first we include a ground truth data reddit post made on various mental health support community prior work ha established that in these community individual selfdisclose a variety of mental health challenge explicitly parallelly we utilize another set of reddit post made on generic subreddits unrelated to mental health to be a control next we build a machine learning classifier to distinguish between these two type of post then we learn feature that could detect whether an post shared in a university subreddit could be an expression of some mental health concern we discus these step in detail in the following subsection,"['present', 'methodology', 'identifying', 'post', 'shared', 'university', 'subreddits', 'likely', 'mental', 'health', 'expression', 'note', 'reddit', 'data', 'doe', 'contain', 'gold', 'standard', 'information', 'around', 'whether', 'post', 'shared', 'university', 'subreddit', 'one', 'mental', 'health', 'experience', 'condition', 'proposed', 'method', 'overcomes', 'challenge', 'employing', 'inductive', 'transfer', 'learning', 'approach', 'first', 'include', 'ground', 'truth', 'data', 'reddit', 'post', 'made', 'various', 'mental', 'health', 'support', 'community', 'prior', 'work', 'ha', 'established', 'community', 'individual', 'selfdisclose', 'variety', 'mental', 'health', 'challenge', 'explicitly', 'parallelly', 'utilize', 'another', 'set', 'reddit', 'post', 'made', 'generic', 'subreddits', 'unrelated', 'mental', 'health', 'control', 'next', 'build', 'machine', 'learning', 'classifier', 'distinguish', 'two', 'type', 'post', 'learn', 'feature', 'could', 'detect', 'whether', 'post', 'shared', 'university', 'subreddit', 'could', 'expression', 'mental', 'health', 'concern', 'discus', 'step', 'detail', 'following', 'subsection']","['present methodology', 'methodology identifying', 'identifying post', 'post shared', 'shared university', 'university subreddits', 'subreddits likely', 'likely mental', 'mental health', 'health expression', 'expression note', 'note reddit', 'reddit data', 'data doe', 'doe contain', 'contain gold', 'gold standard', 'standard information', 'information around', 'around whether', 'whether post', 'post shared', 'shared university', 'university subreddit', 'subreddit one', 'one mental', 'mental health', 'health experience', 'experience condition', 'condition proposed', 'proposed method', 'method overcomes', 'overcomes challenge', 'challenge employing', 'employing inductive', 'inductive transfer', 'transfer learning', 'learning approach', 'approach first', 'first include', 'include ground', 'ground truth', 'truth data', 'data reddit', 'reddit post', 'post made', 'made various', 'various mental', 'mental health', 'health support', 'support community', 'community prior', 'prior work', 'work ha', 'ha established', 'established community', 'community individual', 'individual selfdisclose', 'selfdisclose variety', 'variety mental', 'mental health', 'health challenge', 'challenge explicitly', 'explicitly parallelly', 'parallelly utilize', 'utilize another', 'another set', 'set reddit', 'reddit post', 'post made', 'made generic', 'generic subreddits', 'subreddits unrelated', 'unrelated mental', 'mental health', 'health control', 'control next', 'next build', 'build machine', 'machine learning', 'learning classifier', 'classifier distinguish', 'distinguish two', 'two type', 'type post', 'post learn', 'learn feature', 'feature could', 'could detect', 'detect whether', 'whether post', 'post shared', 'shared university', 'university subreddit', 'subreddit could', 'could expression', 'expression mental', 'mental health', 'health concern', 'concern discus', 'discus step', 'step detail', 'detail following', 'following subsection']","['present methodology identifying', 'methodology identifying post', 'identifying post shared', 'post shared university', 'shared university subreddits', 'university subreddits likely', 'subreddits likely mental', 'likely mental health', 'mental health expression', 'health expression note', 'expression note reddit', 'note reddit data', 'reddit data doe', 'data doe contain', 'doe contain gold', 'contain gold standard', 'gold standard information', 'standard information around', 'information around whether', 'around whether post', 'whether post shared', 'post shared university', 'shared university subreddit', 'university subreddit one', 'subreddit one mental', 'one mental health', 'mental health experience', 'health experience condition', 'experience condition proposed', 'condition proposed method', 'proposed method overcomes', 'method overcomes challenge', 'overcomes challenge employing', 'challenge employing inductive', 'employing inductive transfer', 'inductive transfer learning', 'transfer learning approach', 'learning approach first', 'approach first include', 'first include ground', 'include ground truth', 'ground truth data', 'truth data reddit', 'data reddit post', 'reddit post made', 'post made various', 'made various mental', 'various mental health', 'mental health support', 'health support community', 'support community prior', 'community prior work', 'prior work ha', 'work ha established', 'ha established community', 'established community individual', 'community individual selfdisclose', 'individual selfdisclose variety', 'selfdisclose variety mental', 'variety mental health', 'mental health challenge', 'health challenge explicitly', 'challenge explicitly parallelly', 'explicitly parallelly utilize', 'parallelly utilize another', 'utilize another set', 'another set reddit', 'set reddit post', 'reddit post made', 'post made generic', 'made generic subreddits', 'generic subreddits unrelated', 'subreddits unrelated mental', 'unrelated mental health', 'mental health control', 'health control next', 'control next build', 'next build machine', 'build machine learning', 'machine learning classifier', 'learning classifier distinguish', 'classifier distinguish two', 'distinguish two type', 'two type post', 'type post learn', 'post learn feature', 'learn feature could', 'feature could detect', 'could detect whether', 'detect whether post', 'whether post shared', 'post shared university', 'shared university subreddit', 'university subreddit could', 'subreddit could expression', 'could expression mental', 'expression mental health', 'mental health concern', 'health concern discus', 'concern discus step', 'discus step detail', 'step detail following', 'detail following subsection']",,,,,,,,
https://ieeexplore.ieee.org/abstract/document/7752434,0,In this work we are focused on two main type of features (linguistic and behavioral). TF-IDF is adopted to model the linguist features of patients and Pattern of Life Features (PLF) adopted from the work of Coppersmith et al. [1] is used to model the behavioral style of patients. TF-IDF Features To capture the frequent and representative words used by the patients TF-IDF is applied on the unigram and bigrams collected from all the patients' tweets. Pattern of Life Features (PLF) These features reveal the emotional patterns and behavioral tendency of users by measuring polarity emotion and social interactions. In order to fully compose the PLF we combined the following list of features: Age and Gender: Twitter does not publicly provide information about the age and gender of its users mainly due to privacy concerns so we adopted the work of Sap et al. [5] to fill in this information. Polarity Features: The Sentiment140 API 3 was used to label each tweet as either positive negative or neutral. The polarity is furthermore transformed into five different values to capture the affective traits of each user: 1) Positive Ratio: the percentage of positive tweets 2) Negative Ratio: the percentage of negative tweets 3) Positive Combo: captures the mania and hypomania traits of patients which is determined by the number of continuous positive posts appearing more than x amount of times within a period of time in minutes T. 4) Negative Combos: captures the depression traits of patients and is determined by the number of continuous negative posts appearing more than x amount of times within a period of time in minutes T. 5) Flips Ratio: quantifies the emotional unstableness and is determined by counting how frequently two continuous tweets with different polarity (either positive to negative or negative to positive) appear together within a period of time in minutes T. In our work x is set to 2 and T is set to 30 minutes. Social Features: These features can demonstrate how users are behaving with respect to their environment. The following are the social features designed for each user: 1) Tweeting Frequency; the frequency of daily posts; 2) Mention Ratio: the percentage of posts which contain at least one mention of another user; 3) Frequent Mentions: the number of Twitter users mentioned more than three times which is a measurement of how many close friends a particular user may have; 4) Unique Mentions: the number of unique users mentioned which is a measure of the width of a user's social network.,in this work we are focused on two main type of feature linguistic and behavioral tfidf is adopted to model the linguist feature of patient and pattern of life feature plf adopted from the work of coppersmith et al is used to model the behavioral style of patient tfidf feature to capture the frequent and representative word used by the patient tfidf is applied on the unigram and bigram collected from all the patient tweet pattern of life feature plf these feature reveal the emotional pattern and behavioral tendency of user by measuring polarity emotion and social interaction in order to fully compose the plf we combined the following list of feature age and gender twitter doe not publicly provide information about the age and gender of it user mainly due to privacy concern so we adopted the work of sap et al to fill in this information polarity feature the sentiment api wa used to label each tweet a either positive negative or neutral the polarity is furthermore transformed into five different value to capture the affective trait of each user positive ratio the percentage of positive tweet negative ratio the percentage of negative tweet positive combo capture the mania and hypomania trait of patient which is determined by the number of continuous positive post appearing more than x amount of time within a period of time in minute t negative combo capture the depression trait of patient and is determined by the number of continuous negative post appearing more than x amount of time within a period of time in minute t flip ratio quantifies the emotional unstableness and is determined by counting how frequently two continuous tweet with different polarity either positive to negative or negative to positive appear together within a period of time in minute t in our work x is set to and t is set to minute social feature these feature can demonstrate how user are behaving with respect to their environment the following are the social feature designed for each user tweeting frequency the frequency of daily post mention ratio the percentage of post which contain at least one mention of another user frequent mention the number of twitter user mentioned more than three time which is a measurement of how many close friend a particular user may have unique mention the number of unique user mentioned which is a measure of the width of a user social network,"['work', 'focused', 'two', 'main', 'type', 'feature', 'linguistic', 'behavioral', 'tfidf', 'adopted', 'model', 'linguist', 'feature', 'patient', 'pattern', 'life', 'feature', 'plf', 'adopted', 'work', 'coppersmith', 'et', 'al', 'used', 'model', 'behavioral', 'style', 'patient', 'tfidf', 'feature', 'capture', 'frequent', 'representative', 'word', 'used', 'patient', 'tfidf', 'applied', 'unigram', 'bigram', 'collected', 'patient', 'tweet', 'pattern', 'life', 'feature', 'plf', 'feature', 'reveal', 'emotional', 'pattern', 'behavioral', 'tendency', 'user', 'measuring', 'polarity', 'emotion', 'social', 'interaction', 'order', 'fully', 'compose', 'plf', 'combined', 'following', 'list', 'feature', 'age', 'gender', 'twitter', 'doe', 'publicly', 'provide', 'information', 'age', 'gender', 'user', 'mainly', 'due', 'privacy', 'concern', 'adopted', 'work', 'sap', 'et', 'al', 'fill', 'information', 'polarity', 'feature', 'sentiment', 'api', 'wa', 'used', 'label', 'tweet', 'either', 'positive', 'negative', 'neutral', 'polarity', 'furthermore', 'transformed', 'five', 'different', 'value', 'capture', 'affective', 'trait', 'user', 'positive', 'ratio', 'percentage', 'positive', 'tweet', 'negative', 'ratio', 'percentage', 'negative', 'tweet', 'positive', 'combo', 'capture', 'mania', 'hypomania', 'trait', 'patient', 'determined', 'number', 'continuous', 'positive', 'post', 'appearing', 'x', 'amount', 'time', 'within', 'period', 'time', 'minute', 'negative', 'combo', 'capture', 'depression', 'trait', 'patient', 'determined', 'number', 'continuous', 'negative', 'post', 'appearing', 'x', 'amount', 'time', 'within', 'period', 'time', 'minute', 'flip', 'ratio', 'quantifies', 'emotional', 'unstableness', 'determined', 'counting', 'frequently', 'two', 'continuous', 'tweet', 'different', 'polarity', 'either', 'positive', 'negative', 'negative', 'positive', 'appear', 'together', 'within', 'period', 'time', 'minute', 'work', 'x', 'set', 'set', 'minute', 'social', 'feature', 'feature', 'demonstrate', 'user', 'behaving', 'respect', 'environment', 'following', 'social', 'feature', 'designed', 'user', 'tweeting', 'frequency', 'frequency', 'daily', 'post', 'mention', 'ratio', 'percentage', 'post', 'contain', 'least', 'one', 'mention', 'another', 'user', 'frequent', 'mention', 'number', 'twitter', 'user', 'mentioned', 'three', 'time', 'measurement', 'many', 'close', 'friend', 'particular', 'user', 'may', 'unique', 'mention', 'number', 'unique', 'user', 'mentioned', 'measure', 'width', 'user', 'social', 'network']","['work focused', 'focused two', 'two main', 'main type', 'type feature', 'feature linguistic', 'linguistic behavioral', 'behavioral tfidf', 'tfidf adopted', 'adopted model', 'model linguist', 'linguist feature', 'feature patient', 'patient pattern', 'pattern life', 'life feature', 'feature plf', 'plf adopted', 'adopted work', 'work coppersmith', 'coppersmith et', 'et al', 'al used', 'used model', 'model behavioral', 'behavioral style', 'style patient', 'patient tfidf', 'tfidf feature', 'feature capture', 'capture frequent', 'frequent representative', 'representative word', 'word used', 'used patient', 'patient tfidf', 'tfidf applied', 'applied unigram', 'unigram bigram', 'bigram collected', 'collected patient', 'patient tweet', 'tweet pattern', 'pattern life', 'life feature', 'feature plf', 'plf feature', 'feature reveal', 'reveal emotional', 'emotional pattern', 'pattern behavioral', 'behavioral tendency', 'tendency user', 'user measuring', 'measuring polarity', 'polarity emotion', 'emotion social', 'social interaction', 'interaction order', 'order fully', 'fully compose', 'compose plf', 'plf combined', 'combined following', 'following list', 'list feature', 'feature age', 'age gender', 'gender twitter', 'twitter doe', 'doe publicly', 'publicly provide', 'provide information', 'information age', 'age gender', 'gender user', 'user mainly', 'mainly due', 'due privacy', 'privacy concern', 'concern adopted', 'adopted work', 'work sap', 'sap et', 'et al', 'al fill', 'fill information', 'information polarity', 'polarity feature', 'feature sentiment', 'sentiment api', 'api wa', 'wa used', 'used label', 'label tweet', 'tweet either', 'either positive', 'positive negative', 'negative neutral', 'neutral polarity', 'polarity furthermore', 'furthermore transformed', 'transformed five', 'five different', 'different value', 'value capture', 'capture affective', 'affective trait', 'trait user', 'user positive', 'positive ratio', 'ratio percentage', 'percentage positive', 'positive tweet', 'tweet negative', 'negative ratio', 'ratio percentage', 'percentage negative', 'negative tweet', 'tweet positive', 'positive combo', 'combo capture', 'capture mania', 'mania hypomania', 'hypomania trait', 'trait patient', 'patient determined', 'determined number', 'number continuous', 'continuous positive', 'positive post', 'post appearing', 'appearing x', 'x amount', 'amount time', 'time within', 'within period', 'period time', 'time minute', 'minute negative', 'negative combo', 'combo capture', 'capture depression', 'depression trait', 'trait patient', 'patient determined', 'determined number', 'number continuous', 'continuous negative', 'negative post', 'post appearing', 'appearing x', 'x amount', 'amount time', 'time within', 'within period', 'period time', 'time minute', 'minute flip', 'flip ratio', 'ratio quantifies', 'quantifies emotional', 'emotional unstableness', 'unstableness determined', 'determined counting', 'counting frequently', 'frequently two', 'two continuous', 'continuous tweet', 'tweet different', 'different polarity', 'polarity either', 'either positive', 'positive negative', 'negative negative', 'negative positive', 'positive appear', 'appear together', 'together within', 'within period', 'period time', 'time minute', 'minute work', 'work x', 'x set', 'set set', 'set minute', 'minute social', 'social feature', 'feature feature', 'feature demonstrate', 'demonstrate user', 'user behaving', 'behaving respect', 'respect environment', 'environment following', 'following social', 'social feature', 'feature designed', 'designed user', 'user tweeting', 'tweeting frequency', 'frequency frequency', 'frequency daily', 'daily post', 'post mention', 'mention ratio', 'ratio percentage', 'percentage post', 'post contain', 'contain least', 'least one', 'one mention', 'mention another', 'another user', 'user frequent', 'frequent mention', 'mention number', 'number twitter', 'twitter user', 'user mentioned', 'mentioned three', 'three time', 'time measurement', 'measurement many', 'many close', 'close friend', 'friend particular', 'particular user', 'user may', 'may unique', 'unique mention', 'mention number', 'number unique', 'unique user', 'user mentioned', 'mentioned measure', 'measure width', 'width user', 'user social', 'social network']","['work focused two', 'focused two main', 'two main type', 'main type feature', 'type feature linguistic', 'feature linguistic behavioral', 'linguistic behavioral tfidf', 'behavioral tfidf adopted', 'tfidf adopted model', 'adopted model linguist', 'model linguist feature', 'linguist feature patient', 'feature patient pattern', 'patient pattern life', 'pattern life feature', 'life feature plf', 'feature plf adopted', 'plf adopted work', 'adopted work coppersmith', 'work coppersmith et', 'coppersmith et al', 'et al used', 'al used model', 'used model behavioral', 'model behavioral style', 'behavioral style patient', 'style patient tfidf', 'patient tfidf feature', 'tfidf feature capture', 'feature capture frequent', 'capture frequent representative', 'frequent representative word', 'representative word used', 'word used patient', 'used patient tfidf', 'patient tfidf applied', 'tfidf applied unigram', 'applied unigram bigram', 'unigram bigram collected', 'bigram collected patient', 'collected patient tweet', 'patient tweet pattern', 'tweet pattern life', 'pattern life feature', 'life feature plf', 'feature plf feature', 'plf feature reveal', 'feature reveal emotional', 'reveal emotional pattern', 'emotional pattern behavioral', 'pattern behavioral tendency', 'behavioral tendency user', 'tendency user measuring', 'user measuring polarity', 'measuring polarity emotion', 'polarity emotion social', 'emotion social interaction', 'social interaction order', 'interaction order fully', 'order fully compose', 'fully compose plf', 'compose plf combined', 'plf combined following', 'combined following list', 'following list feature', 'list feature age', 'feature age gender', 'age gender twitter', 'gender twitter doe', 'twitter doe publicly', 'doe publicly provide', 'publicly provide information', 'provide information age', 'information age gender', 'age gender user', 'gender user mainly', 'user mainly due', 'mainly due privacy', 'due privacy concern', 'privacy concern adopted', 'concern adopted work', 'adopted work sap', 'work sap et', 'sap et al', 'et al fill', 'al fill information', 'fill information polarity', 'information polarity feature', 'polarity feature sentiment', 'feature sentiment api', 'sentiment api wa', 'api wa used', 'wa used label', 'used label tweet', 'label tweet either', 'tweet either positive', 'either positive negative', 'positive negative neutral', 'negative neutral polarity', 'neutral polarity furthermore', 'polarity furthermore transformed', 'furthermore transformed five', 'transformed five different', 'five different value', 'different value capture', 'value capture affective', 'capture affective trait', 'affective trait user', 'trait user positive', 'user positive ratio', 'positive ratio percentage', 'ratio percentage positive', 'percentage positive tweet', 'positive tweet negative', 'tweet negative ratio', 'negative ratio percentage', 'ratio percentage negative', 'percentage negative tweet', 'negative tweet positive', 'tweet positive combo', 'positive combo capture', 'combo capture mania', 'capture mania hypomania', 'mania hypomania trait', 'hypomania trait patient', 'trait patient determined', 'patient determined number', 'determined number continuous', 'number continuous positive', 'continuous positive post', 'positive post appearing', 'post appearing x', 'appearing x amount', 'x amount time', 'amount time within', 'time within period', 'within period time', 'period time minute', 'time minute negative', 'minute negative combo', 'negative combo capture', 'combo capture depression', 'capture depression trait', 'depression trait patient', 'trait patient determined', 'patient determined number', 'determined number continuous', 'number continuous negative', 'continuous negative post', 'negative post appearing', 'post appearing x', 'appearing x amount', 'x amount time', 'amount time within', 'time within period', 'within period time', 'period time minute', 'time minute flip', 'minute flip ratio', 'flip ratio quantifies', 'ratio quantifies emotional', 'quantifies emotional unstableness', 'emotional unstableness determined', 'unstableness determined counting', 'determined counting frequently', 'counting frequently two', 'frequently two continuous', 'two continuous tweet', 'continuous tweet different', 'tweet different polarity', 'different polarity either', 'polarity either positive', 'either positive negative', 'positive negative negative', 'negative negative positive', 'negative positive appear', 'positive appear together', 'appear together within', 'together within period', 'within period time', 'period time minute', 'time minute work', 'minute work x', 'work x set', 'x set set', 'set set minute', 'set minute social', 'minute social feature', 'social feature feature', 'feature feature demonstrate', 'feature demonstrate user', 'demonstrate user behaving', 'user behaving respect', 'behaving respect environment', 'respect environment following', 'environment following social', 'following social feature', 'social feature designed', 'feature designed user', 'designed user tweeting', 'user tweeting frequency', 'tweeting frequency frequency', 'frequency frequency daily', 'frequency daily post', 'daily post mention', 'post mention ratio', 'mention ratio percentage', 'ratio percentage post', 'percentage post contain', 'post contain least', 'contain least one', 'least one mention', 'one mention another', 'mention another user', 'another user frequent', 'user frequent mention', 'frequent mention number', 'mention number twitter', 'number twitter user', 'twitter user mentioned', 'user mentioned three', 'mentioned three time', 'three time measurement', 'time measurement many', 'measurement many close', 'many close friend', 'close friend particular', 'friend particular user', 'particular user may', 'user may unique', 'may unique mention', 'unique mention number', 'mention number unique', 'number unique user', 'unique user mentioned', 'user mentioned measure', 'mentioned measure width', 'measure width user', 'width user social', 'user social network']",,,,,,,,
https://www.nature.com/articles/s41598-020-68764-y,1,The data pre-processing procedure for the collected post data is presented in Fig. 1. After collecting the data each title was combined with its corresponding post. We removed unnecessary punctuation marks and white spaces for each post. Then we used the natural language toolkit (NLTK) implemented in Python to tokenize users’ posts and filter frequently employed words (stop words). Porter Stemmer a tool used to define a series of guidelines for exploring word meaning and source was employed on the tokenized words to convert a word to its root meaning and to decrease the number of word corpus. After this procedure data from 228060 users with 488472 posts in total were employed for the analysis.,the data preprocessing procedure for the collected post data is presented in fig after collecting the data each title wa combined with it corresponding post we removed unnecessary punctuation mark and white space for each post then we used the natural language toolkit nltk implemented in python to tokenize user post and filter frequently employed word stop word porter stemmer a tool used to define a series of guideline for exploring word meaning and source wa employed on the tokenized word to convert a word to it root meaning and to decrease the number of word corpus after this procedure data from user with post in total were employed for the analysis,"['data', 'preprocessing', 'procedure', 'collected', 'post', 'data', 'presented', 'fig', 'collecting', 'data', 'title', 'wa', 'combined', 'corresponding', 'post', 'removed', 'unnecessary', 'punctuation', 'mark', 'white', 'space', 'post', 'used', 'natural', 'language', 'toolkit', 'nltk', 'implemented', 'python', 'tokenize', 'user', 'post', 'filter', 'frequently', 'employed', 'word', 'stop', 'word', 'porter', 'stemmer', 'tool', 'used', 'define', 'series', 'guideline', 'exploring', 'word', 'meaning', 'source', 'wa', 'employed', 'tokenized', 'word', 'convert', 'word', 'root', 'meaning', 'decrease', 'number', 'word', 'corpus', 'procedure', 'data', 'user', 'post', 'total', 'employed', 'analysis']","['data preprocessing', 'preprocessing procedure', 'procedure collected', 'collected post', 'post data', 'data presented', 'presented fig', 'fig collecting', 'collecting data', 'data title', 'title wa', 'wa combined', 'combined corresponding', 'corresponding post', 'post removed', 'removed unnecessary', 'unnecessary punctuation', 'punctuation mark', 'mark white', 'white space', 'space post', 'post used', 'used natural', 'natural language', 'language toolkit', 'toolkit nltk', 'nltk implemented', 'implemented python', 'python tokenize', 'tokenize user', 'user post', 'post filter', 'filter frequently', 'frequently employed', 'employed word', 'word stop', 'stop word', 'word porter', 'porter stemmer', 'stemmer tool', 'tool used', 'used define', 'define series', 'series guideline', 'guideline exploring', 'exploring word', 'word meaning', 'meaning source', 'source wa', 'wa employed', 'employed tokenized', 'tokenized word', 'word convert', 'convert word', 'word root', 'root meaning', 'meaning decrease', 'decrease number', 'number word', 'word corpus', 'corpus procedure', 'procedure data', 'data user', 'user post', 'post total', 'total employed', 'employed analysis']","['data preprocessing procedure', 'preprocessing procedure collected', 'procedure collected post', 'collected post data', 'post data presented', 'data presented fig', 'presented fig collecting', 'fig collecting data', 'collecting data title', 'data title wa', 'title wa combined', 'wa combined corresponding', 'combined corresponding post', 'corresponding post removed', 'post removed unnecessary', 'removed unnecessary punctuation', 'unnecessary punctuation mark', 'punctuation mark white', 'mark white space', 'white space post', 'space post used', 'post used natural', 'used natural language', 'natural language toolkit', 'language toolkit nltk', 'toolkit nltk implemented', 'nltk implemented python', 'implemented python tokenize', 'python tokenize user', 'tokenize user post', 'user post filter', 'post filter frequently', 'filter frequently employed', 'frequently employed word', 'employed word stop', 'word stop word', 'stop word porter', 'word porter stemmer', 'porter stemmer tool', 'stemmer tool used', 'tool used define', 'used define series', 'define series guideline', 'series guideline exploring', 'guideline exploring word', 'exploring word meaning', 'word meaning source', 'meaning source wa', 'source wa employed', 'wa employed tokenized', 'employed tokenized word', 'tokenized word convert', 'word convert word', 'convert word root', 'word root meaning', 'root meaning decrease', 'meaning decrease number', 'decrease number word', 'number word corpus', 'word corpus procedure', 'corpus procedure data', 'procedure data user', 'data user post', 'user post total', 'post total employed', 'total employed analysis']",,,,,,,,
https://aclanthology.org/W19-3013.pdf,0,We now briefly describe our approach for cohortbased studies over social media. A more detailed description of the proposed methodology will appear in a forthcoming publication. Most works on social media analysis estimate trends by aggregating document-level signals inferred from arbitrary (and biased) data samples selected to match a predefined outcome. While some recent work has begun incorporating demographic information to contextualize analyses (Mandel et al. 2012; Mitchell et al. 2013; Huang et al. 2017 2019) and to improve representativeness of the data (Coppersmith et al. 2015b; Dos Reis and Culotta 2015) these studies still select on specific outcomes. We depart from these works by constructing a demographically representative digital cohort of social media users prior to the analyses and then conducting cohort-based studies over this preselected population. While a significant undertaking in most medical studies the vast quantities of available social media data make assembling social media cohorts feasible. Such cohorts can be used to support longitudinal and cross-sectional studies allowing experts to contextualize the outcomes produce externally valid trends from inherently biased samples and extrapolate those trends to a broader population. Similar strategies have been utilized in online surveys which can have comparable validity to other survey modalities simply by controlling for basic demographic features such as the location age ethnicity and gender (Duffy et al. 2005).,we now briefly describe our approach for cohortbased study over social medium a more detailed description of the proposed methodology will appear in a forthcoming publication most work on social medium analysis estimate trend by aggregating documentlevel signal inferred from arbitrary and biased data sample selected to match a predefined outcome while some recent work ha begun incorporating demographic information to contextualize analysis mandel et al mitchell et al huang et al and to improve representativeness of the data coppersmith et al b do real and culotta these study still select on specific outcome we depart from these work by constructing a demographically representative digital cohort of social medium user prior to the analysis and then conducting cohortbased study over this preselected population while a significant undertaking in most medical study the vast quantity of available social medium data make assembling social medium cohort feasible such cohort can be used to support longitudinal and crosssectional study allowing expert to contextualize the outcome produce externally valid trend from inherently biased sample and extrapolate those trend to a broader population similar strategy have been utilized in online survey which can have comparable validity to other survey modality simply by controlling for basic demographic feature such a the location age ethnicity and gender duffy et al,"['briefly', 'describe', 'approach', 'cohortbased', 'study', 'social', 'medium', 'detailed', 'description', 'proposed', 'methodology', 'appear', 'forthcoming', 'publication', 'work', 'social', 'medium', 'analysis', 'estimate', 'trend', 'aggregating', 'documentlevel', 'signal', 'inferred', 'arbitrary', 'biased', 'data', 'sample', 'selected', 'match', 'predefined', 'outcome', 'recent', 'work', 'ha', 'begun', 'incorporating', 'demographic', 'information', 'contextualize', 'analysis', 'mandel', 'et', 'al', 'mitchell', 'et', 'al', 'huang', 'et', 'al', 'improve', 'representativeness', 'data', 'coppersmith', 'et', 'al', 'b', 'real', 'culotta', 'study', 'still', 'select', 'specific', 'outcome', 'depart', 'work', 'constructing', 'demographically', 'representative', 'digital', 'cohort', 'social', 'medium', 'user', 'prior', 'analysis', 'conducting', 'cohortbased', 'study', 'preselected', 'population', 'significant', 'undertaking', 'medical', 'study', 'vast', 'quantity', 'available', 'social', 'medium', 'data', 'make', 'assembling', 'social', 'medium', 'cohort', 'feasible', 'cohort', 'used', 'support', 'longitudinal', 'crosssectional', 'study', 'allowing', 'expert', 'contextualize', 'outcome', 'produce', 'externally', 'valid', 'trend', 'inherently', 'biased', 'sample', 'extrapolate', 'trend', 'broader', 'population', 'similar', 'strategy', 'utilized', 'online', 'survey', 'comparable', 'validity', 'survey', 'modality', 'simply', 'controlling', 'basic', 'demographic', 'feature', 'location', 'age', 'ethnicity', 'gender', 'duffy', 'et', 'al']","['briefly describe', 'describe approach', 'approach cohortbased', 'cohortbased study', 'study social', 'social medium', 'medium detailed', 'detailed description', 'description proposed', 'proposed methodology', 'methodology appear', 'appear forthcoming', 'forthcoming publication', 'publication work', 'work social', 'social medium', 'medium analysis', 'analysis estimate', 'estimate trend', 'trend aggregating', 'aggregating documentlevel', 'documentlevel signal', 'signal inferred', 'inferred arbitrary', 'arbitrary biased', 'biased data', 'data sample', 'sample selected', 'selected match', 'match predefined', 'predefined outcome', 'outcome recent', 'recent work', 'work ha', 'ha begun', 'begun incorporating', 'incorporating demographic', 'demographic information', 'information contextualize', 'contextualize analysis', 'analysis mandel', 'mandel et', 'et al', 'al mitchell', 'mitchell et', 'et al', 'al huang', 'huang et', 'et al', 'al improve', 'improve representativeness', 'representativeness data', 'data coppersmith', 'coppersmith et', 'et al', 'al b', 'b real', 'real culotta', 'culotta study', 'study still', 'still select', 'select specific', 'specific outcome', 'outcome depart', 'depart work', 'work constructing', 'constructing demographically', 'demographically representative', 'representative digital', 'digital cohort', 'cohort social', 'social medium', 'medium user', 'user prior', 'prior analysis', 'analysis conducting', 'conducting cohortbased', 'cohortbased study', 'study preselected', 'preselected population', 'population significant', 'significant undertaking', 'undertaking medical', 'medical study', 'study vast', 'vast quantity', 'quantity available', 'available social', 'social medium', 'medium data', 'data make', 'make assembling', 'assembling social', 'social medium', 'medium cohort', 'cohort feasible', 'feasible cohort', 'cohort used', 'used support', 'support longitudinal', 'longitudinal crosssectional', 'crosssectional study', 'study allowing', 'allowing expert', 'expert contextualize', 'contextualize outcome', 'outcome produce', 'produce externally', 'externally valid', 'valid trend', 'trend inherently', 'inherently biased', 'biased sample', 'sample extrapolate', 'extrapolate trend', 'trend broader', 'broader population', 'population similar', 'similar strategy', 'strategy utilized', 'utilized online', 'online survey', 'survey comparable', 'comparable validity', 'validity survey', 'survey modality', 'modality simply', 'simply controlling', 'controlling basic', 'basic demographic', 'demographic feature', 'feature location', 'location age', 'age ethnicity', 'ethnicity gender', 'gender duffy', 'duffy et', 'et al']","['briefly describe approach', 'describe approach cohortbased', 'approach cohortbased study', 'cohortbased study social', 'study social medium', 'social medium detailed', 'medium detailed description', 'detailed description proposed', 'description proposed methodology', 'proposed methodology appear', 'methodology appear forthcoming', 'appear forthcoming publication', 'forthcoming publication work', 'publication work social', 'work social medium', 'social medium analysis', 'medium analysis estimate', 'analysis estimate trend', 'estimate trend aggregating', 'trend aggregating documentlevel', 'aggregating documentlevel signal', 'documentlevel signal inferred', 'signal inferred arbitrary', 'inferred arbitrary biased', 'arbitrary biased data', 'biased data sample', 'data sample selected', 'sample selected match', 'selected match predefined', 'match predefined outcome', 'predefined outcome recent', 'outcome recent work', 'recent work ha', 'work ha begun', 'ha begun incorporating', 'begun incorporating demographic', 'incorporating demographic information', 'demographic information contextualize', 'information contextualize analysis', 'contextualize analysis mandel', 'analysis mandel et', 'mandel et al', 'et al mitchell', 'al mitchell et', 'mitchell et al', 'et al huang', 'al huang et', 'huang et al', 'et al improve', 'al improve representativeness', 'improve representativeness data', 'representativeness data coppersmith', 'data coppersmith et', 'coppersmith et al', 'et al b', 'al b real', 'b real culotta', 'real culotta study', 'culotta study still', 'study still select', 'still select specific', 'select specific outcome', 'specific outcome depart', 'outcome depart work', 'depart work constructing', 'work constructing demographically', 'constructing demographically representative', 'demographically representative digital', 'representative digital cohort', 'digital cohort social', 'cohort social medium', 'social medium user', 'medium user prior', 'user prior analysis', 'prior analysis conducting', 'analysis conducting cohortbased', 'conducting cohortbased study', 'cohortbased study preselected', 'study preselected population', 'preselected population significant', 'population significant undertaking', 'significant undertaking medical', 'undertaking medical study', 'medical study vast', 'study vast quantity', 'vast quantity available', 'quantity available social', 'available social medium', 'social medium data', 'medium data make', 'data make assembling', 'make assembling social', 'assembling social medium', 'social medium cohort', 'medium cohort feasible', 'cohort feasible cohort', 'feasible cohort used', 'cohort used support', 'used support longitudinal', 'support longitudinal crosssectional', 'longitudinal crosssectional study', 'crosssectional study allowing', 'study allowing expert', 'allowing expert contextualize', 'expert contextualize outcome', 'contextualize outcome produce', 'outcome produce externally', 'produce externally valid', 'externally valid trend', 'valid trend inherently', 'trend inherently biased', 'inherently biased sample', 'biased sample extrapolate', 'sample extrapolate trend', 'extrapolate trend broader', 'trend broader population', 'broader population similar', 'population similar strategy', 'similar strategy utilized', 'strategy utilized online', 'utilized online survey', 'online survey comparable', 'survey comparable validity', 'comparable validity survey', 'validity survey modality', 'survey modality simply', 'modality simply controlling', 'simply controlling basic', 'controlling basic demographic', 'basic demographic feature', 'demographic feature location', 'feature location age', 'location age ethnicity', 'age ethnicity gender', 'ethnicity gender duffy', 'gender duffy et', 'duffy et al']",,,,,,,,
https://link.springer.com/chapter/10.1007/978-3-319-67186-4_6,1,First we removed journals with no text and those with fewer than 20 characters1 leaving 1.1 million journals for topic modelling. Next we pre-processed the text using the Stanford Tweet Tokenizer which is a “Twitter-aware” tokenizer designed to handle short informal text [1]. We used the option that truncates characters repeating 3 or more times converting phrases such as “I’m sooooo happyy” to “I’m soo happyy”. On average the number of tokens per journal was 27.7. Since we are interested in topics we removed stopwords and tokens with fewer than two letters and we only retained nouns which appear in the WordNet corpus [10]. After this filtering the average number of nouns per journal was 7. Examples of frequently appearing nouns in alphabetical order include “anxiety” “class” “dinner” “family” “god” “job” “lunch” “miss” “school” “sick” “sleep” and “work”. We then iteratively clustered the journals into topics (details below) and removed nouns that do not refer to topics such as numbers timings (e.g. “today” “yesterday”) general feelings (e.g. “feel” “like”) proper nouns and nouns that have ambiguous meanings (e.g. “overall” “true”). Lastly we only retained nouns that appeared more than ten times in the dataset. This process resulted in a vocabulary of 8386 words for topic modelling. Each journal is represented as a 8386-dimensional term frequency vector with each component denoting the term-frequency/ inverse-document-frequency (TF-IDF) of the corresponding term. Algorithm 1 summarizes our topic modelling methodology. Given a TF-IDF term frequency vector for each journal we run non-negative matrix factorization (NMF) [8] implemented in Python’s scikit-learn package [12]. The objective of NMF is to find two matrices whose product approximates the original matrix. In our case one matrix is the weighted set of topics in each journal and the other is the weighted set of words that belong to each topic. Hence each journal is represented as a combination of topics which are themselves composed of a weighted combination of words. We chose NMF because its non-negativity constraint aids with interpretability. In the context of analyzing word frequencies negative presence of a word would not be interpretable. This is because we only track word occurrences and not semantics or syntax. Unlike other matrix factorization methods NMF reconstructs each document from a sum of positive parts which enables us to easily manually label the discovered topics. Iterating from 4 to 40 topics we derived 37 different topic matrices (steps 1 and 2 of Algorithm 1). Each matrix consists of one topic per row. Each topic has a positive weight for each word in the vocabulary. Stronger weights indicate higher relevance to the topic. The final topic matrix we used has 14 topics and is shown in Table 1. We show the first six words in this table for simplicity where we sorted the words associated with each topic from highest relevance to lowest. When judging the topic matrices we considered the top twenty most important words per topic. Using this information we manually labeled each row in the matrix with a corresponding topic. Furthermore we manually evaluated each matrix based on the distinctness between topics consistency within topics and interpretability. During this process we compiled a custom list of removed words that we mentioned earlier in this section. The groups of words we removed appeared as stand-alone topics that did not offer information about what the journal was about. For example proper nouns appeared as a stand-alone topic. Other words which we deemed too general or ambiguous appeared across several topics and hence did not provide discriminative information. We tested different levels of regularization to enforce sparseness in our models (see [8] for a discussion) but did not find significant differences. However one important modification we made to regularize each topic was to make their first words only as strong as their second ones (by default first words are stronger than second words which are stronger than third words and so on). This is since the most relevant word for each topic tended to be too strong of a signal regardless of how we changed the number of topics pre-processing procedure or regularization in the objective function. For example the word “love” in a journal about sports would be so strong that the journal would be labeled as relating to romantic love. Lowering the importance of first words was sufficient to eliminate the false positives we identified. Given the final topic matrix (summarized in Table 1) the next step is to use it to assign labels to journals (steps 3 and 4 of Algorithm 1). We plotted the distribution of how important each topic was to all journals in the dataset with importance ranging from zero to one. Each distribution had a similar shape with a clear inflection point between 0.05 to 0.15 importance. Figure 4 shows an example importance distribution for the topic “Work” where the inflection point occurs at 0.1 importance.,first we removed journal with no text and those with fewer than character leaving million journal for topic modelling next we preprocessed the text using the stanford tweet tokenizer which is a twitteraware tokenizer designed to handle short informal text we used the option that truncates character repeating or more time converting phrase such a im sooooo happyy to im soo happyy on average the number of token per journal wa since we are interested in topic we removed stopwords and token with fewer than two letter and we only retained noun which appear in the wordnet corpus after this filtering the average number of noun per journal wa example of frequently appearing noun in alphabetical order include anxiety class dinner family god job lunch miss school sick sleep and work we then iteratively clustered the journal into topic detail below and removed noun that do not refer to topic such a number timing eg today yesterday general feeling eg feel like proper noun and noun that have ambiguous meaning eg overall true lastly we only retained noun that appeared more than ten time in the dataset this process resulted in a vocabulary of word for topic modelling each journal is represented a a dimensional term frequency vector with each component denoting the termfrequency inversedocumentfrequency tfidf of the corresponding term algorithm summarizes our topic modelling methodology given a tfidf term frequency vector for each journal we run nonnegative matrix factorization nmf implemented in python scikitlearn package the objective of nmf is to find two matrix whose product approximates the original matrix in our case one matrix is the weighted set of topic in each journal and the other is the weighted set of word that belong to each topic hence each journal is represented a a combination of topic which are themselves composed of a weighted combination of word we chose nmf because it nonnegativity constraint aid with interpretability in the context of analyzing word frequency negative presence of a word would not be interpretable this is because we only track word occurrence and not semantics or syntax unlike other matrix factorization method nmf reconstructs each document from a sum of positive part which enables u to easily manually label the discovered topic iterating from to topic we derived different topic matrix step and of algorithm each matrix consists of one topic per row each topic ha a positive weight for each word in the vocabulary stronger weight indicate higher relevance to the topic the final topic matrix we used ha topic and is shown in table we show the first six word in this table for simplicity where we sorted the word associated with each topic from highest relevance to lowest when judging the topic matrix we considered the top twenty most important word per topic using this information we manually labeled each row in the matrix with a corresponding topic furthermore we manually evaluated each matrix based on the distinctness between topic consistency within topic and interpretability during this process we compiled a custom list of removed word that we mentioned earlier in this section the group of word we removed appeared a standalone topic that did not offer information about what the journal wa about for example proper noun appeared a a standalone topic other word which we deemed too general or ambiguous appeared across several topic and hence did not provide discriminative information we tested different level of regularization to enforce sparseness in our model see for a discussion but did not find significant difference however one important modification we made to regularize each topic wa to make their first word only a strong a their second one by default first word are stronger than second word which are stronger than third word and so on this is since the most relevant word for each topic tended to be too strong of a signal regardless of how we changed the number of topic preprocessing procedure or regularization in the objective function for example the word love in a journal about sport would be so strong that the journal would be labeled a relating to romantic love lowering the importance of first word wa sufficient to eliminate the false positive we identified given the final topic matrix summarized in table the next step is to use it to assign label to journal step and of algorithm we plotted the distribution of how important each topic wa to all journal in the dataset with importance ranging from zero to one each distribution had a similar shape with a clear inflection point between to importance figure show an example importance distribution for the topic work where the inflection point occurs at importance,"['first', 'removed', 'journal', 'text', 'fewer', 'character', 'leaving', 'million', 'journal', 'topic', 'modelling', 'next', 'preprocessed', 'text', 'using', 'stanford', 'tweet', 'tokenizer', 'twitteraware', 'tokenizer', 'designed', 'handle', 'short', 'informal', 'text', 'used', 'option', 'truncates', 'character', 'repeating', 'time', 'converting', 'phrase', 'im', 'sooooo', 'happyy', 'im', 'soo', 'happyy', 'average', 'number', 'token', 'per', 'journal', 'wa', 'since', 'interested', 'topic', 'removed', 'stopwords', 'token', 'fewer', 'two', 'letter', 'retained', 'noun', 'appear', 'wordnet', 'corpus', 'filtering', 'average', 'number', 'noun', 'per', 'journal', 'wa', 'example', 'frequently', 'appearing', 'noun', 'alphabetical', 'order', 'include', 'anxiety', 'class', 'dinner', 'family', 'god', 'job', 'lunch', 'miss', 'school', 'sick', 'sleep', 'work', 'iteratively', 'clustered', 'journal', 'topic', 'detail', 'removed', 'noun', 'refer', 'topic', 'number', 'timing', 'eg', 'today', 'yesterday', 'general', 'feeling', 'eg', 'feel', 'like', 'proper', 'noun', 'noun', 'ambiguous', 'meaning', 'eg', 'overall', 'true', 'lastly', 'retained', 'noun', 'appeared', 'ten', 'time', 'dataset', 'process', 'resulted', 'vocabulary', 'word', 'topic', 'modelling', 'journal', 'represented', 'dimensional', 'term', 'frequency', 'vector', 'component', 'denoting', 'termfrequency', 'inversedocumentfrequency', 'tfidf', 'corresponding', 'term', 'algorithm', 'summarizes', 'topic', 'modelling', 'methodology', 'given', 'tfidf', 'term', 'frequency', 'vector', 'journal', 'run', 'nonnegative', 'matrix', 'factorization', 'nmf', 'implemented', 'python', 'scikitlearn', 'package', 'objective', 'nmf', 'find', 'two', 'matrix', 'whose', 'product', 'approximates', 'original', 'matrix', 'case', 'one', 'matrix', 'weighted', 'set', 'topic', 'journal', 'weighted', 'set', 'word', 'belong', 'topic', 'hence', 'journal', 'represented', 'combination', 'topic', 'composed', 'weighted', 'combination', 'word', 'chose', 'nmf', 'nonnegativity', 'constraint', 'aid', 'interpretability', 'context', 'analyzing', 'word', 'frequency', 'negative', 'presence', 'word', 'would', 'interpretable', 'track', 'word', 'occurrence', 'semantics', 'syntax', 'unlike', 'matrix', 'factorization', 'method', 'nmf', 'reconstructs', 'document', 'sum', 'positive', 'part', 'enables', 'u', 'easily', 'manually', 'label', 'discovered', 'topic', 'iterating', 'topic', 'derived', 'different', 'topic', 'matrix', 'step', 'algorithm', 'matrix', 'consists', 'one', 'topic', 'per', 'row', 'topic', 'ha', 'positive', 'weight', 'word', 'vocabulary', 'stronger', 'weight', 'indicate', 'higher', 'relevance', 'topic', 'final', 'topic', 'matrix', 'used', 'ha', 'topic', 'shown', 'table', 'show', 'first', 'six', 'word', 'table', 'simplicity', 'sorted', 'word', 'associated', 'topic', 'highest', 'relevance', 'lowest', 'judging', 'topic', 'matrix', 'considered', 'top', 'twenty', 'important', 'word', 'per', 'topic', 'using', 'information', 'manually', 'labeled', 'row', 'matrix', 'corresponding', 'topic', 'furthermore', 'manually', 'evaluated', 'matrix', 'based', 'distinctness', 'topic', 'consistency', 'within', 'topic', 'interpretability', 'process', 'compiled', 'custom', 'list', 'removed', 'word', 'mentioned', 'earlier', 'section', 'group', 'word', 'removed', 'appeared', 'standalone', 'topic', 'offer', 'information', 'journal', 'wa', 'example', 'proper', 'noun', 'appeared', 'standalone', 'topic', 'word', 'deemed', 'general', 'ambiguous', 'appeared', 'across', 'several', 'topic', 'hence', 'provide', 'discriminative', 'information', 'tested', 'different', 'level', 'regularization', 'enforce', 'sparseness', 'model', 'see', 'discussion', 'find', 'significant', 'difference', 'however', 'one', 'important', 'modification', 'made', 'regularize', 'topic', 'wa', 'make', 'first', 'word', 'strong', 'second', 'one', 'default', 'first', 'word', 'stronger', 'second', 'word', 'stronger', 'third', 'word', 'since', 'relevant', 'word', 'topic', 'tended', 'strong', 'signal', 'regardless', 'changed', 'number', 'topic', 'preprocessing', 'procedure', 'regularization', 'objective', 'function', 'example', 'word', 'love', 'journal', 'sport', 'would', 'strong', 'journal', 'would', 'labeled', 'relating', 'romantic', 'love', 'lowering', 'importance', 'first', 'word', 'wa', 'sufficient', 'eliminate', 'false', 'positive', 'identified', 'given', 'final', 'topic', 'matrix', 'summarized', 'table', 'next', 'step', 'use', 'assign', 'label', 'journal', 'step', 'algorithm', 'plotted', 'distribution', 'important', 'topic', 'wa', 'journal', 'dataset', 'importance', 'ranging', 'zero', 'one', 'distribution', 'similar', 'shape', 'clear', 'inflection', 'point', 'importance', 'figure', 'show', 'example', 'importance', 'distribution', 'topic', 'work', 'inflection', 'point', 'occurs', 'importance']","['first removed', 'removed journal', 'journal text', 'text fewer', 'fewer character', 'character leaving', 'leaving million', 'million journal', 'journal topic', 'topic modelling', 'modelling next', 'next preprocessed', 'preprocessed text', 'text using', 'using stanford', 'stanford tweet', 'tweet tokenizer', 'tokenizer twitteraware', 'twitteraware tokenizer', 'tokenizer designed', 'designed handle', 'handle short', 'short informal', 'informal text', 'text used', 'used option', 'option truncates', 'truncates character', 'character repeating', 'repeating time', 'time converting', 'converting phrase', 'phrase im', 'im sooooo', 'sooooo happyy', 'happyy im', 'im soo', 'soo happyy', 'happyy average', 'average number', 'number token', 'token per', 'per journal', 'journal wa', 'wa since', 'since interested', 'interested topic', 'topic removed', 'removed stopwords', 'stopwords token', 'token fewer', 'fewer two', 'two letter', 'letter retained', 'retained noun', 'noun appear', 'appear wordnet', 'wordnet corpus', 'corpus filtering', 'filtering average', 'average number', 'number noun', 'noun per', 'per journal', 'journal wa', 'wa example', 'example frequently', 'frequently appearing', 'appearing noun', 'noun alphabetical', 'alphabetical order', 'order include', 'include anxiety', 'anxiety class', 'class dinner', 'dinner family', 'family god', 'god job', 'job lunch', 'lunch miss', 'miss school', 'school sick', 'sick sleep', 'sleep work', 'work iteratively', 'iteratively clustered', 'clustered journal', 'journal topic', 'topic detail', 'detail removed', 'removed noun', 'noun refer', 'refer topic', 'topic number', 'number timing', 'timing eg', 'eg today', 'today yesterday', 'yesterday general', 'general feeling', 'feeling eg', 'eg feel', 'feel like', 'like proper', 'proper noun', 'noun noun', 'noun ambiguous', 'ambiguous meaning', 'meaning eg', 'eg overall', 'overall true', 'true lastly', 'lastly retained', 'retained noun', 'noun appeared', 'appeared ten', 'ten time', 'time dataset', 'dataset process', 'process resulted', 'resulted vocabulary', 'vocabulary word', 'word topic', 'topic modelling', 'modelling journal', 'journal represented', 'represented dimensional', 'dimensional term', 'term frequency', 'frequency vector', 'vector component', 'component denoting', 'denoting termfrequency', 'termfrequency inversedocumentfrequency', 'inversedocumentfrequency tfidf', 'tfidf corresponding', 'corresponding term', 'term algorithm', 'algorithm summarizes', 'summarizes topic', 'topic modelling', 'modelling methodology', 'methodology given', 'given tfidf', 'tfidf term', 'term frequency', 'frequency vector', 'vector journal', 'journal run', 'run nonnegative', 'nonnegative matrix', 'matrix factorization', 'factorization nmf', 'nmf implemented', 'implemented python', 'python scikitlearn', 'scikitlearn package', 'package objective', 'objective nmf', 'nmf find', 'find two', 'two matrix', 'matrix whose', 'whose product', 'product approximates', 'approximates original', 'original matrix', 'matrix case', 'case one', 'one matrix', 'matrix weighted', 'weighted set', 'set topic', 'topic journal', 'journal weighted', 'weighted set', 'set word', 'word belong', 'belong topic', 'topic hence', 'hence journal', 'journal represented', 'represented combination', 'combination topic', 'topic composed', 'composed weighted', 'weighted combination', 'combination word', 'word chose', 'chose nmf', 'nmf nonnegativity', 'nonnegativity constraint', 'constraint aid', 'aid interpretability', 'interpretability context', 'context analyzing', 'analyzing word', 'word frequency', 'frequency negative', 'negative presence', 'presence word', 'word would', 'would interpretable', 'interpretable track', 'track word', 'word occurrence', 'occurrence semantics', 'semantics syntax', 'syntax unlike', 'unlike matrix', 'matrix factorization', 'factorization method', 'method nmf', 'nmf reconstructs', 'reconstructs document', 'document sum', 'sum positive', 'positive part', 'part enables', 'enables u', 'u easily', 'easily manually', 'manually label', 'label discovered', 'discovered topic', 'topic iterating', 'iterating topic', 'topic derived', 'derived different', 'different topic', 'topic matrix', 'matrix step', 'step algorithm', 'algorithm matrix', 'matrix consists', 'consists one', 'one topic', 'topic per', 'per row', 'row topic', 'topic ha', 'ha positive', 'positive weight', 'weight word', 'word vocabulary', 'vocabulary stronger', 'stronger weight', 'weight indicate', 'indicate higher', 'higher relevance', 'relevance topic', 'topic final', 'final topic', 'topic matrix', 'matrix used', 'used ha', 'ha topic', 'topic shown', 'shown table', 'table show', 'show first', 'first six', 'six word', 'word table', 'table simplicity', 'simplicity sorted', 'sorted word', 'word associated', 'associated topic', 'topic highest', 'highest relevance', 'relevance lowest', 'lowest judging', 'judging topic', 'topic matrix', 'matrix considered', 'considered top', 'top twenty', 'twenty important', 'important word', 'word per', 'per topic', 'topic using', 'using information', 'information manually', 'manually labeled', 'labeled row', 'row matrix', 'matrix corresponding', 'corresponding topic', 'topic furthermore', 'furthermore manually', 'manually evaluated', 'evaluated matrix', 'matrix based', 'based distinctness', 'distinctness topic', 'topic consistency', 'consistency within', 'within topic', 'topic interpretability', 'interpretability process', 'process compiled', 'compiled custom', 'custom list', 'list removed', 'removed word', 'word mentioned', 'mentioned earlier', 'earlier section', 'section group', 'group word', 'word removed', 'removed appeared', 'appeared standalone', 'standalone topic', 'topic offer', 'offer information', 'information journal', 'journal wa', 'wa example', 'example proper', 'proper noun', 'noun appeared', 'appeared standalone', 'standalone topic', 'topic word', 'word deemed', 'deemed general', 'general ambiguous', 'ambiguous appeared', 'appeared across', 'across several', 'several topic', 'topic hence', 'hence provide', 'provide discriminative', 'discriminative information', 'information tested', 'tested different', 'different level', 'level regularization', 'regularization enforce', 'enforce sparseness', 'sparseness model', 'model see', 'see discussion', 'discussion find', 'find significant', 'significant difference', 'difference however', 'however one', 'one important', 'important modification', 'modification made', 'made regularize', 'regularize topic', 'topic wa', 'wa make', 'make first', 'first word', 'word strong', 'strong second', 'second one', 'one default', 'default first', 'first word', 'word stronger', 'stronger second', 'second word', 'word stronger', 'stronger third', 'third word', 'word since', 'since relevant', 'relevant word', 'word topic', 'topic tended', 'tended strong', 'strong signal', 'signal regardless', 'regardless changed', 'changed number', 'number topic', 'topic preprocessing', 'preprocessing procedure', 'procedure regularization', 'regularization objective', 'objective function', 'function example', 'example word', 'word love', 'love journal', 'journal sport', 'sport would', 'would strong', 'strong journal', 'journal would', 'would labeled', 'labeled relating', 'relating romantic', 'romantic love', 'love lowering', 'lowering importance', 'importance first', 'first word', 'word wa', 'wa sufficient', 'sufficient eliminate', 'eliminate false', 'false positive', 'positive identified', 'identified given', 'given final', 'final topic', 'topic matrix', 'matrix summarized', 'summarized table', 'table next', 'next step', 'step use', 'use assign', 'assign label', 'label journal', 'journal step', 'step algorithm', 'algorithm plotted', 'plotted distribution', 'distribution important', 'important topic', 'topic wa', 'wa journal', 'journal dataset', 'dataset importance', 'importance ranging', 'ranging zero', 'zero one', 'one distribution', 'distribution similar', 'similar shape', 'shape clear', 'clear inflection', 'inflection point', 'point importance', 'importance figure', 'figure show', 'show example', 'example importance', 'importance distribution', 'distribution topic', 'topic work', 'work inflection', 'inflection point', 'point occurs', 'occurs importance']","['first removed journal', 'removed journal text', 'journal text fewer', 'text fewer character', 'fewer character leaving', 'character leaving million', 'leaving million journal', 'million journal topic', 'journal topic modelling', 'topic modelling next', 'modelling next preprocessed', 'next preprocessed text', 'preprocessed text using', 'text using stanford', 'using stanford tweet', 'stanford tweet tokenizer', 'tweet tokenizer twitteraware', 'tokenizer twitteraware tokenizer', 'twitteraware tokenizer designed', 'tokenizer designed handle', 'designed handle short', 'handle short informal', 'short informal text', 'informal text used', 'text used option', 'used option truncates', 'option truncates character', 'truncates character repeating', 'character repeating time', 'repeating time converting', 'time converting phrase', 'converting phrase im', 'phrase im sooooo', 'im sooooo happyy', 'sooooo happyy im', 'happyy im soo', 'im soo happyy', 'soo happyy average', 'happyy average number', 'average number token', 'number token per', 'token per journal', 'per journal wa', 'journal wa since', 'wa since interested', 'since interested topic', 'interested topic removed', 'topic removed stopwords', 'removed stopwords token', 'stopwords token fewer', 'token fewer two', 'fewer two letter', 'two letter retained', 'letter retained noun', 'retained noun appear', 'noun appear wordnet', 'appear wordnet corpus', 'wordnet corpus filtering', 'corpus filtering average', 'filtering average number', 'average number noun', 'number noun per', 'noun per journal', 'per journal wa', 'journal wa example', 'wa example frequently', 'example frequently appearing', 'frequently appearing noun', 'appearing noun alphabetical', 'noun alphabetical order', 'alphabetical order include', 'order include anxiety', 'include anxiety class', 'anxiety class dinner', 'class dinner family', 'dinner family god', 'family god job', 'god job lunch', 'job lunch miss', 'lunch miss school', 'miss school sick', 'school sick sleep', 'sick sleep work', 'sleep work iteratively', 'work iteratively clustered', 'iteratively clustered journal', 'clustered journal topic', 'journal topic detail', 'topic detail removed', 'detail removed noun', 'removed noun refer', 'noun refer topic', 'refer topic number', 'topic number timing', 'number timing eg', 'timing eg today', 'eg today yesterday', 'today yesterday general', 'yesterday general feeling', 'general feeling eg', 'feeling eg feel', 'eg feel like', 'feel like proper', 'like proper noun', 'proper noun noun', 'noun noun ambiguous', 'noun ambiguous meaning', 'ambiguous meaning eg', 'meaning eg overall', 'eg overall true', 'overall true lastly', 'true lastly retained', 'lastly retained noun', 'retained noun appeared', 'noun appeared ten', 'appeared ten time', 'ten time dataset', 'time dataset process', 'dataset process resulted', 'process resulted vocabulary', 'resulted vocabulary word', 'vocabulary word topic', 'word topic modelling', 'topic modelling journal', 'modelling journal represented', 'journal represented dimensional', 'represented dimensional term', 'dimensional term frequency', 'term frequency vector', 'frequency vector component', 'vector component denoting', 'component denoting termfrequency', 'denoting termfrequency inversedocumentfrequency', 'termfrequency inversedocumentfrequency tfidf', 'inversedocumentfrequency tfidf corresponding', 'tfidf corresponding term', 'corresponding term algorithm', 'term algorithm summarizes', 'algorithm summarizes topic', 'summarizes topic modelling', 'topic modelling methodology', 'modelling methodology given', 'methodology given tfidf', 'given tfidf term', 'tfidf term frequency', 'term frequency vector', 'frequency vector journal', 'vector journal run', 'journal run nonnegative', 'run nonnegative matrix', 'nonnegative matrix factorization', 'matrix factorization nmf', 'factorization nmf implemented', 'nmf implemented python', 'implemented python scikitlearn', 'python scikitlearn package', 'scikitlearn package objective', 'package objective nmf', 'objective nmf find', 'nmf find two', 'find two matrix', 'two matrix whose', 'matrix whose product', 'whose product approximates', 'product approximates original', 'approximates original matrix', 'original matrix case', 'matrix case one', 'case one matrix', 'one matrix weighted', 'matrix weighted set', 'weighted set topic', 'set topic journal', 'topic journal weighted', 'journal weighted set', 'weighted set word', 'set word belong', 'word belong topic', 'belong topic hence', 'topic hence journal', 'hence journal represented', 'journal represented combination', 'represented combination topic', 'combination topic composed', 'topic composed weighted', 'composed weighted combination', 'weighted combination word', 'combination word chose', 'word chose nmf', 'chose nmf nonnegativity', 'nmf nonnegativity constraint', 'nonnegativity constraint aid', 'constraint aid interpretability', 'aid interpretability context', 'interpretability context analyzing', 'context analyzing word', 'analyzing word frequency', 'word frequency negative', 'frequency negative presence', 'negative presence word', 'presence word would', 'word would interpretable', 'would interpretable track', 'interpretable track word', 'track word occurrence', 'word occurrence semantics', 'occurrence semantics syntax', 'semantics syntax unlike', 'syntax unlike matrix', 'unlike matrix factorization', 'matrix factorization method', 'factorization method nmf', 'method nmf reconstructs', 'nmf reconstructs document', 'reconstructs document sum', 'document sum positive', 'sum positive part', 'positive part enables', 'part enables u', 'enables u easily', 'u easily manually', 'easily manually label', 'manually label discovered', 'label discovered topic', 'discovered topic iterating', 'topic iterating topic', 'iterating topic derived', 'topic derived different', 'derived different topic', 'different topic matrix', 'topic matrix step', 'matrix step algorithm', 'step algorithm matrix', 'algorithm matrix consists', 'matrix consists one', 'consists one topic', 'one topic per', 'topic per row', 'per row topic', 'row topic ha', 'topic ha positive', 'ha positive weight', 'positive weight word', 'weight word vocabulary', 'word vocabulary stronger', 'vocabulary stronger weight', 'stronger weight indicate', 'weight indicate higher', 'indicate higher relevance', 'higher relevance topic', 'relevance topic final', 'topic final topic', 'final topic matrix', 'topic matrix used', 'matrix used ha', 'used ha topic', 'ha topic shown', 'topic shown table', 'shown table show', 'table show first', 'show first six', 'first six word', 'six word table', 'word table simplicity', 'table simplicity sorted', 'simplicity sorted word', 'sorted word associated', 'word associated topic', 'associated topic highest', 'topic highest relevance', 'highest relevance lowest', 'relevance lowest judging', 'lowest judging topic', 'judging topic matrix', 'topic matrix considered', 'matrix considered top', 'considered top twenty', 'top twenty important', 'twenty important word', 'important word per', 'word per topic', 'per topic using', 'topic using information', 'using information manually', 'information manually labeled', 'manually labeled row', 'labeled row matrix', 'row matrix corresponding', 'matrix corresponding topic', 'corresponding topic furthermore', 'topic furthermore manually', 'furthermore manually evaluated', 'manually evaluated matrix', 'evaluated matrix based', 'matrix based distinctness', 'based distinctness topic', 'distinctness topic consistency', 'topic consistency within', 'consistency within topic', 'within topic interpretability', 'topic interpretability process', 'interpretability process compiled', 'process compiled custom', 'compiled custom list', 'custom list removed', 'list removed word', 'removed word mentioned', 'word mentioned earlier', 'mentioned earlier section', 'earlier section group', 'section group word', 'group word removed', 'word removed appeared', 'removed appeared standalone', 'appeared standalone topic', 'standalone topic offer', 'topic offer information', 'offer information journal', 'information journal wa', 'journal wa example', 'wa example proper', 'example proper noun', 'proper noun appeared', 'noun appeared standalone', 'appeared standalone topic', 'standalone topic word', 'topic word deemed', 'word deemed general', 'deemed general ambiguous', 'general ambiguous appeared', 'ambiguous appeared across', 'appeared across several', 'across several topic', 'several topic hence', 'topic hence provide', 'hence provide discriminative', 'provide discriminative information', 'discriminative information tested', 'information tested different', 'tested different level', 'different level regularization', 'level regularization enforce', 'regularization enforce sparseness', 'enforce sparseness model', 'sparseness model see', 'model see discussion', 'see discussion find', 'discussion find significant', 'find significant difference', 'significant difference however', 'difference however one', 'however one important', 'one important modification', 'important modification made', 'modification made regularize', 'made regularize topic', 'regularize topic wa', 'topic wa make', 'wa make first', 'make first word', 'first word strong', 'word strong second', 'strong second one', 'second one default', 'one default first', 'default first word', 'first word stronger', 'word stronger second', 'stronger second word', 'second word stronger', 'word stronger third', 'stronger third word', 'third word since', 'word since relevant', 'since relevant word', 'relevant word topic', 'word topic tended', 'topic tended strong', 'tended strong signal', 'strong signal regardless', 'signal regardless changed', 'regardless changed number', 'changed number topic', 'number topic preprocessing', 'topic preprocessing procedure', 'preprocessing procedure regularization', 'procedure regularization objective', 'regularization objective function', 'objective function example', 'function example word', 'example word love', 'word love journal', 'love journal sport', 'journal sport would', 'sport would strong', 'would strong journal', 'strong journal would', 'journal would labeled', 'would labeled relating', 'labeled relating romantic', 'relating romantic love', 'romantic love lowering', 'love lowering importance', 'lowering importance first', 'importance first word', 'first word wa', 'word wa sufficient', 'wa sufficient eliminate', 'sufficient eliminate false', 'eliminate false positive', 'false positive identified', 'positive identified given', 'identified given final', 'given final topic', 'final topic matrix', 'topic matrix summarized', 'matrix summarized table', 'summarized table next', 'table next step', 'next step use', 'step use assign', 'use assign label', 'assign label journal', 'label journal step', 'journal step algorithm', 'step algorithm plotted', 'algorithm plotted distribution', 'plotted distribution important', 'distribution important topic', 'important topic wa', 'topic wa journal', 'wa journal dataset', 'journal dataset importance', 'dataset importance ranging', 'importance ranging zero', 'ranging zero one', 'zero one distribution', 'one distribution similar', 'distribution similar shape', 'similar shape clear', 'shape clear inflection', 'clear inflection point', 'inflection point importance', 'point importance figure', 'importance figure show', 'figure show example', 'show example importance', 'example importance distribution', 'importance distribution topic', 'distribution topic work', 'topic work inflection', 'work inflection point', 'inflection point occurs', 'point occurs importance']",,,,,,,,
https://dl.acm.org/doi/abs/10.1145/2556288.2557214,0,We focus on the social media platform Twitter a popular microblogging service used by 18% of U.S. Internet users and whose popularity continues to increase [28]. Twitter is particularly interesting to study since nearly all posts are public; the public nature of tweets provides an interesting counterpoint to the private nature of search engine activity. We gathered a 15-month sample of Twitter’s Firehose stream (which includes all public tweets) between November 1 2011 and March 31 2013 made available to us under contract focusing on English-language tweets. Twitter post count and unique user count were computed for each condition and aggregated over the full time period. Specifically we considered a post to belong to a certain health condition if there was a regular expression match of the condition to the text of the post (this would not permit substring matches within terms). To reduce noise we excluded posts that were retweets or contained hyperlinks since they were likely related to general news and not a user’s personal health. Using this method we obtained 125166549 tweets on the 165 health conditions from 62269225 users in the time period of interest. The median number of posts was 51687 per condition from a median of 40152 users per condition.,we focus on the social medium platform twitter a popular microblogging service used by of u internet user and whose popularity continues to increase twitter is particularly interesting to study since nearly all post are public the public nature of tweet provides an interesting counterpoint to the private nature of search engine activity we gathered a month sample of twitter firehose stream which includes all public tweet between november and march made available to u under contract focusing on englishlanguage tweet twitter post count and unique user count were computed for each condition and aggregated over the full time period specifically we considered a post to belong to a certain health condition if there wa a regular expression match of the condition to the text of the post this would not permit substring match within term to reduce noise we excluded post that were retweets or contained hyperlink since they were likely related to general news and not a user personal health using this method we obtained tweet on the health condition from user in the time period of interest the median number of post wa per condition from a median of user per condition,"['focus', 'social', 'medium', 'platform', 'twitter', 'popular', 'microblogging', 'service', 'used', 'u', 'internet', 'user', 'whose', 'popularity', 'continues', 'increase', 'twitter', 'particularly', 'interesting', 'study', 'since', 'nearly', 'post', 'public', 'public', 'nature', 'tweet', 'provides', 'interesting', 'counterpoint', 'private', 'nature', 'search', 'engine', 'activity', 'gathered', 'month', 'sample', 'twitter', 'firehose', 'stream', 'includes', 'public', 'tweet', 'november', 'march', 'made', 'available', 'u', 'contract', 'focusing', 'englishlanguage', 'tweet', 'twitter', 'post', 'count', 'unique', 'user', 'count', 'computed', 'condition', 'aggregated', 'full', 'time', 'period', 'specifically', 'considered', 'post', 'belong', 'certain', 'health', 'condition', 'wa', 'regular', 'expression', 'match', 'condition', 'text', 'post', 'would', 'permit', 'substring', 'match', 'within', 'term', 'reduce', 'noise', 'excluded', 'post', 'retweets', 'contained', 'hyperlink', 'since', 'likely', 'related', 'general', 'news', 'user', 'personal', 'health', 'using', 'method', 'obtained', 'tweet', 'health', 'condition', 'user', 'time', 'period', 'interest', 'median', 'number', 'post', 'wa', 'per', 'condition', 'median', 'user', 'per', 'condition']","['focus social', 'social medium', 'medium platform', 'platform twitter', 'twitter popular', 'popular microblogging', 'microblogging service', 'service used', 'used u', 'u internet', 'internet user', 'user whose', 'whose popularity', 'popularity continues', 'continues increase', 'increase twitter', 'twitter particularly', 'particularly interesting', 'interesting study', 'study since', 'since nearly', 'nearly post', 'post public', 'public public', 'public nature', 'nature tweet', 'tweet provides', 'provides interesting', 'interesting counterpoint', 'counterpoint private', 'private nature', 'nature search', 'search engine', 'engine activity', 'activity gathered', 'gathered month', 'month sample', 'sample twitter', 'twitter firehose', 'firehose stream', 'stream includes', 'includes public', 'public tweet', 'tweet november', 'november march', 'march made', 'made available', 'available u', 'u contract', 'contract focusing', 'focusing englishlanguage', 'englishlanguage tweet', 'tweet twitter', 'twitter post', 'post count', 'count unique', 'unique user', 'user count', 'count computed', 'computed condition', 'condition aggregated', 'aggregated full', 'full time', 'time period', 'period specifically', 'specifically considered', 'considered post', 'post belong', 'belong certain', 'certain health', 'health condition', 'condition wa', 'wa regular', 'regular expression', 'expression match', 'match condition', 'condition text', 'text post', 'post would', 'would permit', 'permit substring', 'substring match', 'match within', 'within term', 'term reduce', 'reduce noise', 'noise excluded', 'excluded post', 'post retweets', 'retweets contained', 'contained hyperlink', 'hyperlink since', 'since likely', 'likely related', 'related general', 'general news', 'news user', 'user personal', 'personal health', 'health using', 'using method', 'method obtained', 'obtained tweet', 'tweet health', 'health condition', 'condition user', 'user time', 'time period', 'period interest', 'interest median', 'median number', 'number post', 'post wa', 'wa per', 'per condition', 'condition median', 'median user', 'user per', 'per condition']","['focus social medium', 'social medium platform', 'medium platform twitter', 'platform twitter popular', 'twitter popular microblogging', 'popular microblogging service', 'microblogging service used', 'service used u', 'used u internet', 'u internet user', 'internet user whose', 'user whose popularity', 'whose popularity continues', 'popularity continues increase', 'continues increase twitter', 'increase twitter particularly', 'twitter particularly interesting', 'particularly interesting study', 'interesting study since', 'study since nearly', 'since nearly post', 'nearly post public', 'post public public', 'public public nature', 'public nature tweet', 'nature tweet provides', 'tweet provides interesting', 'provides interesting counterpoint', 'interesting counterpoint private', 'counterpoint private nature', 'private nature search', 'nature search engine', 'search engine activity', 'engine activity gathered', 'activity gathered month', 'gathered month sample', 'month sample twitter', 'sample twitter firehose', 'twitter firehose stream', 'firehose stream includes', 'stream includes public', 'includes public tweet', 'public tweet november', 'tweet november march', 'november march made', 'march made available', 'made available u', 'available u contract', 'u contract focusing', 'contract focusing englishlanguage', 'focusing englishlanguage tweet', 'englishlanguage tweet twitter', 'tweet twitter post', 'twitter post count', 'post count unique', 'count unique user', 'unique user count', 'user count computed', 'count computed condition', 'computed condition aggregated', 'condition aggregated full', 'aggregated full time', 'full time period', 'time period specifically', 'period specifically considered', 'specifically considered post', 'considered post belong', 'post belong certain', 'belong certain health', 'certain health condition', 'health condition wa', 'condition wa regular', 'wa regular expression', 'regular expression match', 'expression match condition', 'match condition text', 'condition text post', 'text post would', 'post would permit', 'would permit substring', 'permit substring match', 'substring match within', 'match within term', 'within term reduce', 'term reduce noise', 'reduce noise excluded', 'noise excluded post', 'excluded post retweets', 'post retweets contained', 'retweets contained hyperlink', 'contained hyperlink since', 'hyperlink since likely', 'since likely related', 'likely related general', 'related general news', 'general news user', 'news user personal', 'user personal health', 'personal health using', 'health using method', 'using method obtained', 'method obtained tweet', 'obtained tweet health', 'tweet health condition', 'health condition user', 'condition user time', 'user time period', 'time period interest', 'period interest median', 'interest median number', 'median number post', 'number post wa', 'post wa per', 'wa per condition', 'per condition median', 'condition median user', 'median user per', 'user per condition']",,,,,,,,
https://www.aaai.org/ocs/index.php/ICWSM/ICWSM14/paper/viewPaper/8075,1,To understand the nature of self-disclosure in reddit posts we first examine the general linguistic attributes manifested in their content. In Table 3 we first present a list of the most popular (stopword eliminated) unigrams that appear in reddit postings. We intended to look at these highly shared unigrams more deeply and systematically hence we organized these unigrams (stopword inclusive) in various semantic categories provided by the psycholinguistic lexicon LIWC (http://www.liwc.net/). We find that among the unigrams in Table 3 there are words that extensively span emotional or affective expressions (happy love bad anxiety good hate) e.g.: “I've been recently wondering if my love to numb the world around me has turned me into an alcoholic…” “Has anyone else battled numbness loss of feeling during recovery? Does it ever get better? i've been sober for about 2 years and still have pretty severe anxiety at times”. We observe presence of relationships and social life words too (family friends people person parents) e.g.: “i get really anxious out when i go home for big events.” “i do love my family they're just really loud and argumentative sometimes”. Temporal indicators in reddit discourse is also visible— e.g. time day years months: “hi all i'm ten weeks sober today and while i wish i could say i'm physically and mentally in great shape the truth is i judge my days by how i feel less bad as oppose to good”. Work and daily grind oriented words are common as well because lifestyle irregularities are often associated with the psychopathology of mental illness (Prigerson et al. 1995)—e.g. life school work job: “I am completely broke can't afford rehab and can't take time off work”. We also find a fair number of cognitive words in these highly used unigrams (felt hard feeling lot) e.g.: “I'm new here but having anxiety like I haven't felt in a long time” “I find a lot of strength in going to a concert. I have been understanding of my anxiety and depression since i was about 8 and i hated it”. These observations are supported by psychology literature where cognitive biases as manifested through dysfunctional attitudes depressive attributional biases and negative automatic thoughts were found to be characteristic of mental illness (Eaves & Rush 1984). Further inhibition words like avoid deny safe demonstrate that redditors are perhaps using the platform to broadcast their thoughts to an audience of strangers or weak ties on issues and topics they might consider to be socially stigmatic to be discussed elsewhere: “i can't escape the feeling of fright i have at all times. i don't feel safe in my own home” “ive been denying my (assumed) depression symptoms for close to two years now writing them off …” Comparing across different LIWC semantic categories over all posts we observe noticeable differences— KruskalWallis one-way analysis of variance indicated the differences across categories to be significant (χ2 (39; N=20411)=9.24; p<10-4). Table 4 reports the top 8 most common LIWC categories the mean proportion of words from each category in the posts and the corresponding standard deviation. Note that the percentages over all categories sum to greater than 100% since a word could belong to multiple categories. Observing closely many of the categories whose corresponding unigrams appeared in Table 3 are also highly prominent categories globally over all posts as given in Table 4. Not shown in Table 4 perhaps intuitively negative emotion anger and sadness words were considerably more prominent than positive emotion words (a Wilcoxon signed rank test reveals that the differences are statistically significant (z=-6.08 p<.001)). Likely these redditors experience several negative emotions: hence mental instability helplessness loneliness restlessness manifest in their postings (Rude et al. 2004). the health and social issues they are facing. In fact high selfattentional focus is a known psychological attribute of mental illness sufferers (Chung & Pennebaker 2007).,to understand the nature of selfdisclosure in reddit post we first examine the general linguistic attribute manifested in their content in table we first present a list of the most popular stopword eliminated unigrams that appear in reddit posting we intended to look at these highly shared unigrams more deeply and systematically hence we organized these unigrams stopword inclusive in various semantic category provided by the psycholinguistic lexicon liwc httpwwwliwcnet we find that among the unigrams in table there are word that extensively span emotional or affective expression happy love bad anxiety good hate eg ive been recently wondering if my love to numb the world around me ha turned me into an alcoholic ha anyone else battled numbness loss of feeling during recovery doe it ever get better ive been sober for about year and still have pretty severe anxiety at time we observe presence of relationship and social life word too family friend people person parent eg i get really anxious out when i go home for big event i do love my family theyre just really loud and argumentative sometimes temporal indicator in reddit discourse is also visible eg time day year month hi all im ten week sober today and while i wish i could say im physically and mentally in great shape the truth is i judge my day by how i feel le bad a oppose to good work and daily grind oriented word are common a well because lifestyle irregularity are often associated with the psychopathology of mental illness prigerson et al eg life school work job i am completely broke cant afford rehab and cant take time off work we also find a fair number of cognitive word in these highly used unigrams felt hard feeling lot eg im new here but having anxiety like i havent felt in a long time i find a lot of strength in going to a concert i have been understanding of my anxiety and depression since i wa about and i hated it these observation are supported by psychology literature where cognitive bias a manifested through dysfunctional attitude depressive attributional bias and negative automatic thought were found to be characteristic of mental illness eaves rush further inhibition word like avoid deny safe demonstrate that redditors are perhaps using the platform to broadcast their thought to an audience of stranger or weak tie on issue and topic they might consider to be socially stigmatic to be discussed elsewhere i cant escape the feeling of fright i have at all time i dont feel safe in my own home ive been denying my assumed depression symptom for close to two year now writing them off comparing across different liwc semantic category over all post we observe noticeable difference kruskalwallis oneway analysis of variance indicated the difference across category to be significant n p table report the top most common liwc category the mean proportion of word from each category in the post and the corresponding standard deviation note that the percentage over all category sum to greater than since a word could belong to multiple category observing closely many of the category whose corresponding unigrams appeared in table are also highly prominent category globally over all post a given in table not shown in table perhaps intuitively negative emotion anger and sadness word were considerably more prominent than positive emotion word a wilcoxon signed rank test reveals that the difference are statistically significant z p likely these redditors experience several negative emotion hence mental instability helplessness loneliness restlessness manifest in their posting rude et al the health and social issue they are facing in fact high selfattentional focus is a known psychological attribute of mental illness sufferer chung pennebaker,"['understand', 'nature', 'selfdisclosure', 'reddit', 'post', 'first', 'examine', 'general', 'linguistic', 'attribute', 'manifested', 'content', 'table', 'first', 'present', 'list', 'popular', 'stopword', 'eliminated', 'unigrams', 'appear', 'reddit', 'posting', 'intended', 'look', 'highly', 'shared', 'unigrams', 'deeply', 'systematically', 'hence', 'organized', 'unigrams', 'stopword', 'inclusive', 'various', 'semantic', 'category', 'provided', 'psycholinguistic', 'lexicon', 'liwc', 'httpwwwliwcnet', 'find', 'among', 'unigrams', 'table', 'word', 'extensively', 'span', 'emotional', 'affective', 'expression', 'happy', 'love', 'bad', 'anxiety', 'good', 'hate', 'eg', 'ive', 'recently', 'wondering', 'love', 'numb', 'world', 'around', 'ha', 'turned', 'alcoholic', 'ha', 'anyone', 'else', 'battled', 'numbness', 'loss', 'feeling', 'recovery', 'doe', 'ever', 'get', 'better', 'ive', 'sober', 'year', 'still', 'pretty', 'severe', 'anxiety', 'time', 'observe', 'presence', 'relationship', 'social', 'life', 'word', 'family', 'friend', 'people', 'person', 'parent', 'eg', 'get', 'really', 'anxious', 'go', 'home', 'big', 'event', 'love', 'family', 'theyre', 'really', 'loud', 'argumentative', 'sometimes', 'temporal', 'indicator', 'reddit', 'discourse', 'also', 'visible', 'eg', 'time', 'day', 'year', 'month', 'hi', 'im', 'ten', 'week', 'sober', 'today', 'wish', 'could', 'say', 'im', 'physically', 'mentally', 'great', 'shape', 'truth', 'judge', 'day', 'feel', 'le', 'bad', 'oppose', 'good', 'work', 'daily', 'grind', 'oriented', 'word', 'common', 'well', 'lifestyle', 'irregularity', 'often', 'associated', 'psychopathology', 'mental', 'illness', 'prigerson', 'et', 'al', 'eg', 'life', 'school', 'work', 'job', 'completely', 'broke', 'cant', 'afford', 'rehab', 'cant', 'take', 'time', 'work', 'also', 'find', 'fair', 'number', 'cognitive', 'word', 'highly', 'used', 'unigrams', 'felt', 'hard', 'feeling', 'lot', 'eg', 'im', 'new', 'anxiety', 'like', 'havent', 'felt', 'long', 'time', 'find', 'lot', 'strength', 'going', 'concert', 'understanding', 'anxiety', 'depression', 'since', 'wa', 'hated', 'observation', 'supported', 'psychology', 'literature', 'cognitive', 'bias', 'manifested', 'dysfunctional', 'attitude', 'depressive', 'attributional', 'bias', 'negative', 'automatic', 'thought', 'found', 'characteristic', 'mental', 'illness', 'eaves', 'rush', 'inhibition', 'word', 'like', 'avoid', 'deny', 'safe', 'demonstrate', 'redditors', 'perhaps', 'using', 'platform', 'broadcast', 'thought', 'audience', 'stranger', 'weak', 'tie', 'issue', 'topic', 'might', 'consider', 'socially', 'stigmatic', 'discussed', 'elsewhere', 'cant', 'escape', 'feeling', 'fright', 'time', 'dont', 'feel', 'safe', 'home', 'ive', 'denying', 'assumed', 'depression', 'symptom', 'close', 'two', 'year', 'writing', 'comparing', 'across', 'different', 'liwc', 'semantic', 'category', 'post', 'observe', 'noticeable', 'difference', 'kruskalwallis', 'oneway', 'analysis', 'variance', 'indicated', 'difference', 'across', 'category', 'significant', 'n', 'p', 'table', 'report', 'top', 'common', 'liwc', 'category', 'mean', 'proportion', 'word', 'category', 'post', 'corresponding', 'standard', 'deviation', 'note', 'percentage', 'category', 'sum', 'greater', 'since', 'word', 'could', 'belong', 'multiple', 'category', 'observing', 'closely', 'many', 'category', 'whose', 'corresponding', 'unigrams', 'appeared', 'table', 'also', 'highly', 'prominent', 'category', 'globally', 'post', 'given', 'table', 'shown', 'table', 'perhaps', 'intuitively', 'negative', 'emotion', 'anger', 'sadness', 'word', 'considerably', 'prominent', 'positive', 'emotion', 'word', 'wilcoxon', 'signed', 'rank', 'test', 'reveals', 'difference', 'statistically', 'significant', 'z', 'p', 'likely', 'redditors', 'experience', 'several', 'negative', 'emotion', 'hence', 'mental', 'instability', 'helplessness', 'loneliness', 'restlessness', 'manifest', 'posting', 'rude', 'et', 'al', 'health', 'social', 'issue', 'facing', 'fact', 'high', 'selfattentional', 'focus', 'known', 'psychological', 'attribute', 'mental', 'illness', 'sufferer', 'chung', 'pennebaker']","['understand nature', 'nature selfdisclosure', 'selfdisclosure reddit', 'reddit post', 'post first', 'first examine', 'examine general', 'general linguistic', 'linguistic attribute', 'attribute manifested', 'manifested content', 'content table', 'table first', 'first present', 'present list', 'list popular', 'popular stopword', 'stopword eliminated', 'eliminated unigrams', 'unigrams appear', 'appear reddit', 'reddit posting', 'posting intended', 'intended look', 'look highly', 'highly shared', 'shared unigrams', 'unigrams deeply', 'deeply systematically', 'systematically hence', 'hence organized', 'organized unigrams', 'unigrams stopword', 'stopword inclusive', 'inclusive various', 'various semantic', 'semantic category', 'category provided', 'provided psycholinguistic', 'psycholinguistic lexicon', 'lexicon liwc', 'liwc httpwwwliwcnet', 'httpwwwliwcnet find', 'find among', 'among unigrams', 'unigrams table', 'table word', 'word extensively', 'extensively span', 'span emotional', 'emotional affective', 'affective expression', 'expression happy', 'happy love', 'love bad', 'bad anxiety', 'anxiety good', 'good hate', 'hate eg', 'eg ive', 'ive recently', 'recently wondering', 'wondering love', 'love numb', 'numb world', 'world around', 'around ha', 'ha turned', 'turned alcoholic', 'alcoholic ha', 'ha anyone', 'anyone else', 'else battled', 'battled numbness', 'numbness loss', 'loss feeling', 'feeling recovery', 'recovery doe', 'doe ever', 'ever get', 'get better', 'better ive', 'ive sober', 'sober year', 'year still', 'still pretty', 'pretty severe', 'severe anxiety', 'anxiety time', 'time observe', 'observe presence', 'presence relationship', 'relationship social', 'social life', 'life word', 'word family', 'family friend', 'friend people', 'people person', 'person parent', 'parent eg', 'eg get', 'get really', 'really anxious', 'anxious go', 'go home', 'home big', 'big event', 'event love', 'love family', 'family theyre', 'theyre really', 'really loud', 'loud argumentative', 'argumentative sometimes', 'sometimes temporal', 'temporal indicator', 'indicator reddit', 'reddit discourse', 'discourse also', 'also visible', 'visible eg', 'eg time', 'time day', 'day year', 'year month', 'month hi', 'hi im', 'im ten', 'ten week', 'week sober', 'sober today', 'today wish', 'wish could', 'could say', 'say im', 'im physically', 'physically mentally', 'mentally great', 'great shape', 'shape truth', 'truth judge', 'judge day', 'day feel', 'feel le', 'le bad', 'bad oppose', 'oppose good', 'good work', 'work daily', 'daily grind', 'grind oriented', 'oriented word', 'word common', 'common well', 'well lifestyle', 'lifestyle irregularity', 'irregularity often', 'often associated', 'associated psychopathology', 'psychopathology mental', 'mental illness', 'illness prigerson', 'prigerson et', 'et al', 'al eg', 'eg life', 'life school', 'school work', 'work job', 'job completely', 'completely broke', 'broke cant', 'cant afford', 'afford rehab', 'rehab cant', 'cant take', 'take time', 'time work', 'work also', 'also find', 'find fair', 'fair number', 'number cognitive', 'cognitive word', 'word highly', 'highly used', 'used unigrams', 'unigrams felt', 'felt hard', 'hard feeling', 'feeling lot', 'lot eg', 'eg im', 'im new', 'new anxiety', 'anxiety like', 'like havent', 'havent felt', 'felt long', 'long time', 'time find', 'find lot', 'lot strength', 'strength going', 'going concert', 'concert understanding', 'understanding anxiety', 'anxiety depression', 'depression since', 'since wa', 'wa hated', 'hated observation', 'observation supported', 'supported psychology', 'psychology literature', 'literature cognitive', 'cognitive bias', 'bias manifested', 'manifested dysfunctional', 'dysfunctional attitude', 'attitude depressive', 'depressive attributional', 'attributional bias', 'bias negative', 'negative automatic', 'automatic thought', 'thought found', 'found characteristic', 'characteristic mental', 'mental illness', 'illness eaves', 'eaves rush', 'rush inhibition', 'inhibition word', 'word like', 'like avoid', 'avoid deny', 'deny safe', 'safe demonstrate', 'demonstrate redditors', 'redditors perhaps', 'perhaps using', 'using platform', 'platform broadcast', 'broadcast thought', 'thought audience', 'audience stranger', 'stranger weak', 'weak tie', 'tie issue', 'issue topic', 'topic might', 'might consider', 'consider socially', 'socially stigmatic', 'stigmatic discussed', 'discussed elsewhere', 'elsewhere cant', 'cant escape', 'escape feeling', 'feeling fright', 'fright time', 'time dont', 'dont feel', 'feel safe', 'safe home', 'home ive', 'ive denying', 'denying assumed', 'assumed depression', 'depression symptom', 'symptom close', 'close two', 'two year', 'year writing', 'writing comparing', 'comparing across', 'across different', 'different liwc', 'liwc semantic', 'semantic category', 'category post', 'post observe', 'observe noticeable', 'noticeable difference', 'difference kruskalwallis', 'kruskalwallis oneway', 'oneway analysis', 'analysis variance', 'variance indicated', 'indicated difference', 'difference across', 'across category', 'category significant', 'significant n', 'n p', 'p table', 'table report', 'report top', 'top common', 'common liwc', 'liwc category', 'category mean', 'mean proportion', 'proportion word', 'word category', 'category post', 'post corresponding', 'corresponding standard', 'standard deviation', 'deviation note', 'note percentage', 'percentage category', 'category sum', 'sum greater', 'greater since', 'since word', 'word could', 'could belong', 'belong multiple', 'multiple category', 'category observing', 'observing closely', 'closely many', 'many category', 'category whose', 'whose corresponding', 'corresponding unigrams', 'unigrams appeared', 'appeared table', 'table also', 'also highly', 'highly prominent', 'prominent category', 'category globally', 'globally post', 'post given', 'given table', 'table shown', 'shown table', 'table perhaps', 'perhaps intuitively', 'intuitively negative', 'negative emotion', 'emotion anger', 'anger sadness', 'sadness word', 'word considerably', 'considerably prominent', 'prominent positive', 'positive emotion', 'emotion word', 'word wilcoxon', 'wilcoxon signed', 'signed rank', 'rank test', 'test reveals', 'reveals difference', 'difference statistically', 'statistically significant', 'significant z', 'z p', 'p likely', 'likely redditors', 'redditors experience', 'experience several', 'several negative', 'negative emotion', 'emotion hence', 'hence mental', 'mental instability', 'instability helplessness', 'helplessness loneliness', 'loneliness restlessness', 'restlessness manifest', 'manifest posting', 'posting rude', 'rude et', 'et al', 'al health', 'health social', 'social issue', 'issue facing', 'facing fact', 'fact high', 'high selfattentional', 'selfattentional focus', 'focus known', 'known psychological', 'psychological attribute', 'attribute mental', 'mental illness', 'illness sufferer', 'sufferer chung', 'chung pennebaker']","['understand nature selfdisclosure', 'nature selfdisclosure reddit', 'selfdisclosure reddit post', 'reddit post first', 'post first examine', 'first examine general', 'examine general linguistic', 'general linguistic attribute', 'linguistic attribute manifested', 'attribute manifested content', 'manifested content table', 'content table first', 'table first present', 'first present list', 'present list popular', 'list popular stopword', 'popular stopword eliminated', 'stopword eliminated unigrams', 'eliminated unigrams appear', 'unigrams appear reddit', 'appear reddit posting', 'reddit posting intended', 'posting intended look', 'intended look highly', 'look highly shared', 'highly shared unigrams', 'shared unigrams deeply', 'unigrams deeply systematically', 'deeply systematically hence', 'systematically hence organized', 'hence organized unigrams', 'organized unigrams stopword', 'unigrams stopword inclusive', 'stopword inclusive various', 'inclusive various semantic', 'various semantic category', 'semantic category provided', 'category provided psycholinguistic', 'provided psycholinguistic lexicon', 'psycholinguistic lexicon liwc', 'lexicon liwc httpwwwliwcnet', 'liwc httpwwwliwcnet find', 'httpwwwliwcnet find among', 'find among unigrams', 'among unigrams table', 'unigrams table word', 'table word extensively', 'word extensively span', 'extensively span emotional', 'span emotional affective', 'emotional affective expression', 'affective expression happy', 'expression happy love', 'happy love bad', 'love bad anxiety', 'bad anxiety good', 'anxiety good hate', 'good hate eg', 'hate eg ive', 'eg ive recently', 'ive recently wondering', 'recently wondering love', 'wondering love numb', 'love numb world', 'numb world around', 'world around ha', 'around ha turned', 'ha turned alcoholic', 'turned alcoholic ha', 'alcoholic ha anyone', 'ha anyone else', 'anyone else battled', 'else battled numbness', 'battled numbness loss', 'numbness loss feeling', 'loss feeling recovery', 'feeling recovery doe', 'recovery doe ever', 'doe ever get', 'ever get better', 'get better ive', 'better ive sober', 'ive sober year', 'sober year still', 'year still pretty', 'still pretty severe', 'pretty severe anxiety', 'severe anxiety time', 'anxiety time observe', 'time observe presence', 'observe presence relationship', 'presence relationship social', 'relationship social life', 'social life word', 'life word family', 'word family friend', 'family friend people', 'friend people person', 'people person parent', 'person parent eg', 'parent eg get', 'eg get really', 'get really anxious', 'really anxious go', 'anxious go home', 'go home big', 'home big event', 'big event love', 'event love family', 'love family theyre', 'family theyre really', 'theyre really loud', 'really loud argumentative', 'loud argumentative sometimes', 'argumentative sometimes temporal', 'sometimes temporal indicator', 'temporal indicator reddit', 'indicator reddit discourse', 'reddit discourse also', 'discourse also visible', 'also visible eg', 'visible eg time', 'eg time day', 'time day year', 'day year month', 'year month hi', 'month hi im', 'hi im ten', 'im ten week', 'ten week sober', 'week sober today', 'sober today wish', 'today wish could', 'wish could say', 'could say im', 'say im physically', 'im physically mentally', 'physically mentally great', 'mentally great shape', 'great shape truth', 'shape truth judge', 'truth judge day', 'judge day feel', 'day feel le', 'feel le bad', 'le bad oppose', 'bad oppose good', 'oppose good work', 'good work daily', 'work daily grind', 'daily grind oriented', 'grind oriented word', 'oriented word common', 'word common well', 'common well lifestyle', 'well lifestyle irregularity', 'lifestyle irregularity often', 'irregularity often associated', 'often associated psychopathology', 'associated psychopathology mental', 'psychopathology mental illness', 'mental illness prigerson', 'illness prigerson et', 'prigerson et al', 'et al eg', 'al eg life', 'eg life school', 'life school work', 'school work job', 'work job completely', 'job completely broke', 'completely broke cant', 'broke cant afford', 'cant afford rehab', 'afford rehab cant', 'rehab cant take', 'cant take time', 'take time work', 'time work also', 'work also find', 'also find fair', 'find fair number', 'fair number cognitive', 'number cognitive word', 'cognitive word highly', 'word highly used', 'highly used unigrams', 'used unigrams felt', 'unigrams felt hard', 'felt hard feeling', 'hard feeling lot', 'feeling lot eg', 'lot eg im', 'eg im new', 'im new anxiety', 'new anxiety like', 'anxiety like havent', 'like havent felt', 'havent felt long', 'felt long time', 'long time find', 'time find lot', 'find lot strength', 'lot strength going', 'strength going concert', 'going concert understanding', 'concert understanding anxiety', 'understanding anxiety depression', 'anxiety depression since', 'depression since wa', 'since wa hated', 'wa hated observation', 'hated observation supported', 'observation supported psychology', 'supported psychology literature', 'psychology literature cognitive', 'literature cognitive bias', 'cognitive bias manifested', 'bias manifested dysfunctional', 'manifested dysfunctional attitude', 'dysfunctional attitude depressive', 'attitude depressive attributional', 'depressive attributional bias', 'attributional bias negative', 'bias negative automatic', 'negative automatic thought', 'automatic thought found', 'thought found characteristic', 'found characteristic mental', 'characteristic mental illness', 'mental illness eaves', 'illness eaves rush', 'eaves rush inhibition', 'rush inhibition word', 'inhibition word like', 'word like avoid', 'like avoid deny', 'avoid deny safe', 'deny safe demonstrate', 'safe demonstrate redditors', 'demonstrate redditors perhaps', 'redditors perhaps using', 'perhaps using platform', 'using platform broadcast', 'platform broadcast thought', 'broadcast thought audience', 'thought audience stranger', 'audience stranger weak', 'stranger weak tie', 'weak tie issue', 'tie issue topic', 'issue topic might', 'topic might consider', 'might consider socially', 'consider socially stigmatic', 'socially stigmatic discussed', 'stigmatic discussed elsewhere', 'discussed elsewhere cant', 'elsewhere cant escape', 'cant escape feeling', 'escape feeling fright', 'feeling fright time', 'fright time dont', 'time dont feel', 'dont feel safe', 'feel safe home', 'safe home ive', 'home ive denying', 'ive denying assumed', 'denying assumed depression', 'assumed depression symptom', 'depression symptom close', 'symptom close two', 'close two year', 'two year writing', 'year writing comparing', 'writing comparing across', 'comparing across different', 'across different liwc', 'different liwc semantic', 'liwc semantic category', 'semantic category post', 'category post observe', 'post observe noticeable', 'observe noticeable difference', 'noticeable difference kruskalwallis', 'difference kruskalwallis oneway', 'kruskalwallis oneway analysis', 'oneway analysis variance', 'analysis variance indicated', 'variance indicated difference', 'indicated difference across', 'difference across category', 'across category significant', 'category significant n', 'significant n p', 'n p table', 'p table report', 'table report top', 'report top common', 'top common liwc', 'common liwc category', 'liwc category mean', 'category mean proportion', 'mean proportion word', 'proportion word category', 'word category post', 'category post corresponding', 'post corresponding standard', 'corresponding standard deviation', 'standard deviation note', 'deviation note percentage', 'note percentage category', 'percentage category sum', 'category sum greater', 'sum greater since', 'greater since word', 'since word could', 'word could belong', 'could belong multiple', 'belong multiple category', 'multiple category observing', 'category observing closely', 'observing closely many', 'closely many category', 'many category whose', 'category whose corresponding', 'whose corresponding unigrams', 'corresponding unigrams appeared', 'unigrams appeared table', 'appeared table also', 'table also highly', 'also highly prominent', 'highly prominent category', 'prominent category globally', 'category globally post', 'globally post given', 'post given table', 'given table shown', 'table shown table', 'shown table perhaps', 'table perhaps intuitively', 'perhaps intuitively negative', 'intuitively negative emotion', 'negative emotion anger', 'emotion anger sadness', 'anger sadness word', 'sadness word considerably', 'word considerably prominent', 'considerably prominent positive', 'prominent positive emotion', 'positive emotion word', 'emotion word wilcoxon', 'word wilcoxon signed', 'wilcoxon signed rank', 'signed rank test', 'rank test reveals', 'test reveals difference', 'reveals difference statistically', 'difference statistically significant', 'statistically significant z', 'significant z p', 'z p likely', 'p likely redditors', 'likely redditors experience', 'redditors experience several', 'experience several negative', 'several negative emotion', 'negative emotion hence', 'emotion hence mental', 'hence mental instability', 'mental instability helplessness', 'instability helplessness loneliness', 'helplessness loneliness restlessness', 'loneliness restlessness manifest', 'restlessness manifest posting', 'manifest posting rude', 'posting rude et', 'rude et al', 'et al health', 'al health social', 'health social issue', 'social issue facing', 'issue facing fact', 'facing fact high', 'fact high selfattentional', 'high selfattentional focus', 'selfattentional focus known', 'focus known psychological', 'known psychological attribute', 'psychological attribute mental', 'attribute mental illness', 'mental illness sufferer', 'illness sufferer chung', 'sufferer chung pennebaker']",,,,,,,,
https://dl.acm.org/doi/abs/10.1145/3025453.3025932,1,Finally to complement the visual themes (our research goal RQ 3) we identify themes from the captions and hashtags (textual data) associated with the Instagram images in our dataset. We refer to these latent topics as linguistic themes. Existing literature [42] emphasizes the importance of studying language since it reflects a variety of thoughts functions as a signal of identity and emphasizes the social distance. We believe the linguistic themes may therefore help us contrast the visual themes around how individuals engage in mental health disclosure on Instagram. We used TwitterLDA4 to extract these linguistic themes. This method was developed for topic modeling of short text corpora for mining the latent topics. As typically done in topic modeling we pre-processed the data by removing a standard list of stop words words with very high frequency and words that occur fewer than five times. Since LDA is an unsupervised learning approach identifying the correct number of topics is challenging. We used the default hyper-parameter settings and 10 topics which we determined based on the value of average corpus likelihood over ten runs. These 10 topics constituted what is known as lifted forms of linguistic vocabulary. On these extracted linguistic vocabulary to arrive at interpretive descriptions (we call them linguistic themes) we adopted a similar semi-open coding approach as the visual themes that involved the same two researchers as above. The raters referred to the mental health literature and identified the best possible description that characterized the tokens in the linguistic vocabulary corresponding to each of the 10 linguistic topics. To characterize and represent each of the visual and the linguistic themes we propose a measure of visual diversity. This measure estimates how coherent images are with respect to the each other in a theme. To measure the diversity in terms of the latent visual features images are expressed in terms of their principle components within a theme. In the component space distance between a pair of images are computed by employing the cosine theta similarity function. To perform these set of operations we utilize the Python scikit-learn library.,finally to complement the visual theme our research goal rq we identify theme from the caption and hashtags textual data associated with the instagram image in our dataset we refer to these latent topic a linguistic theme existing literature emphasizes the importance of studying language since it reflects a variety of thought function a a signal of identity and emphasizes the social distance we believe the linguistic theme may therefore help u contrast the visual theme around how individual engage in mental health disclosure on instagram we used twitterlda to extract these linguistic theme this method wa developed for topic modeling of short text corpus for mining the latent topic a typically done in topic modeling we preprocessed the data by removing a standard list of stop word word with very high frequency and word that occur fewer than five time since lda is an unsupervised learning approach identifying the correct number of topic is challenging we used the default hyperparameter setting and topic which we determined based on the value of average corpus likelihood over ten run these topic constituted what is known a lifted form of linguistic vocabulary on these extracted linguistic vocabulary to arrive at interpretive description we call them linguistic theme we adopted a similar semiopen coding approach a the visual theme that involved the same two researcher a above the raters referred to the mental health literature and identified the best possible description that characterized the token in the linguistic vocabulary corresponding to each of the linguistic topic to characterize and represent each of the visual and the linguistic theme we propose a measure of visual diversity this measure estimate how coherent image are with respect to the each other in a theme to measure the diversity in term of the latent visual feature image are expressed in term of their principle component within a theme in the component space distance between a pair of image are computed by employing the cosine theta similarity function to perform these set of operation we utilize the python scikitlearn library,"['finally', 'complement', 'visual', 'theme', 'research', 'goal', 'rq', 'identify', 'theme', 'caption', 'hashtags', 'textual', 'data', 'associated', 'instagram', 'image', 'dataset', 'refer', 'latent', 'topic', 'linguistic', 'theme', 'existing', 'literature', 'emphasizes', 'importance', 'studying', 'language', 'since', 'reflects', 'variety', 'thought', 'function', 'signal', 'identity', 'emphasizes', 'social', 'distance', 'believe', 'linguistic', 'theme', 'may', 'therefore', 'help', 'u', 'contrast', 'visual', 'theme', 'around', 'individual', 'engage', 'mental', 'health', 'disclosure', 'instagram', 'used', 'twitterlda', 'extract', 'linguistic', 'theme', 'method', 'wa', 'developed', 'topic', 'modeling', 'short', 'text', 'corpus', 'mining', 'latent', 'topic', 'typically', 'done', 'topic', 'modeling', 'preprocessed', 'data', 'removing', 'standard', 'list', 'stop', 'word', 'word', 'high', 'frequency', 'word', 'occur', 'fewer', 'five', 'time', 'since', 'lda', 'unsupervised', 'learning', 'approach', 'identifying', 'correct', 'number', 'topic', 'challenging', 'used', 'default', 'hyperparameter', 'setting', 'topic', 'determined', 'based', 'value', 'average', 'corpus', 'likelihood', 'ten', 'run', 'topic', 'constituted', 'known', 'lifted', 'form', 'linguistic', 'vocabulary', 'extracted', 'linguistic', 'vocabulary', 'arrive', 'interpretive', 'description', 'call', 'linguistic', 'theme', 'adopted', 'similar', 'semiopen', 'coding', 'approach', 'visual', 'theme', 'involved', 'two', 'researcher', 'raters', 'referred', 'mental', 'health', 'literature', 'identified', 'best', 'possible', 'description', 'characterized', 'token', 'linguistic', 'vocabulary', 'corresponding', 'linguistic', 'topic', 'characterize', 'represent', 'visual', 'linguistic', 'theme', 'propose', 'measure', 'visual', 'diversity', 'measure', 'estimate', 'coherent', 'image', 'respect', 'theme', 'measure', 'diversity', 'term', 'latent', 'visual', 'feature', 'image', 'expressed', 'term', 'principle', 'component', 'within', 'theme', 'component', 'space', 'distance', 'pair', 'image', 'computed', 'employing', 'cosine', 'theta', 'similarity', 'function', 'perform', 'set', 'operation', 'utilize', 'python', 'scikitlearn', 'library']","['finally complement', 'complement visual', 'visual theme', 'theme research', 'research goal', 'goal rq', 'rq identify', 'identify theme', 'theme caption', 'caption hashtags', 'hashtags textual', 'textual data', 'data associated', 'associated instagram', 'instagram image', 'image dataset', 'dataset refer', 'refer latent', 'latent topic', 'topic linguistic', 'linguistic theme', 'theme existing', 'existing literature', 'literature emphasizes', 'emphasizes importance', 'importance studying', 'studying language', 'language since', 'since reflects', 'reflects variety', 'variety thought', 'thought function', 'function signal', 'signal identity', 'identity emphasizes', 'emphasizes social', 'social distance', 'distance believe', 'believe linguistic', 'linguistic theme', 'theme may', 'may therefore', 'therefore help', 'help u', 'u contrast', 'contrast visual', 'visual theme', 'theme around', 'around individual', 'individual engage', 'engage mental', 'mental health', 'health disclosure', 'disclosure instagram', 'instagram used', 'used twitterlda', 'twitterlda extract', 'extract linguistic', 'linguistic theme', 'theme method', 'method wa', 'wa developed', 'developed topic', 'topic modeling', 'modeling short', 'short text', 'text corpus', 'corpus mining', 'mining latent', 'latent topic', 'topic typically', 'typically done', 'done topic', 'topic modeling', 'modeling preprocessed', 'preprocessed data', 'data removing', 'removing standard', 'standard list', 'list stop', 'stop word', 'word word', 'word high', 'high frequency', 'frequency word', 'word occur', 'occur fewer', 'fewer five', 'five time', 'time since', 'since lda', 'lda unsupervised', 'unsupervised learning', 'learning approach', 'approach identifying', 'identifying correct', 'correct number', 'number topic', 'topic challenging', 'challenging used', 'used default', 'default hyperparameter', 'hyperparameter setting', 'setting topic', 'topic determined', 'determined based', 'based value', 'value average', 'average corpus', 'corpus likelihood', 'likelihood ten', 'ten run', 'run topic', 'topic constituted', 'constituted known', 'known lifted', 'lifted form', 'form linguistic', 'linguistic vocabulary', 'vocabulary extracted', 'extracted linguistic', 'linguistic vocabulary', 'vocabulary arrive', 'arrive interpretive', 'interpretive description', 'description call', 'call linguistic', 'linguistic theme', 'theme adopted', 'adopted similar', 'similar semiopen', 'semiopen coding', 'coding approach', 'approach visual', 'visual theme', 'theme involved', 'involved two', 'two researcher', 'researcher raters', 'raters referred', 'referred mental', 'mental health', 'health literature', 'literature identified', 'identified best', 'best possible', 'possible description', 'description characterized', 'characterized token', 'token linguistic', 'linguistic vocabulary', 'vocabulary corresponding', 'corresponding linguistic', 'linguistic topic', 'topic characterize', 'characterize represent', 'represent visual', 'visual linguistic', 'linguistic theme', 'theme propose', 'propose measure', 'measure visual', 'visual diversity', 'diversity measure', 'measure estimate', 'estimate coherent', 'coherent image', 'image respect', 'respect theme', 'theme measure', 'measure diversity', 'diversity term', 'term latent', 'latent visual', 'visual feature', 'feature image', 'image expressed', 'expressed term', 'term principle', 'principle component', 'component within', 'within theme', 'theme component', 'component space', 'space distance', 'distance pair', 'pair image', 'image computed', 'computed employing', 'employing cosine', 'cosine theta', 'theta similarity', 'similarity function', 'function perform', 'perform set', 'set operation', 'operation utilize', 'utilize python', 'python scikitlearn', 'scikitlearn library']","['finally complement visual', 'complement visual theme', 'visual theme research', 'theme research goal', 'research goal rq', 'goal rq identify', 'rq identify theme', 'identify theme caption', 'theme caption hashtags', 'caption hashtags textual', 'hashtags textual data', 'textual data associated', 'data associated instagram', 'associated instagram image', 'instagram image dataset', 'image dataset refer', 'dataset refer latent', 'refer latent topic', 'latent topic linguistic', 'topic linguistic theme', 'linguistic theme existing', 'theme existing literature', 'existing literature emphasizes', 'literature emphasizes importance', 'emphasizes importance studying', 'importance studying language', 'studying language since', 'language since reflects', 'since reflects variety', 'reflects variety thought', 'variety thought function', 'thought function signal', 'function signal identity', 'signal identity emphasizes', 'identity emphasizes social', 'emphasizes social distance', 'social distance believe', 'distance believe linguistic', 'believe linguistic theme', 'linguistic theme may', 'theme may therefore', 'may therefore help', 'therefore help u', 'help u contrast', 'u contrast visual', 'contrast visual theme', 'visual theme around', 'theme around individual', 'around individual engage', 'individual engage mental', 'engage mental health', 'mental health disclosure', 'health disclosure instagram', 'disclosure instagram used', 'instagram used twitterlda', 'used twitterlda extract', 'twitterlda extract linguistic', 'extract linguistic theme', 'linguistic theme method', 'theme method wa', 'method wa developed', 'wa developed topic', 'developed topic modeling', 'topic modeling short', 'modeling short text', 'short text corpus', 'text corpus mining', 'corpus mining latent', 'mining latent topic', 'latent topic typically', 'topic typically done', 'typically done topic', 'done topic modeling', 'topic modeling preprocessed', 'modeling preprocessed data', 'preprocessed data removing', 'data removing standard', 'removing standard list', 'standard list stop', 'list stop word', 'stop word word', 'word word high', 'word high frequency', 'high frequency word', 'frequency word occur', 'word occur fewer', 'occur fewer five', 'fewer five time', 'five time since', 'time since lda', 'since lda unsupervised', 'lda unsupervised learning', 'unsupervised learning approach', 'learning approach identifying', 'approach identifying correct', 'identifying correct number', 'correct number topic', 'number topic challenging', 'topic challenging used', 'challenging used default', 'used default hyperparameter', 'default hyperparameter setting', 'hyperparameter setting topic', 'setting topic determined', 'topic determined based', 'determined based value', 'based value average', 'value average corpus', 'average corpus likelihood', 'corpus likelihood ten', 'likelihood ten run', 'ten run topic', 'run topic constituted', 'topic constituted known', 'constituted known lifted', 'known lifted form', 'lifted form linguistic', 'form linguistic vocabulary', 'linguistic vocabulary extracted', 'vocabulary extracted linguistic', 'extracted linguistic vocabulary', 'linguistic vocabulary arrive', 'vocabulary arrive interpretive', 'arrive interpretive description', 'interpretive description call', 'description call linguistic', 'call linguistic theme', 'linguistic theme adopted', 'theme adopted similar', 'adopted similar semiopen', 'similar semiopen coding', 'semiopen coding approach', 'coding approach visual', 'approach visual theme', 'visual theme involved', 'theme involved two', 'involved two researcher', 'two researcher raters', 'researcher raters referred', 'raters referred mental', 'referred mental health', 'mental health literature', 'health literature identified', 'literature identified best', 'identified best possible', 'best possible description', 'possible description characterized', 'description characterized token', 'characterized token linguistic', 'token linguistic vocabulary', 'linguistic vocabulary corresponding', 'vocabulary corresponding linguistic', 'corresponding linguistic topic', 'linguistic topic characterize', 'topic characterize represent', 'characterize represent visual', 'represent visual linguistic', 'visual linguistic theme', 'linguistic theme propose', 'theme propose measure', 'propose measure visual', 'measure visual diversity', 'visual diversity measure', 'diversity measure estimate', 'measure estimate coherent', 'estimate coherent image', 'coherent image respect', 'image respect theme', 'respect theme measure', 'theme measure diversity', 'measure diversity term', 'diversity term latent', 'term latent visual', 'latent visual feature', 'visual feature image', 'feature image expressed', 'image expressed term', 'expressed term principle', 'term principle component', 'principle component within', 'component within theme', 'within theme component', 'theme component space', 'component space distance', 'space distance pair', 'distance pair image', 'pair image computed', 'image computed employing', 'computed employing cosine', 'employing cosine theta', 'cosine theta similarity', 'theta similarity function', 'similarity function perform', 'function perform set', 'perform set operation', 'set operation utilize', 'operation utilize python', 'utilize python scikitlearn', 'python scikitlearn library']",,,,,,,,
https://www.sciencedirect.com/science/article/pii/S0747563215300996,0,Tweets about depression were collected by Simply Measured a company that specializes in social media measurement and analytics (Simply Measured 2014). Simply Measured has access to the Twitter “firehose” (or full volume of tweets) via Gnip a licensed company that can retrieve the full Twitter data stream. All tweets in the English language that contained at least either “depressed” “#depressed” “depression” or “#depression” were collected between April 11 and May 4 2014. We scanned a random sample of the tweets to identify common phrases that included our keywords of interest but were not about mental health. In SAS version 9.3 (SAS Institute Inc. Cary NC) we used the index function which searches a character expression (in this case the text of the tweet) for a specific string of characters to locate and remove such tweets from our sample. We removed tweets that included the following terms regardless of capitalization: “Great Depression” “economic depression” “during the depression” “depression era” “tropical depression” and “depressed real estate”. The popularity and influence of the Tweeters was described using the distribution of followers and Klout Scores. While number of followers is a measure of popularity Klout Score is a measure of influence. Klout Scores range from 0 to 100 with a higher score indicating higher influence. Klout Score is calculated based on an algorithm that considers over 400 signals from eight different online networks. Examples of signals include the amount of retweets a person generates in relation to the amount of tweets shared and the amount of engagement a user drives from unique individuals (e.g. lots of retweets from different individuals as opposed to lots of retweets from one person) (Klout Inc. 2014).,tweet about depression were collected by simply measured a company that specializes in social medium measurement and analytics simply measured simply measured ha access to the twitter firehose or full volume of tweet via gnip a licensed company that can retrieve the full twitter data stream all tweet in the english language that contained at least either depressed depressed depression or depression were collected between april and may we scanned a random sample of the tweet to identify common phrase that included our keywords of interest but were not about mental health in sa version sa institute inc cary nc we used the index function which search a character expression in this case the text of the tweet for a specific string of character to locate and remove such tweet from our sample we removed tweet that included the following term regardless of capitalization great depression economic depression during the depression depression era tropical depression and depressed real estate the popularity and influence of the tweeter wa described using the distribution of follower and klout score while number of follower is a measure of popularity klout score is a measure of influence klout score range from to with a higher score indicating higher influence klout score is calculated based on an algorithm that considers over signal from eight different online network example of signal include the amount of retweets a person generates in relation to the amount of tweet shared and the amount of engagement a user drive from unique individual eg lot of retweets from different individual a opposed to lot of retweets from one person klout inc,"['tweet', 'depression', 'collected', 'simply', 'measured', 'company', 'specializes', 'social', 'medium', 'measurement', 'analytics', 'simply', 'measured', 'simply', 'measured', 'ha', 'access', 'twitter', 'firehose', 'full', 'volume', 'tweet', 'via', 'gnip', 'licensed', 'company', 'retrieve', 'full', 'twitter', 'data', 'stream', 'tweet', 'english', 'language', 'contained', 'least', 'either', 'depressed', 'depressed', 'depression', 'depression', 'collected', 'april', 'may', 'scanned', 'random', 'sample', 'tweet', 'identify', 'common', 'phrase', 'included', 'keywords', 'interest', 'mental', 'health', 'sa', 'version', 'sa', 'institute', 'inc', 'cary', 'nc', 'used', 'index', 'function', 'search', 'character', 'expression', 'case', 'text', 'tweet', 'specific', 'string', 'character', 'locate', 'remove', 'tweet', 'sample', 'removed', 'tweet', 'included', 'following', 'term', 'regardless', 'capitalization', 'great', 'depression', 'economic', 'depression', 'depression', 'depression', 'era', 'tropical', 'depression', 'depressed', 'real', 'estate', 'popularity', 'influence', 'tweeter', 'wa', 'described', 'using', 'distribution', 'follower', 'klout', 'score', 'number', 'follower', 'measure', 'popularity', 'klout', 'score', 'measure', 'influence', 'klout', 'score', 'range', 'higher', 'score', 'indicating', 'higher', 'influence', 'klout', 'score', 'calculated', 'based', 'algorithm', 'considers', 'signal', 'eight', 'different', 'online', 'network', 'example', 'signal', 'include', 'amount', 'retweets', 'person', 'generates', 'relation', 'amount', 'tweet', 'shared', 'amount', 'engagement', 'user', 'drive', 'unique', 'individual', 'eg', 'lot', 'retweets', 'different', 'individual', 'opposed', 'lot', 'retweets', 'one', 'person', 'klout', 'inc']","['tweet depression', 'depression collected', 'collected simply', 'simply measured', 'measured company', 'company specializes', 'specializes social', 'social medium', 'medium measurement', 'measurement analytics', 'analytics simply', 'simply measured', 'measured simply', 'simply measured', 'measured ha', 'ha access', 'access twitter', 'twitter firehose', 'firehose full', 'full volume', 'volume tweet', 'tweet via', 'via gnip', 'gnip licensed', 'licensed company', 'company retrieve', 'retrieve full', 'full twitter', 'twitter data', 'data stream', 'stream tweet', 'tweet english', 'english language', 'language contained', 'contained least', 'least either', 'either depressed', 'depressed depressed', 'depressed depression', 'depression depression', 'depression collected', 'collected april', 'april may', 'may scanned', 'scanned random', 'random sample', 'sample tweet', 'tweet identify', 'identify common', 'common phrase', 'phrase included', 'included keywords', 'keywords interest', 'interest mental', 'mental health', 'health sa', 'sa version', 'version sa', 'sa institute', 'institute inc', 'inc cary', 'cary nc', 'nc used', 'used index', 'index function', 'function search', 'search character', 'character expression', 'expression case', 'case text', 'text tweet', 'tweet specific', 'specific string', 'string character', 'character locate', 'locate remove', 'remove tweet', 'tweet sample', 'sample removed', 'removed tweet', 'tweet included', 'included following', 'following term', 'term regardless', 'regardless capitalization', 'capitalization great', 'great depression', 'depression economic', 'economic depression', 'depression depression', 'depression depression', 'depression era', 'era tropical', 'tropical depression', 'depression depressed', 'depressed real', 'real estate', 'estate popularity', 'popularity influence', 'influence tweeter', 'tweeter wa', 'wa described', 'described using', 'using distribution', 'distribution follower', 'follower klout', 'klout score', 'score number', 'number follower', 'follower measure', 'measure popularity', 'popularity klout', 'klout score', 'score measure', 'measure influence', 'influence klout', 'klout score', 'score range', 'range higher', 'higher score', 'score indicating', 'indicating higher', 'higher influence', 'influence klout', 'klout score', 'score calculated', 'calculated based', 'based algorithm', 'algorithm considers', 'considers signal', 'signal eight', 'eight different', 'different online', 'online network', 'network example', 'example signal', 'signal include', 'include amount', 'amount retweets', 'retweets person', 'person generates', 'generates relation', 'relation amount', 'amount tweet', 'tweet shared', 'shared amount', 'amount engagement', 'engagement user', 'user drive', 'drive unique', 'unique individual', 'individual eg', 'eg lot', 'lot retweets', 'retweets different', 'different individual', 'individual opposed', 'opposed lot', 'lot retweets', 'retweets one', 'one person', 'person klout', 'klout inc']","['tweet depression collected', 'depression collected simply', 'collected simply measured', 'simply measured company', 'measured company specializes', 'company specializes social', 'specializes social medium', 'social medium measurement', 'medium measurement analytics', 'measurement analytics simply', 'analytics simply measured', 'simply measured simply', 'measured simply measured', 'simply measured ha', 'measured ha access', 'ha access twitter', 'access twitter firehose', 'twitter firehose full', 'firehose full volume', 'full volume tweet', 'volume tweet via', 'tweet via gnip', 'via gnip licensed', 'gnip licensed company', 'licensed company retrieve', 'company retrieve full', 'retrieve full twitter', 'full twitter data', 'twitter data stream', 'data stream tweet', 'stream tweet english', 'tweet english language', 'english language contained', 'language contained least', 'contained least either', 'least either depressed', 'either depressed depressed', 'depressed depressed depression', 'depressed depression depression', 'depression depression collected', 'depression collected april', 'collected april may', 'april may scanned', 'may scanned random', 'scanned random sample', 'random sample tweet', 'sample tweet identify', 'tweet identify common', 'identify common phrase', 'common phrase included', 'phrase included keywords', 'included keywords interest', 'keywords interest mental', 'interest mental health', 'mental health sa', 'health sa version', 'sa version sa', 'version sa institute', 'sa institute inc', 'institute inc cary', 'inc cary nc', 'cary nc used', 'nc used index', 'used index function', 'index function search', 'function search character', 'search character expression', 'character expression case', 'expression case text', 'case text tweet', 'text tweet specific', 'tweet specific string', 'specific string character', 'string character locate', 'character locate remove', 'locate remove tweet', 'remove tweet sample', 'tweet sample removed', 'sample removed tweet', 'removed tweet included', 'tweet included following', 'included following term', 'following term regardless', 'term regardless capitalization', 'regardless capitalization great', 'capitalization great depression', 'great depression economic', 'depression economic depression', 'economic depression depression', 'depression depression depression', 'depression depression era', 'depression era tropical', 'era tropical depression', 'tropical depression depressed', 'depression depressed real', 'depressed real estate', 'real estate popularity', 'estate popularity influence', 'popularity influence tweeter', 'influence tweeter wa', 'tweeter wa described', 'wa described using', 'described using distribution', 'using distribution follower', 'distribution follower klout', 'follower klout score', 'klout score number', 'score number follower', 'number follower measure', 'follower measure popularity', 'measure popularity klout', 'popularity klout score', 'klout score measure', 'score measure influence', 'measure influence klout', 'influence klout score', 'klout score range', 'score range higher', 'range higher score', 'higher score indicating', 'score indicating higher', 'indicating higher influence', 'higher influence klout', 'influence klout score', 'klout score calculated', 'score calculated based', 'calculated based algorithm', 'based algorithm considers', 'algorithm considers signal', 'considers signal eight', 'signal eight different', 'eight different online', 'different online network', 'online network example', 'network example signal', 'example signal include', 'signal include amount', 'include amount retweets', 'amount retweets person', 'retweets person generates', 'person generates relation', 'generates relation amount', 'relation amount tweet', 'amount tweet shared', 'tweet shared amount', 'shared amount engagement', 'amount engagement user', 'engagement user drive', 'user drive unique', 'drive unique individual', 'unique individual eg', 'individual eg lot', 'eg lot retweets', 'lot retweets different', 'retweets different individual', 'different individual opposed', 'individual opposed lot', 'opposed lot retweets', 'lot retweets one', 'retweets one person', 'one person klout', 'person klout inc']",,,,,,,,
https://aclanthology.org/W15-1202.pdf,1,We follow the data acquisition and curation process of Coppersmith et al. (2014a) summarizing the major points here: Social media such as Twitter contains frequent public statements by users reporting diagnoses for various medical conditions. Many talk about physical health conditions (e.g. cancer flu) but some also discuss mental illness including schizophrenia. There are a variety of motivations for users to share this information on social media: to offer or seek support to fight the stigma of mental illness or perhaps to offer an explanation for certain behaviors.4 We obtain messages with these self-reported diagnoses using the Twitter API and filtered via (caseinsensitive) regular expression to require “schizo” or a close phonetic approximation to be present; our expression matched “schizophrenia” its subtypes and various approximations: “schizo” “skitzo” “skitso” “schizotypal” “schizoid” etc. All data we collect are public posts made between 2008 and 2015 and exclude any message marked as ‘private’ by the author. All use of the data reported in this paper has been approved by the appropriate Institutional Review Board (IRB). Each self-stated diagnosis included in this study was examined by a human annotator (one of the authors) to verify that it appeared to be a genuine statement of a schizophrenia diagnosis excluding jokes quotes or disingenuous statements. We obtained 174 users with an apparently genuine selfstated diagnosis of a schizophrenia-related condition. Note that we cannot be certain that the Twitter user was actually diagnosed with schizophrenia only that their statement of being diagnosed appears to be genuine. Previous work indicates that interannotator agreement for this task is good: κ = 0.77 (Coppersmith et al. 2014a). For each user we obtained a set of their public Twitter posts via the Twitter API collecting up to 3200 tweets.5 As we wish to focus on user-authored content we exclude from analysis all retweets and any tweets that contain a URL (which often contain text that the user did not author). We lowercase all words and convert any non-standard characters (including emoji) to a systematic ASCII representation via Unidecode.6 For our community controls we used randomlyselected Twitter users who primarily tweet in English. Specifically during a two week period in early 2014 each Twitter user who was included in Twitter’s 1% “spritzer” sample had an equal chance for inclusion in our pool of community controls. We then collected some of their historic tweets and assessed the language(s) they tweeted in according to the Chromium Compact Language Detector.7 Users were excluded from our community controls if their tweets were less than 75% English.8,we follow the data acquisition and curation process of coppersmith et al a summarizing the major point here social medium such a twitter contains frequent public statement by user reporting diagnosis for various medical condition many talk about physical health condition eg cancer flu but some also discus mental illness including schizophrenia there are a variety of motivation for user to share this information on social medium to offer or seek support to fight the stigma of mental illness or perhaps to offer an explanation for certain behavior we obtain message with these selfreported diagnosis using the twitter api and filtered via caseinsensitive regular expression to require schizo or a close phonetic approximation to be present our expression matched schizophrenia it subtypes and various approximation schizo skitzo skitso schizotypal schizoid etc all data we collect are public post made between and and exclude any message marked a private by the author all use of the data reported in this paper ha been approved by the appropriate institutional review board irb each selfstated diagnosis included in this study wa examined by a human annotator one of the author to verify that it appeared to be a genuine statement of a schizophrenia diagnosis excluding joke quote or disingenuous statement we obtained user with an apparently genuine selfstated diagnosis of a schizophreniarelated condition note that we cannot be certain that the twitter user wa actually diagnosed with schizophrenia only that their statement of being diagnosed appears to be genuine previous work indicates that interannotator agreement for this task is good coppersmith et al a for each user we obtained a set of their public twitter post via the twitter api collecting up to tweet a we wish to focus on userauthored content we exclude from analysis all retweets and any tweet that contain a url which often contain text that the user did not author we lowercase all word and convert any nonstandard character including emoji to a systematic ascii representation via unidecode for our community control we used randomlyselected twitter user who primarily tweet in english specifically during a two week period in early each twitter user who wa included in twitter spritzer sample had an equal chance for inclusion in our pool of community control we then collected some of their historic tweet and assessed the language they tweeted in according to the chromium compact language detector user were excluded from our community control if their tweet were le than english,"['follow', 'data', 'acquisition', 'curation', 'process', 'coppersmith', 'et', 'al', 'summarizing', 'major', 'point', 'social', 'medium', 'twitter', 'contains', 'frequent', 'public', 'statement', 'user', 'reporting', 'diagnosis', 'various', 'medical', 'condition', 'many', 'talk', 'physical', 'health', 'condition', 'eg', 'cancer', 'flu', 'also', 'discus', 'mental', 'illness', 'including', 'schizophrenia', 'variety', 'motivation', 'user', 'share', 'information', 'social', 'medium', 'offer', 'seek', 'support', 'fight', 'stigma', 'mental', 'illness', 'perhaps', 'offer', 'explanation', 'certain', 'behavior', 'obtain', 'message', 'selfreported', 'diagnosis', 'using', 'twitter', 'api', 'filtered', 'via', 'caseinsensitive', 'regular', 'expression', 'require', 'schizo', 'close', 'phonetic', 'approximation', 'present', 'expression', 'matched', 'schizophrenia', 'subtypes', 'various', 'approximation', 'schizo', 'skitzo', 'skitso', 'schizotypal', 'schizoid', 'etc', 'data', 'collect', 'public', 'post', 'made', 'exclude', 'message', 'marked', 'private', 'author', 'use', 'data', 'reported', 'paper', 'ha', 'approved', 'appropriate', 'institutional', 'review', 'board', 'irb', 'selfstated', 'diagnosis', 'included', 'study', 'wa', 'examined', 'human', 'annotator', 'one', 'author', 'verify', 'appeared', 'genuine', 'statement', 'schizophrenia', 'diagnosis', 'excluding', 'joke', 'quote', 'disingenuous', 'statement', 'obtained', 'user', 'apparently', 'genuine', 'selfstated', 'diagnosis', 'schizophreniarelated', 'condition', 'note', 'cannot', 'certain', 'twitter', 'user', 'wa', 'actually', 'diagnosed', 'schizophrenia', 'statement', 'diagnosed', 'appears', 'genuine', 'previous', 'work', 'indicates', 'interannotator', 'agreement', 'task', 'good', 'coppersmith', 'et', 'al', 'user', 'obtained', 'set', 'public', 'twitter', 'post', 'via', 'twitter', 'api', 'collecting', 'tweet', 'wish', 'focus', 'userauthored', 'content', 'exclude', 'analysis', 'retweets', 'tweet', 'contain', 'url', 'often', 'contain', 'text', 'user', 'author', 'lowercase', 'word', 'convert', 'nonstandard', 'character', 'including', 'emoji', 'systematic', 'ascii', 'representation', 'via', 'unidecode', 'community', 'control', 'used', 'randomlyselected', 'twitter', 'user', 'primarily', 'tweet', 'english', 'specifically', 'two', 'week', 'period', 'early', 'twitter', 'user', 'wa', 'included', 'twitter', 'spritzer', 'sample', 'equal', 'chance', 'inclusion', 'pool', 'community', 'control', 'collected', 'historic', 'tweet', 'assessed', 'language', 'tweeted', 'according', 'chromium', 'compact', 'language', 'detector', 'user', 'excluded', 'community', 'control', 'tweet', 'le', 'english']","['follow data', 'data acquisition', 'acquisition curation', 'curation process', 'process coppersmith', 'coppersmith et', 'et al', 'al summarizing', 'summarizing major', 'major point', 'point social', 'social medium', 'medium twitter', 'twitter contains', 'contains frequent', 'frequent public', 'public statement', 'statement user', 'user reporting', 'reporting diagnosis', 'diagnosis various', 'various medical', 'medical condition', 'condition many', 'many talk', 'talk physical', 'physical health', 'health condition', 'condition eg', 'eg cancer', 'cancer flu', 'flu also', 'also discus', 'discus mental', 'mental illness', 'illness including', 'including schizophrenia', 'schizophrenia variety', 'variety motivation', 'motivation user', 'user share', 'share information', 'information social', 'social medium', 'medium offer', 'offer seek', 'seek support', 'support fight', 'fight stigma', 'stigma mental', 'mental illness', 'illness perhaps', 'perhaps offer', 'offer explanation', 'explanation certain', 'certain behavior', 'behavior obtain', 'obtain message', 'message selfreported', 'selfreported diagnosis', 'diagnosis using', 'using twitter', 'twitter api', 'api filtered', 'filtered via', 'via caseinsensitive', 'caseinsensitive regular', 'regular expression', 'expression require', 'require schizo', 'schizo close', 'close phonetic', 'phonetic approximation', 'approximation present', 'present expression', 'expression matched', 'matched schizophrenia', 'schizophrenia subtypes', 'subtypes various', 'various approximation', 'approximation schizo', 'schizo skitzo', 'skitzo skitso', 'skitso schizotypal', 'schizotypal schizoid', 'schizoid etc', 'etc data', 'data collect', 'collect public', 'public post', 'post made', 'made exclude', 'exclude message', 'message marked', 'marked private', 'private author', 'author use', 'use data', 'data reported', 'reported paper', 'paper ha', 'ha approved', 'approved appropriate', 'appropriate institutional', 'institutional review', 'review board', 'board irb', 'irb selfstated', 'selfstated diagnosis', 'diagnosis included', 'included study', 'study wa', 'wa examined', 'examined human', 'human annotator', 'annotator one', 'one author', 'author verify', 'verify appeared', 'appeared genuine', 'genuine statement', 'statement schizophrenia', 'schizophrenia diagnosis', 'diagnosis excluding', 'excluding joke', 'joke quote', 'quote disingenuous', 'disingenuous statement', 'statement obtained', 'obtained user', 'user apparently', 'apparently genuine', 'genuine selfstated', 'selfstated diagnosis', 'diagnosis schizophreniarelated', 'schizophreniarelated condition', 'condition note', 'note cannot', 'cannot certain', 'certain twitter', 'twitter user', 'user wa', 'wa actually', 'actually diagnosed', 'diagnosed schizophrenia', 'schizophrenia statement', 'statement diagnosed', 'diagnosed appears', 'appears genuine', 'genuine previous', 'previous work', 'work indicates', 'indicates interannotator', 'interannotator agreement', 'agreement task', 'task good', 'good coppersmith', 'coppersmith et', 'et al', 'al user', 'user obtained', 'obtained set', 'set public', 'public twitter', 'twitter post', 'post via', 'via twitter', 'twitter api', 'api collecting', 'collecting tweet', 'tweet wish', 'wish focus', 'focus userauthored', 'userauthored content', 'content exclude', 'exclude analysis', 'analysis retweets', 'retweets tweet', 'tweet contain', 'contain url', 'url often', 'often contain', 'contain text', 'text user', 'user author', 'author lowercase', 'lowercase word', 'word convert', 'convert nonstandard', 'nonstandard character', 'character including', 'including emoji', 'emoji systematic', 'systematic ascii', 'ascii representation', 'representation via', 'via unidecode', 'unidecode community', 'community control', 'control used', 'used randomlyselected', 'randomlyselected twitter', 'twitter user', 'user primarily', 'primarily tweet', 'tweet english', 'english specifically', 'specifically two', 'two week', 'week period', 'period early', 'early twitter', 'twitter user', 'user wa', 'wa included', 'included twitter', 'twitter spritzer', 'spritzer sample', 'sample equal', 'equal chance', 'chance inclusion', 'inclusion pool', 'pool community', 'community control', 'control collected', 'collected historic', 'historic tweet', 'tweet assessed', 'assessed language', 'language tweeted', 'tweeted according', 'according chromium', 'chromium compact', 'compact language', 'language detector', 'detector user', 'user excluded', 'excluded community', 'community control', 'control tweet', 'tweet le', 'le english']","['follow data acquisition', 'data acquisition curation', 'acquisition curation process', 'curation process coppersmith', 'process coppersmith et', 'coppersmith et al', 'et al summarizing', 'al summarizing major', 'summarizing major point', 'major point social', 'point social medium', 'social medium twitter', 'medium twitter contains', 'twitter contains frequent', 'contains frequent public', 'frequent public statement', 'public statement user', 'statement user reporting', 'user reporting diagnosis', 'reporting diagnosis various', 'diagnosis various medical', 'various medical condition', 'medical condition many', 'condition many talk', 'many talk physical', 'talk physical health', 'physical health condition', 'health condition eg', 'condition eg cancer', 'eg cancer flu', 'cancer flu also', 'flu also discus', 'also discus mental', 'discus mental illness', 'mental illness including', 'illness including schizophrenia', 'including schizophrenia variety', 'schizophrenia variety motivation', 'variety motivation user', 'motivation user share', 'user share information', 'share information social', 'information social medium', 'social medium offer', 'medium offer seek', 'offer seek support', 'seek support fight', 'support fight stigma', 'fight stigma mental', 'stigma mental illness', 'mental illness perhaps', 'illness perhaps offer', 'perhaps offer explanation', 'offer explanation certain', 'explanation certain behavior', 'certain behavior obtain', 'behavior obtain message', 'obtain message selfreported', 'message selfreported diagnosis', 'selfreported diagnosis using', 'diagnosis using twitter', 'using twitter api', 'twitter api filtered', 'api filtered via', 'filtered via caseinsensitive', 'via caseinsensitive regular', 'caseinsensitive regular expression', 'regular expression require', 'expression require schizo', 'require schizo close', 'schizo close phonetic', 'close phonetic approximation', 'phonetic approximation present', 'approximation present expression', 'present expression matched', 'expression matched schizophrenia', 'matched schizophrenia subtypes', 'schizophrenia subtypes various', 'subtypes various approximation', 'various approximation schizo', 'approximation schizo skitzo', 'schizo skitzo skitso', 'skitzo skitso schizotypal', 'skitso schizotypal schizoid', 'schizotypal schizoid etc', 'schizoid etc data', 'etc data collect', 'data collect public', 'collect public post', 'public post made', 'post made exclude', 'made exclude message', 'exclude message marked', 'message marked private', 'marked private author', 'private author use', 'author use data', 'use data reported', 'data reported paper', 'reported paper ha', 'paper ha approved', 'ha approved appropriate', 'approved appropriate institutional', 'appropriate institutional review', 'institutional review board', 'review board irb', 'board irb selfstated', 'irb selfstated diagnosis', 'selfstated diagnosis included', 'diagnosis included study', 'included study wa', 'study wa examined', 'wa examined human', 'examined human annotator', 'human annotator one', 'annotator one author', 'one author verify', 'author verify appeared', 'verify appeared genuine', 'appeared genuine statement', 'genuine statement schizophrenia', 'statement schizophrenia diagnosis', 'schizophrenia diagnosis excluding', 'diagnosis excluding joke', 'excluding joke quote', 'joke quote disingenuous', 'quote disingenuous statement', 'disingenuous statement obtained', 'statement obtained user', 'obtained user apparently', 'user apparently genuine', 'apparently genuine selfstated', 'genuine selfstated diagnosis', 'selfstated diagnosis schizophreniarelated', 'diagnosis schizophreniarelated condition', 'schizophreniarelated condition note', 'condition note cannot', 'note cannot certain', 'cannot certain twitter', 'certain twitter user', 'twitter user wa', 'user wa actually', 'wa actually diagnosed', 'actually diagnosed schizophrenia', 'diagnosed schizophrenia statement', 'schizophrenia statement diagnosed', 'statement diagnosed appears', 'diagnosed appears genuine', 'appears genuine previous', 'genuine previous work', 'previous work indicates', 'work indicates interannotator', 'indicates interannotator agreement', 'interannotator agreement task', 'agreement task good', 'task good coppersmith', 'good coppersmith et', 'coppersmith et al', 'et al user', 'al user obtained', 'user obtained set', 'obtained set public', 'set public twitter', 'public twitter post', 'twitter post via', 'post via twitter', 'via twitter api', 'twitter api collecting', 'api collecting tweet', 'collecting tweet wish', 'tweet wish focus', 'wish focus userauthored', 'focus userauthored content', 'userauthored content exclude', 'content exclude analysis', 'exclude analysis retweets', 'analysis retweets tweet', 'retweets tweet contain', 'tweet contain url', 'contain url often', 'url often contain', 'often contain text', 'contain text user', 'text user author', 'user author lowercase', 'author lowercase word', 'lowercase word convert', 'word convert nonstandard', 'convert nonstandard character', 'nonstandard character including', 'character including emoji', 'including emoji systematic', 'emoji systematic ascii', 'systematic ascii representation', 'ascii representation via', 'representation via unidecode', 'via unidecode community', 'unidecode community control', 'community control used', 'control used randomlyselected', 'used randomlyselected twitter', 'randomlyselected twitter user', 'twitter user primarily', 'user primarily tweet', 'primarily tweet english', 'tweet english specifically', 'english specifically two', 'specifically two week', 'two week period', 'week period early', 'period early twitter', 'early twitter user', 'twitter user wa', 'user wa included', 'wa included twitter', 'included twitter spritzer', 'twitter spritzer sample', 'spritzer sample equal', 'sample equal chance', 'equal chance inclusion', 'chance inclusion pool', 'inclusion pool community', 'pool community control', 'community control collected', 'control collected historic', 'collected historic tweet', 'historic tweet assessed', 'tweet assessed language', 'assessed language tweeted', 'language tweeted according', 'tweeted according chromium', 'according chromium compact', 'chromium compact language', 'compact language detector', 'language detector user', 'detector user excluded', 'user excluded community', 'excluded community control', 'community control tweet', 'control tweet le', 'tweet le english']",,,,,,,,
https://dl.acm.org/doi/abs/10.1145/2700171.2791026,0,We propose four categories of linguistic and non-linguistic attributes to examine preceding/succeeding celebrity suicides. These are: (1) affective attributes (2) cognitive attributes (3) linguistic style attributes and (4) social attributes. Measures belonging to all of these attribute categories are largely based on the psycholinguistic lexicon LIWC [40] and were motivated from prior literature that examine associations between the behavioral expression of individuals and their responses to traumatic context and crises including vulnerability due to mental illness [7 11]. Note that LIWC has been extensively validated to perform well on Internet language [7 18]. (1) We consider two measures of affect derived from LIWC: positive affect (PA) and negative affect (NA) and four other measures of emotional expression: anger anxiety sadness and swear. (2) We use LIWC to define the cognitive measures as well: (a) cognition comprising cognitive mech discrepancies inhibition negation death causation certainty and tentativeness; and (b) perception comprising set of words in LIWC around see hear feel percept insight and relative. (3) Next we consider four measures of linguistic style: (a) Lexical Density: consisting of words that are verbs auxiliary verbs nouns adjectives (identified using NLTK’s [2] POS tagger) and adverbs. (b) Temporal References: consisting of past present and future tenses. (c) Social/Personal Concerns: words belonging to family friends social work health humans religion bio body money achievement home and sexual. (d) Interpersonal Awareness and Focus: words that are 1st person singular 1st person plural 2nd person and 3rd person pronouns. (4) For social attributes we utilized a variety of content sharing social interaction and social support indicators. These are: post length number of comments vote difference (difference between upvotes and downvotes divided by total upvotes and downvotes) comment arrival rate (average time difference between any two subsequent comments in a post’s comment thread) time to first comment (time elapsed between the first comment and the timestamp of the corresponding post) and median comment length9 . We compute each of the above linguistic measures of behavior at the post level – the value of a measure is given by the ratio of the number of words in a post that match words belonging to the measure to the total number of words in the post. For each measure we take the average across all celebrities to ensure each suicide event is equally weighted i.e. to avoid skew due to a single suicide. For statistical comparison we used the Welch t-test; a negative tstatistic value means the measure increased after suicide.,we propose four category of linguistic and nonlinguistic attribute to examine precedingsucceeding celebrity suicide these are affective attribute cognitive attribute linguistic style attribute and social attribute measure belonging to all of these attribute category are largely based on the psycholinguistic lexicon liwc and were motivated from prior literature that examine association between the behavioral expression of individual and their response to traumatic context and crisis including vulnerability due to mental illness note that liwc ha been extensively validated to perform well on internet language we consider two measure of affect derived from liwc positive affect pa and negative affect na and four other measure of emotional expression anger anxiety sadness and swear we use liwc to define the cognitive measure a well a cognition comprising cognitive mech discrepancy inhibition negation death causation certainty and tentativeness and b perception comprising set of word in liwc around see hear feel percept insight and relative next we consider four measure of linguistic style a lexical density consisting of word that are verb auxiliary verb noun adjective identified using nltks po tagger and adverb b temporal reference consisting of past present and future tense c socialpersonal concern word belonging to family friend social work health human religion bio body money achievement home and sexual d interpersonal awareness and focus word that are st person singular st person plural nd person and rd person pronoun for social attribute we utilized a variety of content sharing social interaction and social support indicator these are post length number of comment vote difference difference between upvotes and downvotes divided by total upvotes and downvotes comment arrival rate average time difference between any two subsequent comment in a post comment thread time to first comment time elapsed between the first comment and the timestamp of the corresponding post and median comment length we compute each of the above linguistic measure of behavior at the post level the value of a measure is given by the ratio of the number of word in a post that match word belonging to the measure to the total number of word in the post for each measure we take the average across all celebrity to ensure each suicide event is equally weighted ie to avoid skew due to a single suicide for statistical comparison we used the welch ttest a negative tstatistic value mean the measure increased after suicide,"['propose', 'four', 'category', 'linguistic', 'nonlinguistic', 'attribute', 'examine', 'precedingsucceeding', 'celebrity', 'suicide', 'affective', 'attribute', 'cognitive', 'attribute', 'linguistic', 'style', 'attribute', 'social', 'attribute', 'measure', 'belonging', 'attribute', 'category', 'largely', 'based', 'psycholinguistic', 'lexicon', 'liwc', 'motivated', 'prior', 'literature', 'examine', 'association', 'behavioral', 'expression', 'individual', 'response', 'traumatic', 'context', 'crisis', 'including', 'vulnerability', 'due', 'mental', 'illness', 'note', 'liwc', 'ha', 'extensively', 'validated', 'perform', 'well', 'internet', 'language', 'consider', 'two', 'measure', 'affect', 'derived', 'liwc', 'positive', 'affect', 'pa', 'negative', 'affect', 'na', 'four', 'measure', 'emotional', 'expression', 'anger', 'anxiety', 'sadness', 'swear', 'use', 'liwc', 'define', 'cognitive', 'measure', 'well', 'cognition', 'comprising', 'cognitive', 'mech', 'discrepancy', 'inhibition', 'negation', 'death', 'causation', 'certainty', 'tentativeness', 'b', 'perception', 'comprising', 'set', 'word', 'liwc', 'around', 'see', 'hear', 'feel', 'percept', 'insight', 'relative', 'next', 'consider', 'four', 'measure', 'linguistic', 'style', 'lexical', 'density', 'consisting', 'word', 'verb', 'auxiliary', 'verb', 'noun', 'adjective', 'identified', 'using', 'nltks', 'po', 'tagger', 'adverb', 'b', 'temporal', 'reference', 'consisting', 'past', 'present', 'future', 'tense', 'c', 'socialpersonal', 'concern', 'word', 'belonging', 'family', 'friend', 'social', 'work', 'health', 'human', 'religion', 'bio', 'body', 'money', 'achievement', 'home', 'sexual', 'interpersonal', 'awareness', 'focus', 'word', 'st', 'person', 'singular', 'st', 'person', 'plural', 'nd', 'person', 'rd', 'person', 'pronoun', 'social', 'attribute', 'utilized', 'variety', 'content', 'sharing', 'social', 'interaction', 'social', 'support', 'indicator', 'post', 'length', 'number', 'comment', 'vote', 'difference', 'difference', 'upvotes', 'downvotes', 'divided', 'total', 'upvotes', 'downvotes', 'comment', 'arrival', 'rate', 'average', 'time', 'difference', 'two', 'subsequent', 'comment', 'post', 'comment', 'thread', 'time', 'first', 'comment', 'time', 'elapsed', 'first', 'comment', 'timestamp', 'corresponding', 'post', 'median', 'comment', 'length', 'compute', 'linguistic', 'measure', 'behavior', 'post', 'level', 'value', 'measure', 'given', 'ratio', 'number', 'word', 'post', 'match', 'word', 'belonging', 'measure', 'total', 'number', 'word', 'post', 'measure', 'take', 'average', 'across', 'celebrity', 'ensure', 'suicide', 'event', 'equally', 'weighted', 'ie', 'avoid', 'skew', 'due', 'single', 'suicide', 'statistical', 'comparison', 'used', 'welch', 'ttest', 'negative', 'tstatistic', 'value', 'mean', 'measure', 'increased', 'suicide']","['propose four', 'four category', 'category linguistic', 'linguistic nonlinguistic', 'nonlinguistic attribute', 'attribute examine', 'examine precedingsucceeding', 'precedingsucceeding celebrity', 'celebrity suicide', 'suicide affective', 'affective attribute', 'attribute cognitive', 'cognitive attribute', 'attribute linguistic', 'linguistic style', 'style attribute', 'attribute social', 'social attribute', 'attribute measure', 'measure belonging', 'belonging attribute', 'attribute category', 'category largely', 'largely based', 'based psycholinguistic', 'psycholinguistic lexicon', 'lexicon liwc', 'liwc motivated', 'motivated prior', 'prior literature', 'literature examine', 'examine association', 'association behavioral', 'behavioral expression', 'expression individual', 'individual response', 'response traumatic', 'traumatic context', 'context crisis', 'crisis including', 'including vulnerability', 'vulnerability due', 'due mental', 'mental illness', 'illness note', 'note liwc', 'liwc ha', 'ha extensively', 'extensively validated', 'validated perform', 'perform well', 'well internet', 'internet language', 'language consider', 'consider two', 'two measure', 'measure affect', 'affect derived', 'derived liwc', 'liwc positive', 'positive affect', 'affect pa', 'pa negative', 'negative affect', 'affect na', 'na four', 'four measure', 'measure emotional', 'emotional expression', 'expression anger', 'anger anxiety', 'anxiety sadness', 'sadness swear', 'swear use', 'use liwc', 'liwc define', 'define cognitive', 'cognitive measure', 'measure well', 'well cognition', 'cognition comprising', 'comprising cognitive', 'cognitive mech', 'mech discrepancy', 'discrepancy inhibition', 'inhibition negation', 'negation death', 'death causation', 'causation certainty', 'certainty tentativeness', 'tentativeness b', 'b perception', 'perception comprising', 'comprising set', 'set word', 'word liwc', 'liwc around', 'around see', 'see hear', 'hear feel', 'feel percept', 'percept insight', 'insight relative', 'relative next', 'next consider', 'consider four', 'four measure', 'measure linguistic', 'linguistic style', 'style lexical', 'lexical density', 'density consisting', 'consisting word', 'word verb', 'verb auxiliary', 'auxiliary verb', 'verb noun', 'noun adjective', 'adjective identified', 'identified using', 'using nltks', 'nltks po', 'po tagger', 'tagger adverb', 'adverb b', 'b temporal', 'temporal reference', 'reference consisting', 'consisting past', 'past present', 'present future', 'future tense', 'tense c', 'c socialpersonal', 'socialpersonal concern', 'concern word', 'word belonging', 'belonging family', 'family friend', 'friend social', 'social work', 'work health', 'health human', 'human religion', 'religion bio', 'bio body', 'body money', 'money achievement', 'achievement home', 'home sexual', 'sexual interpersonal', 'interpersonal awareness', 'awareness focus', 'focus word', 'word st', 'st person', 'person singular', 'singular st', 'st person', 'person plural', 'plural nd', 'nd person', 'person rd', 'rd person', 'person pronoun', 'pronoun social', 'social attribute', 'attribute utilized', 'utilized variety', 'variety content', 'content sharing', 'sharing social', 'social interaction', 'interaction social', 'social support', 'support indicator', 'indicator post', 'post length', 'length number', 'number comment', 'comment vote', 'vote difference', 'difference difference', 'difference upvotes', 'upvotes downvotes', 'downvotes divided', 'divided total', 'total upvotes', 'upvotes downvotes', 'downvotes comment', 'comment arrival', 'arrival rate', 'rate average', 'average time', 'time difference', 'difference two', 'two subsequent', 'subsequent comment', 'comment post', 'post comment', 'comment thread', 'thread time', 'time first', 'first comment', 'comment time', 'time elapsed', 'elapsed first', 'first comment', 'comment timestamp', 'timestamp corresponding', 'corresponding post', 'post median', 'median comment', 'comment length', 'length compute', 'compute linguistic', 'linguistic measure', 'measure behavior', 'behavior post', 'post level', 'level value', 'value measure', 'measure given', 'given ratio', 'ratio number', 'number word', 'word post', 'post match', 'match word', 'word belonging', 'belonging measure', 'measure total', 'total number', 'number word', 'word post', 'post measure', 'measure take', 'take average', 'average across', 'across celebrity', 'celebrity ensure', 'ensure suicide', 'suicide event', 'event equally', 'equally weighted', 'weighted ie', 'ie avoid', 'avoid skew', 'skew due', 'due single', 'single suicide', 'suicide statistical', 'statistical comparison', 'comparison used', 'used welch', 'welch ttest', 'ttest negative', 'negative tstatistic', 'tstatistic value', 'value mean', 'mean measure', 'measure increased', 'increased suicide']","['propose four category', 'four category linguistic', 'category linguistic nonlinguistic', 'linguistic nonlinguistic attribute', 'nonlinguistic attribute examine', 'attribute examine precedingsucceeding', 'examine precedingsucceeding celebrity', 'precedingsucceeding celebrity suicide', 'celebrity suicide affective', 'suicide affective attribute', 'affective attribute cognitive', 'attribute cognitive attribute', 'cognitive attribute linguistic', 'attribute linguistic style', 'linguistic style attribute', 'style attribute social', 'attribute social attribute', 'social attribute measure', 'attribute measure belonging', 'measure belonging attribute', 'belonging attribute category', 'attribute category largely', 'category largely based', 'largely based psycholinguistic', 'based psycholinguistic lexicon', 'psycholinguistic lexicon liwc', 'lexicon liwc motivated', 'liwc motivated prior', 'motivated prior literature', 'prior literature examine', 'literature examine association', 'examine association behavioral', 'association behavioral expression', 'behavioral expression individual', 'expression individual response', 'individual response traumatic', 'response traumatic context', 'traumatic context crisis', 'context crisis including', 'crisis including vulnerability', 'including vulnerability due', 'vulnerability due mental', 'due mental illness', 'mental illness note', 'illness note liwc', 'note liwc ha', 'liwc ha extensively', 'ha extensively validated', 'extensively validated perform', 'validated perform well', 'perform well internet', 'well internet language', 'internet language consider', 'language consider two', 'consider two measure', 'two measure affect', 'measure affect derived', 'affect derived liwc', 'derived liwc positive', 'liwc positive affect', 'positive affect pa', 'affect pa negative', 'pa negative affect', 'negative affect na', 'affect na four', 'na four measure', 'four measure emotional', 'measure emotional expression', 'emotional expression anger', 'expression anger anxiety', 'anger anxiety sadness', 'anxiety sadness swear', 'sadness swear use', 'swear use liwc', 'use liwc define', 'liwc define cognitive', 'define cognitive measure', 'cognitive measure well', 'measure well cognition', 'well cognition comprising', 'cognition comprising cognitive', 'comprising cognitive mech', 'cognitive mech discrepancy', 'mech discrepancy inhibition', 'discrepancy inhibition negation', 'inhibition negation death', 'negation death causation', 'death causation certainty', 'causation certainty tentativeness', 'certainty tentativeness b', 'tentativeness b perception', 'b perception comprising', 'perception comprising set', 'comprising set word', 'set word liwc', 'word liwc around', 'liwc around see', 'around see hear', 'see hear feel', 'hear feel percept', 'feel percept insight', 'percept insight relative', 'insight relative next', 'relative next consider', 'next consider four', 'consider four measure', 'four measure linguistic', 'measure linguistic style', 'linguistic style lexical', 'style lexical density', 'lexical density consisting', 'density consisting word', 'consisting word verb', 'word verb auxiliary', 'verb auxiliary verb', 'auxiliary verb noun', 'verb noun adjective', 'noun adjective identified', 'adjective identified using', 'identified using nltks', 'using nltks po', 'nltks po tagger', 'po tagger adverb', 'tagger adverb b', 'adverb b temporal', 'b temporal reference', 'temporal reference consisting', 'reference consisting past', 'consisting past present', 'past present future', 'present future tense', 'future tense c', 'tense c socialpersonal', 'c socialpersonal concern', 'socialpersonal concern word', 'concern word belonging', 'word belonging family', 'belonging family friend', 'family friend social', 'friend social work', 'social work health', 'work health human', 'health human religion', 'human religion bio', 'religion bio body', 'bio body money', 'body money achievement', 'money achievement home', 'achievement home sexual', 'home sexual interpersonal', 'sexual interpersonal awareness', 'interpersonal awareness focus', 'awareness focus word', 'focus word st', 'word st person', 'st person singular', 'person singular st', 'singular st person', 'st person plural', 'person plural nd', 'plural nd person', 'nd person rd', 'person rd person', 'rd person pronoun', 'person pronoun social', 'pronoun social attribute', 'social attribute utilized', 'attribute utilized variety', 'utilized variety content', 'variety content sharing', 'content sharing social', 'sharing social interaction', 'social interaction social', 'interaction social support', 'social support indicator', 'support indicator post', 'indicator post length', 'post length number', 'length number comment', 'number comment vote', 'comment vote difference', 'vote difference difference', 'difference difference upvotes', 'difference upvotes downvotes', 'upvotes downvotes divided', 'downvotes divided total', 'divided total upvotes', 'total upvotes downvotes', 'upvotes downvotes comment', 'downvotes comment arrival', 'comment arrival rate', 'arrival rate average', 'rate average time', 'average time difference', 'time difference two', 'difference two subsequent', 'two subsequent comment', 'subsequent comment post', 'comment post comment', 'post comment thread', 'comment thread time', 'thread time first', 'time first comment', 'first comment time', 'comment time elapsed', 'time elapsed first', 'elapsed first comment', 'first comment timestamp', 'comment timestamp corresponding', 'timestamp corresponding post', 'corresponding post median', 'post median comment', 'median comment length', 'comment length compute', 'length compute linguistic', 'compute linguistic measure', 'linguistic measure behavior', 'measure behavior post', 'behavior post level', 'post level value', 'level value measure', 'value measure given', 'measure given ratio', 'given ratio number', 'ratio number word', 'number word post', 'word post match', 'post match word', 'match word belonging', 'word belonging measure', 'belonging measure total', 'measure total number', 'total number word', 'number word post', 'word post measure', 'post measure take', 'measure take average', 'take average across', 'average across celebrity', 'across celebrity ensure', 'celebrity ensure suicide', 'ensure suicide event', 'suicide event equally', 'event equally weighted', 'equally weighted ie', 'weighted ie avoid', 'ie avoid skew', 'avoid skew due', 'skew due single', 'due single suicide', 'single suicide statistical', 'suicide statistical comparison', 'statistical comparison used', 'comparison used welch', 'used welch ttest', 'welch ttest negative', 'ttest negative tstatistic', 'negative tstatistic value', 'tstatistic value mean', 'value mean measure', 'mean measure increased', 'measure increased suicide']",,,,,,,,
https://aclanthology.org/W17-3110.pdf,0,This study aimed to examine the prevalence of affective micropatterns in social media posts and highlight differences in micropattern occurrence that might be relevant to quantifying mental health. Primarily we do this through comparison of users with anxiety disorders eating disorders schizophrenia suicide attempt history and their matched controls. We use a straightforward and well-understood method for sentiment analysis VADER (Hutto and Gilbert 2014) to produce a trinary label for each message: positive neutral or negative. VADER outputs a [0 1] score for each sentiment label; we use the label with the maximum score. Specifically we examined trajectories of posted emotional content in three subsequent tweets no more than three hours from earliest to latest. The same tweet will be counted in more than one over lapping micropattern if more than three tweets occur in the three-hour time window – so if 5 tweets occur in 3 hours 3 micropatterns will be recorded from those 5 tweets likewise for 4 tweets 2 micropatterns will be recorded. The potential overlap exists for both patients and neurotypical users and subsequent analyses (e.g. classifying users based on proportion of micropatterns) were designed to be robust to this property of overlapping micropattern generation. The number of sequential tweets to examine was chosen to minimize the complexity of the analysis while allowing significant variability to be observed. Critically we aimed for the resulting dimensions (i.e. number of distinct micropatterns) to be small enough for meaningful interpretation by clinical psychologists.,this study aimed to examine the prevalence of affective micropatterns in social medium post and highlight difference in micropattern occurrence that might be relevant to quantifying mental health primarily we do this through comparison of user with anxiety disorder eating disorder schizophrenia suicide attempt history and their matched control we use a straightforward and wellunderstood method for sentiment analysis vader hutto and gilbert to produce a trinary label for each message positive neutral or negative vader output a score for each sentiment label we use the label with the maximum score specifically we examined trajectory of posted emotional content in three subsequent tweet no more than three hour from earliest to latest the same tweet will be counted in more than one over lapping micropattern if more than three tweet occur in the threehour time window so if tweet occur in hour micropatterns will be recorded from those tweet likewise for tweet micropatterns will be recorded the potential overlap exists for both patient and neurotypical user and subsequent analysis eg classifying user based on proportion of micropatterns were designed to be robust to this property of overlapping micropattern generation the number of sequential tweet to examine wa chosen to minimize the complexity of the analysis while allowing significant variability to be observed critically we aimed for the resulting dimension ie number of distinct micropatterns to be small enough for meaningful interpretation by clinical psychologist,"['study', 'aimed', 'examine', 'prevalence', 'affective', 'micropatterns', 'social', 'medium', 'post', 'highlight', 'difference', 'micropattern', 'occurrence', 'might', 'relevant', 'quantifying', 'mental', 'health', 'primarily', 'comparison', 'user', 'anxiety', 'disorder', 'eating', 'disorder', 'schizophrenia', 'suicide', 'attempt', 'history', 'matched', 'control', 'use', 'straightforward', 'wellunderstood', 'method', 'sentiment', 'analysis', 'vader', 'hutto', 'gilbert', 'produce', 'trinary', 'label', 'message', 'positive', 'neutral', 'negative', 'vader', 'output', 'score', 'sentiment', 'label', 'use', 'label', 'maximum', 'score', 'specifically', 'examined', 'trajectory', 'posted', 'emotional', 'content', 'three', 'subsequent', 'tweet', 'three', 'hour', 'earliest', 'latest', 'tweet', 'counted', 'one', 'lapping', 'micropattern', 'three', 'tweet', 'occur', 'threehour', 'time', 'window', 'tweet', 'occur', 'hour', 'micropatterns', 'recorded', 'tweet', 'likewise', 'tweet', 'micropatterns', 'recorded', 'potential', 'overlap', 'exists', 'patient', 'neurotypical', 'user', 'subsequent', 'analysis', 'eg', 'classifying', 'user', 'based', 'proportion', 'micropatterns', 'designed', 'robust', 'property', 'overlapping', 'micropattern', 'generation', 'number', 'sequential', 'tweet', 'examine', 'wa', 'chosen', 'minimize', 'complexity', 'analysis', 'allowing', 'significant', 'variability', 'observed', 'critically', 'aimed', 'resulting', 'dimension', 'ie', 'number', 'distinct', 'micropatterns', 'small', 'enough', 'meaningful', 'interpretation', 'clinical', 'psychologist']","['study aimed', 'aimed examine', 'examine prevalence', 'prevalence affective', 'affective micropatterns', 'micropatterns social', 'social medium', 'medium post', 'post highlight', 'highlight difference', 'difference micropattern', 'micropattern occurrence', 'occurrence might', 'might relevant', 'relevant quantifying', 'quantifying mental', 'mental health', 'health primarily', 'primarily comparison', 'comparison user', 'user anxiety', 'anxiety disorder', 'disorder eating', 'eating disorder', 'disorder schizophrenia', 'schizophrenia suicide', 'suicide attempt', 'attempt history', 'history matched', 'matched control', 'control use', 'use straightforward', 'straightforward wellunderstood', 'wellunderstood method', 'method sentiment', 'sentiment analysis', 'analysis vader', 'vader hutto', 'hutto gilbert', 'gilbert produce', 'produce trinary', 'trinary label', 'label message', 'message positive', 'positive neutral', 'neutral negative', 'negative vader', 'vader output', 'output score', 'score sentiment', 'sentiment label', 'label use', 'use label', 'label maximum', 'maximum score', 'score specifically', 'specifically examined', 'examined trajectory', 'trajectory posted', 'posted emotional', 'emotional content', 'content three', 'three subsequent', 'subsequent tweet', 'tweet three', 'three hour', 'hour earliest', 'earliest latest', 'latest tweet', 'tweet counted', 'counted one', 'one lapping', 'lapping micropattern', 'micropattern three', 'three tweet', 'tweet occur', 'occur threehour', 'threehour time', 'time window', 'window tweet', 'tweet occur', 'occur hour', 'hour micropatterns', 'micropatterns recorded', 'recorded tweet', 'tweet likewise', 'likewise tweet', 'tweet micropatterns', 'micropatterns recorded', 'recorded potential', 'potential overlap', 'overlap exists', 'exists patient', 'patient neurotypical', 'neurotypical user', 'user subsequent', 'subsequent analysis', 'analysis eg', 'eg classifying', 'classifying user', 'user based', 'based proportion', 'proportion micropatterns', 'micropatterns designed', 'designed robust', 'robust property', 'property overlapping', 'overlapping micropattern', 'micropattern generation', 'generation number', 'number sequential', 'sequential tweet', 'tweet examine', 'examine wa', 'wa chosen', 'chosen minimize', 'minimize complexity', 'complexity analysis', 'analysis allowing', 'allowing significant', 'significant variability', 'variability observed', 'observed critically', 'critically aimed', 'aimed resulting', 'resulting dimension', 'dimension ie', 'ie number', 'number distinct', 'distinct micropatterns', 'micropatterns small', 'small enough', 'enough meaningful', 'meaningful interpretation', 'interpretation clinical', 'clinical psychologist']","['study aimed examine', 'aimed examine prevalence', 'examine prevalence affective', 'prevalence affective micropatterns', 'affective micropatterns social', 'micropatterns social medium', 'social medium post', 'medium post highlight', 'post highlight difference', 'highlight difference micropattern', 'difference micropattern occurrence', 'micropattern occurrence might', 'occurrence might relevant', 'might relevant quantifying', 'relevant quantifying mental', 'quantifying mental health', 'mental health primarily', 'health primarily comparison', 'primarily comparison user', 'comparison user anxiety', 'user anxiety disorder', 'anxiety disorder eating', 'disorder eating disorder', 'eating disorder schizophrenia', 'disorder schizophrenia suicide', 'schizophrenia suicide attempt', 'suicide attempt history', 'attempt history matched', 'history matched control', 'matched control use', 'control use straightforward', 'use straightforward wellunderstood', 'straightforward wellunderstood method', 'wellunderstood method sentiment', 'method sentiment analysis', 'sentiment analysis vader', 'analysis vader hutto', 'vader hutto gilbert', 'hutto gilbert produce', 'gilbert produce trinary', 'produce trinary label', 'trinary label message', 'label message positive', 'message positive neutral', 'positive neutral negative', 'neutral negative vader', 'negative vader output', 'vader output score', 'output score sentiment', 'score sentiment label', 'sentiment label use', 'label use label', 'use label maximum', 'label maximum score', 'maximum score specifically', 'score specifically examined', 'specifically examined trajectory', 'examined trajectory posted', 'trajectory posted emotional', 'posted emotional content', 'emotional content three', 'content three subsequent', 'three subsequent tweet', 'subsequent tweet three', 'tweet three hour', 'three hour earliest', 'hour earliest latest', 'earliest latest tweet', 'latest tweet counted', 'tweet counted one', 'counted one lapping', 'one lapping micropattern', 'lapping micropattern three', 'micropattern three tweet', 'three tweet occur', 'tweet occur threehour', 'occur threehour time', 'threehour time window', 'time window tweet', 'window tweet occur', 'tweet occur hour', 'occur hour micropatterns', 'hour micropatterns recorded', 'micropatterns recorded tweet', 'recorded tweet likewise', 'tweet likewise tweet', 'likewise tweet micropatterns', 'tweet micropatterns recorded', 'micropatterns recorded potential', 'recorded potential overlap', 'potential overlap exists', 'overlap exists patient', 'exists patient neurotypical', 'patient neurotypical user', 'neurotypical user subsequent', 'user subsequent analysis', 'subsequent analysis eg', 'analysis eg classifying', 'eg classifying user', 'classifying user based', 'user based proportion', 'based proportion micropatterns', 'proportion micropatterns designed', 'micropatterns designed robust', 'designed robust property', 'robust property overlapping', 'property overlapping micropattern', 'overlapping micropattern generation', 'micropattern generation number', 'generation number sequential', 'number sequential tweet', 'sequential tweet examine', 'tweet examine wa', 'examine wa chosen', 'wa chosen minimize', 'chosen minimize complexity', 'minimize complexity analysis', 'complexity analysis allowing', 'analysis allowing significant', 'allowing significant variability', 'significant variability observed', 'variability observed critically', 'observed critically aimed', 'critically aimed resulting', 'aimed resulting dimension', 'resulting dimension ie', 'dimension ie number', 'ie number distinct', 'number distinct micropatterns', 'distinct micropatterns small', 'micropatterns small enough', 'small enough meaningful', 'enough meaningful interpretation', 'meaningful interpretation clinical', 'interpretation clinical psychologist']",,,,,,,,
https://dl.acm.org/doi/abs/10.1145/2998181.2998220,1,For inferring the country name corresponding to a user we adopted a stepwise approach as follows: First we cleaned the location strings reported in the location field of Twitter user profiles — including normalization of character case and removal of non-English word roots. Then we performed a location matching exercise wherein we split the cleaned location strings into single words iteratively created all possible 5-to-1-gram substrings and then matched each n-gram to a location database based on GeoNames (http://www.geonames.org) preferring larger n-grams over smaller ones (“New York City” to “York”). Third we performed disambiguation by computing geographic distances between matched text-adjacent places and assigned high likelihood to those matches that are close to each other geographically. We then sorted equally likely location alternatives by population size and choose the top one. We note that compared to geo-located Twitter posts this location field string lookup method has been known to yield better coverage in social media data [22].,for inferring the country name corresponding to a user we adopted a stepwise approach a follows first we cleaned the location string reported in the location field of twitter user profile including normalization of character case and removal of nonenglish word root then we performed a location matching exercise wherein we split the cleaned location string into single word iteratively created all possible togram substring and then matched each ngram to a location database based on geonames httpwwwgeonamesorg preferring larger ngrams over smaller one new york city to york third we performed disambiguation by computing geographic distance between matched textadjacent place and assigned high likelihood to those match that are close to each other geographically we then sorted equally likely location alternative by population size and choose the top one we note that compared to geolocated twitter post this location field string lookup method ha been known to yield better coverage in social medium data,"['inferring', 'country', 'name', 'corresponding', 'user', 'adopted', 'stepwise', 'approach', 'follows', 'first', 'cleaned', 'location', 'string', 'reported', 'location', 'field', 'twitter', 'user', 'profile', 'including', 'normalization', 'character', 'case', 'removal', 'nonenglish', 'word', 'root', 'performed', 'location', 'matching', 'exercise', 'wherein', 'split', 'cleaned', 'location', 'string', 'single', 'word', 'iteratively', 'created', 'possible', 'togram', 'substring', 'matched', 'ngram', 'location', 'database', 'based', 'geonames', 'httpwwwgeonamesorg', 'preferring', 'larger', 'ngrams', 'smaller', 'one', 'new', 'york', 'city', 'york', 'third', 'performed', 'disambiguation', 'computing', 'geographic', 'distance', 'matched', 'textadjacent', 'place', 'assigned', 'high', 'likelihood', 'match', 'close', 'geographically', 'sorted', 'equally', 'likely', 'location', 'alternative', 'population', 'size', 'choose', 'top', 'one', 'note', 'compared', 'geolocated', 'twitter', 'post', 'location', 'field', 'string', 'lookup', 'method', 'ha', 'known', 'yield', 'better', 'coverage', 'social', 'medium', 'data']","['inferring country', 'country name', 'name corresponding', 'corresponding user', 'user adopted', 'adopted stepwise', 'stepwise approach', 'approach follows', 'follows first', 'first cleaned', 'cleaned location', 'location string', 'string reported', 'reported location', 'location field', 'field twitter', 'twitter user', 'user profile', 'profile including', 'including normalization', 'normalization character', 'character case', 'case removal', 'removal nonenglish', 'nonenglish word', 'word root', 'root performed', 'performed location', 'location matching', 'matching exercise', 'exercise wherein', 'wherein split', 'split cleaned', 'cleaned location', 'location string', 'string single', 'single word', 'word iteratively', 'iteratively created', 'created possible', 'possible togram', 'togram substring', 'substring matched', 'matched ngram', 'ngram location', 'location database', 'database based', 'based geonames', 'geonames httpwwwgeonamesorg', 'httpwwwgeonamesorg preferring', 'preferring larger', 'larger ngrams', 'ngrams smaller', 'smaller one', 'one new', 'new york', 'york city', 'city york', 'york third', 'third performed', 'performed disambiguation', 'disambiguation computing', 'computing geographic', 'geographic distance', 'distance matched', 'matched textadjacent', 'textadjacent place', 'place assigned', 'assigned high', 'high likelihood', 'likelihood match', 'match close', 'close geographically', 'geographically sorted', 'sorted equally', 'equally likely', 'likely location', 'location alternative', 'alternative population', 'population size', 'size choose', 'choose top', 'top one', 'one note', 'note compared', 'compared geolocated', 'geolocated twitter', 'twitter post', 'post location', 'location field', 'field string', 'string lookup', 'lookup method', 'method ha', 'ha known', 'known yield', 'yield better', 'better coverage', 'coverage social', 'social medium', 'medium data']","['inferring country name', 'country name corresponding', 'name corresponding user', 'corresponding user adopted', 'user adopted stepwise', 'adopted stepwise approach', 'stepwise approach follows', 'approach follows first', 'follows first cleaned', 'first cleaned location', 'cleaned location string', 'location string reported', 'string reported location', 'reported location field', 'location field twitter', 'field twitter user', 'twitter user profile', 'user profile including', 'profile including normalization', 'including normalization character', 'normalization character case', 'character case removal', 'case removal nonenglish', 'removal nonenglish word', 'nonenglish word root', 'word root performed', 'root performed location', 'performed location matching', 'location matching exercise', 'matching exercise wherein', 'exercise wherein split', 'wherein split cleaned', 'split cleaned location', 'cleaned location string', 'location string single', 'string single word', 'single word iteratively', 'word iteratively created', 'iteratively created possible', 'created possible togram', 'possible togram substring', 'togram substring matched', 'substring matched ngram', 'matched ngram location', 'ngram location database', 'location database based', 'database based geonames', 'based geonames httpwwwgeonamesorg', 'geonames httpwwwgeonamesorg preferring', 'httpwwwgeonamesorg preferring larger', 'preferring larger ngrams', 'larger ngrams smaller', 'ngrams smaller one', 'smaller one new', 'one new york', 'new york city', 'york city york', 'city york third', 'york third performed', 'third performed disambiguation', 'performed disambiguation computing', 'disambiguation computing geographic', 'computing geographic distance', 'geographic distance matched', 'distance matched textadjacent', 'matched textadjacent place', 'textadjacent place assigned', 'place assigned high', 'assigned high likelihood', 'high likelihood match', 'likelihood match close', 'match close geographically', 'close geographically sorted', 'geographically sorted equally', 'sorted equally likely', 'equally likely location', 'likely location alternative', 'location alternative population', 'alternative population size', 'population size choose', 'size choose top', 'choose top one', 'top one note', 'one note compared', 'note compared geolocated', 'compared geolocated twitter', 'geolocated twitter post', 'twitter post location', 'post location field', 'location field string', 'field string lookup', 'string lookup method', 'lookup method ha', 'method ha known', 'ha known yield', 'known yield better', 'yield better coverage', 'better coverage social', 'coverage social medium', 'social medium data']",,,,,,,,
https://www.jmir.org/2019/6/e14199/,0,The selection of the tweets and their users was based on the filtered real-time streaming support provided by the Twitter API. In the first step we selected the users who showed potential signs of depression on Twitter on the basis of the 20 most frequent words in Spanish expressed by patients suffering from depression in clinical settings. These words were jointly identified and selected by a psychologist and a family physician with clinical experience and were based on the definition and general features of depression according to the Diagnostic and Statistical Manual of Mental Disorders [42]. The list of words used and their English translations are shown in Textbox 1. During June 2018 1470000 tweets including 1 or more occurrences of the words listed in Textbox 1 were collected. From this collection of tweets and to select the users who publicly stated in the textual description associated to their profile that they suffered from depression all the profile descriptions including 1 or more occurrences of the word “depr” and all the possible derivations related to the word depression in Spanish such as “depre” “depresión” “depresivo” “depresiva” “deprimido” and “deprimida” were considered. From the 720 users who included 1 or more of these words in their description profile 90 users who stated they suffered from depression or were receiving treatment for depression were selected for the analysis. This selection was performed by a psychologist verifying that the statements were related to real expressions of depression excluding quotes jokes or fake ones. For each of these depressed Twitter users we collected all the most recent tweets from their timeline up to a maximum of about 3200 tweets. Thus a total of 189669 tweets were collected a figure that was reduced to 140946 after discarding the retweets. These 140946 tweets constituted the depressive users dataset. Examples of sentences appearing in the user profiles that were used for selecting the depressive users are: “Paciente psiquiátrico con depresión crónica” (Psychiatric patient with chronic depression; example of a profile sentence that indicates depression). “Colecciono errores traducidos a tweets depresivos y a uno que otro impulso de amor” (I gather errors translated into depressing tweets and into one or another love impulse; example of a profile sentence that does not indicate depression). Once the users with profile sentences indicating depression had been retrieved their Twitter timelines were collected. Only those users having in their timeline at least 10 tweets that suggested signs of depression were retained for further analyses. For each user the selection of these tweets was performed by manually inspecting the tweets of the user’s complete timeline in reverse temporal order starting from the most recent one to the oldest tweet of the timeline retrieved by means of the Twitter API . Finally a total number of 1000 tweets issued by the 90 depressive users suggesting signs of depression were detected and used for the analysis. This set of tweets provided us with the depressive tweets dataset which was used to analyze linguistic features of tweets showing signs of depression. It has to be mentioned that these 1000 tweets were not to be included in the depressive users dataset (see Figure 1). At the same time more than 97500000 tweets were also collected in June 2018: such tweets were gathered by listening to the public Twitter stream during this time span by only considering tweets with Spanish textual contents (as detected by Twitter language identification support). Given that Twitter requires more restrictive filters than just the language of the tweets we used a list of the most frequently used Spanish words (stopwords) to retrieve all tweets that included 1 or more of these words. The vast majority of Spanish tweets should match this criterion. A sample of 450 users who did not mention in their profile the word depression and its derivations were selected randomly from the 97500000 tweets. The complete timelines of these users were compiled (1141021 tweets) which were reduced to 712589 once retweets were removed. These 712589 tweets constituted the control dataset. To identify the language of a tweet we relied on the language automatically identified by Twitter for each tweet selecting tweets in Spanish. It has to be noted that these data can contain some tweets from unidentified depressive users.,the selection of the tweet and their user wa based on the filtered realtime streaming support provided by the twitter api in the first step we selected the user who showed potential sign of depression on twitter on the basis of the most frequent word in spanish expressed by patient suffering from depression in clinical setting these word were jointly identified and selected by a psychologist and a family physician with clinical experience and were based on the definition and general feature of depression according to the diagnostic and statistical manual of mental disorder the list of word used and their english translation are shown in textbox during june tweet including or more occurrence of the word listed in textbox were collected from this collection of tweet and to select the user who publicly stated in the textual description associated to their profile that they suffered from depression all the profile description including or more occurrence of the word depr and all the possible derivation related to the word depression in spanish such a depre depresin depresivo depresiva deprimido and deprimida were considered from the user who included or more of these word in their description profile user who stated they suffered from depression or were receiving treatment for depression were selected for the analysis this selection wa performed by a psychologist verifying that the statement were related to real expression of depression excluding quote joke or fake one for each of these depressed twitter user we collected all the most recent tweet from their timeline up to a maximum of about tweet thus a total of tweet were collected a figure that wa reduced to after discarding the retweets these tweet constituted the depressive user dataset example of sentence appearing in the user profile that were used for selecting the depressive user are paciente psiquitrico con depresin crnica psychiatric patient with chronic depression example of a profile sentence that indicates depression colecciono errores traducidos a tweet depresivos y a uno que otro impulso de amor i gather error translated into depressing tweet and into one or another love impulse example of a profile sentence that doe not indicate depression once the user with profile sentence indicating depression had been retrieved their twitter timeline were collected only those user having in their timeline at least tweet that suggested sign of depression were retained for further analysis for each user the selection of these tweet wa performed by manually inspecting the tweet of the user complete timeline in reverse temporal order starting from the most recent one to the oldest tweet of the timeline retrieved by mean of the twitter api finally a total number of tweet issued by the depressive user suggesting sign of depression were detected and used for the analysis this set of tweet provided u with the depressive tweet dataset which wa used to analyze linguistic feature of tweet showing sign of depression it ha to be mentioned that these tweet were not to be included in the depressive user dataset see figure at the same time more than tweet were also collected in june such tweet were gathered by listening to the public twitter stream during this time span by only considering tweet with spanish textual content a detected by twitter language identification support given that twitter requires more restrictive filter than just the language of the tweet we used a list of the most frequently used spanish word stopwords to retrieve all tweet that included or more of these word the vast majority of spanish tweet should match this criterion a sample of user who did not mention in their profile the word depression and it derivation were selected randomly from the tweet the complete timeline of these user were compiled tweet which were reduced to once retweets were removed these tweet constituted the control dataset to identify the language of a tweet we relied on the language automatically identified by twitter for each tweet selecting tweet in spanish it ha to be noted that these data can contain some tweet from unidentified depressive user,"['selection', 'tweet', 'user', 'wa', 'based', 'filtered', 'realtime', 'streaming', 'support', 'provided', 'twitter', 'api', 'first', 'step', 'selected', 'user', 'showed', 'potential', 'sign', 'depression', 'twitter', 'basis', 'frequent', 'word', 'spanish', 'expressed', 'patient', 'suffering', 'depression', 'clinical', 'setting', 'word', 'jointly', 'identified', 'selected', 'psychologist', 'family', 'physician', 'clinical', 'experience', 'based', 'definition', 'general', 'feature', 'depression', 'according', 'diagnostic', 'statistical', 'manual', 'mental', 'disorder', 'list', 'word', 'used', 'english', 'translation', 'shown', 'textbox', 'june', 'tweet', 'including', 'occurrence', 'word', 'listed', 'textbox', 'collected', 'collection', 'tweet', 'select', 'user', 'publicly', 'stated', 'textual', 'description', 'associated', 'profile', 'suffered', 'depression', 'profile', 'description', 'including', 'occurrence', 'word', 'depr', 'possible', 'derivation', 'related', 'word', 'depression', 'spanish', 'depre', 'depresin', 'depresivo', 'depresiva', 'deprimido', 'deprimida', 'considered', 'user', 'included', 'word', 'description', 'profile', 'user', 'stated', 'suffered', 'depression', 'receiving', 'treatment', 'depression', 'selected', 'analysis', 'selection', 'wa', 'performed', 'psychologist', 'verifying', 'statement', 'related', 'real', 'expression', 'depression', 'excluding', 'quote', 'joke', 'fake', 'one', 'depressed', 'twitter', 'user', 'collected', 'recent', 'tweet', 'timeline', 'maximum', 'tweet', 'thus', 'total', 'tweet', 'collected', 'figure', 'wa', 'reduced', 'discarding', 'retweets', 'tweet', 'constituted', 'depressive', 'user', 'dataset', 'example', 'sentence', 'appearing', 'user', 'profile', 'used', 'selecting', 'depressive', 'user', 'paciente', 'psiquitrico', 'con', 'depresin', 'crnica', 'psychiatric', 'patient', 'chronic', 'depression', 'example', 'profile', 'sentence', 'indicates', 'depression', 'colecciono', 'errores', 'traducidos', 'tweet', 'depresivos', 'uno', 'que', 'otro', 'impulso', 'de', 'amor', 'gather', 'error', 'translated', 'depressing', 'tweet', 'one', 'another', 'love', 'impulse', 'example', 'profile', 'sentence', 'doe', 'indicate', 'depression', 'user', 'profile', 'sentence', 'indicating', 'depression', 'retrieved', 'twitter', 'timeline', 'collected', 'user', 'timeline', 'least', 'tweet', 'suggested', 'sign', 'depression', 'retained', 'analysis', 'user', 'selection', 'tweet', 'wa', 'performed', 'manually', 'inspecting', 'tweet', 'user', 'complete', 'timeline', 'reverse', 'temporal', 'order', 'starting', 'recent', 'one', 'oldest', 'tweet', 'timeline', 'retrieved', 'mean', 'twitter', 'api', 'finally', 'total', 'number', 'tweet', 'issued', 'depressive', 'user', 'suggesting', 'sign', 'depression', 'detected', 'used', 'analysis', 'set', 'tweet', 'provided', 'u', 'depressive', 'tweet', 'dataset', 'wa', 'used', 'analyze', 'linguistic', 'feature', 'tweet', 'showing', 'sign', 'depression', 'ha', 'mentioned', 'tweet', 'included', 'depressive', 'user', 'dataset', 'see', 'figure', 'time', 'tweet', 'also', 'collected', 'june', 'tweet', 'gathered', 'listening', 'public', 'twitter', 'stream', 'time', 'span', 'considering', 'tweet', 'spanish', 'textual', 'content', 'detected', 'twitter', 'language', 'identification', 'support', 'given', 'twitter', 'requires', 'restrictive', 'filter', 'language', 'tweet', 'used', 'list', 'frequently', 'used', 'spanish', 'word', 'stopwords', 'retrieve', 'tweet', 'included', 'word', 'vast', 'majority', 'spanish', 'tweet', 'match', 'criterion', 'sample', 'user', 'mention', 'profile', 'word', 'depression', 'derivation', 'selected', 'randomly', 'tweet', 'complete', 'timeline', 'user', 'compiled', 'tweet', 'reduced', 'retweets', 'removed', 'tweet', 'constituted', 'control', 'dataset', 'identify', 'language', 'tweet', 'relied', 'language', 'automatically', 'identified', 'twitter', 'tweet', 'selecting', 'tweet', 'spanish', 'ha', 'noted', 'data', 'contain', 'tweet', 'unidentified', 'depressive', 'user']","['selection tweet', 'tweet user', 'user wa', 'wa based', 'based filtered', 'filtered realtime', 'realtime streaming', 'streaming support', 'support provided', 'provided twitter', 'twitter api', 'api first', 'first step', 'step selected', 'selected user', 'user showed', 'showed potential', 'potential sign', 'sign depression', 'depression twitter', 'twitter basis', 'basis frequent', 'frequent word', 'word spanish', 'spanish expressed', 'expressed patient', 'patient suffering', 'suffering depression', 'depression clinical', 'clinical setting', 'setting word', 'word jointly', 'jointly identified', 'identified selected', 'selected psychologist', 'psychologist family', 'family physician', 'physician clinical', 'clinical experience', 'experience based', 'based definition', 'definition general', 'general feature', 'feature depression', 'depression according', 'according diagnostic', 'diagnostic statistical', 'statistical manual', 'manual mental', 'mental disorder', 'disorder list', 'list word', 'word used', 'used english', 'english translation', 'translation shown', 'shown textbox', 'textbox june', 'june tweet', 'tweet including', 'including occurrence', 'occurrence word', 'word listed', 'listed textbox', 'textbox collected', 'collected collection', 'collection tweet', 'tweet select', 'select user', 'user publicly', 'publicly stated', 'stated textual', 'textual description', 'description associated', 'associated profile', 'profile suffered', 'suffered depression', 'depression profile', 'profile description', 'description including', 'including occurrence', 'occurrence word', 'word depr', 'depr possible', 'possible derivation', 'derivation related', 'related word', 'word depression', 'depression spanish', 'spanish depre', 'depre depresin', 'depresin depresivo', 'depresivo depresiva', 'depresiva deprimido', 'deprimido deprimida', 'deprimida considered', 'considered user', 'user included', 'included word', 'word description', 'description profile', 'profile user', 'user stated', 'stated suffered', 'suffered depression', 'depression receiving', 'receiving treatment', 'treatment depression', 'depression selected', 'selected analysis', 'analysis selection', 'selection wa', 'wa performed', 'performed psychologist', 'psychologist verifying', 'verifying statement', 'statement related', 'related real', 'real expression', 'expression depression', 'depression excluding', 'excluding quote', 'quote joke', 'joke fake', 'fake one', 'one depressed', 'depressed twitter', 'twitter user', 'user collected', 'collected recent', 'recent tweet', 'tweet timeline', 'timeline maximum', 'maximum tweet', 'tweet thus', 'thus total', 'total tweet', 'tweet collected', 'collected figure', 'figure wa', 'wa reduced', 'reduced discarding', 'discarding retweets', 'retweets tweet', 'tweet constituted', 'constituted depressive', 'depressive user', 'user dataset', 'dataset example', 'example sentence', 'sentence appearing', 'appearing user', 'user profile', 'profile used', 'used selecting', 'selecting depressive', 'depressive user', 'user paciente', 'paciente psiquitrico', 'psiquitrico con', 'con depresin', 'depresin crnica', 'crnica psychiatric', 'psychiatric patient', 'patient chronic', 'chronic depression', 'depression example', 'example profile', 'profile sentence', 'sentence indicates', 'indicates depression', 'depression colecciono', 'colecciono errores', 'errores traducidos', 'traducidos tweet', 'tweet depresivos', 'depresivos uno', 'uno que', 'que otro', 'otro impulso', 'impulso de', 'de amor', 'amor gather', 'gather error', 'error translated', 'translated depressing', 'depressing tweet', 'tweet one', 'one another', 'another love', 'love impulse', 'impulse example', 'example profile', 'profile sentence', 'sentence doe', 'doe indicate', 'indicate depression', 'depression user', 'user profile', 'profile sentence', 'sentence indicating', 'indicating depression', 'depression retrieved', 'retrieved twitter', 'twitter timeline', 'timeline collected', 'collected user', 'user timeline', 'timeline least', 'least tweet', 'tweet suggested', 'suggested sign', 'sign depression', 'depression retained', 'retained analysis', 'analysis user', 'user selection', 'selection tweet', 'tweet wa', 'wa performed', 'performed manually', 'manually inspecting', 'inspecting tweet', 'tweet user', 'user complete', 'complete timeline', 'timeline reverse', 'reverse temporal', 'temporal order', 'order starting', 'starting recent', 'recent one', 'one oldest', 'oldest tweet', 'tweet timeline', 'timeline retrieved', 'retrieved mean', 'mean twitter', 'twitter api', 'api finally', 'finally total', 'total number', 'number tweet', 'tweet issued', 'issued depressive', 'depressive user', 'user suggesting', 'suggesting sign', 'sign depression', 'depression detected', 'detected used', 'used analysis', 'analysis set', 'set tweet', 'tweet provided', 'provided u', 'u depressive', 'depressive tweet', 'tweet dataset', 'dataset wa', 'wa used', 'used analyze', 'analyze linguistic', 'linguistic feature', 'feature tweet', 'tweet showing', 'showing sign', 'sign depression', 'depression ha', 'ha mentioned', 'mentioned tweet', 'tweet included', 'included depressive', 'depressive user', 'user dataset', 'dataset see', 'see figure', 'figure time', 'time tweet', 'tweet also', 'also collected', 'collected june', 'june tweet', 'tweet gathered', 'gathered listening', 'listening public', 'public twitter', 'twitter stream', 'stream time', 'time span', 'span considering', 'considering tweet', 'tweet spanish', 'spanish textual', 'textual content', 'content detected', 'detected twitter', 'twitter language', 'language identification', 'identification support', 'support given', 'given twitter', 'twitter requires', 'requires restrictive', 'restrictive filter', 'filter language', 'language tweet', 'tweet used', 'used list', 'list frequently', 'frequently used', 'used spanish', 'spanish word', 'word stopwords', 'stopwords retrieve', 'retrieve tweet', 'tweet included', 'included word', 'word vast', 'vast majority', 'majority spanish', 'spanish tweet', 'tweet match', 'match criterion', 'criterion sample', 'sample user', 'user mention', 'mention profile', 'profile word', 'word depression', 'depression derivation', 'derivation selected', 'selected randomly', 'randomly tweet', 'tweet complete', 'complete timeline', 'timeline user', 'user compiled', 'compiled tweet', 'tweet reduced', 'reduced retweets', 'retweets removed', 'removed tweet', 'tweet constituted', 'constituted control', 'control dataset', 'dataset identify', 'identify language', 'language tweet', 'tweet relied', 'relied language', 'language automatically', 'automatically identified', 'identified twitter', 'twitter tweet', 'tweet selecting', 'selecting tweet', 'tweet spanish', 'spanish ha', 'ha noted', 'noted data', 'data contain', 'contain tweet', 'tweet unidentified', 'unidentified depressive', 'depressive user']","['selection tweet user', 'tweet user wa', 'user wa based', 'wa based filtered', 'based filtered realtime', 'filtered realtime streaming', 'realtime streaming support', 'streaming support provided', 'support provided twitter', 'provided twitter api', 'twitter api first', 'api first step', 'first step selected', 'step selected user', 'selected user showed', 'user showed potential', 'showed potential sign', 'potential sign depression', 'sign depression twitter', 'depression twitter basis', 'twitter basis frequent', 'basis frequent word', 'frequent word spanish', 'word spanish expressed', 'spanish expressed patient', 'expressed patient suffering', 'patient suffering depression', 'suffering depression clinical', 'depression clinical setting', 'clinical setting word', 'setting word jointly', 'word jointly identified', 'jointly identified selected', 'identified selected psychologist', 'selected psychologist family', 'psychologist family physician', 'family physician clinical', 'physician clinical experience', 'clinical experience based', 'experience based definition', 'based definition general', 'definition general feature', 'general feature depression', 'feature depression according', 'depression according diagnostic', 'according diagnostic statistical', 'diagnostic statistical manual', 'statistical manual mental', 'manual mental disorder', 'mental disorder list', 'disorder list word', 'list word used', 'word used english', 'used english translation', 'english translation shown', 'translation shown textbox', 'shown textbox june', 'textbox june tweet', 'june tweet including', 'tweet including occurrence', 'including occurrence word', 'occurrence word listed', 'word listed textbox', 'listed textbox collected', 'textbox collected collection', 'collected collection tweet', 'collection tweet select', 'tweet select user', 'select user publicly', 'user publicly stated', 'publicly stated textual', 'stated textual description', 'textual description associated', 'description associated profile', 'associated profile suffered', 'profile suffered depression', 'suffered depression profile', 'depression profile description', 'profile description including', 'description including occurrence', 'including occurrence word', 'occurrence word depr', 'word depr possible', 'depr possible derivation', 'possible derivation related', 'derivation related word', 'related word depression', 'word depression spanish', 'depression spanish depre', 'spanish depre depresin', 'depre depresin depresivo', 'depresin depresivo depresiva', 'depresivo depresiva deprimido', 'depresiva deprimido deprimida', 'deprimido deprimida considered', 'deprimida considered user', 'considered user included', 'user included word', 'included word description', 'word description profile', 'description profile user', 'profile user stated', 'user stated suffered', 'stated suffered depression', 'suffered depression receiving', 'depression receiving treatment', 'receiving treatment depression', 'treatment depression selected', 'depression selected analysis', 'selected analysis selection', 'analysis selection wa', 'selection wa performed', 'wa performed psychologist', 'performed psychologist verifying', 'psychologist verifying statement', 'verifying statement related', 'statement related real', 'related real expression', 'real expression depression', 'expression depression excluding', 'depression excluding quote', 'excluding quote joke', 'quote joke fake', 'joke fake one', 'fake one depressed', 'one depressed twitter', 'depressed twitter user', 'twitter user collected', 'user collected recent', 'collected recent tweet', 'recent tweet timeline', 'tweet timeline maximum', 'timeline maximum tweet', 'maximum tweet thus', 'tweet thus total', 'thus total tweet', 'total tweet collected', 'tweet collected figure', 'collected figure wa', 'figure wa reduced', 'wa reduced discarding', 'reduced discarding retweets', 'discarding retweets tweet', 'retweets tweet constituted', 'tweet constituted depressive', 'constituted depressive user', 'depressive user dataset', 'user dataset example', 'dataset example sentence', 'example sentence appearing', 'sentence appearing user', 'appearing user profile', 'user profile used', 'profile used selecting', 'used selecting depressive', 'selecting depressive user', 'depressive user paciente', 'user paciente psiquitrico', 'paciente psiquitrico con', 'psiquitrico con depresin', 'con depresin crnica', 'depresin crnica psychiatric', 'crnica psychiatric patient', 'psychiatric patient chronic', 'patient chronic depression', 'chronic depression example', 'depression example profile', 'example profile sentence', 'profile sentence indicates', 'sentence indicates depression', 'indicates depression colecciono', 'depression colecciono errores', 'colecciono errores traducidos', 'errores traducidos tweet', 'traducidos tweet depresivos', 'tweet depresivos uno', 'depresivos uno que', 'uno que otro', 'que otro impulso', 'otro impulso de', 'impulso de amor', 'de amor gather', 'amor gather error', 'gather error translated', 'error translated depressing', 'translated depressing tweet', 'depressing tweet one', 'tweet one another', 'one another love', 'another love impulse', 'love impulse example', 'impulse example profile', 'example profile sentence', 'profile sentence doe', 'sentence doe indicate', 'doe indicate depression', 'indicate depression user', 'depression user profile', 'user profile sentence', 'profile sentence indicating', 'sentence indicating depression', 'indicating depression retrieved', 'depression retrieved twitter', 'retrieved twitter timeline', 'twitter timeline collected', 'timeline collected user', 'collected user timeline', 'user timeline least', 'timeline least tweet', 'least tweet suggested', 'tweet suggested sign', 'suggested sign depression', 'sign depression retained', 'depression retained analysis', 'retained analysis user', 'analysis user selection', 'user selection tweet', 'selection tweet wa', 'tweet wa performed', 'wa performed manually', 'performed manually inspecting', 'manually inspecting tweet', 'inspecting tweet user', 'tweet user complete', 'user complete timeline', 'complete timeline reverse', 'timeline reverse temporal', 'reverse temporal order', 'temporal order starting', 'order starting recent', 'starting recent one', 'recent one oldest', 'one oldest tweet', 'oldest tweet timeline', 'tweet timeline retrieved', 'timeline retrieved mean', 'retrieved mean twitter', 'mean twitter api', 'twitter api finally', 'api finally total', 'finally total number', 'total number tweet', 'number tweet issued', 'tweet issued depressive', 'issued depressive user', 'depressive user suggesting', 'user suggesting sign', 'suggesting sign depression', 'sign depression detected', 'depression detected used', 'detected used analysis', 'used analysis set', 'analysis set tweet', 'set tweet provided', 'tweet provided u', 'provided u depressive', 'u depressive tweet', 'depressive tweet dataset', 'tweet dataset wa', 'dataset wa used', 'wa used analyze', 'used analyze linguistic', 'analyze linguistic feature', 'linguistic feature tweet', 'feature tweet showing', 'tweet showing sign', 'showing sign depression', 'sign depression ha', 'depression ha mentioned', 'ha mentioned tweet', 'mentioned tweet included', 'tweet included depressive', 'included depressive user', 'depressive user dataset', 'user dataset see', 'dataset see figure', 'see figure time', 'figure time tweet', 'time tweet also', 'tweet also collected', 'also collected june', 'collected june tweet', 'june tweet gathered', 'tweet gathered listening', 'gathered listening public', 'listening public twitter', 'public twitter stream', 'twitter stream time', 'stream time span', 'time span considering', 'span considering tweet', 'considering tweet spanish', 'tweet spanish textual', 'spanish textual content', 'textual content detected', 'content detected twitter', 'detected twitter language', 'twitter language identification', 'language identification support', 'identification support given', 'support given twitter', 'given twitter requires', 'twitter requires restrictive', 'requires restrictive filter', 'restrictive filter language', 'filter language tweet', 'language tweet used', 'tweet used list', 'used list frequently', 'list frequently used', 'frequently used spanish', 'used spanish word', 'spanish word stopwords', 'word stopwords retrieve', 'stopwords retrieve tweet', 'retrieve tweet included', 'tweet included word', 'included word vast', 'word vast majority', 'vast majority spanish', 'majority spanish tweet', 'spanish tweet match', 'tweet match criterion', 'match criterion sample', 'criterion sample user', 'sample user mention', 'user mention profile', 'mention profile word', 'profile word depression', 'word depression derivation', 'depression derivation selected', 'derivation selected randomly', 'selected randomly tweet', 'randomly tweet complete', 'tweet complete timeline', 'complete timeline user', 'timeline user compiled', 'user compiled tweet', 'compiled tweet reduced', 'tweet reduced retweets', 'reduced retweets removed', 'retweets removed tweet', 'removed tweet constituted', 'tweet constituted control', 'constituted control dataset', 'control dataset identify', 'dataset identify language', 'identify language tweet', 'language tweet relied', 'tweet relied language', 'relied language automatically', 'language automatically identified', 'automatically identified twitter', 'identified twitter tweet', 'twitter tweet selecting', 'tweet selecting tweet', 'selecting tweet spanish', 'tweet spanish ha', 'spanish ha noted', 'ha noted data', 'noted data contain', 'data contain tweet', 'contain tweet unidentified', 'tweet unidentified depressive', 'unidentified depressive user']",,,,,,,,
https://aclanthology.org/W18-0608.pdf,0,Data was collected from 7 Cups of Tea an anonymous online chat-based peer support community for emotional distress1 . Users agree at signup that their data may be used for the purposes of research. All the data used for the current study was anonymous and securely stored. This research was performed in line with the ethical and privacy protocols outlined in detail in (Benton et al. 2017). Data from 7 Cups takes the form of written dialogue between users of the service and volunteers who are trained as “active listeners”. A fragment of an exchange between the user of the service (U) and the volunteer (V) might go as follows: For the analyses reported in this paper we used only text generated by users of the service not the volunteers providing peer support. Users who reported depression as their primary concern at sign up were eligible for inclusion in analyses. Our original sample was comprised of 23048 conversations involving 1937 unique users. Users were excluded from the sample if they did not indicate their culture or if they selected ‘Other’. This resulted in the exclusion of 199 and 130 users respectively. The original sample also included users identifying as Native American or American Indian. This group was excluded from analyses since the majority of the data among these users was not English. This resulted in the removal of 15 users leaving a total sample size of 1593.,data wa collected from cup of tea an anonymous online chatbased peer support community for emotional distress user agree at signup that their data may be used for the purpose of research all the data used for the current study wa anonymous and securely stored this research wa performed in line with the ethical and privacy protocol outlined in detail in benton et al data from cup take the form of written dialogue between user of the service and volunteer who are trained a active listener a fragment of an exchange between the user of the service u and the volunteer v might go a follows for the analysis reported in this paper we used only text generated by user of the service not the volunteer providing peer support user who reported depression a their primary concern at sign up were eligible for inclusion in analysis our original sample wa comprised of conversation involving unique user user were excluded from the sample if they did not indicate their culture or if they selected other this resulted in the exclusion of and user respectively the original sample also included user identifying a native american or american indian this group wa excluded from analysis since the majority of the data among these user wa not english this resulted in the removal of user leaving a total sample size of,"['data', 'wa', 'collected', 'cup', 'tea', 'anonymous', 'online', 'chatbased', 'peer', 'support', 'community', 'emotional', 'distress', 'user', 'agree', 'signup', 'data', 'may', 'used', 'purpose', 'research', 'data', 'used', 'current', 'study', 'wa', 'anonymous', 'securely', 'stored', 'research', 'wa', 'performed', 'line', 'ethical', 'privacy', 'protocol', 'outlined', 'detail', 'benton', 'et', 'al', 'data', 'cup', 'take', 'form', 'written', 'dialogue', 'user', 'service', 'volunteer', 'trained', 'active', 'listener', 'fragment', 'exchange', 'user', 'service', 'u', 'volunteer', 'v', 'might', 'go', 'follows', 'analysis', 'reported', 'paper', 'used', 'text', 'generated', 'user', 'service', 'volunteer', 'providing', 'peer', 'support', 'user', 'reported', 'depression', 'primary', 'concern', 'sign', 'eligible', 'inclusion', 'analysis', 'original', 'sample', 'wa', 'comprised', 'conversation', 'involving', 'unique', 'user', 'user', 'excluded', 'sample', 'indicate', 'culture', 'selected', 'resulted', 'exclusion', 'user', 'respectively', 'original', 'sample', 'also', 'included', 'user', 'identifying', 'native', 'american', 'american', 'indian', 'group', 'wa', 'excluded', 'analysis', 'since', 'majority', 'data', 'among', 'user', 'wa', 'english', 'resulted', 'removal', 'user', 'leaving', 'total', 'sample', 'size']","['data wa', 'wa collected', 'collected cup', 'cup tea', 'tea anonymous', 'anonymous online', 'online chatbased', 'chatbased peer', 'peer support', 'support community', 'community emotional', 'emotional distress', 'distress user', 'user agree', 'agree signup', 'signup data', 'data may', 'may used', 'used purpose', 'purpose research', 'research data', 'data used', 'used current', 'current study', 'study wa', 'wa anonymous', 'anonymous securely', 'securely stored', 'stored research', 'research wa', 'wa performed', 'performed line', 'line ethical', 'ethical privacy', 'privacy protocol', 'protocol outlined', 'outlined detail', 'detail benton', 'benton et', 'et al', 'al data', 'data cup', 'cup take', 'take form', 'form written', 'written dialogue', 'dialogue user', 'user service', 'service volunteer', 'volunteer trained', 'trained active', 'active listener', 'listener fragment', 'fragment exchange', 'exchange user', 'user service', 'service u', 'u volunteer', 'volunteer v', 'v might', 'might go', 'go follows', 'follows analysis', 'analysis reported', 'reported paper', 'paper used', 'used text', 'text generated', 'generated user', 'user service', 'service volunteer', 'volunteer providing', 'providing peer', 'peer support', 'support user', 'user reported', 'reported depression', 'depression primary', 'primary concern', 'concern sign', 'sign eligible', 'eligible inclusion', 'inclusion analysis', 'analysis original', 'original sample', 'sample wa', 'wa comprised', 'comprised conversation', 'conversation involving', 'involving unique', 'unique user', 'user user', 'user excluded', 'excluded sample', 'sample indicate', 'indicate culture', 'culture selected', 'selected resulted', 'resulted exclusion', 'exclusion user', 'user respectively', 'respectively original', 'original sample', 'sample also', 'also included', 'included user', 'user identifying', 'identifying native', 'native american', 'american american', 'american indian', 'indian group', 'group wa', 'wa excluded', 'excluded analysis', 'analysis since', 'since majority', 'majority data', 'data among', 'among user', 'user wa', 'wa english', 'english resulted', 'resulted removal', 'removal user', 'user leaving', 'leaving total', 'total sample', 'sample size']","['data wa collected', 'wa collected cup', 'collected cup tea', 'cup tea anonymous', 'tea anonymous online', 'anonymous online chatbased', 'online chatbased peer', 'chatbased peer support', 'peer support community', 'support community emotional', 'community emotional distress', 'emotional distress user', 'distress user agree', 'user agree signup', 'agree signup data', 'signup data may', 'data may used', 'may used purpose', 'used purpose research', 'purpose research data', 'research data used', 'data used current', 'used current study', 'current study wa', 'study wa anonymous', 'wa anonymous securely', 'anonymous securely stored', 'securely stored research', 'stored research wa', 'research wa performed', 'wa performed line', 'performed line ethical', 'line ethical privacy', 'ethical privacy protocol', 'privacy protocol outlined', 'protocol outlined detail', 'outlined detail benton', 'detail benton et', 'benton et al', 'et al data', 'al data cup', 'data cup take', 'cup take form', 'take form written', 'form written dialogue', 'written dialogue user', 'dialogue user service', 'user service volunteer', 'service volunteer trained', 'volunteer trained active', 'trained active listener', 'active listener fragment', 'listener fragment exchange', 'fragment exchange user', 'exchange user service', 'user service u', 'service u volunteer', 'u volunteer v', 'volunteer v might', 'v might go', 'might go follows', 'go follows analysis', 'follows analysis reported', 'analysis reported paper', 'reported paper used', 'paper used text', 'used text generated', 'text generated user', 'generated user service', 'user service volunteer', 'service volunteer providing', 'volunteer providing peer', 'providing peer support', 'peer support user', 'support user reported', 'user reported depression', 'reported depression primary', 'depression primary concern', 'primary concern sign', 'concern sign eligible', 'sign eligible inclusion', 'eligible inclusion analysis', 'inclusion analysis original', 'analysis original sample', 'original sample wa', 'sample wa comprised', 'wa comprised conversation', 'comprised conversation involving', 'conversation involving unique', 'involving unique user', 'unique user user', 'user user excluded', 'user excluded sample', 'excluded sample indicate', 'sample indicate culture', 'indicate culture selected', 'culture selected resulted', 'selected resulted exclusion', 'resulted exclusion user', 'exclusion user respectively', 'user respectively original', 'respectively original sample', 'original sample also', 'sample also included', 'also included user', 'included user identifying', 'user identifying native', 'identifying native american', 'native american american', 'american american indian', 'american indian group', 'indian group wa', 'group wa excluded', 'wa excluded analysis', 'excluded analysis since', 'analysis since majority', 'since majority data', 'majority data among', 'data among user', 'among user wa', 'user wa english', 'wa english resulted', 'english resulted removal', 'resulted removal user', 'removal user leaving', 'user leaving total', 'leaving total sample', 'total sample size']",,,,,,,,
https://dl.acm.org/doi/pdf/10.1145/3359169,1,"4.2.1 General linguistic differences. To better understand the content-based differences between posts from individuals in the minority sample we compare the top unigrams bigrams and trigrams from each minority country to those from the majority sample. We filter out any words that were only one character as well as the Natural Language Toolkit’s built in stopwords [16]. The top 5 n-grams unigrams bigrams and trigrams can be seen in Table 3. N-grams that have been redacted are the usernames of Talklife users that were discussed in plain text by others on the website. We can make a few observations from the relative frequency of n-grams. First English is the most commonly used language within each community on Talklife as words from languages native to each country were not the majority of any of the n-grams. Second top-5 n-grams from the majority sample are not common to the top-5 n-grams in any of the minority sample suggesting a difference in how people express mental health. Even when people are talking about the same theme such as getting support they tend to use different phrases: the majority sample uses “need someone to talk to” while Indians prefer “like to talk to friends”. Third while much of the language most often expressed by users in the majority sample is support language expressing individual distress (such as “i’m” or “feel”) we find that people from the minority sample are more likely to talk about themselves in relation to other people as illustrated by the bolded n-grams in Table 3. For example individuals from the minority sample use the term “us” at a higher amount than those in the majority sample. Further individuals from India often talk about wanting or needing friends whereas individuals from Malaysia and Filipino refer to loneliness (using the word “alone”) more often than people in the majority sample. This use of terms related to interpersonal connections to express mental distress is seen in India [73] Malaysia [92] and the Philippines [35] when individuals experiencing distress are asked to describe how they are feeling. Our finding extends this work in the context of online mental health support communities. Finally references to religion (such as “god” or “pray find peace”) are more common in posts from Malaysia and the Philippines. This follows past research on the expression of mental health in both Malaysia [65] and the Philippines [55 84] showing that religion is often used as a foundation for how people from these cultural backgrounds express mental health concerns and for how people support one another. The higher presence of references to religion among users from Malaysia and the Philippines might also suggest that index posts that discuss religion but do not specifically discuss distress might actually be one culturally-sanctioned method of signposting a state of mental distress as seen in past research in offline contexts [24 55]. 4.2.2 Differences in Clinical Language Use. Based on past research showing that individuals from minority countries often express mental distress in non-clinical terms [52 53 64] we examine the use of clinical language on online mental health support communities. For this analysis of clinical language around mental distress from different countries we use data from the top 25% of users based on number of posts on Talklife to analyze a greater number of posts. As Table 4 shows the amount of clinical language consistently differs between the majority sample and minority samples. We find that clinical language is used less frequently in posts from the minority sample than in the majority sample. For Indians Malaysians and Filipinos 14.2-17.5% of index posts have clinical language respectively compared to 22.2% for the majority sample. We see a similar but smaller difference for support responses: 6.6-7.9% of support responses tend to have clinical language for the minority sample compared to 8.6% for the majority sample.These differences are significant with a p < .01 unless otherwise indicated.3 For all countries though we find consistently that support responses have less amounts of clinical language than index posts. However if we look at the frequency of clinical language within posts the difference between the minority sample and the majority sample is small. Combined with the evidence that the fraction of posts with at least one use of clinical language is substantially lower for the minority sample this implies that there must be higher use of clinical language terms per post for each post from the minority sample that does contain clinical language. This implication is verified by the last two rows of Table 4: when we restrict our analysis to only those posts that do contain clinical language posts from the minority sample use more clinical mental health language in comparison to the majority sample for both index posts and support responses with a statistically significant difference at p = .01. Thus while a smaller fraction of posts in the minority sample use clinical language overall there is a big variation in its use. Posts that do use clinical language tend to use many more terms per post than those from majority countries suggesting that people who do use clinical language from minority countries are participating in a standard and globalized language around describing clinical mental health. Table 5 confirms the above result when we look at the top unigrams bigrams and trigrams of clinical language used by people from different countries. The percentages for each term or n-gram correspond to the fraction of occurrences of the n-gram relative to all clinical n-grams. We find that there is not much variation between the clinical language terms that are used across the different countries. The disorders that are talked about most explicitly (such as “borderline personality disorder” “personality disorder” or “social anxiety”) are used at relatively consistent rates between both the minority sample and the majority sample. As posited above it is possible that this consistency in types of clinical language hints at some form of standardization in how those users who use clinical language use it a globalized online clinical language around mental health and a potential difference between past work on expressions of distress in offline settings. That said it should be noted that some of the most popular n-grams especially unigrams such as “sleep” and “night” can function as false positives as not every mention of sleep or night on Talklife is a reference to a mental health issue (such as “sleep paralysis” or “night terrors”). These terms may also be used in different (and culturally bound) ways. More specific terms (bigrams such as ""anxiety disorder"" and trigrams such as ""borderline personality disorder"" are used rarely. Finally we also report potential cross-cultural differences when individuals first begin to use clinical language on Talklife. On average Indians tend to use clinical language on the 6th post (mean = 6.3 σ = 12.12) whereas Malaysians Filipinos and the population from the majority sample tend to use clinical language on the 4th post (mean = 4.31 4.51 4.13 σ = 5.7 6.8 6.6). Interpretation of these results will require more work investigations the reasons why Indians use clinical language later in a thread. On balance these results shows that individuals from the minority sample use lesser amounts of clinical language overall but also show more variation in use than people from the majority sample and add nuanced perspective to past work [52 53 64] showing that individuals from these minority countries are often less likely to use clinical language when conceptualizing and describing their experience of mental distress.",general linguistic difference to better understand the contentbased difference between post from individual in the minority sample we compare the top unigrams bigram and trigram from each minority country to those from the majority sample we filter out any word that were only one character a well a the natural language toolkits built in stopwords the top ngrams unigrams bigram and trigram can be seen in table ngrams that have been redacted are the usernames of talklife user that were discussed in plain text by others on the website we can make a few observation from the relative frequency of ngrams first english is the most commonly used language within each community on talklife a word from language native to each country were not the majority of any of the ngrams second top ngrams from the majority sample are not common to the top ngrams in any of the minority sample suggesting a difference in how people express mental health even when people are talking about the same theme such a getting support they tend to use different phrase the majority sample us need someone to talk to while indian prefer like to talk to friend third while much of the language most often expressed by user in the majority sample is support language expressing individual distress such a im or feel we find that people from the minority sample are more likely to talk about themselves in relation to other people a illustrated by the bolded ngrams in table for example individual from the minority sample use the term u at a higher amount than those in the majority sample further individual from india often talk about wanting or needing friend whereas individual from malaysia and filipino refer to loneliness using the word alone more often than people in the majority sample this use of term related to interpersonal connection to express mental distress is seen in india malaysia and the philippine when individual experiencing distress are asked to describe how they are feeling our finding extends this work in the context of online mental health support community finally reference to religion such a god or pray find peace are more common in post from malaysia and the philippine this follows past research on the expression of mental health in both malaysia and the philippine showing that religion is often used a a foundation for how people from these cultural background express mental health concern and for how people support one another the higher presence of reference to religion among user from malaysia and the philippine might also suggest that index post that discus religion but do not specifically discus distress might actually be one culturallysanctioned method of signposting a state of mental distress a seen in past research in offline context difference in clinical language use based on past research showing that individual from minority country often express mental distress in nonclinical term we examine the use of clinical language on online mental health support community for this analysis of clinical language around mental distress from different country we use data from the top of user based on number of post on talklife to analyze a greater number of post a table show the amount of clinical language consistently differs between the majority sample and minority sample we find that clinical language is used le frequently in post from the minority sample than in the majority sample for indian malaysian and filipino of index post have clinical language respectively compared to for the majority sample we see a similar but smaller difference for support response of support response tend to have clinical language for the minority sample compared to for the majority samplethese difference are significant with a p unless otherwise indicated for all country though we find consistently that support response have le amount of clinical language than index post however if we look at the frequency of clinical language within post the difference between the minority sample and the majority sample is small combined with the evidence that the fraction of post with at least one use of clinical language is substantially lower for the minority sample this implies that there must be higher use of clinical language term per post for each post from the minority sample that doe contain clinical language this implication is verified by the last two row of table when we restrict our analysis to only those post that do contain clinical language post from the minority sample use more clinical mental health language in comparison to the majority sample for both index post and support response with a statistically significant difference at p thus while a smaller fraction of post in the minority sample use clinical language overall there is a big variation in it use post that do use clinical language tend to use many more term per post than those from majority country suggesting that people who do use clinical language from minority country are participating in a standard and globalized language around describing clinical mental health table confirms the above result when we look at the top unigrams bigram and trigram of clinical language used by people from different country the percentage for each term or ngram correspond to the fraction of occurrence of the ngram relative to all clinical ngrams we find that there is not much variation between the clinical language term that are used across the different country the disorder that are talked about most explicitly such a borderline personality disorder personality disorder or social anxiety are used at relatively consistent rate between both the minority sample and the majority sample a posited above it is possible that this consistency in type of clinical language hint at some form of standardization in how those user who use clinical language use it a globalized online clinical language around mental health and a potential difference between past work on expression of distress in offline setting that said it should be noted that some of the most popular ngrams especially unigrams such a sleep and night can function a false positive a not every mention of sleep or night on talklife is a reference to a mental health issue such a sleep paralysis or night terror these term may also be used in different and culturally bound way more specific term bigram such a anxiety disorder and trigram such a borderline personality disorder are used rarely finally we also report potential crosscultural difference when individual first begin to use clinical language on talklife on average indian tend to use clinical language on the th post mean whereas malaysian filipino and the population from the majority sample tend to use clinical language on the th post mean interpretation of these result will require more work investigation the reason why indian use clinical language later in a thread on balance these result show that individual from the minority sample use lesser amount of clinical language overall but also show more variation in use than people from the majority sample and add nuanced perspective to past work showing that individual from these minority country are often le likely to use clinical language when conceptualizing and describing their experience of mental distress,"['general', 'linguistic', 'difference', 'better', 'understand', 'contentbased', 'difference', 'post', 'individual', 'minority', 'sample', 'compare', 'top', 'unigrams', 'bigram', 'trigram', 'minority', 'country', 'majority', 'sample', 'filter', 'word', 'one', 'character', 'well', 'natural', 'language', 'toolkits', 'built', 'stopwords', 'top', 'ngrams', 'unigrams', 'bigram', 'trigram', 'seen', 'table', 'ngrams', 'redacted', 'usernames', 'talklife', 'user', 'discussed', 'plain', 'text', 'others', 'website', 'make', 'observation', 'relative', 'frequency', 'ngrams', 'first', 'english', 'commonly', 'used', 'language', 'within', 'community', 'talklife', 'word', 'language', 'native', 'country', 'majority', 'ngrams', 'second', 'top', 'ngrams', 'majority', 'sample', 'common', 'top', 'ngrams', 'minority', 'sample', 'suggesting', 'difference', 'people', 'express', 'mental', 'health', 'even', 'people', 'talking', 'theme', 'getting', 'support', 'tend', 'use', 'different', 'phrase', 'majority', 'sample', 'us', 'need', 'someone', 'talk', 'indian', 'prefer', 'like', 'talk', 'friend', 'third', 'much', 'language', 'often', 'expressed', 'user', 'majority', 'sample', 'support', 'language', 'expressing', 'individual', 'distress', 'im', 'feel', 'find', 'people', 'minority', 'sample', 'likely', 'talk', 'relation', 'people', 'illustrated', 'bolded', 'ngrams', 'table', 'example', 'individual', 'minority', 'sample', 'use', 'term', 'u', 'higher', 'amount', 'majority', 'sample', 'individual', 'india', 'often', 'talk', 'wanting', 'needing', 'friend', 'whereas', 'individual', 'malaysia', 'filipino', 'refer', 'loneliness', 'using', 'word', 'alone', 'often', 'people', 'majority', 'sample', 'use', 'term', 'related', 'interpersonal', 'connection', 'express', 'mental', 'distress', 'seen', 'india', 'malaysia', 'philippine', 'individual', 'experiencing', 'distress', 'asked', 'describe', 'feeling', 'finding', 'extends', 'work', 'context', 'online', 'mental', 'health', 'support', 'community', 'finally', 'reference', 'religion', 'god', 'pray', 'find', 'peace', 'common', 'post', 'malaysia', 'philippine', 'follows', 'past', 'research', 'expression', 'mental', 'health', 'malaysia', 'philippine', 'showing', 'religion', 'often', 'used', 'foundation', 'people', 'cultural', 'background', 'express', 'mental', 'health', 'concern', 'people', 'support', 'one', 'another', 'higher', 'presence', 'reference', 'religion', 'among', 'user', 'malaysia', 'philippine', 'might', 'also', 'suggest', 'index', 'post', 'discus', 'religion', 'specifically', 'discus', 'distress', 'might', 'actually', 'one', 'culturallysanctioned', 'method', 'signposting', 'state', 'mental', 'distress', 'seen', 'past', 'research', 'offline', 'context', 'difference', 'clinical', 'language', 'use', 'based', 'past', 'research', 'showing', 'individual', 'minority', 'country', 'often', 'express', 'mental', 'distress', 'nonclinical', 'term', 'examine', 'use', 'clinical', 'language', 'online', 'mental', 'health', 'support', 'community', 'analysis', 'clinical', 'language', 'around', 'mental', 'distress', 'different', 'country', 'use', 'data', 'top', 'user', 'based', 'number', 'post', 'talklife', 'analyze', 'greater', 'number', 'post', 'table', 'show', 'amount', 'clinical', 'language', 'consistently', 'differs', 'majority', 'sample', 'minority', 'sample', 'find', 'clinical', 'language', 'used', 'le', 'frequently', 'post', 'minority', 'sample', 'majority', 'sample', 'indian', 'malaysian', 'filipino', 'index', 'post', 'clinical', 'language', 'respectively', 'compared', 'majority', 'sample', 'see', 'similar', 'smaller', 'difference', 'support', 'response', 'support', 'response', 'tend', 'clinical', 'language', 'minority', 'sample', 'compared', 'majority', 'samplethese', 'difference', 'significant', 'p', 'unless', 'otherwise', 'indicated', 'country', 'though', 'find', 'consistently', 'support', 'response', 'le', 'amount', 'clinical', 'language', 'index', 'post', 'however', 'look', 'frequency', 'clinical', 'language', 'within', 'post', 'difference', 'minority', 'sample', 'majority', 'sample', 'small', 'combined', 'evidence', 'fraction', 'post', 'least', 'one', 'use', 'clinical', 'language', 'substantially', 'lower', 'minority', 'sample', 'implies', 'must', 'higher', 'use', 'clinical', 'language', 'term', 'per', 'post', 'post', 'minority', 'sample', 'doe', 'contain', 'clinical', 'language', 'implication', 'verified', 'last', 'two', 'row', 'table', 'restrict', 'analysis', 'post', 'contain', 'clinical', 'language', 'post', 'minority', 'sample', 'use', 'clinical', 'mental', 'health', 'language', 'comparison', 'majority', 'sample', 'index', 'post', 'support', 'response', 'statistically', 'significant', 'difference', 'p', 'thus', 'smaller', 'fraction', 'post', 'minority', 'sample', 'use', 'clinical', 'language', 'overall', 'big', 'variation', 'use', 'post', 'use', 'clinical', 'language', 'tend', 'use', 'many', 'term', 'per', 'post', 'majority', 'country', 'suggesting', 'people', 'use', 'clinical', 'language', 'minority', 'country', 'participating', 'standard', 'globalized', 'language', 'around', 'describing', 'clinical', 'mental', 'health', 'table', 'confirms', 'result', 'look', 'top', 'unigrams', 'bigram', 'trigram', 'clinical', 'language', 'used', 'people', 'different', 'country', 'percentage', 'term', 'ngram', 'correspond', 'fraction', 'occurrence', 'ngram', 'relative', 'clinical', 'ngrams', 'find', 'much', 'variation', 'clinical', 'language', 'term', 'used', 'across', 'different', 'country', 'disorder', 'talked', 'explicitly', 'borderline', 'personality', 'disorder', 'personality', 'disorder', 'social', 'anxiety', 'used', 'relatively', 'consistent', 'rate', 'minority', 'sample', 'majority', 'sample', 'posited', 'possible', 'consistency', 'type', 'clinical', 'language', 'hint', 'form', 'standardization', 'user', 'use', 'clinical', 'language', 'use', 'globalized', 'online', 'clinical', 'language', 'around', 'mental', 'health', 'potential', 'difference', 'past', 'work', 'expression', 'distress', 'offline', 'setting', 'said', 'noted', 'popular', 'ngrams', 'especially', 'unigrams', 'sleep', 'night', 'function', 'false', 'positive', 'every', 'mention', 'sleep', 'night', 'talklife', 'reference', 'mental', 'health', 'issue', 'sleep', 'paralysis', 'night', 'terror', 'term', 'may', 'also', 'used', 'different', 'culturally', 'bound', 'way', 'specific', 'term', 'bigram', 'anxiety', 'disorder', 'trigram', 'borderline', 'personality', 'disorder', 'used', 'rarely', 'finally', 'also', 'report', 'potential', 'crosscultural', 'difference', 'individual', 'first', 'begin', 'use', 'clinical', 'language', 'talklife', 'average', 'indian', 'tend', 'use', 'clinical', 'language', 'th', 'post', 'mean', 'whereas', 'malaysian', 'filipino', 'population', 'majority', 'sample', 'tend', 'use', 'clinical', 'language', 'th', 'post', 'mean', 'interpretation', 'result', 'require', 'work', 'investigation', 'reason', 'indian', 'use', 'clinical', 'language', 'later', 'thread', 'balance', 'result', 'show', 'individual', 'minority', 'sample', 'use', 'lesser', 'amount', 'clinical', 'language', 'overall', 'also', 'show', 'variation', 'use', 'people', 'majority', 'sample', 'add', 'nuanced', 'perspective', 'past', 'work', 'showing', 'individual', 'minority', 'country', 'often', 'le', 'likely', 'use', 'clinical', 'language', 'conceptualizing', 'describing', 'experience', 'mental', 'distress']","['general linguistic', 'linguistic difference', 'difference better', 'better understand', 'understand contentbased', 'contentbased difference', 'difference post', 'post individual', 'individual minority', 'minority sample', 'sample compare', 'compare top', 'top unigrams', 'unigrams bigram', 'bigram trigram', 'trigram minority', 'minority country', 'country majority', 'majority sample', 'sample filter', 'filter word', 'word one', 'one character', 'character well', 'well natural', 'natural language', 'language toolkits', 'toolkits built', 'built stopwords', 'stopwords top', 'top ngrams', 'ngrams unigrams', 'unigrams bigram', 'bigram trigram', 'trigram seen', 'seen table', 'table ngrams', 'ngrams redacted', 'redacted usernames', 'usernames talklife', 'talklife user', 'user discussed', 'discussed plain', 'plain text', 'text others', 'others website', 'website make', 'make observation', 'observation relative', 'relative frequency', 'frequency ngrams', 'ngrams first', 'first english', 'english commonly', 'commonly used', 'used language', 'language within', 'within community', 'community talklife', 'talklife word', 'word language', 'language native', 'native country', 'country majority', 'majority ngrams', 'ngrams second', 'second top', 'top ngrams', 'ngrams majority', 'majority sample', 'sample common', 'common top', 'top ngrams', 'ngrams minority', 'minority sample', 'sample suggesting', 'suggesting difference', 'difference people', 'people express', 'express mental', 'mental health', 'health even', 'even people', 'people talking', 'talking theme', 'theme getting', 'getting support', 'support tend', 'tend use', 'use different', 'different phrase', 'phrase majority', 'majority sample', 'sample us', 'us need', 'need someone', 'someone talk', 'talk indian', 'indian prefer', 'prefer like', 'like talk', 'talk friend', 'friend third', 'third much', 'much language', 'language often', 'often expressed', 'expressed user', 'user majority', 'majority sample', 'sample support', 'support language', 'language expressing', 'expressing individual', 'individual distress', 'distress im', 'im feel', 'feel find', 'find people', 'people minority', 'minority sample', 'sample likely', 'likely talk', 'talk relation', 'relation people', 'people illustrated', 'illustrated bolded', 'bolded ngrams', 'ngrams table', 'table example', 'example individual', 'individual minority', 'minority sample', 'sample use', 'use term', 'term u', 'u higher', 'higher amount', 'amount majority', 'majority sample', 'sample individual', 'individual india', 'india often', 'often talk', 'talk wanting', 'wanting needing', 'needing friend', 'friend whereas', 'whereas individual', 'individual malaysia', 'malaysia filipino', 'filipino refer', 'refer loneliness', 'loneliness using', 'using word', 'word alone', 'alone often', 'often people', 'people majority', 'majority sample', 'sample use', 'use term', 'term related', 'related interpersonal', 'interpersonal connection', 'connection express', 'express mental', 'mental distress', 'distress seen', 'seen india', 'india malaysia', 'malaysia philippine', 'philippine individual', 'individual experiencing', 'experiencing distress', 'distress asked', 'asked describe', 'describe feeling', 'feeling finding', 'finding extends', 'extends work', 'work context', 'context online', 'online mental', 'mental health', 'health support', 'support community', 'community finally', 'finally reference', 'reference religion', 'religion god', 'god pray', 'pray find', 'find peace', 'peace common', 'common post', 'post malaysia', 'malaysia philippine', 'philippine follows', 'follows past', 'past research', 'research expression', 'expression mental', 'mental health', 'health malaysia', 'malaysia philippine', 'philippine showing', 'showing religion', 'religion often', 'often used', 'used foundation', 'foundation people', 'people cultural', 'cultural background', 'background express', 'express mental', 'mental health', 'health concern', 'concern people', 'people support', 'support one', 'one another', 'another higher', 'higher presence', 'presence reference', 'reference religion', 'religion among', 'among user', 'user malaysia', 'malaysia philippine', 'philippine might', 'might also', 'also suggest', 'suggest index', 'index post', 'post discus', 'discus religion', 'religion specifically', 'specifically discus', 'discus distress', 'distress might', 'might actually', 'actually one', 'one culturallysanctioned', 'culturallysanctioned method', 'method signposting', 'signposting state', 'state mental', 'mental distress', 'distress seen', 'seen past', 'past research', 'research offline', 'offline context', 'context difference', 'difference clinical', 'clinical language', 'language use', 'use based', 'based past', 'past research', 'research showing', 'showing individual', 'individual minority', 'minority country', 'country often', 'often express', 'express mental', 'mental distress', 'distress nonclinical', 'nonclinical term', 'term examine', 'examine use', 'use clinical', 'clinical language', 'language online', 'online mental', 'mental health', 'health support', 'support community', 'community analysis', 'analysis clinical', 'clinical language', 'language around', 'around mental', 'mental distress', 'distress different', 'different country', 'country use', 'use data', 'data top', 'top user', 'user based', 'based number', 'number post', 'post talklife', 'talklife analyze', 'analyze greater', 'greater number', 'number post', 'post table', 'table show', 'show amount', 'amount clinical', 'clinical language', 'language consistently', 'consistently differs', 'differs majority', 'majority sample', 'sample minority', 'minority sample', 'sample find', 'find clinical', 'clinical language', 'language used', 'used le', 'le frequently', 'frequently post', 'post minority', 'minority sample', 'sample majority', 'majority sample', 'sample indian', 'indian malaysian', 'malaysian filipino', 'filipino index', 'index post', 'post clinical', 'clinical language', 'language respectively', 'respectively compared', 'compared majority', 'majority sample', 'sample see', 'see similar', 'similar smaller', 'smaller difference', 'difference support', 'support response', 'response support', 'support response', 'response tend', 'tend clinical', 'clinical language', 'language minority', 'minority sample', 'sample compared', 'compared majority', 'majority samplethese', 'samplethese difference', 'difference significant', 'significant p', 'p unless', 'unless otherwise', 'otherwise indicated', 'indicated country', 'country though', 'though find', 'find consistently', 'consistently support', 'support response', 'response le', 'le amount', 'amount clinical', 'clinical language', 'language index', 'index post', 'post however', 'however look', 'look frequency', 'frequency clinical', 'clinical language', 'language within', 'within post', 'post difference', 'difference minority', 'minority sample', 'sample majority', 'majority sample', 'sample small', 'small combined', 'combined evidence', 'evidence fraction', 'fraction post', 'post least', 'least one', 'one use', 'use clinical', 'clinical language', 'language substantially', 'substantially lower', 'lower minority', 'minority sample', 'sample implies', 'implies must', 'must higher', 'higher use', 'use clinical', 'clinical language', 'language term', 'term per', 'per post', 'post post', 'post minority', 'minority sample', 'sample doe', 'doe contain', 'contain clinical', 'clinical language', 'language implication', 'implication verified', 'verified last', 'last two', 'two row', 'row table', 'table restrict', 'restrict analysis', 'analysis post', 'post contain', 'contain clinical', 'clinical language', 'language post', 'post minority', 'minority sample', 'sample use', 'use clinical', 'clinical mental', 'mental health', 'health language', 'language comparison', 'comparison majority', 'majority sample', 'sample index', 'index post', 'post support', 'support response', 'response statistically', 'statistically significant', 'significant difference', 'difference p', 'p thus', 'thus smaller', 'smaller fraction', 'fraction post', 'post minority', 'minority sample', 'sample use', 'use clinical', 'clinical language', 'language overall', 'overall big', 'big variation', 'variation use', 'use post', 'post use', 'use clinical', 'clinical language', 'language tend', 'tend use', 'use many', 'many term', 'term per', 'per post', 'post majority', 'majority country', 'country suggesting', 'suggesting people', 'people use', 'use clinical', 'clinical language', 'language minority', 'minority country', 'country participating', 'participating standard', 'standard globalized', 'globalized language', 'language around', 'around describing', 'describing clinical', 'clinical mental', 'mental health', 'health table', 'table confirms', 'confirms result', 'result look', 'look top', 'top unigrams', 'unigrams bigram', 'bigram trigram', 'trigram clinical', 'clinical language', 'language used', 'used people', 'people different', 'different country', 'country percentage', 'percentage term', 'term ngram', 'ngram correspond', 'correspond fraction', 'fraction occurrence', 'occurrence ngram', 'ngram relative', 'relative clinical', 'clinical ngrams', 'ngrams find', 'find much', 'much variation', 'variation clinical', 'clinical language', 'language term', 'term used', 'used across', 'across different', 'different country', 'country disorder', 'disorder talked', 'talked explicitly', 'explicitly borderline', 'borderline personality', 'personality disorder', 'disorder personality', 'personality disorder', 'disorder social', 'social anxiety', 'anxiety used', 'used relatively', 'relatively consistent', 'consistent rate', 'rate minority', 'minority sample', 'sample majority', 'majority sample', 'sample posited', 'posited possible', 'possible consistency', 'consistency type', 'type clinical', 'clinical language', 'language hint', 'hint form', 'form standardization', 'standardization user', 'user use', 'use clinical', 'clinical language', 'language use', 'use globalized', 'globalized online', 'online clinical', 'clinical language', 'language around', 'around mental', 'mental health', 'health potential', 'potential difference', 'difference past', 'past work', 'work expression', 'expression distress', 'distress offline', 'offline setting', 'setting said', 'said noted', 'noted popular', 'popular ngrams', 'ngrams especially', 'especially unigrams', 'unigrams sleep', 'sleep night', 'night function', 'function false', 'false positive', 'positive every', 'every mention', 'mention sleep', 'sleep night', 'night talklife', 'talklife reference', 'reference mental', 'mental health', 'health issue', 'issue sleep', 'sleep paralysis', 'paralysis night', 'night terror', 'terror term', 'term may', 'may also', 'also used', 'used different', 'different culturally', 'culturally bound', 'bound way', 'way specific', 'specific term', 'term bigram', 'bigram anxiety', 'anxiety disorder', 'disorder trigram', 'trigram borderline', 'borderline personality', 'personality disorder', 'disorder used', 'used rarely', 'rarely finally', 'finally also', 'also report', 'report potential', 'potential crosscultural', 'crosscultural difference', 'difference individual', 'individual first', 'first begin', 'begin use', 'use clinical', 'clinical language', 'language talklife', 'talklife average', 'average indian', 'indian tend', 'tend use', 'use clinical', 'clinical language', 'language th', 'th post', 'post mean', 'mean whereas', 'whereas malaysian', 'malaysian filipino', 'filipino population', 'population majority', 'majority sample', 'sample tend', 'tend use', 'use clinical', 'clinical language', 'language th', 'th post', 'post mean', 'mean interpretation', 'interpretation result', 'result require', 'require work', 'work investigation', 'investigation reason', 'reason indian', 'indian use', 'use clinical', 'clinical language', 'language later', 'later thread', 'thread balance', 'balance result', 'result show', 'show individual', 'individual minority', 'minority sample', 'sample use', 'use lesser', 'lesser amount', 'amount clinical', 'clinical language', 'language overall', 'overall also', 'also show', 'show variation', 'variation use', 'use people', 'people majority', 'majority sample', 'sample add', 'add nuanced', 'nuanced perspective', 'perspective past', 'past work', 'work showing', 'showing individual', 'individual minority', 'minority country', 'country often', 'often le', 'le likely', 'likely use', 'use clinical', 'clinical language', 'language conceptualizing', 'conceptualizing describing', 'describing experience', 'experience mental', 'mental distress']","['general linguistic difference', 'linguistic difference better', 'difference better understand', 'better understand contentbased', 'understand contentbased difference', 'contentbased difference post', 'difference post individual', 'post individual minority', 'individual minority sample', 'minority sample compare', 'sample compare top', 'compare top unigrams', 'top unigrams bigram', 'unigrams bigram trigram', 'bigram trigram minority', 'trigram minority country', 'minority country majority', 'country majority sample', 'majority sample filter', 'sample filter word', 'filter word one', 'word one character', 'one character well', 'character well natural', 'well natural language', 'natural language toolkits', 'language toolkits built', 'toolkits built stopwords', 'built stopwords top', 'stopwords top ngrams', 'top ngrams unigrams', 'ngrams unigrams bigram', 'unigrams bigram trigram', 'bigram trigram seen', 'trigram seen table', 'seen table ngrams', 'table ngrams redacted', 'ngrams redacted usernames', 'redacted usernames talklife', 'usernames talklife user', 'talklife user discussed', 'user discussed plain', 'discussed plain text', 'plain text others', 'text others website', 'others website make', 'website make observation', 'make observation relative', 'observation relative frequency', 'relative frequency ngrams', 'frequency ngrams first', 'ngrams first english', 'first english commonly', 'english commonly used', 'commonly used language', 'used language within', 'language within community', 'within community talklife', 'community talklife word', 'talklife word language', 'word language native', 'language native country', 'native country majority', 'country majority ngrams', 'majority ngrams second', 'ngrams second top', 'second top ngrams', 'top ngrams majority', 'ngrams majority sample', 'majority sample common', 'sample common top', 'common top ngrams', 'top ngrams minority', 'ngrams minority sample', 'minority sample suggesting', 'sample suggesting difference', 'suggesting difference people', 'difference people express', 'people express mental', 'express mental health', 'mental health even', 'health even people', 'even people talking', 'people talking theme', 'talking theme getting', 'theme getting support', 'getting support tend', 'support tend use', 'tend use different', 'use different phrase', 'different phrase majority', 'phrase majority sample', 'majority sample us', 'sample us need', 'us need someone', 'need someone talk', 'someone talk indian', 'talk indian prefer', 'indian prefer like', 'prefer like talk', 'like talk friend', 'talk friend third', 'friend third much', 'third much language', 'much language often', 'language often expressed', 'often expressed user', 'expressed user majority', 'user majority sample', 'majority sample support', 'sample support language', 'support language expressing', 'language expressing individual', 'expressing individual distress', 'individual distress im', 'distress im feel', 'im feel find', 'feel find people', 'find people minority', 'people minority sample', 'minority sample likely', 'sample likely talk', 'likely talk relation', 'talk relation people', 'relation people illustrated', 'people illustrated bolded', 'illustrated bolded ngrams', 'bolded ngrams table', 'ngrams table example', 'table example individual', 'example individual minority', 'individual minority sample', 'minority sample use', 'sample use term', 'use term u', 'term u higher', 'u higher amount', 'higher amount majority', 'amount majority sample', 'majority sample individual', 'sample individual india', 'individual india often', 'india often talk', 'often talk wanting', 'talk wanting needing', 'wanting needing friend', 'needing friend whereas', 'friend whereas individual', 'whereas individual malaysia', 'individual malaysia filipino', 'malaysia filipino refer', 'filipino refer loneliness', 'refer loneliness using', 'loneliness using word', 'using word alone', 'word alone often', 'alone often people', 'often people majority', 'people majority sample', 'majority sample use', 'sample use term', 'use term related', 'term related interpersonal', 'related interpersonal connection', 'interpersonal connection express', 'connection express mental', 'express mental distress', 'mental distress seen', 'distress seen india', 'seen india malaysia', 'india malaysia philippine', 'malaysia philippine individual', 'philippine individual experiencing', 'individual experiencing distress', 'experiencing distress asked', 'distress asked describe', 'asked describe feeling', 'describe feeling finding', 'feeling finding extends', 'finding extends work', 'extends work context', 'work context online', 'context online mental', 'online mental health', 'mental health support', 'health support community', 'support community finally', 'community finally reference', 'finally reference religion', 'reference religion god', 'religion god pray', 'god pray find', 'pray find peace', 'find peace common', 'peace common post', 'common post malaysia', 'post malaysia philippine', 'malaysia philippine follows', 'philippine follows past', 'follows past research', 'past research expression', 'research expression mental', 'expression mental health', 'mental health malaysia', 'health malaysia philippine', 'malaysia philippine showing', 'philippine showing religion', 'showing religion often', 'religion often used', 'often used foundation', 'used foundation people', 'foundation people cultural', 'people cultural background', 'cultural background express', 'background express mental', 'express mental health', 'mental health concern', 'health concern people', 'concern people support', 'people support one', 'support one another', 'one another higher', 'another higher presence', 'higher presence reference', 'presence reference religion', 'reference religion among', 'religion among user', 'among user malaysia', 'user malaysia philippine', 'malaysia philippine might', 'philippine might also', 'might also suggest', 'also suggest index', 'suggest index post', 'index post discus', 'post discus religion', 'discus religion specifically', 'religion specifically discus', 'specifically discus distress', 'discus distress might', 'distress might actually', 'might actually one', 'actually one culturallysanctioned', 'one culturallysanctioned method', 'culturallysanctioned method signposting', 'method signposting state', 'signposting state mental', 'state mental distress', 'mental distress seen', 'distress seen past', 'seen past research', 'past research offline', 'research offline context', 'offline context difference', 'context difference clinical', 'difference clinical language', 'clinical language use', 'language use based', 'use based past', 'based past research', 'past research showing', 'research showing individual', 'showing individual minority', 'individual minority country', 'minority country often', 'country often express', 'often express mental', 'express mental distress', 'mental distress nonclinical', 'distress nonclinical term', 'nonclinical term examine', 'term examine use', 'examine use clinical', 'use clinical language', 'clinical language online', 'language online mental', 'online mental health', 'mental health support', 'health support community', 'support community analysis', 'community analysis clinical', 'analysis clinical language', 'clinical language around', 'language around mental', 'around mental distress', 'mental distress different', 'distress different country', 'different country use', 'country use data', 'use data top', 'data top user', 'top user based', 'user based number', 'based number post', 'number post talklife', 'post talklife analyze', 'talklife analyze greater', 'analyze greater number', 'greater number post', 'number post table', 'post table show', 'table show amount', 'show amount clinical', 'amount clinical language', 'clinical language consistently', 'language consistently differs', 'consistently differs majority', 'differs majority sample', 'majority sample minority', 'sample minority sample', 'minority sample find', 'sample find clinical', 'find clinical language', 'clinical language used', 'language used le', 'used le frequently', 'le frequently post', 'frequently post minority', 'post minority sample', 'minority sample majority', 'sample majority sample', 'majority sample indian', 'sample indian malaysian', 'indian malaysian filipino', 'malaysian filipino index', 'filipino index post', 'index post clinical', 'post clinical language', 'clinical language respectively', 'language respectively compared', 'respectively compared majority', 'compared majority sample', 'majority sample see', 'sample see similar', 'see similar smaller', 'similar smaller difference', 'smaller difference support', 'difference support response', 'support response support', 'response support response', 'support response tend', 'response tend clinical', 'tend clinical language', 'clinical language minority', 'language minority sample', 'minority sample compared', 'sample compared majority', 'compared majority samplethese', 'majority samplethese difference', 'samplethese difference significant', 'difference significant p', 'significant p unless', 'p unless otherwise', 'unless otherwise indicated', 'otherwise indicated country', 'indicated country though', 'country though find', 'though find consistently', 'find consistently support', 'consistently support response', 'support response le', 'response le amount', 'le amount clinical', 'amount clinical language', 'clinical language index', 'language index post', 'index post however', 'post however look', 'however look frequency', 'look frequency clinical', 'frequency clinical language', 'clinical language within', 'language within post', 'within post difference', 'post difference minority', 'difference minority sample', 'minority sample majority', 'sample majority sample', 'majority sample small', 'sample small combined', 'small combined evidence', 'combined evidence fraction', 'evidence fraction post', 'fraction post least', 'post least one', 'least one use', 'one use clinical', 'use clinical language', 'clinical language substantially', 'language substantially lower', 'substantially lower minority', 'lower minority sample', 'minority sample implies', 'sample implies must', 'implies must higher', 'must higher use', 'higher use clinical', 'use clinical language', 'clinical language term', 'language term per', 'term per post', 'per post post', 'post post minority', 'post minority sample', 'minority sample doe', 'sample doe contain', 'doe contain clinical', 'contain clinical language', 'clinical language implication', 'language implication verified', 'implication verified last', 'verified last two', 'last two row', 'two row table', 'row table restrict', 'table restrict analysis', 'restrict analysis post', 'analysis post contain', 'post contain clinical', 'contain clinical language', 'clinical language post', 'language post minority', 'post minority sample', 'minority sample use', 'sample use clinical', 'use clinical mental', 'clinical mental health', 'mental health language', 'health language comparison', 'language comparison majority', 'comparison majority sample', 'majority sample index', 'sample index post', 'index post support', 'post support response', 'support response statistically', 'response statistically significant', 'statistically significant difference', 'significant difference p', 'difference p thus', 'p thus smaller', 'thus smaller fraction', 'smaller fraction post', 'fraction post minority', 'post minority sample', 'minority sample use', 'sample use clinical', 'use clinical language', 'clinical language overall', 'language overall big', 'overall big variation', 'big variation use', 'variation use post', 'use post use', 'post use clinical', 'use clinical language', 'clinical language tend', 'language tend use', 'tend use many', 'use many term', 'many term per', 'term per post', 'per post majority', 'post majority country', 'majority country suggesting', 'country suggesting people', 'suggesting people use', 'people use clinical', 'use clinical language', 'clinical language minority', 'language minority country', 'minority country participating', 'country participating standard', 'participating standard globalized', 'standard globalized language', 'globalized language around', 'language around describing', 'around describing clinical', 'describing clinical mental', 'clinical mental health', 'mental health table', 'health table confirms', 'table confirms result', 'confirms result look', 'result look top', 'look top unigrams', 'top unigrams bigram', 'unigrams bigram trigram', 'bigram trigram clinical', 'trigram clinical language', 'clinical language used', 'language used people', 'used people different', 'people different country', 'different country percentage', 'country percentage term', 'percentage term ngram', 'term ngram correspond', 'ngram correspond fraction', 'correspond fraction occurrence', 'fraction occurrence ngram', 'occurrence ngram relative', 'ngram relative clinical', 'relative clinical ngrams', 'clinical ngrams find', 'ngrams find much', 'find much variation', 'much variation clinical', 'variation clinical language', 'clinical language term', 'language term used', 'term used across', 'used across different', 'across different country', 'different country disorder', 'country disorder talked', 'disorder talked explicitly', 'talked explicitly borderline', 'explicitly borderline personality', 'borderline personality disorder', 'personality disorder personality', 'disorder personality disorder', 'personality disorder social', 'disorder social anxiety', 'social anxiety used', 'anxiety used relatively', 'used relatively consistent', 'relatively consistent rate', 'consistent rate minority', 'rate minority sample', 'minority sample majority', 'sample majority sample', 'majority sample posited', 'sample posited possible', 'posited possible consistency', 'possible consistency type', 'consistency type clinical', 'type clinical language', 'clinical language hint', 'language hint form', 'hint form standardization', 'form standardization user', 'standardization user use', 'user use clinical', 'use clinical language', 'clinical language use', 'language use globalized', 'use globalized online', 'globalized online clinical', 'online clinical language', 'clinical language around', 'language around mental', 'around mental health', 'mental health potential', 'health potential difference', 'potential difference past', 'difference past work', 'past work expression', 'work expression distress', 'expression distress offline', 'distress offline setting', 'offline setting said', 'setting said noted', 'said noted popular', 'noted popular ngrams', 'popular ngrams especially', 'ngrams especially unigrams', 'especially unigrams sleep', 'unigrams sleep night', 'sleep night function', 'night function false', 'function false positive', 'false positive every', 'positive every mention', 'every mention sleep', 'mention sleep night', 'sleep night talklife', 'night talklife reference', 'talklife reference mental', 'reference mental health', 'mental health issue', 'health issue sleep', 'issue sleep paralysis', 'sleep paralysis night', 'paralysis night terror', 'night terror term', 'terror term may', 'term may also', 'may also used', 'also used different', 'used different culturally', 'different culturally bound', 'culturally bound way', 'bound way specific', 'way specific term', 'specific term bigram', 'term bigram anxiety', 'bigram anxiety disorder', 'anxiety disorder trigram', 'disorder trigram borderline', 'trigram borderline personality', 'borderline personality disorder', 'personality disorder used', 'disorder used rarely', 'used rarely finally', 'rarely finally also', 'finally also report', 'also report potential', 'report potential crosscultural', 'potential crosscultural difference', 'crosscultural difference individual', 'difference individual first', 'individual first begin', 'first begin use', 'begin use clinical', 'use clinical language', 'clinical language talklife', 'language talklife average', 'talklife average indian', 'average indian tend', 'indian tend use', 'tend use clinical', 'use clinical language', 'clinical language th', 'language th post', 'th post mean', 'post mean whereas', 'mean whereas malaysian', 'whereas malaysian filipino', 'malaysian filipino population', 'filipino population majority', 'population majority sample', 'majority sample tend', 'sample tend use', 'tend use clinical', 'use clinical language', 'clinical language th', 'language th post', 'th post mean', 'post mean interpretation', 'mean interpretation result', 'interpretation result require', 'result require work', 'require work investigation', 'work investigation reason', 'investigation reason indian', 'reason indian use', 'indian use clinical', 'use clinical language', 'clinical language later', 'language later thread', 'later thread balance', 'thread balance result', 'balance result show', 'result show individual', 'show individual minority', 'individual minority sample', 'minority sample use', 'sample use lesser', 'use lesser amount', 'lesser amount clinical', 'amount clinical language', 'clinical language overall', 'language overall also', 'overall also show', 'also show variation', 'show variation use', 'variation use people', 'use people majority', 'people majority sample', 'majority sample add', 'sample add nuanced', 'add nuanced perspective', 'nuanced perspective past', 'perspective past work', 'past work showing', 'work showing individual', 'showing individual minority', 'individual minority country', 'minority country often', 'country often le', 'often le likely', 'le likely use', 'likely use clinical', 'use clinical language', 'clinical language conceptualizing', 'language conceptualizing describing', 'conceptualizing describing experience', 'describing experience mental', 'experience mental distress']",,,,,,,,
https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-018-2197-z,1,In this section we first present the data gathered and used in our analysis. Researchers interested in the code and the data are invited to contact the authors. Reddit is a website which enables users to aggregate rate and discuss news entertainment politics and many other topics. According to Alexa it is the 8th most popular website in the world. It was estimated by the Pew research center that 6% of online adults use Reddit [26]. The site is organized into a collection of “subreddits” each focused on a particular topic and administered by a collection of moderators. The subreddit r/SuicideWatch is a forum in which online users are encouraged to post their thoughts regarding suicide. At the time of our data collection it had over 58000 subscribers. Sometimes users express a preoccupation with the thought of suicide. Other times users discuss immediate plans to take their own life. These posts often contain a description of their mental state including depression reaction to stress their feelings of being alone and having a low self-esteem. While most online sources of data are notoriously noisy this particular subreddit is remarkably clean. Given the serious nature of the subreddit individuals are less likely to post harassing comments or off-topic remarks. When users post such comments the moderators of the subreddit quickly remove them. We collected all posts from its inception in 2008 to 2016. Each post is often commented on by other individuals. In this work we focused on the original post as it most often represents the suicidal ideation of a user and comments often represent emotional support from other users. We cleaned this data. First we removed empty posts in which the content had been deleted. Second we removed links and replaced them with the word “link”. Third we concatenated the text of the post to the title as many users begin their post in the title and continue in the body of the post. Finally we removed punctuation and other special characters. After cleaning this data we had 131728 posts with 27978246 words of which 84607 words were unique posted by 63252 unique users.,in this section we first present the data gathered and used in our analysis researcher interested in the code and the data are invited to contact the author reddit is a website which enables user to aggregate rate and discus news entertainment politics and many other topic according to alexa it is the th most popular website in the world it wa estimated by the pew research center that of online adult use reddit the site is organized into a collection of subreddits each focused on a particular topic and administered by a collection of moderator the subreddit rsuicidewatch is a forum in which online user are encouraged to post their thought regarding suicide at the time of our data collection it had over subscriber sometimes user express a preoccupation with the thought of suicide other time user discus immediate plan to take their own life these post often contain a description of their mental state including depression reaction to stress their feeling of being alone and having a low selfesteem while most online source of data are notoriously noisy this particular subreddit is remarkably clean given the serious nature of the subreddit individual are le likely to post harassing comment or offtopic remark when user post such comment the moderator of the subreddit quickly remove them we collected all post from it inception in to each post is often commented on by other individual in this work we focused on the original post a it most often represents the suicidal ideation of a user and comment often represent emotional support from other user we cleaned this data first we removed empty post in which the content had been deleted second we removed link and replaced them with the word link third we concatenated the text of the post to the title a many user begin their post in the title and continue in the body of the post finally we removed punctuation and other special character after cleaning this data we had post with word of which word were unique posted by unique user,"['section', 'first', 'present', 'data', 'gathered', 'used', 'analysis', 'researcher', 'interested', 'code', 'data', 'invited', 'contact', 'author', 'reddit', 'website', 'enables', 'user', 'aggregate', 'rate', 'discus', 'news', 'entertainment', 'politics', 'many', 'topic', 'according', 'alexa', 'th', 'popular', 'website', 'world', 'wa', 'estimated', 'pew', 'research', 'center', 'online', 'adult', 'use', 'reddit', 'site', 'organized', 'collection', 'subreddits', 'focused', 'particular', 'topic', 'administered', 'collection', 'moderator', 'subreddit', 'rsuicidewatch', 'forum', 'online', 'user', 'encouraged', 'post', 'thought', 'regarding', 'suicide', 'time', 'data', 'collection', 'subscriber', 'sometimes', 'user', 'express', 'preoccupation', 'thought', 'suicide', 'time', 'user', 'discus', 'immediate', 'plan', 'take', 'life', 'post', 'often', 'contain', 'description', 'mental', 'state', 'including', 'depression', 'reaction', 'stress', 'feeling', 'alone', 'low', 'selfesteem', 'online', 'source', 'data', 'notoriously', 'noisy', 'particular', 'subreddit', 'remarkably', 'clean', 'given', 'serious', 'nature', 'subreddit', 'individual', 'le', 'likely', 'post', 'harassing', 'comment', 'offtopic', 'remark', 'user', 'post', 'comment', 'moderator', 'subreddit', 'quickly', 'remove', 'collected', 'post', 'inception', 'post', 'often', 'commented', 'individual', 'work', 'focused', 'original', 'post', 'often', 'represents', 'suicidal', 'ideation', 'user', 'comment', 'often', 'represent', 'emotional', 'support', 'user', 'cleaned', 'data', 'first', 'removed', 'empty', 'post', 'content', 'deleted', 'second', 'removed', 'link', 'replaced', 'word', 'link', 'third', 'concatenated', 'text', 'post', 'title', 'many', 'user', 'begin', 'post', 'title', 'continue', 'body', 'post', 'finally', 'removed', 'punctuation', 'special', 'character', 'cleaning', 'data', 'post', 'word', 'word', 'unique', 'posted', 'unique', 'user']","['section first', 'first present', 'present data', 'data gathered', 'gathered used', 'used analysis', 'analysis researcher', 'researcher interested', 'interested code', 'code data', 'data invited', 'invited contact', 'contact author', 'author reddit', 'reddit website', 'website enables', 'enables user', 'user aggregate', 'aggregate rate', 'rate discus', 'discus news', 'news entertainment', 'entertainment politics', 'politics many', 'many topic', 'topic according', 'according alexa', 'alexa th', 'th popular', 'popular website', 'website world', 'world wa', 'wa estimated', 'estimated pew', 'pew research', 'research center', 'center online', 'online adult', 'adult use', 'use reddit', 'reddit site', 'site organized', 'organized collection', 'collection subreddits', 'subreddits focused', 'focused particular', 'particular topic', 'topic administered', 'administered collection', 'collection moderator', 'moderator subreddit', 'subreddit rsuicidewatch', 'rsuicidewatch forum', 'forum online', 'online user', 'user encouraged', 'encouraged post', 'post thought', 'thought regarding', 'regarding suicide', 'suicide time', 'time data', 'data collection', 'collection subscriber', 'subscriber sometimes', 'sometimes user', 'user express', 'express preoccupation', 'preoccupation thought', 'thought suicide', 'suicide time', 'time user', 'user discus', 'discus immediate', 'immediate plan', 'plan take', 'take life', 'life post', 'post often', 'often contain', 'contain description', 'description mental', 'mental state', 'state including', 'including depression', 'depression reaction', 'reaction stress', 'stress feeling', 'feeling alone', 'alone low', 'low selfesteem', 'selfesteem online', 'online source', 'source data', 'data notoriously', 'notoriously noisy', 'noisy particular', 'particular subreddit', 'subreddit remarkably', 'remarkably clean', 'clean given', 'given serious', 'serious nature', 'nature subreddit', 'subreddit individual', 'individual le', 'le likely', 'likely post', 'post harassing', 'harassing comment', 'comment offtopic', 'offtopic remark', 'remark user', 'user post', 'post comment', 'comment moderator', 'moderator subreddit', 'subreddit quickly', 'quickly remove', 'remove collected', 'collected post', 'post inception', 'inception post', 'post often', 'often commented', 'commented individual', 'individual work', 'work focused', 'focused original', 'original post', 'post often', 'often represents', 'represents suicidal', 'suicidal ideation', 'ideation user', 'user comment', 'comment often', 'often represent', 'represent emotional', 'emotional support', 'support user', 'user cleaned', 'cleaned data', 'data first', 'first removed', 'removed empty', 'empty post', 'post content', 'content deleted', 'deleted second', 'second removed', 'removed link', 'link replaced', 'replaced word', 'word link', 'link third', 'third concatenated', 'concatenated text', 'text post', 'post title', 'title many', 'many user', 'user begin', 'begin post', 'post title', 'title continue', 'continue body', 'body post', 'post finally', 'finally removed', 'removed punctuation', 'punctuation special', 'special character', 'character cleaning', 'cleaning data', 'data post', 'post word', 'word word', 'word unique', 'unique posted', 'posted unique', 'unique user']","['section first present', 'first present data', 'present data gathered', 'data gathered used', 'gathered used analysis', 'used analysis researcher', 'analysis researcher interested', 'researcher interested code', 'interested code data', 'code data invited', 'data invited contact', 'invited contact author', 'contact author reddit', 'author reddit website', 'reddit website enables', 'website enables user', 'enables user aggregate', 'user aggregate rate', 'aggregate rate discus', 'rate discus news', 'discus news entertainment', 'news entertainment politics', 'entertainment politics many', 'politics many topic', 'many topic according', 'topic according alexa', 'according alexa th', 'alexa th popular', 'th popular website', 'popular website world', 'website world wa', 'world wa estimated', 'wa estimated pew', 'estimated pew research', 'pew research center', 'research center online', 'center online adult', 'online adult use', 'adult use reddit', 'use reddit site', 'reddit site organized', 'site organized collection', 'organized collection subreddits', 'collection subreddits focused', 'subreddits focused particular', 'focused particular topic', 'particular topic administered', 'topic administered collection', 'administered collection moderator', 'collection moderator subreddit', 'moderator subreddit rsuicidewatch', 'subreddit rsuicidewatch forum', 'rsuicidewatch forum online', 'forum online user', 'online user encouraged', 'user encouraged post', 'encouraged post thought', 'post thought regarding', 'thought regarding suicide', 'regarding suicide time', 'suicide time data', 'time data collection', 'data collection subscriber', 'collection subscriber sometimes', 'subscriber sometimes user', 'sometimes user express', 'user express preoccupation', 'express preoccupation thought', 'preoccupation thought suicide', 'thought suicide time', 'suicide time user', 'time user discus', 'user discus immediate', 'discus immediate plan', 'immediate plan take', 'plan take life', 'take life post', 'life post often', 'post often contain', 'often contain description', 'contain description mental', 'description mental state', 'mental state including', 'state including depression', 'including depression reaction', 'depression reaction stress', 'reaction stress feeling', 'stress feeling alone', 'feeling alone low', 'alone low selfesteem', 'low selfesteem online', 'selfesteem online source', 'online source data', 'source data notoriously', 'data notoriously noisy', 'notoriously noisy particular', 'noisy particular subreddit', 'particular subreddit remarkably', 'subreddit remarkably clean', 'remarkably clean given', 'clean given serious', 'given serious nature', 'serious nature subreddit', 'nature subreddit individual', 'subreddit individual le', 'individual le likely', 'le likely post', 'likely post harassing', 'post harassing comment', 'harassing comment offtopic', 'comment offtopic remark', 'offtopic remark user', 'remark user post', 'user post comment', 'post comment moderator', 'comment moderator subreddit', 'moderator subreddit quickly', 'subreddit quickly remove', 'quickly remove collected', 'remove collected post', 'collected post inception', 'post inception post', 'inception post often', 'post often commented', 'often commented individual', 'commented individual work', 'individual work focused', 'work focused original', 'focused original post', 'original post often', 'post often represents', 'often represents suicidal', 'represents suicidal ideation', 'suicidal ideation user', 'ideation user comment', 'user comment often', 'comment often represent', 'often represent emotional', 'represent emotional support', 'emotional support user', 'support user cleaned', 'user cleaned data', 'cleaned data first', 'data first removed', 'first removed empty', 'removed empty post', 'empty post content', 'post content deleted', 'content deleted second', 'deleted second removed', 'second removed link', 'removed link replaced', 'link replaced word', 'replaced word link', 'word link third', 'link third concatenated', 'third concatenated text', 'concatenated text post', 'text post title', 'post title many', 'title many user', 'many user begin', 'user begin post', 'begin post title', 'post title continue', 'title continue body', 'continue body post', 'body post finally', 'post finally removed', 'finally removed punctuation', 'removed punctuation special', 'punctuation special character', 'special character cleaning', 'character cleaning data', 'cleaning data post', 'data post word', 'post word word', 'word word unique', 'word unique posted', 'unique posted unique', 'posted unique user']",,,,,,,,
https://ieeexplore.ieee.org/abstract/document/8609647,0,Reddit is a multilingual Online Social Network founded in 2005 and organized in subcommunities by areas of interest called subreddits. We obtained data from the Reddit's data repository4 focusing on four subreddits where people discuss issues related to mental heath disorders: Depression (/r/depression) Suicide Watch (/r/Suicide Watch) Anxiety (/r/anxiety) and Bipolar (/r/bipolar). Our dataset is comprised of user activities (posts and comments) that took place between 2011 and 201 7. Here we focus on data from January 2017 to December 2017. In total we obtained 261511 posts and 1256669 comments from 184708 unique users. Table I shows the total number of users posts and comments per subreddit. The total number of comments in each community is at least 4.2 times larger than the number of posts which suggests a supportive behavior among users.,reddit is a multilingual online social network founded in and organized in subcommunities by area of interest called subreddits we obtained data from the reddits data repository focusing on four subreddits where people discus issue related to mental heath disorder depression rdepression suicide watch rsuicide watch anxiety ranxiety and bipolar rbipolar our dataset is comprised of user activity post and comment that took place between and here we focus on data from january to december in total we obtained post and comment from unique user table i show the total number of user post and comment per subreddit the total number of comment in each community is at least time larger than the number of post which suggests a supportive behavior among user,"['reddit', 'multilingual', 'online', 'social', 'network', 'founded', 'organized', 'subcommunities', 'area', 'interest', 'called', 'subreddits', 'obtained', 'data', 'reddits', 'data', 'repository', 'focusing', 'four', 'subreddits', 'people', 'discus', 'issue', 'related', 'mental', 'heath', 'disorder', 'depression', 'rdepression', 'suicide', 'watch', 'rsuicide', 'watch', 'anxiety', 'ranxiety', 'bipolar', 'rbipolar', 'dataset', 'comprised', 'user', 'activity', 'post', 'comment', 'took', 'place', 'focus', 'data', 'january', 'december', 'total', 'obtained', 'post', 'comment', 'unique', 'user', 'table', 'show', 'total', 'number', 'user', 'post', 'comment', 'per', 'subreddit', 'total', 'number', 'comment', 'community', 'least', 'time', 'larger', 'number', 'post', 'suggests', 'supportive', 'behavior', 'among', 'user']","['reddit multilingual', 'multilingual online', 'online social', 'social network', 'network founded', 'founded organized', 'organized subcommunities', 'subcommunities area', 'area interest', 'interest called', 'called subreddits', 'subreddits obtained', 'obtained data', 'data reddits', 'reddits data', 'data repository', 'repository focusing', 'focusing four', 'four subreddits', 'subreddits people', 'people discus', 'discus issue', 'issue related', 'related mental', 'mental heath', 'heath disorder', 'disorder depression', 'depression rdepression', 'rdepression suicide', 'suicide watch', 'watch rsuicide', 'rsuicide watch', 'watch anxiety', 'anxiety ranxiety', 'ranxiety bipolar', 'bipolar rbipolar', 'rbipolar dataset', 'dataset comprised', 'comprised user', 'user activity', 'activity post', 'post comment', 'comment took', 'took place', 'place focus', 'focus data', 'data january', 'january december', 'december total', 'total obtained', 'obtained post', 'post comment', 'comment unique', 'unique user', 'user table', 'table show', 'show total', 'total number', 'number user', 'user post', 'post comment', 'comment per', 'per subreddit', 'subreddit total', 'total number', 'number comment', 'comment community', 'community least', 'least time', 'time larger', 'larger number', 'number post', 'post suggests', 'suggests supportive', 'supportive behavior', 'behavior among', 'among user']","['reddit multilingual online', 'multilingual online social', 'online social network', 'social network founded', 'network founded organized', 'founded organized subcommunities', 'organized subcommunities area', 'subcommunities area interest', 'area interest called', 'interest called subreddits', 'called subreddits obtained', 'subreddits obtained data', 'obtained data reddits', 'data reddits data', 'reddits data repository', 'data repository focusing', 'repository focusing four', 'focusing four subreddits', 'four subreddits people', 'subreddits people discus', 'people discus issue', 'discus issue related', 'issue related mental', 'related mental heath', 'mental heath disorder', 'heath disorder depression', 'disorder depression rdepression', 'depression rdepression suicide', 'rdepression suicide watch', 'suicide watch rsuicide', 'watch rsuicide watch', 'rsuicide watch anxiety', 'watch anxiety ranxiety', 'anxiety ranxiety bipolar', 'ranxiety bipolar rbipolar', 'bipolar rbipolar dataset', 'rbipolar dataset comprised', 'dataset comprised user', 'comprised user activity', 'user activity post', 'activity post comment', 'post comment took', 'comment took place', 'took place focus', 'place focus data', 'focus data january', 'data january december', 'january december total', 'december total obtained', 'total obtained post', 'obtained post comment', 'post comment unique', 'comment unique user', 'unique user table', 'user table show', 'table show total', 'show total number', 'total number user', 'number user post', 'user post comment', 'post comment per', 'comment per subreddit', 'per subreddit total', 'subreddit total number', 'total number comment', 'number comment community', 'comment community least', 'community least time', 'least time larger', 'time larger number', 'larger number post', 'number post suggests', 'post suggests supportive', 'suggests supportive behavior', 'supportive behavior among', 'behavior among user']",,,,,,,,
