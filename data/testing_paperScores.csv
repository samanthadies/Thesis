Link to paper,doc_num,section_num,section_name,researchers,num_researchers,choudhury,drezde,coppersmith,year,text,data_source_i,class_collection_i,random_sample_i,replication_i,dem_dist_i,informed_consent_i,data_public_i,irb_i,human_subject_protection_i,ground_truth_size_i,ground_truth_discussion_i,limitations_i,preprocess_anonymity_i,preprocess_drop_i,preprocess_missing_values_i,preprocess_noise_i,preprocess_text_i,preprocess_feature_reconstruction_i,preprocess_i,ethics_section_i
https://dl.acm.org/doi/abs/10.1145/2464464.2464480,1,1,,"De Choudhury, Counts, Horvits",3,1,0,0,2013,"Depression is a serious and widespread public health challenge. We examine the potential for leveraging social media postings as a new type of lens in understanding depression in populations. Information gleaned from social media bears potential to complement traditional survey techniques in its ability to provide finer grained measurements over time while radically expanding population sample sizes. We present work on using a crowdsourcing methodology to build a large corpus of postings on Twitter that have been shared by individuals diagnosed with clinical depression. Next, we develop a probabilistic model trained on this corpus to determine if posts could indicate depression. The model leverages signals of social activity, emotion, and language manifested on Twitter. Using the model, we introduce a social media depression index that may serve to characterize levels of depression in populations. Geographical, demographic and seasonal patterns of depression given by the measure confirm psychiatric findings and correlate highly with depression statistics reported by the Centers for Disease Control and Prevention (CDC).",2,2,2,1,2,2,2,0,4,2,2,2,2,1,0,2,2,2,9,0
https://dl.acm.org/doi/abs/10.1145/2464464.2464480,1,2,,"De Choudhury, Counts, Horvits",3,1,0,0,2013,"Depression affects more than 27 million Americans and is believed to be responsible for the more than 30,000 suicides each year [2,14]. Besides being directly debilitating to sufferers, depression can adversely affect chronic health conditions, such as cardiovascular disease, cancer, diabetes, and obesity. It is also known to have negative influences on individuals’ family and personal relationships, work or school life, and sleeping and eating habits. Over the coming 20 years, depression is projected to be the leading cause of disability in high-income nations such as the United States [16]. The World Health Organization (WHO) now ranks major depression as one of the most burdensome diseases in the world [2,16]. Although a number of primary care programs have been devised for its detection and treatment, the majority of the millions of Americans who meet depression criteria are untreated or undertreated [11]. Furthermore, ethnic minority groups such as Mexican Americans and African Americans are significantly less likely to receive depression therapies than are other ethnic groups [9]. As part of a national-scale effort to curb depression, every few years the Centers for Disease Control and Prevention (CDC) administers the Behavioral Risk Factor Surveillance System (BRFSS) survey via telephone to estimate the rate of depression among adults in the US [2]. However the large temporal gaps across which these measurements are made, as well as the limited number of participant responses (on the order of thousands) makes it difficult for agencies to track and identify risk factors that may be associated with mental illness, or to develop effective intervention programs. We examine the potential of social media as a new tool for mental health measurement and surveillance. Platforms such as Twitter and Facebook are increasingly gaining traction among individuals allowing them to share their thoughts and emotions around a variety of happenings in everyday life. The emotion and language used in social media postings may indicate feelings of worthlessness, guilt, helplessness, and self-hatred that characterize depression as manifested in everyday life. Additionally, depression sufferers often show withdrawal from social situations and activities—i.e., the etiology of depression typically includes social environmental factors [17]. Characterization of social media activity and changing social ties within social media can provide measurement of such withdrawal and capture the depression sufferers’ social context in a manner that might help detect depression in populations. Relying on social media as a behavioral health assessment tool has other advantages as well. For instance, in contrast to the self-report methodology in behavioral surveys, where responses are prompted by the experimenter and typically comprise recollection of (sometimes subjective) health facts, social media measurement of behavior captures social activity and language expression in a naturalistic setting. Such activity is real-time, and happens in the course of a person’s day-to-day life. Hence it is less vulnerable to memory bias or experimenter demand effects, and can help track concerns at a fine-grained temporal scale. Our main contributions in this paper are as follows: (1) Using crowdsourcing techniques, we gather a ground truth set of 69K Twitter postings shared by individuals suffering from clinical depression—depression was measured using the CES-D (Center for Epidemiologic Studies Depression Scale) screening test [22]. (2) We develop statistical models (an SVM classifier) that can predict whether or not a Twitter post in a test set could be depression-indicative. To construct and test the predictive models, we harness evidence from a variety of measures, spanning emotional expression, linguistic style, user engagement, and egocentric social network properties. We demonstrate that our models can predict if a post is depression-indicative, with accuracy of more than 70% and precision of 0.82. (3) Finally we propose a metric we refer to as the social media depression index (SMDI). SMDI uses the above prediction models to determine depressive-indicative postings on Twitter, and thereby helps characterize the levels of depression in populations. We conduct a variety of analyses at population scale, examining depression levels (as given by SMDI) across geography (US cities and states), demographics (gender), and time, including diurnal and seasonal patterns. Our findings from these analyses align with CDC reported statistics of depression in US population, as well as confirm known characteristics of depression given in clinical literature. We believe that, when tied to behavioral health records from agencies, information derived from our prediction models and analyses can be valuable to epidemiologists who study macro-trends of individuals suffering from depression or other types of mental health disorders.",2,2,2,1,2,2,2,0,4,2,2,2,2,1,0,2,2,2,9,0
https://dl.acm.org/doi/abs/10.1145/2464464.2464480,1,3,,"De Choudhury, Counts, Horvits",3,1,0,0,2013,"Social/Psychological Context and Depression Offline social networks and attributes relating to the psychological environment of individuals have consistently been used to study behavioral health concerns. Billings and Moos [1] studied the roles of stress, social resources, and coping among individuals entering treatment for depression. Neils Rosenquist, Fowler, and Christakis [17] found that levels of depression showed diffusion upto three degrees of separation in a large social network, suggesting a network influence component to depression. On similar lines, in [10], Kawachi et al. explored the role of social ties and social capital in the maintenance of psychological wellbeing and treatment of behavioral health concerns. This prior research provides strong evidence that individuals’ social environments contain vital information useful for understanding and intervening on mental health. In the field of psycholinguistics, Oxman et al. [18] demonstrated that linguistic analysis of speech could classify patients into diagnostic groups such as those suffering depression and paranoia. Computerized analysis of written text has also been known to reveal cues about neurotic tendencies and psychiatric disorders [23]. Utilizing such analyses, particularly of social media given their strong connection to people’s social environment, can help us overcome the limitations of surveys for understanding the social/psychological context of individuals. Public Health using Online Data Leveraging internet data for modeling and analyzing public health behaviors has been a ripe area of research in the recent past. Google Flu Trends1 provides nuanced predictions of flu infections based on online search queries. Paul and Dredze [20] developed a disease-specific topic model based on Twitter’s posts in order to model behavior around a variety of diseases of importance in public health. Through language modeling of Twitter posts, Collier et al. [4] found evidence of high correlation between social media signals and diagnostic influenza case data. Sadelik et al. [24] developed statistical models that predicted infectious disease (e.g. flu) spread in individuals based on geotagged postings made on Twitter (also see [13]). While this body of work has investigated a range of challenges around public health, research on harnessing social media for understanding behavioral health disorders is still in its infancy. Park et al. [19] found initial evidence that people do post about their depression and even their treatment for depression on Twitter. In other related work [6], we examined linguistic and emotional correlates for postnatal course of new mothers, and thereafter built a model to predict extreme behavioral changes in new mothers. This early work thus points to the potential of social media as a signal to leverage in the study of depression. With the present work we expand the scope of social media-based studies of depression by examining general depression at population scale.",2,2,2,1,2,2,2,0,4,2,2,2,2,1,0,2,2,2,9,0
https://dl.acm.org/doi/abs/10.1145/2464464.2464480,1,4,,"De Choudhury, Counts, Horvits",3,1,0,0,2013,"A primary challenge is the gathering of adequate social media data, in particular Twitter posts, which bear information on whether or not they could contain depression-bearing/indicative content. We tackle this issue by collecting actual depression information on a set of Twitter users. The idea is that the postings made by a user suffering from depression will contain cues or signals that could reflect their mental condition. Such postings would thus help construct the positive/target class (i.e. set of posts that are depression-indicative) necessary for classification. Identifying Clinically Depressed Users We employ a crowdsourcing methodology to collect information on the status of depression of a set of users. Using Amazon’s Mechanical Turk interface, we designed HITs wherein crowdworkers were asked to take a clinical depression survey, followed by self-report seeking questions on their depression history. The crowdworkers could also opt-in to share their Twitter usernames if they had a public profile. We sought responses from crowdworkers who were located in the US, and had an approval rating greater than or equal to 90%. Each crowdworker was restricted to take the HIT exactly once, and was paid 90 cents for completing the task. We used the CES-D (Center for Epidemiologic Studies Depression Scale) questionnaire as the primary tool to determine the depression levels in the crowdworkers. The CES-D is a 20-item self-report scale designed to measure depressive symptoms in the general population [22]. Higher scores in the test indicate the presence of more symptomatology. In our study, we also collected information about the crowdworkers’ depression history through the following questions: Whether or not they have been diagnosed with clinical depression in the past. If so, when. If the answer above was positive, what was the estimated date of onset of depression. From the date of depression onset, how many depression episodes had they experienced. Finally, we asked the crowdworkers to share their (public) Twitter username for research purposes, under the privacy clause that their data may be mined using a computer program and analyzed anonymously. Statistics of Crowdsourcing Study A total of 1,583 crowdworkers completed our HITs between September 15 and October 31, 2012. However, not all crowdworkers opted in to share their Twitter usernames—637 participants (~40%) agreed to provide us access to their Twitter feeds. Next, we eliminated noisy respondents who took too little time to complete the task. Finally, we sought data from individuals with depression onset dates anytime in the last one year, but no later than three months before the survey was taken. These constraints were assumed to ensure that, for each user detected with depression, we would have sufficient postings on Twitter after the onset and until the point in time that they took the survey. We also sought users who had reported to be clinically depressed (and were currently so as well based on the scoring on the survey) with at least two depression episodes in the studied period. This helped us to focus on high precision data for which we had recurring signs of depression; depression would be a prevalent condition in these individuals. A set of 489 users was obtained, who indicated either they have clinical depression with onset in or after September 2011 and before June 2012, or did not have depression anytime. The set contained 251 males, 238 females, and the median age was 25 years. A distribution of the users per their depression scores is displayed in Figure 1. Based on the standard cut-off of 30 or above for high-range depression [21], we found 117 users with signs of severe depression. Separately, we obtained a set of 157 users, with scores in the range (0-10), who showed very little likelihood/almost no sign of depression in the period of our interest. Building Ground Truth Dataset We now discuss how we constructed a dataset of Twitter posts, with ground-truth label information (on whether or not the post is depression-indicative). We use Twitter postings of the two sets of users obtained above—(1) for the positive class (depression-indicative posts), and (2) for the negative class (standard posts). For each user in both sets, we use all of their postings in a three-month period. For the positive class of posts, we collect all postings made by users in the first set during the three months after their indicated onset of depression. For the negative class, we collect all postings made by users in the second set, during the three months before the date they took our survey. For collecting the posts, we utilized the Firehose made available to us via our organization’s contract with Twitter. Table 1 gives the final statistics of the dataset thus constructed. A few examples of posts from the target class of depression indicative posts are given in Table 2.",2,2,2,1,2,2,2,0,4,2,2,2,2,1,0,2,2,2,9,0
https://dl.acm.org/doi/abs/10.1145/2464464.2464480,1,5,,"De Choudhury, Counts, Horvits",3,1,0,0,2013,"We propose several features to characterize the postings in our dataset. The features can be categorized into two types: post-centric and user-centric—the former captures properties in the post, while the latter characterizes the behavior of the post’s author. Several of our features are motivated from [6], where greater details can be accessed by the readers. Post Features Emotion. We consider four features of the emotional state manifested in the posts: positive affect (PA), negative affect (NA), activation, and dominance. Measurements of PA and NA per post are computed using the psycholinguistic resource LIWC (http://www.liwc.net/), whose emotion categories have been validated to perform well for determining affect in Twitter [3,6]. We use the ANEW lexicon [5,6] for computing activation and dominance. Time. We define a measure that uses the timestamp information of a post, that is, whether it was a day time or night time post (in terms of local time of the post author). Our motivation springs from observations in the depression literature indicating that users showing depression signs tend to be relatively more active during the evening and night [15]. For the purpose we define a “night” window in a given day as “9PM—5:59AM” (consequently the “day” window for the same user, in local time, would be “6AM8:59PM”). For each post, we therefore assign a “time” index 1 or -1, depending on whether it was posted respectively in the night or day window. Linguistic Style. We also introduce features to characterize posts based on the use of linguistic styles [3,6]. We again use LIWC for determining 22 specific linguistic styles: articles, auxiliary verbs, conjunctions, adverbs, impersonal pronouns, personal pronouns, prepositions, functional words, fillers, assent, negation, certainty and quantifiers. n-grams. We also extract unigrams and bigrams from the posts, in order to account for the general language use. User Features Engagement. We utilize a set of engagement measures of the authors of posts in order to characterize the general behavior associated with them—we believe users found to be clinically depressed (or not) will bear distinctive behavioral markers in their postings. A measure of overall engagement of author of a post in social media is volume, defined as the number of posts the user has made so far on social media. We define a second engagement feature to be the proportion of reply posts (@-replies) from a post’s author. The third feature is the fraction of retweets from a post author. The proportion of links (urls) shared by each post author comprises our fourth feature. We define a fifth feature as the fraction of question-centric posts from a post’s author. Ego-network. We define two features that characterize a post author’s egocentric social network: (1) the number of followers or inlinks of the user, (2) the count of her followees or outlinks.",2,2,2,1,2,2,2,0,4,2,2,2,2,1,0,2,2,2,9,0
https://dl.acm.org/doi/abs/10.1145/2464464.2464480,1,6,,"De Choudhury, Counts, Horvits",3,1,0,0,2013,"Based on the features discussed so far, we present some descriptive analyses of differences in the two classes of posts: the “depression-indicative”, and the “standard” posts. Table 3 gives a list of the high frequency unigrams that appear in the postings of the two classes. For the negative class (standard posts), most of the unigrams relate to commonplace details of daily life, ranging from work to entertainment (work, friends, life, tomorrow, movie, football). A number of positive emotion words are also observed, such as brilliant, love, beautiful, perfect, great. On the other hand, in the case of depression-indicative posts, many words are emotional in nature (e.g., sad, happiness, uncomfortable, hurts, painful, hate, hope, worry). However we notice a strong inclination towards unigrams representing negative affect and low intensity emotions—possibly reflecting the mental instability and helplessness of the individuals sharing these posts, including symptoms of their likely depression (loser, depress*, weak, useless, suicidal, unsuccessful). Some posts also contain references to therapy and medication, possibly because individuals may be interested to exchange information about these with their audiences. We extend these findings through observations of the difference in means across the two classes in terms of various features (except the n-grams)—for a particular feature f, it is defined as μ(f)=fd – fs where fd (or fs) is the mean value of f for the positive (or negative) class. This is shown in Table 4. There are considerable differences across the two classes, as noted in the differences of means as well as the statistical significance tests. For instance, for the depressionindicative posts, we observe considerable decrease in user engagement features, such as volume, RTs, and replies. This indicates that the authors of the depression-indicative posts are posting less, suggesting a possible loss of social connectedness. Additionally, lowered numbers of followers and followees shows that the authors of the depressionindicative posts exhibit reduced desire to socialize or tendency to consume external information and remain connected with others. There is also high NA characterizing these post authors— possibly reflecting their mental instability and helplessness. Moreover, low activation and dominance may indicate loneliness, restlessness, exhaustion, lack of energy, and sleep deprivation, all of which are known to be consistent depression symptoms [6,18,23]. Finally, we find that the presence of the first-person pronoun is considerably high in posts of the same class, reflecting the users’ high attention to self and psychological distancing from others [3,6]. We weave together these observations, and the outcomes of the t-tests demonstrating statistically significant differences across the classes (see p-values in Table 4). It appears that our choice of features can adequately capture the distinctions across the depression-indicative and other posts. In this light, in the following section, we present a prediction framework that can classify a given post into one of the two classes, leveraging signal from these features.",2,2,2,1,2,2,2,0,4,2,2,2,2,1,0,2,2,2,9,0
https://dl.acm.org/doi/abs/10.1145/2464464.2464480,1,7,,"De Choudhury, Counts, Horvits",3,1,0,0,2013,"Classification Framework We use a supervised learning setup for classifying whether or not a given post is depression-indicative. We represent Twitter posts as vectors of the features presented earlier (e.g., emotion, time, n-grams, style, engagement features). Before feature extraction, the posts are lowercased, and numbers are normalized into a canonical form. Finally the posts are tokenized. After feature extraction, features that occur fewer than five times are removed in a first step of feature reduction. We then randomly split the data into five folds for cross-validation. The high dimensionality of the feature space can lead to overfitting to the training data— therefore we deploy principal component analysis (PCA) [7]. The classification algorithm is a standard Support Vector Machine classifier with an RBF kernel [7], although we experimented with other parametric and non-parametric supervised learning methods. We use five-fold cross validation, and conduct 100 randomized experimental runs. Prediction Performance Using the above model, we examine prediction performance in identifying the two classes of posts. In order to understand the importance of various feature types, we trained one model each using: (1) engagement and egonetwork features; (2) n-grams; (3) linguistic style; (4) emotion and time features; (5) all features; and (6) dimensionality-reduced set of features. We present the results of these prediction models in Table 5. The results indicate that, in our test set, the best performing model (dimension-reduced features) yields an average accuracy of ~73% and high precision of 0.82, corresponding to the class of depression-indicative posts. Good performance of this classifier is also evident from the receiver-operator characteristic (ROC) curves in Figure 2. We find that the dimension reduced feature model gives slightly greater traction in prediction compared to the model that uses all features—demonstrating that reducing feature redundancy is important. We also observe better performance for models that uses the linguistic style features alone, and the one that uses emotion and time features. Results in prior literature suggest that use of linguistic styles such as pronouns and articles provide information about how individuals respond to psychological triggers [3]. We conjecture that the general social and psychological distancing characterizing the circumstances of clinically depressed individuals is linked to high attentional focus on oneself—as indicated by use of these styles (see also Table 4). Hence style features turn out to be a strong predictor of depression. Next, the relatively better performance of the model using emotion and time features shows that expression of PA, NA as well as activation and dominance are central, so is the pattern of posting through the course of a day. Note that, as discussed earlier, one of the main characteristics of depression is disturbed processing of emotional information as indexed by disturbed startle reflex modulation, as well as a reduced sense of arousal in day-to-day activities [1,18]. Hence we observe better performance of these features in the prediction task. Similarly, psychiatric literature on depression indicates that 8 out of 10 people suffering from depression tend to worsen their symptoms during night [15]. Night time Internet activity is a known characteristic of these individuals [15]. This explains why the time feature, capturing the timestamp of post, is able to help predict if the post could be depression-indicative. In essence, we conclude that social media activity provides useful signals that can be utilized to classify and predict whether a post could be indicative of depression.",2,2,2,1,2,2,2,0,4,2,2,2,2,1,0,2,2,2,9,0
https://dl.acm.org/doi/abs/10.1145/2464464.2464480,1,8,,"De Choudhury, Counts, Horvits",3,1,0,0,2013,"Can the ability to predict whether or not a Twitter post is depression-indicative provide the basis for an accurate, reliable model of real-world depression rates in large populations? To this end, we use our predictive model, that considers whether a given post is depression-indicative, to automatically label a large corpus of posts shared on Twitter on any given day. Thereafter, we define a metric called the social media depression index (SMDI). This index gives a measure of the degree of depression as manifested by Twitter users in their daily postings. On a given day t, we define it as the standardized difference between the frequencies of depression-indicative posts nd(t) and “standard” (or non-depression indicative) posts ns(t): where µd (correspondingly µs) and σd (correspondingly σs) are the mean and standard deviations of the number of depression-indicative (correspondingly standard) posts shared in a fixed time period before t (here between k and t1, such that 1≤k≤t-1). Note that we consider separate terms for depression-indicative posts and standard posts. This allows depression and non-depression expressions in posts to be weighted equally (since their relative volumes are likely to be different—see, e.g., Table 1; also some days may be more “depression-inducing” than others). By standardizing depression and non-depression expression separately, we also focus on variation in each class separately. That is, even if per one’s behavior, some individuals dramatically under-express depression in their postings, each day’s relative depression expression compared to non-depression expression will be informative as a measure. In the above equation, note that SMDI(t) is zero when there are as many standardized depressionindicative posts as non-depression indicative ones; while it will be positive for high depression days and negative for low depression ones. We note that Kramer [14] used a similar formulation while using Facebook for computing the Gross National Happiness index. We also define an individual-centric metric of SMDI: Given a user u, we define her SMDI on day t as: where nd(u,t) (correspondingly ns(u,t)) is the number of depression-indicative (correspondingly standard) posts from user u on day t, and µd(t) (correspondingly µs(t)) and σd(t) (correspondingly σs(t)) are the mean and standard deviations of the number of depression-indicative (correspondingly standard) posts shared on the same day t.",2,2,2,1,2,2,2,0,4,2,2,2,2,1,0,2,2,2,9,0
https://dl.acm.org/doi/abs/10.1145/2464464.2464480,1,9,,"De Choudhury, Counts, Horvits",3,1,0,0,2013,"We now utilize SMDI to understand a variety of population characteristics of depression. Geographical Analysis We present analyses on how our social media depression index reflects the known depression rates in several US cities and states. First we collected a list of 20 cities reported by businessweek.com to be the “unhappiest US cities”. The list includes a per-city depression rank, as measured by prescription drug claims for common antidepressants2 . These statistics were reported at end of 2011. Using Twitter’s Firehose, we crawled 30% random samples of Twitter posts (English language), per month between Jan 1 2011 and Dec 31 2011, for each city, to approximately match the time period around which these numbers were reported. Note that matching a post with a city was made based on the post author’s self-report location on the Twitter profile, although other sophisticated location inference schemes may be adopted in future work. Table 6 gives statistics of the data crawled for each city. We used our trained prediction model to label all the posts shared every day from each city on whether they were depression-indicative, and thereafter computed the corresponding SMDI per day (eqn. (2)). We further calculated a mean SMDI per city throughout the time of analysis. Figure 3 shows the social media depression indices of all the 20 cities (y-axis) against their reported ranking in terms of depression rates (x-axis). The main observation in the figure is that we observe reasonable correlation (R 2 =0.64) between our computed SMDI and known depression rates in the various cities. Note that, the slope of the trend line in the figure is positive (0.082); this is because we are predicting depression rank, for which lower rank implies worse condition. On examination of the trend, it appears that we may be overestimating the degree of depression in more cities (e.g., Portland, Cleveland, Louisville, Atlanta, Kansas City), than the number of cities for which we underestimating the degree of depression (e.g., New Orleans, Detroit). To the extent this is true, this may be an artifact of a general tendency for people to express relatively more negativity on Twitter. Prior literature shows that negative affect is more predominantly expressed on Twitter than is positive affect [5]. Thus our model may be misinterpreting some expression of negativity as depression-indicative. Next we expand our geographical analysis to understand how SMDI reflects actual depression rates in the 50 US states. In this case, we obtain data on depression rates in the various states from the Centers for Disease Control and Prevention (CDC), per the Behavioral Risk Surveillance Survey (BRFSS) conducted in 2008 [2]. For computing SMDI, we follow a similar approach as done with the cities. That is, we compile a set of Twitter posts for each state based on regex matching of the state’s name with the user’s self-reported location on Twitter. Our Twitter data go back to June 2010, and thus to best align with the 2008 CDC data, we collected a 30% sample of Twitter data over a six month period between Jun 1, 2010 and Dec 31, 2010. There were a total of approximately 132M posts over all the 40 states in our sample thus compiled. Figure 4 shows heat map rendering of actual and predicted depression rates in various states—higher intensity colors imply greater depression rate reported by CDC (top) or greater SMDI (bottom). Using the exact percent of each state’s population that is depressed (as opposed to the binned values in Figure 3), the Pearson correlation between actual per-state rate of depression and those predicted by SMDI is found to be 0.51. This demonstrates that our metric can capture to a reasonable extent, the actual rates of depression known in these states. Demographic Analysis: Gender Differences In this section we perform a demographic-centric analysis of depression, comparing men and women. Our goal is to be able to examine whether the rates of depression as found by the BRFSS survey (same source as above) for men and women align with the SMDI rates. For the purpose, we utilize the same corpus of Twitter posts used in the analysis reported in Figure 4. This analysis however necessitated that we know the gender of the author of a post, which is not an available attribute from Twitter. Hence we utilized a gender classifier [6] that uses regex matching of Twitter’s self-reported first names of users with a gazetteer of names and gender collated from US Census, baby name lists and public Facebook profiles. As found in [6], a gender classifier of this type is known to yield accuracy between 85-90%. In our corpus, we obtained about 61% women and the rest men, after about 65% coverage over all users. For each post for which the author’s gender was inferred, we computed the individual-centric measure of SMDI (see eqn. (3)). Figure 5 shows the results. We observe that we can reasonably predict the percentages of men and women depressed. Women are known to suffer from 1.3 times more depression than men [11]—approximately that ratio is observed in our prediction as well (1.5 times in our case as given by the actual rates of percent depressed in Figure 5). This is denoted by the nearly parallel lines for predicted and actual percent of each gender who are depressed. However as with our previous analysis on geography, we observe a slight overestimation of depression when using our method. Interestingly, it appears that overestimation is higher for women than for men (notice the SMDI values specifically). One possibility is that women generally show greater emotional expressivity than men, typically negative in affect [8], hence leading to a bias in our model. Temporal Analysis In this final section on population-scale analyses, we focus on understanding some of the temporal patterns of depression, at diurnal and annual scales. We first use the same data as used in Figure 5 to study levels of SMDI over the course of a typical day (in local time) across men and women. This is shown in Figure 6. As in correspondence with our findings in Figure 5, we see that diurnal SMDI values for women (μ=-0.47) are higher than that of men (μ=-01.39; p<.001); although they have high correlation (R 2 =0.78). An interesting finding of this study is that, for both men and women, SMDI is higher during the night than during the day, with SMDI values peaking at around midnight, and the lowest value appearing around noon. As mentioned before, psychiatric literature on depression indicates that 8 out of 10 people suffering from depression exhibit worsened symptoms at night [15]: People become more prone to worrying thoughts at night due to loneliness, break from work, lack of energy, or other interactions between light/darkness and the nervous system. Next, we study the rhythms of SMDI over the course of a year, aggregated in pit-stops of months. For this purpose, we expand the above dataset, and derive two 30% samples of Twitter posts ranging between Jun 2010-May 2011, and Jan 2012-Dec 2012 (307M posts in all). The yearly patterns of SMDI for several US cities (chosen for exemplary purposes) with high depression rate (see Figure for details) are shown in Figure 7 (averaged across the two dataset snapshots). We also show an average trend for all posts, combining all locations (denoted as “All”). We observe that the yearly depression pattern shows a seasonal trend, with highest depression observed during winter season in the US (μ=0.47; Dec-Jan), while the lowest during summer (μ=-1.88; Jun-Aug) and to some extent fall (μ=-0.73; Sep-Nov). Clinical literature indicates that there is greater prevalence of depressive anxiety during winter at northern latitudes, sometimes attributed to insufficient exposure to light during winter [25]. Across the cities, we observe that SMDI in certain cities is more associated with weather conditions than others. Jacksonville and Seattle are both ranked high in terms of depression rates, however the variation in SMDI trend for Seattle (σ 2 =4.45) is much higher than that for Jacksonville (σ 2 =0.85). In fact, the percent difference between Seattle and Jacksonville’s SMDI during winter is 8% higher than that during summer. Note that Seattle’s seasonal weather variations are more extreme than those for Jacksonville, per National Oceanic and Atmospheric Administration (NOAA). As also supported by clinical literature, we thus conjecture that Twitter users based in Seattle are more prone to depressive symptoms during winter than in Jacksonville, or other low weather variability cities. However, notice that even during the summer, cities like Jacksonville and other low weather variation cities (e.g., Nashville) show considerable depression prevalence— SMDI for Jacksonville in summer is μ=-1.3, which is higher compared to some of the other cities. That is, it appears that while weather may have a strong role to play in the depression rates, there are likely to be other factors at play, for instance, socio-economic status, unemployment rates, which may explain the higher SMDI in these cities. To summarize, we observe persistent rhythms in depression expression on social media during the course of a day across men and women, and even at a seasonal level across a variety of locations. This provides us with a promising mechanism to monitor fine-grained temporal trends of depression across populations, demographics, and regions.",2,2,2,1,2,2,2,0,4,2,2,2,2,1,0,2,2,2,9,0
https://dl.acm.org/doi/abs/10.1145/2464464.2464480,1,10,,"De Choudhury, Counts, Horvits",3,1,0,0,2013,"Implications Through our experimental findings, we have demonstrated how sets of behavioral markers manifested in social media can be harnessed to predict depression-indicative postings, and thereby understand large-scale depression tendencies in populations. Since our predictions can be made considerably more frequently than BRFSS or CDC surveys, the resultant population-scale estimates of depression can be utilized time to time to enable early detection and rapid treatment of depression. Moreover, a primary challenge in public health is that several behavioral health concerns go under-reported—we hope that through our proposed technique, organizations may be able to gauge better such concerns and improve healthcare support. Note that our method of depression prediction, or the analyses are, however, not meant to be a replacement for traditional surveillance systems, or clinical diagnoses of depression. Rather, the population-scale trends over geography, time or gender may be a mechanism to trigger public health inquiry programs to take appropriate and needful measures, or allocate resources in a timely manner. Limitations Although the analysis of social media postings makes it possible to track depression levels in ways there are not feasible offline, there is an inherent population bias in our study. According to Pew Research Center [21], among the 74% of American adults who use the internet, only about 8% report using Twitter. Additionally, unlike surveys, we have little knowledge about people’s idiosyncratic behavior “behind the scenes”, their social, cultural and psychological environment, or socio-economic status. Potentially, the limitations of Twitter may be tackled in one way by adding complementary sources of behavioral data, such as about social ties from Facebook, web browsing behavior, or search query logs, in conjunction with health records, such as antidepressant purchases, or healthcare claims data. These opportunities remain ripe areas of future research. Privacy This research revolves around analysis individuals’ behavioral health and leverages information that may be considered sensitive, hence privacy is important. In all of our datasets, as well as the initial crowdsourcing study, we did not retain any information about users’ real identity (if available). In the Mechanical Turk task, individuals could opt out, if they wished to, from sharing their Twitter information; and those who did were informed that their data may be used for research purposes. It is also important to also note that we make use of public Twitter postings exclusively, and our predictions are made at the level of posts, instead of individuals, preventing any conspicuous associations that may arise between the observed social media activity of an individual and their actual psychological state.",2,2,2,1,2,2,2,0,4,2,2,2,2,1,0,2,2,2,9,0
https://dl.acm.org/doi/abs/10.1145/2464464.2464480,1,11,,"De Choudhury, Counts, Horvits",3,1,0,0,2013,"Mining and analysis of social media activity in order to understand a variety of public health phenomena has been gaining considerable traction recently among researchers. In this paper, we have demonstrated the potential of using social media as a reliable tool for measuring populationscale depression patterns. We adopted a crowdsourcing strategy of collecting ground truth data on depression from Twitter, and devised a variety of measures such as language, emotion, style and user engagement to build an SVM classifier. The classifier predicted with high accuracy (73%) whether or not a post on Twitter could be depression-indicative. Thereafter, the trained model was leveraged in a population-scale measurement metric of depression—called the social media depression index. Variety of analyses around geography, gender and time showed that SMDI can closely mirror CDC defined statistics on depression. In the future, we are interested in developing individual-centric predictive models that analyze a person’s social media feeds, and provide early warning/intervention if there are behavioral concerns out of the ordinary. Modeling the contagion of depressive disorders in social media is also an exciting future direction. ",2,2,2,1,2,2,2,0,4,2,2,2,2,1,0,2,2,2,9,0
https://aclanthology.org/W14-3207.pdf,2,1,,"Coppersmith, Drezde, Harman",3,0,1,1,2014,"The ubiquity of social media provides a rich opportunity to enhance the data available to mental health clinicians and researchers, enabling a better-informed and better-equipped mental health field. We present analysis of mental health phenomena in publicly available Twitter data, demonstrating how rigorous application of simple natural language processing methods can yield insight into specific disorders as well as mental health writ large, along with evidence that as-of-yet undiscovered linguistic signals relevant to mental health exist in social media. We present a novel method for gathering data for a range of mental illnesses quickly and cheaply, then focus on analysis of four in particular: post-traumatic stress disorder (PTSD), depression, bipolar disorder, and seasonal affective disorder (SAD). We intend for these proof-of-concept results to inform the necessary ethical discussion regarding the balance between the utility of such data and the privacy of mental health related information.",2,2,2,1,0,0,2,0,2,1,2,2,0,2,0,0,2,2,6,0
https://aclanthology.org/W14-3207.pdf,2,2,,"Coppersmith, Drezde, Harman",3,0,1,1,2014,"While mental health issues pose a significant health burden on the general public, mental health research lacks the quantifiable data available to many physical health disciplines. This is partly due to the complexity of the underlying causes of mental illness and partly due to longstanding societal stigma making the subject all but taboo. Lack of data has hampered mental health research in terms of developing reliable diagnoses and effective treatment for many disorders. Moreover, population-level analysis via traditional methods is time consuming, expensive, and often comes with a significant delay. In contrast, social media is plentiful and has enabled diverse research on a wide range of topics, including political science (Boydstun et al., 2013), social science (Al Zamal et al., 2012), and health at an individual and population level (Paul and Dredze, 2011; Dredze, 2012; Aramaki et al., 2011; Hawn, 2009). Of the numerous health topics for which social media has been considered, mental health may actually be the most appropriate. A major component of mental health research requires the study of behavior, which may be manifest in how an individual acts, how they communicate, what activities they engage in and how they interact with the world around them including friends and family. Additionally, capturing population level behavioral trends from Web data has previously provided revolutionary capabilities to health researchers (Ayers et al., 2014). Thus, social media seems like a perfect fit for studying mental health in both individual and overall trends in the population. Such topics have already been the focus of several studies (Coppersmith et al., 2014; De Choudhury et al., 2014; De Choudhury et al., 2013d; De Choudhury et al., 2013b; De Choudhury et al., 2013c; Ayers et al., 2013). What can we expect to learn about mental health by studying social media? How does a service like Twitter inform our knowledge in this area? Numerous studies indicate that language use, social expression and interaction are telling indicators of mental health. The well-known Linguistic Inquiry Word Count (LIWC), a validated tool for the psychometric analysis of language data (Pennebaker et al., 2007), has been repeatedly used to study language associated with all types of disorders (Resnik et al., 2013; Alvarez-Conrad et al., 2001; Tausczik and Pennebaker, 2010). Furthermore, social media is by nature social, which means that social patterns, a critical part of mental health and illness, may be readily observable in raw Twitter data. Thus, Twitter and other social media provide a unique quantifiable perspective on human behavior that may otherwise go unobserved, suggesting it as a powerful tool for mental health researchers. The main vehicle for studying mental health in social media has been the use of surveys, e.g., depression battery (De Choudhury, 2013) or personality test (Schwartz et al., 2013), to determine characteristics of a user coupled with analyzing their corresponding social media data. Work in this area has mostly focused on depression (De Choudhury et al., 2013d; De Choudhury et al., 2013b; De Choudhury et al., 2013c), and the number of users is limited by those that can complete the appropriate survey. For example, De Choudhury et al. (2013d) solicited Twitter users to take the CES-D and to share their public Twitter profile, analyzing linguistic and behavioral patterns. While this type of study has produced high quality data, it is limited in size (by survey respondents) and scope (to diagnoses which have a battery amenable to administration over the internet). In this paper we examine a range of mental health disorders using automatically derived samples from large amounts of Twitter data. Rather than rely on surveys, we automatically identify self-expressions of mental illness diagnoses and leverage these messages to construct a labeled data set for analysis. Using this dataset, we make the following contributions: • We demonstrate the effectiveness of our automatically derived data by showing that statistical classifiers can differentiate users with four different mental health disorders: depression, bipolar, post traumatic stress disorder and seasonal affective disorder. • We conduct a LIWC analysis of each disorder to measure deviations in each illness group from a control group, replicating previous findings for depression and providing new findings for bipolar, PTSD and SAD. • We conduct an open-vocabulary analysis that captures language use relevant to mental health beyond what is captured with LIWC. Our results open the door to a range of large scale analysis of mental health issues using Twitter.",2,2,2,1,0,0,2,0,2,1,2,2,0,2,0,0,2,2,6,0
https://aclanthology.org/W14-3207.pdf,2,3,,"Coppersmith, Drezde, Harman",3,0,1,1,2014,"For a good retrospective and prospective summary of the role of social media in mental health research, we refer the reader to De Choudhury (2013). De Choudhury identifies ways in which NLP has and can be used on social media data to produce what the relevant mental health literature would predict, both at an individual level and a population level. She proceeds to identify ways in which these types of analyses can be used in the near and far term to influence mental health research and interventions alike. Differences in language use have been observed in the personal writing of students who score highly on depression scales (Rude et al., 2004), forum posts for depression (Ramirez-Esparza et al., 2008), self narratives for PTSD (He et al., 2012; D’Andrea et al., 2011; Alvarez-Conrad et al., 2001), and chat rooms for bipolar (Kramer et al., 2004). Specifically in social media, differences have previously been observed between depressed and control groups (as assessed by internet-administered batteries) via LIWC: depressed users more frequently use first person pronouns (Chung and Pennebaker, 2007) and more frequently use negative emotion words and anger words on Twitter, but show no differences in positive emotion word usage (Park et al., 2012). Similarly, an increase in negative emotion and first person pronouns, and a decrease in third person pronouns, (via LIWC) is observed, as well as many manifestations of literature findings in the pattern of life of depressed users (e.g., social engagement, demographics) (De Choudhury et al., 2013d). Differences in language use in social media via LIWC have also been observed between PTSD and control groups (Coppersmith et al., 2014). For population-level analysis, surveys such as the Behavioral Risk Factor Surveillance System (BRFSS) are conducted via telephone (Centers for Disease Control and Prevention (CDC), 2010). Some of these surveys cover relatively few participants (often in the thousands), have significant cost, and have long delays between data collection and dissemination of the findings. However, De Choudhury et al. (2013c) presents a promising population-level analysis of depression that highlights the role of NLP and social media.",2,2,2,1,0,0,2,0,2,1,2,2,0,2,0,0,2,2,6,0
https://aclanthology.org/W14-3207.pdf,2,4,,"Coppersmith, Drezde, Harman",3,0,1,1,2014,"All data we obtain is public, posted between 2008 and 2013, and made available from Twitter via their application programming interface (API). Specifically, this does not include any data that has been marked as ‘private’ by the author or any direct messages. Diagnosed Group We seek users who publicly state that they have been diagnosed with various mental illnesses. Users may make such a statement to seek support from others in their social network, to fight the taboo of mental illness, or perhaps as an explanation of some of their behavior. Tweets were obtained using regular expressions on a large multi-year health related collection, e.g. “I was diagnosed with X.” We searched for four conditions: depression, bipolar disorder, post traumatic stress disorder (PTSD) and seasonal affective disorder (SAD). The matched diagnosis tweets were manually labeled as to whether the tweet contained a genuine statement of a mental health diagnosis. Table 1 shows examples of both genuine statements of diagnosis and disingenuous statements (often jokes or quotes). Next, we retrieved the most recent tweets (up to 3200) for each user with a genuine diagnosis tweet. We then filtered the users to remove those with fewer than 25 tweets and those whose tweets were not at least 75% in English (measured using the Compact Language Detector1 ). These filtering steps left us with users that were considered positive examples. Table 2 indicates the number of users and tweets found for each of the mental health categories examined. We manually examined and annotated only half the diagnosis statements for depression – indicating there are likely 800-900 depression users available via these automatic methods from our collection, compared to the 117 obtained via the methods of De Choudhury et al. (2013d). Additionally, we emphasize the low cost and effort of our automated effort as compared to their crowdsourced survey methods. The difference in collection methods also suggests that the two have a reasonable chance of being complementary. This is especially significant when considering disorders with lower incidence rates than depression (arguably the highest), where respondents to crowdsourced surveys or self-stated diagnoses alike are rare. This method is similar in spirit to that of De Choudhury et al. (2013c), where they inferred a tweet-level classifier for depression from userlevel labels (specifically, tweets from the past three months from users scoring highly on CES-D for the positive class and conversely for the negative). Control Group To build models for analysis and to validate the data, we also need a sample of the general population to use as an approximation of community controls. We follow a similar process: randomly select 10k usernames from a list of Twitter users who posted to a separate random historical collection within a selected two week window, downloaded the 3200 most recent tweets from these users, and apply our two filters: at least 25 tweets and 75% English. This yields a control group of 5728 random users, whose 13.7 million tweets were used as negative examples. Caveats Our method for finding users with mental health diagnoses has significant caveats: 1) the method may only capture a subpopulation of each disorder (i.e., those who are speaking publicly about what is usually a very private matter), which may not truly represent all aspects of the population as a whole. 2) This method in no way verifies whether this diagnosis is genuine (i.e., people are not always truthful in self-reports). However, given the stigma often associated with mental illness, it seems unlikely users would tweet that they are diagnosed with a condition they do not have. 3) The control group is likely contaminated by the presence of users that are diagnosed with the various conditions investigated. We make no attempt to remove these users, and if we assume that the prevalence of each disorder in the general population is similar in our control groups, we likely have hundreds of such diagnosed users contaminating our control training data. 4) Twitter users are not an entirely representative sample of the population as a whole. Despite these caveats, we find that this method yielded promising results as discussed in the next sections. Comorbidity Since some of these disorders have high comorbidity, there are some users in more than one class (e.g., those that state a diagnosis for PTSD and depression): Bipolar and depression have 19 users in common (4.8% of the bipolar users, 4.3% of the depression users), PTSD and depression share 10 (4.0% of PTSD, 2.2% of depression), and bipolar and PTSD share 9 (2.2% of bipolar, 3.6% of PTSD). Two users state diagnosis of bipolar, PTSD and depression (less than 1% of each set). No users stated diagnoses of both SAD and any other condition investigated.",2,2,2,1,0,0,2,0,2,1,2,2,0,2,0,0,2,2,6,0
https://aclanthology.org/W14-3207.pdf,2,5,,"Coppersmith, Drezde, Harman",3,0,1,1,2014,"We quantify various aspects of each user’s language usage and pattern of life via automated methods, extracting features for subsequent machine learning. We use these to (1) replicate previous findings, (2) build classifiers to separate diagnosed from control users, and (3) introspect on those classifiers. Introspection here shows us what quantified signals in the content the classifiers base their decision on, and thus we can gain intuition about what signals are present in the content relevant to mental health.",2,2,2,1,0,0,2,0,2,1,2,2,0,2,0,0,2,2,6,0
https://aclanthology.org/W14-3207.pdf,2,6,,"Coppersmith, Drezde, Harman",3,0,1,1,2014,"LIWC provides clinicians with a tool for gathering quantitative data regarding the state of a patient from the patient’s writing (Pennebaker et al., 2007). Previous work has found signal in the ‘positive affect’ and ‘negative affect’ categories of the LIWC when applied to social media (including Twitter), so we examine their correlations separately, as well as in the context of other LIWC categories (De Choudhury et al., 2013a). In all, we examine some of the LIWC categories directly (Swear, Anger, PosEmo, NegEmo, Anx) and combine pronoun classes by linguistic form: I and We classes are combined to form Pro1, You becomes Pro2 and SheHe and They become Pro3. Each of these classes provides one feature used by subsequent machine learning and our other analyses.",2,2,2,1,0,0,2,0,2,1,2,2,0,2,0,0,2,2,6,0
https://aclanthology.org/W14-3207.pdf,2,7,,"Coppersmith, Drezde, Harman",3,0,1,1,2014,"Language models are commonly used to estimate how likely a given sequence of words is. Generally, an n-gram language model refers to a model that examines strings of up to n words long. This is less than ideal for applications in social media: spelling errors, shortenings, space removal, and other aspects of social media data (especially Twitter) confounds many traditional word-based approaches. Thus, we employ two LMs, first a traditional 1-gram LM (ULM) that examines the probability of each whole word. Second, a character 5-gram LM (CLM) to examine sequences of up to 5 characters. LMs model the likelihood of sequences from training data. In our case, we build one of each model from the positive class (tweets from one class of diagnosed users – e.g., PTSD), yielding ULM+ and CLM+. We also build one of each model from the negative class (control users), yielding ULM− and CLM−. We score each tweet by computing these probabilities and classifying it according to which model has a higher probability (e.g., for a given tweet, is ULM+ > ULM−?).",2,2,2,1,0,0,2,0,2,1,2,2,0,2,0,0,2,2,6,0
https://aclanthology.org/W14-3207.pdf,2,8,,"Coppersmith, Drezde, Harman",3,0,1,1,2014,"For brevity, we only briefly discuss the pattern of life analytics, since they do not depend on significant NLP. They examine how correlates found to be significant in the mental health literature may manifest and be measured in social media data. These are all imperfect proxies for the findings from the literature, but our experiments will demonstrate that they do collectively provide information relevant to mental health. For each of the following analytics we extract one feature to use in subsequent machine learning. Social engagement has been correlated with positive mental health outcomes (Greetham et al., 2011; Berkman et al., 2000; Organization, 2001; De Choudhury et al., 2013d), which is difficult to measure directly so we examine various ways in which this may be manifest in a user’s tweet stream: Tweet rate measures how often a twitter user posts (a measure of overall engagement with this social media platform) and Proportion of tweets with @mentions measures how often a user posts ‘in conversation’ (for lack of better terms) with other users. Number of @mentions is a measure of how often the user in question engages other users, while Number of self @mentions is a measure of how often the user responds to mentions of themselves (since users rarely include their own username in a tweet). To estimate the size of a user’s social network, we calculate Number of unique users @mentioned and Number of users @mentioned at least 3 times, respectively. For each of the following analytics, we calculate the proportion of a user’s tweets that the analytic finds evidence in: Insomnia and sleep disturbance is often a symptom of mental health disorders (Weissman et al., 1996; De Choudhury et al., 2013d), so we calculate the proportion of tweets that a user makes between midnight and 4am according to their local timezone. Exercise has also been correlated with positive mental health outcomes (Penedo and Dahn, 2005; Callaghan, 2004), so we examine tweets mentioning one of a small set of exercise-related terms. We also use an English sentiment analysis lexicon from Mitchell et al. (2013) to score individual tweets according to the presence and valence of sentiment words. We apply no thresholds, so any tweet with a sentiment score above 0 was considered positive, below 0 was considered negative, and those with score 0 were considered to have no sentiment. Thus we use the proportion of Insomnia, Exercise, Positive Sentiment and Negative Sentiment tweets as features in subsequent machine learning and analysis.",2,2,2,1,0,0,2,0,2,1,2,2,0,2,0,0,2,2,6,0
https://aclanthology.org/W14-3207.pdf,2,9,,"Coppersmith, Drezde, Harman",3,0,1,1,2014,"We present three types of experiments to evaluate the quality and character of these data, and to demonstrate some quantifiable mental health signals in Twitter. First, we validate our method for obtaining data by replicating previous findings using LIWC. Next, we build classifiers to distinguish each group from the control group, demonstrating that there is useful signal in the language of each group, and compare these classifiers. Finally, we analyze the correlations between our analytics and classifiers to uncover relationships between them and derive insight into quantifiable and relevant mental health signals in Twitter. Validation First, we provide some validation for our novel method for gathering samples. We demonstrate that language use, as measured by LIWC, is statistically significantly different between control and diagnosed users. Figure 1 shows the proportion of tweets from each user that scores positively on various LIWC categories (i.e., have at least one word from that category). Box-and-whiskers plots (Tukey, 1977)2 summarize a distribution of observations and ease comparison between them (here, each observation is the proportion of a user’s tweets that score positively on LIWC). The median of the distribution is the black horizontal line in the middle of the bar, the bar covers the inter quartile range (where 50% of the observations lie), the whiskers are a robust estimate of the extent of the data, with outliers plotted as circles beyond the whiskers. An approximation of statistical significance is indicated by the pinched in notches on each bar. If the notches on the bars do not overlap, the differences between those distributions is different (α<0.05, 95% confidence interval). Each bar is colored according to diagnosis, and each group of 5 bars notes the scores for one LIWC category. Differences that reach statistical significance from the control group are noted with asterisks (e.g., Pro1, Swear, Anger, NegEmo and Anxiety are statistically significantly different for the depression group). Importantly, this replicates previous findings of significant differences between depressed users (according to an internetadministered diagnostic battery): significant increases are expected in NegEmo, Anger, Pro1 and Pro3 and no change in PosEmo, given all previous work (Park et al., 2012; Chung and Pennebaker, 2007; De Choudhury et al., 2013d). We replicate all these findings except the increase in Pro3 (which only De Choudhury et al. (2013d) found), which validates our data collection methods. Classification We next explore the ability of the various analytics to separate diagnosed from control users and assess performance on a leaveone-out cross-validation task. We train a log linear classifier on the features described in §4 using scikit-learn (Pedregosa et al., 2011). The receiver operating characteristic (ROC) curves in Figures 2 and 3 demonstrate performance of the various classifiers at the task of separating diagnosed from control groups. In all cases, the correct detections (or hits) are on the y-axis and the false detections (or false alarms) are on the x-axis. Figure 2 compares performance across diagnoses, one line per disorder. Figure 3 shows one plot per mental health condition, with the performance of the various analytics, individually and in concert as individual ROC curves. A few trends emerge – 1) All analytics show some ability to separate the classes, indicating they are finding useful signals. 2) The LMs provide superior performance to the other analytics, indicating there are more signals present in the language than are captured by LIWC and pattern-of-life analytics. For readability we do not show the performance of all combinations of analytics, but they perform as expected: any set of them perform equal to or better than their individual components. Taken together, this indicates that there is information relevant to separating diagnosed users from controls in all the analytics discussed here. Furthermore, this highlights that there remains significant signals to be uncovered and understood in the language of social media. These trends also allow us to compare the disorders as manifest in language usage, though this tends to raise more questions than it answers. Generally, the pattern-of-life analytics and LIWC are on par, but this is decidedly not true for depression, where pattern-of-life seems to perform especially poorly, and for SAD, where pattern-of-life seems to perform especially well. This indicates that the depression users have patterns-of-life that look more similar to the controls than is the case for the other disorders (perhaps especially surprising given the inclusion of the sentiment lexicon) and that there may be significant correlation between pattern-of-life factors and SAD.",2,2,2,1,0,0,2,0,2,1,2,2,0,2,0,0,2,2,6,0
https://aclanthology.org/W14-3207.pdf,2,10,,"Coppersmith, Drezde, Harman",3,0,1,1,2014,"To examine correlations between the analytics and the linguistic content they depend on, we scored a random subset of 1 million tweets from control users with each of the linguistic analytics, and plot their Pearson’s correlation coefficients (r) in Figure 4. A simple overlap of wordlists is not sufficient to assess the true utility of these methods since it does not take into account the frequency of occurrence of each word, nor the correlation between these words in real data (e.g., does a classifier based on the LIWC category Swear provide redundant information to the sentiment analysis). Each row and column in Figure 4 represents one of the 17 analytics, in the same order. Colors denote Bonferroni-corrected Pearson’s r for statistically significant correlations between the analytic on the row and column. Correlations that do not reach statistical significance are in aquamarine (corresponding to r=0). Excluded for brevity is a sanity check of a χ 2 test between the analytics to assert they were scoring significantly differently. The strong correlations between the various LIWC analytics, notably Swear, Anger and NegEmo, likely indicates that the analytics are triggered by the same word(s) – in this case profanity. Similarly for LIWC’s PosEmo and the sentiment lexicon – ‘happy’ for example. The correlation between CLM for various diagnoses is particularly intriguingly, as it is in line with known patterns of comorbidity: major depressive disorder, PTSD, and bipolar all have observed comorbidity (Brady et al., 2000; Campbell et al., 2007; McElroy et al., 2001) while SAD is currently considered a specifier of major depressive disorder or bipolar disorder (American Psychiatric Association, 2013; Lurie et al., 2006), without published findings indicating comorbidity. Indeed our small sample dataset follows the same trends, where we observed users with multiple diagnoses exist within depression, PTSD, and bipolar, but none exist with SAD. The correlation observed is too large to be solely attributed to those users shared between the groups, though (correlations at most r = 0.05 would be attributable to that alone). Furthermore, when taken in combination with the different patterns exhibited by the groups as seen in Figure 1, this correlation is not solely attributable to LIWC categories either. At its core, these correlations seem to suggest that similar language is employed by users diagnosed with these occasionally comorbid disorders, and dissimilar language by users with SAD. This should be taken as merely suggestive of the type of analysis one could do, though, since the literature does not present a strong and clear prediction for the comorbidity and exhibited symptoms (to include language use). Interestingly, the lack of (or negative) correlation between most of the analytics again highlights the complexity of the mental illnesses and the divergent signals it presents. Additionally, the lack of correlation between ULM and the other models is to be expected, since they are basing their scores on significantly more words (or different signals as is the case for CLM). Each one of these analytics is highly imperfect, and often give contradictory evidence, but when combined, the machine learning algorithms are able to sort through the conflicting signals with some success.",2,2,2,1,0,0,2,0,2,1,2,2,0,2,0,0,2,2,6,0
https://aclanthology.org/W14-3207.pdf,2,11,,"Coppersmith, Drezde, Harman",3,0,1,1,2014,"We demonstrate quantifiable signals in Twitter data relevant to bipolar disorder, major depressive disorder, post-traumatic-stress disorder and seasonal affective disorder. We introduce a novel method for automatic data collection and validate its veracity by 1) replicating observations of significant differences between depressed and control user groups and 2) constructing classifiers capable of separating diagnosed from control users for each disorder. This data allows us to demonstrate equivalent differences in language use (according to LIWC) for bipolar, PTSD, and SAD. Furthermore, we provide evidence that more information relevant to mental health is encoded in language use in social media (above and beyond that captured by methods based on the mental health literature). By examining correlations between the various analytics investigated, we provide some insight into what quantifiable linguistic information is captured by our classifiers. We finally demonstrate the utility of examining multiple disorders simultaneously and other larger analyses, difficult or impossible with other methods. Crucially, we expect that these novel data collection methods can provide complementary information to existing survey-based methods, rather than supplant them. For many disorders rarer than depression (which has comparatively high incidence rates), we suspect that finding any data will be a challenge, in which case combining these methods with the existing survey collection methods may be the best way to obtain sufficient amounts of data for statistical analyses. Since the LMs take more information into account when modeling the language usage of diagnosed and control users, it is unsurprising that they outperform LIWC and pattern-of-life analyses alone, but this is evidence of as-of-yet undiscovered linguistic differences between diagnosed and control users for all disorders investigated. Uncovering and interpreting these signals can be best accomplished through collaboration between NLP and mental health researchers. Naturally, some caveats come with these results: while identifying genuine self-statements of diagnosis in Twitter works well for some conditions, others exist for which there were few or no diagnoses stated. For Alzheimer’s, the demographic with the majority of diagnoses does not frequently use Twitter (or likely any social media). Eating disorders are also elusive via this method, though related automatic methods (e.g., using disorder-related hashtags) may address this. Finally, those willing to publicly reveal a mental health diagnosis may not be representative of the population suffering from that mental illness. All these experiments, taken together, indicate that there are a diverse set of quantifiable signals relevant to mental health observable in Twitter. They indicate that individual- and population-level analyses can be made cheaper and more timely than current methods, yet there remains as-of-yet untapped information encoded in language use – promising a rich collaboration between the fields of natural language processing and mental health.",2,2,2,1,0,0,2,0,2,1,2,2,0,2,0,0,2,2,6,0
https://www.aaai.org/ocs/index.php/ICWSM/ICWSM14/paper/viewPaper/8079 ,3,1,,"Coppersmith, Drezde, Harman",3,0,1,1,2014,"Traditional mental health studies rely on information primarily collected through personal contact with a health care professional. Recent work has shown the utility of social media data for studying depression, but there have been limited evaluations of other mental health conditions. We consider post traumatic stress disorder (PTSD), a serious condition that affects millions worldwide, with especially high rates in military veterans. We also present a novel method to obtain a PTSD classifier for social media using simple searches of available Twitter data, a significant reduction in training data cost compared to previous work. We demonstrate its utility by examining differences in language use between PTSD and random individuals, building classifiers to separate these two groups and by detecting elevated rates of PTSD at and around U.S. military bases using our classifiers.",2,2,2,0,0,0,0,0,0,2,2,0,1,2,0,0,2,1,6,0
https://www.aaai.org/ocs/index.php/ICWSM/ICWSM14/paper/viewPaper/8079 ,3,2,,"Coppersmith, Drezde, Harman",3,0,1,1,2014,"Mental health conditions affect a significant percentage of the U.S. adult population each year, including depression (6.7%), eating disorders like anorexia and bulimia (1.6%), bipolar disorder (2.6%) and post traumatic stress disorder (PTSD) (3.5%).1 PTSD and other mental illnesses are difficult to diagnose, with competing standards for diagnosis based on self-reports and testimony from friends and relatives.2 In recent years, several studies have turned to social media data to study mental health, since it provides an unbiased collection of a person’s language and behavior, which has been shown to be useful in diagnosing conditions (De Choudhury 2013). Additionally, from a public health standpoint, social media data and Web data in general have enabled large scale analyses of a population’s health status beyond what has previously been possible with traditional methods (Ayers et al. 2013). While social media provides ample data for many types of public health analysis (Paul and Dredze 2011), mental health studies still face serious challenges. First, other health work in social media, such as disease surveillance (Brownstein, Freifeld, and Madoff 2009; Chew and Eysenbach 2010; Lamb, Paul, and Dredze 2013) and modeling (Sadilek, Kautz, and Silenzio 2012), rely on explicit mentions of illness or health issues; if people are sick, they say so. In contrast, mental health conditions largely display implicit changes in language and behavior, such as a switch in the types of topics, a shift in word usage or a shift in frequency of posts. While De Choudhury et al. (2013) find some examples of explicit depression mentions, the focus is on more subtle changes in language (e.g., pronoun use). Second, obtaining labeled data for a mental health condition is challenging since we are examining implicit features of language. De Choudhury et al. (2013) rely on (crowdsourced) volunteers to take depression surveys and offer their Twitter feed for research. While this yields reliable data, it is time-consuming and challenging to build large data sets for a diverse set of mental health conditions. Furthermore, the necessary mental health evaluations such as the DSM (Diagnostic and Statistical Manual of Mental Disorders)3 , are difficult to perform as these evaluations require a trained diagnostician and have been criticized as unscientific and subjective (Insel 2013). Thus, relying on data from crowdsourced volunteers to build datasets of users with diverse mental health conditions is difficult, and perhaps untenable. We provide an alternate method for gathering samples that partially ameliorate these problems – ideally to be used in concert with existing methods. In this paper, we study PTSD in Twitter data, one of the first studies to consider social media for a mental health condition beyond depression (De Choudhury, Counts, and Horvitz 2013; De Choudhury et al. 2013; Rosenquist, Fowler, and Christakis 2010). Rather than rely on traditional PTSD diagnostic tools (Foa 1995) for finding data, we demonstrate that some PTSD users can be easily and automatically identified by scanning for tweets expressing explicit diagnoses. While it is natural to be suspicious of self-identified reporting, we find that self-identifying PTSD users have demonstrably different language usage patterns from the random users, according to the Linguistic Inquiry Word Count (LIWC), a psychometrically validated analysis tool (Pennebaker, Chung, and Ireland 2007). We demonstrate elsewhere (Coppersmith, Dredze, and Harman Submitted 2014) that data obtained in this way replicates analyses performed via LIWC on the crowdsourced survey respondents of De Choudhury et al. (2013). We also demonstrate that users who self-identify are measurably different from random users by learning a classifier to discriminate between self-identified and random users. We further show how this data can be used to train a classifier that detects elevated incidences of PTSD in tweets from U.S. military bases as compared to the general U.S. population, with a further increase around bases that deployed combat troops overseas. We intend for this initial finding (which is small, but statistically significant) to be a demonstration of the types of analysis Twitter data enables for public health. Given the small effect size, replication and further study are called for.",2,2,2,0,0,0,0,0,0,2,2,0,1,2,0,0,2,1,6,0
https://www.aaai.org/ocs/index.php/ICWSM/ICWSM14/paper/viewPaper/8079 ,3,3,,"Coppersmith, Drezde, Harman",3,0,1,1,2014,"We used an automated analysis to find potential PTSD users, and then refined the list manually. First, we had access to a large multi-year historical collection from the Twitter keyword streaming API, where keywords were selected to focus on health topics. We used a regular expression4 to search for statements where the user self-identifies as being diagnosed with PTSD. The 477 matching tweets were manually reviewed to determine if they indicated a genuine statement of a diagnosis for PTSD. Table 1 shows examples from the 260 tweets that indicated a PTSD diagnosis. Next, we selected the username that authored each of these tweets and retrieved up to the 3200 most recent tweets from that user via the Twitter API. We then filtered out users with less than 25 tweets and those whose tweets were not at least 75% in English (measured using an automated language ID system.) This filtering left us with 244 users as positive examples. We repeated this process for a group of randomly selected users. We randomly selected 10,000 usernames from a list of users who posted to our historical collection within a selected two week window. We then downloaded all tweets from these users. After filtering (as above) 5728 random users remain, whose tweets were used as negative examples.",2,2,2,0,0,0,0,0,0,2,2,0,1,2,0,0,2,1,6,0
https://www.aaai.org/ocs/index.php/ICWSM/ICWSM14/paper/viewPaper/8079 ,3,4,,"Coppersmith, Drezde, Harman",3,0,1,1,2014,"We use our positive and negative PTSD data to train three classifiers: one unigram language model (ULM) examining individual whole words, one character n-gram language model (CLM), and one from the LIWC categories above. The LMs have been shown effective for Twitter classification tasks (Bergsma et al. 2012) and LIWC has been previously used for analysis of mental health in Twitter (De Choudhury et al. 2013). The language models measure the probability that a word (ULM) or a string of characters (CLM) was generated by the same underlying process as the training data. Here, one of each language model (clm+ and ulm+) is trained from the tweets of PTSD users, and a second (clm− and ulm−) from the tweets from random users. Each test tweet t is scored by comparing proabilities from each LM: A threshold of 1 for s divides scores into positive and negative classes. In a multi-class setting, the algorithm minimizes the cross entropy, selecting the model with the highest probability. For each user, we calculate the proportion of tweets scored positively by each LIWC category. These proportions are used as a feature vector in a loglinear regression model (Pedregosa et al. 2011). Prior to training, we preprocess the text of each tweet: we replaced all usernames with a single token (USER), lowercased all text, and removed extraneous whitespace. We also excluded any tweet that contained a URL, as these often pertain to events external to the user (e.g., national news stories). In total, we used 463k PTSD tweets and sampled 463k non-PTSD tweets to create a balanced data set.",2,2,2,0,0,0,0,0,0,2,2,0,1,2,0,0,2,1,6,0
https://www.aaai.org/ocs/index.php/ICWSM/ICWSM14/paper/viewPaper/8079 ,3,5,,"Coppersmith, Drezde, Harman",3,0,1,1,2014,"Numerous studies have investigated the language that PTSD sufferers use in “trauma narratives” describing their traumatic experiences (for a review, see (O’Kearney and Perrott 2006)), but only a handful of these studies have used automated analysis of language. The Linguistic Inquiry Word Count (LIWC) (Pennebaker, Chung, and Ireland 2007) has been used to analyze the narratives of 28 female assault victims being treated for chronic PTSD, and found that the LIWC cognitive words category was inversely correlated with post-treatment anxiety, and that social adjustment was negatively related to negative emotion words and death words (Alvarez-Conrad, Zoellner, and Foa 2001). We conduct a LIWC analysis of the PTSD and non-PTSD tweets to determine if there are differences in the language usage of PTSD users. We applied the LIWC battery and examined the distribution of words in their language. Each tweet was tokenized by separating on whitespace. For each user, for a subset of the LIWC categories, we measured the proportion of tweets that contained at least one word from that category. Specifically, we examined the following nine categories: first, second and third person pronouns, swear, anger, positive emotion, negative emotion, death, and anxiety words. Second person pronouns were used significantly less often by PTSD users, while third person pronouns and words about anxiety were used significantly more often. Unsurprisingly, given the stylistic and topical difference between tweets and trauma narratives, we do not observe the same trends as Alvarez-Conrad et al. (2001), but the fact that significant differences in language use exist is an encouraging demonstration of the effectiveness of our data.",2,2,2,0,0,0,0,0,0,2,2,0,1,2,0,0,2,1,6,0
https://www.aaai.org/ocs/index.php/ICWSM/ICWSM14/paper/viewPaper/8079 ,3,6,,"Coppersmith, Drezde, Harman",3,0,1,1,2014,"Previous work on mental health in social media used validated psychological surveys to obtain mental health labels for users (De Choudhury et al. 2013). In comparison, our data labeling method is much faster and easier, though the question remains: are the obtained labels reliable? We answer this question by evaluating various classifiers in terms of their ability to differentiate PTSD and random users. If a classifier can learn to differentiate these users, then we can infer that it is finding a useful signal from the data. If the labels are unreliable, we would expect random performance from the classifier. We evaluated the classifiers via leave-one-out cross validation setting in both a balanced and a non-balanced dataset. In the balanced data set, a single PTSD and non-PTSD user is left out. In the non-balanced setting, each fold held out a single PTSD user and non-PTSD users proportional to the overall ratio between positive and negative training examples, ensuring identical ratios in each training fold. Leaveone-out cross validation provides maximum training data while evaluating every user in turn. We obtained different operating points by varying the classification threshold for s. The results are shown in Figure 1, a receiver operating characteristic (ROC) curve for the proportion of correct detection (y-axis) against the proportion of false alarms (xaxis). In decreasing order of performance is ULM, CLM, and finally LIWC. The non-random performance of our classifiers at separating these classes is further evidence that the data collection method yields sensible data. This additionally indicates that there is more linguistic signal relevant to the separation of users than is captured by LIWC alone.",2,2,2,0,0,0,0,0,0,2,2,0,1,2,0,0,2,1,6,0
https://www.aaai.org/ocs/index.php/ICWSM/ICWSM14/paper/viewPaper/8079 ,3,7,,"Coppersmith, Drezde, Harman",3,0,1,1,2014,"Previous work on detecting health trends for Twitter have shown that imperfect classifiers can still inform us about public health trends. We consider whether our classifier can detect different incidence levels of PTSD in populations with different risk factors, where a higher incidence of PTSD tweets could indicate both an elevated awareness of PTSD, as well as tweets from users with the condition. Since U.S. military personnel have a higher PTSD rate than the general population, we compared these two populations. However, we do not have a mechanism for separating Twitter accounts of military personnel from the general population, so we instead focused on a geographic division. We selected geographic regions in the U.S. that housed troops recently involved as ‘boots on the ground’ in the conflicts overseas. Since statistics for deployments are not widely available, we asked a retired service member (not among the authors) who served overseas during the recent conflicts to select installations that would represent this sample, as well as military installations that deployed less during recent conflicts or were not used as ‘boots on the ground’ since we would expect a lower, but still elevated, rate of PTSD. For each base, we created a bounding box that covered the full base and as little of the surrounding area as possible. These bounding boxes resulted in an average of 407k tweets per box (median 358k, standard deviation 296k) in 2013. To represent the civilian population, we selected both urban and rural areas across the spectrum of civilian life: major cities, vacation spots, and rural townships (Table 2). These bounding boxes resulted in an average of 711k tweets per box (median 494k, standard deviation 774k) in 2013. We collected all geocoded tweets in our bounding boxes during 2013 (Twitter location streaming API). We used our CLM classifier to identify PTSD tweets in these bounding boxes, and computed the incidence rate by normalizing by the total number of tweets in the bounding box. We compare the cartesian product of military bases and civilian areas, noting for each comparison which incidence is greater. Our null hypothesis is that the incidence of PTSDlike tweets is equivalent across military and civilian areas. In 248 out of 342 comparisons military areas have higher PTSD incidence than civilian areas. A binomial test indicates this is statistically significant (p < 3 × 10−17), rejecting the null hypothesis. Our secondary hypothesis – frequently-deploying installations have higher PTSD rates than less-frequently-deploying locations – is tested similarly In 53 of 81 comparisons, the frequently-deploying installations have higher incidence rates (p = 0.007). Finally, we expect no difference between rural and urban areas; indeed, the rates were not statistically significant (p > 0.8). This sort of analysis is difficult using traditional population-analysis methods, so predictions from the literature are few, except work comparing PTSD veterans in rural and urban areas, which found no significant differences (Elhai et al. 2004). Though statistically significant, this effect is small with approximately 1% more PTSD-like tweets in military areas than civilian areas and approximately 0.7% more PTSD-like tweets in frequently-deploying military areas as compared to less-frequently deploying areas. Thus, this result requires further study and replication, but it is suggestive of exciting new avenues for population level mental health research.",2,2,2,0,0,0,0,0,0,2,2,0,1,2,0,0,2,1,6,0
https://www.aaai.org/ocs/index.php/ICWSM/ICWSM14/paper/viewPaper/8079 ,3,8,,"Coppersmith, Drezde, Harman",3,0,1,1,2014,"Mental health is a growing problem, and one for which social media plays a unique role. We have presented the first analysis of social media for the study of individuals with post traumatic stress disorder. Unlike most previous work, our labeled dataset comes from automated searching of raw Twitter data followed by manual curation. Using a classification task, we demonstrate that this dataset captures real differences between PTSD and non-PTSD users. Furthermore, we analyzed our data using the standard LIWC battery and found statistically significant differences in language use. Finally, we used one of our PTSD classifiers to identify and evaluate trends of PTSD incidence in and around U.S. military installations, with an even higher rate in populations more likely to have been deployed into combat – a statistically significant finding, but with a small effect size. In light of this, We treat this finding cautiously – it should be replicated with other classifiers, more geographic regions, and/or a more explicit group of military users but it is at least suggestive of the sort of population-level analysis enabled by this data and these techniques. There remain several important open questions. Do users who self-report diagnoses differ from other diagnosed individuals, perhaps sharing more relevant mental health information? What other mental health conditions can be studied using our approach of identifying self-diagnoses? Finally, what opportunities exist for interventions with identified users? What linguistic signals are present in social media but not captured by LIWC? Pursuing the answers to these questions will provide many exciting opportunities for mental health research and help to address this serious public health concern.",2,2,2,0,0,0,0,0,0,2,2,0,1,2,0,0,2,1,6,0
https://www.pnas.org/content/115/44/11203?utm_source=yxnews&utm_medium=mobile,4,1,,"Eichstaedt, Smith, Merchant, Ungar, Crutchley, Preotiuc-Pietro, Asch, Schwartz",8,0,0,0,2018,"Depression, the most prevalent mental illness, is underdiagnosed and undertreated, highlighting the need to extend the scope of current screening methods. Here, we use language from Facebook posts of consenting individuals to predict depression recorded in electronic medical records. We accessed the history of Facebook statuses posted by 683 patients visiting a large urban academic emergency department, 114 of whom had a diagnosis of depression in their medical records. Using only the language preceding their first documentation of a diagnosis of depression, we could identify depressed patients with fair accuracy [area under the curve (AUC) = 0.69], approximately matching the accuracy of screening surveys benchmarked against medical records. Restricting Facebook data to only the 6 months immediately preceding the first documented diagnosis of depression yielded a higher prediction accuracy (AUC = 0.72) for those users who had sufficient Facebook data. Significant prediction of future depression status was possible as far as 3 months before its first documentation. We found that language predictors of depression include emotional (sadness), interpersonal (loneliness, hostility), and cognitive (preoccupation with the self, rumination) processes. Unobtrusive depression assessment through social media of consenting individuals may become feasible as a scalable complement to existing screening and monitoring procedures.",2,2,2,2,2,2,0,2,4,2,2,1,1,2,0,0,2,2,7,0
https://www.pnas.org/content/115/44/11203?utm_source=yxnews&utm_medium=mobile,4,2,,"Eichstaedt, Smith, Merchant, Ungar, Crutchley, Preotiuc-Pietro, Asch, Schwartz",8,0,0,0,2018,"Each year, 7–26% of the US population experiences depression (1, 2), of whom only 13–49% receive minimally adequate treatment (3). By 2030, unipolar depressive disorders are predicted to be the leading cause of disability in high-income countries (4). The US Preventive Services Task Force recommends screening adults for depression in circumstances in which accurate diagnosis, treatment, and follow-up can be offered (5). These high rates of underdiagnosis and undertreatment suggest that existing procedures for screening and identifying depressed patients are inadequate. Novel methods are needed to identify and treat patients with depression. By using Facebook language data from a sample of consenting patients who presented to a single emergency department, we built a method to predict the first documentation of a diagnosis of depression in the electronic medical record (EMR). Previous research has demonstrated the feasibility of using Twitter (6, 7) and Facebook language and activity data to predict depression (8), postpartum depression (9), suicidality (10), and posttraumatic stress disorder (11), relying on self-report of diagnoses on Twitter (12, 13) or the participants’ responses to screening surveys (6, 7, 9) to establish participants’ mental health status. In contrast to this prior work relying on self-report, we established a depression diagnosis by using medical codes from an EMR. As described by Padrez et al. (14), patients in a single urban academic emergency department (ED) were asked to share access to their medical records and the statuses from their Facebook timelines. We used depression-related International Classification of Diseases (ICD) codes in patients’ medical records as a proxy for the diagnosis of depression, which prior research has shown is feasible with moderate accuracy (15). Of the patients enrolled in the study, 114 had a diagnosis of depression in their medical records. For these patients, we determined the date at which the first documentation of a diagnosis of depression was recorded in the EMR of the hospital system. We analyzed the Facebook data generated by each user before this date. We sought to simulate a realistic screening scenario, and so, for each of these 114 patients, we identified 5 random control patients without a diagnosis of depression in the EMR, examining only the Facebook data they created before the corresponding depressed patient’s first date of a recorded diagnosis of depression. This allowed us to compare depressed and control patients’ data across the same time span and to model the prevalence of depression in the larger population (∼16.7%)",2,2,2,2,2,2,0,2,4,2,2,1,1,2,0,0,2,2,7,0
https://www.pnas.org/content/115/44/11203?utm_source=yxnews&utm_medium=mobile,4,3,,"Eichstaedt, Smith, Merchant, Ungar, Crutchley, Preotiuc-Pietro, Asch, Schwartz",8,0,0,0,2018,"Prediction of Depression. To predict the future diagnosis of depression in the medical record, we built a prediction model by using the textual content of the Facebook posts, post length, frequency of posting, temporal posting patterns, and demographics (Materials and Methods). We then evaluated the performance of this model by comparing the probability of depression estimated by our algorithm against the actual presence or absence of depression for each patient in the medical record (using 10-fold cross-validation to avoid overfitting). Varying the threshold of this probability for diagnosis uniquely determines a combination of true and false positive rates that form the points of a receiver operating characteristic (ROC) curve; overall prediction performance can be summarized as the area under the ROC curve. To yield interpretable and fine-grained language variables, we extracted 200 topics by using latent dirichlet allocation (LDA; ref. 16), a method akin to factor analysis but appropriate for word frequencies. We trained a predictive model based on the relative frequencies with which patients expressed these topics, as well as one-word and two-word phrases, obtaining an area under the curve (AUC) of 0.69, which falls just short of the customary threshold for good discrimination (0.70). As shown in Fig. 1, language features outperform other posting features and demographic characteristics, which do not improve predictive accuracy when added to the language-based model. How do these prediction performances compare against other methods of screening for depression? Noyes et al. (17) assessed the concordance of screening surveys with diagnoses of depression recorded in EMRs as in this study*; the results are shown in Fig. 2 together with our Facebook model. The results suggest that the Facebook prediction model yields prediction accuracies comparable to validated self-report depression scales. Previous work observed that depressed users are more likely to tweet during night hours (6). However, patients with and without a diagnosis of depression in our study differed only modestly in their temporal posting patterns (diurnally and across days of the week; AUC = 0.54). Post length and posting frequency (metafeatures) were approximately as predictive of depression in the medical record as demographic characteristics (AUCs of 0.59 and 0.57, respectively), with the median annual word count across posts being 1,424 words higher for users who ultimately had a diagnosis of depression (Wilcoxon W = 26,594, P = 0.002). Adding temporal pattern features and metafeatures to the languagebased prediction model did not substantially increase prediction performance, suggesting that the language content captures the depression-related variance in the other feature groups.",2,2,2,2,2,2,0,2,4,2,2,1,1,2,0,0,2,2,7,0
https://www.pnas.org/content/115/44/11203?utm_source=yxnews&utm_medium=mobile,4,4,,"Eichstaedt, Smith, Merchant, Ungar, Crutchley, Preotiuc-Pietro, Asch, Schwartz",8,0,0,0,2018,"Comparison with Previous Prediction Studies. In our sample, patients with and without a diagnosis of depression in the medical record were balanced at a 1:5 ratio to simulate true depression prevalence. In previous work, this balance has been closer to 1:1 (e.g., 0.94:1 in ref. 7, 1.78:1 in ref. 6). When limiting our sample to balanced classes (1:1), we obtain an AUC of 0.68 and F1 score (the harmonic mean of precision and recall) of 0.66, which is comparable to the F1 scores of 0.65 reported in ref. 7 and 0.68 reported in ref. 6 based on Twitter data and survey-reported depression. The fact that language content captures the depressionrelated variance in the other feature groups is consistent with what has been seen in previous work (6, 7, 18). However, this work shows that social media can predict diagnoses in medical records, rather than self-report surveys.",2,2,2,2,2,2,0,2,4,2,2,1,1,2,0,0,2,2,7,0
https://www.pnas.org/content/115/44/11203?utm_source=yxnews&utm_medium=mobile,4,5,,"Eichstaedt, Smith, Merchant, Ungar, Crutchley, Preotiuc-Pietro, Asch, Schwartz",8,0,0,0,2018,"Language Markers of Depression. To better understand what specific language may serve as markers of future depression and underlay the prediction performances of the aforementioned machine learning models, we determined how users with and without a diagnosis of depression differed in the expression of the 200 data-driven LDA topics derived from their text.† In Fig. 4, we show the 10 topics most strongly associated with future depression status when controlling for age, gender, and race: 7 (of 200) topics were individually significant at P < 0.05 with Benjamini–Hochberg correction for multiple comparisons. To complement this data-driven approach, we also examined the use of 73 prespecified dictionaries (lists of words) from the Linguistic Inquiry and Word Count (LIWC) software (2015; ref. 19) that is widely used in psychological research. Nine LIWC dictionaries predicted future depression status at Benjamini–Hochberg-corrected significance levels controlling for demographics (Table 1). We observed emotional language markers of depressed mood (topic: tears, cry, pain; standardized regression coefficient β = 0.15; P < 0.001), loneliness (topic: miss, much, baby; β = 0.14; P = 0.001) and hostility (topic: hate, ugh, fuckin; β = 0.12; P = 0.012). The LIWC negative emotion (β = 0.14; P = 0.002; most frequent words: smh, fuck, hate) and sadness dictionaries (β = 0.17; P < 0.001; miss, lost, alone) captured similar information. We observed that users who ultimately had a diagnosis of depression used more first-person singular pronouns (LIWC dictionary: β = 0.19; P < 0.001; I, my, me), suggesting a preoccupation with the self. First-person singular pronouns were found by a recent meta-analysis (20) to be one of the most robust language markers of cross-sectional depression (meta-analytic r = 0.13) and by a preliminary longitudinal study a marker of future depression, as observed in this study (21). Although there is substantial evidence that the use of first-person singular pronouns is associated with depression in private writings (22), this study extends the evidence for this association into the semipublic context of social media. Cognitively, depression is thought to be associated with perseveration and rumination, specifically on self-relevant information (23), which manifests as worry and anxiety when directed toward the future (21). In line with these conceptualizations, we observed language markers suggestive of increased rumination (topic: mind, alot, lot; β = 0.11; P = 0.009) and anxiety (LIWC dictionary: β = 0.08; P = 0.043; scared, upset, worry), albeit not meeting Benjamini–Hochberg-corrected significance thresholds. Depression often presents itself with somatic complaints in primary care settings (24, 25). In our sample, we observed that the text of users who ultimately received a diagnosis of depression contained markers of somatic complaints (topic: hurt, head, bad; β = 0.15; P < 0.001; LIWC dictionary, health: β = 0.11; P = 0.004; life, tired, sick). We also observed increased medical references (topic: hospital, pain, surgery; β = 0.20; P < 0.001), which is consistent with the finding that individuals with depression are known to visit the ED more frequently than individuals without depression (26).‡",2,2,2,2,2,2,0,2,4,2,2,1,1,2,0,0,2,2,7,0
https://www.pnas.org/content/115/44/11203?utm_source=yxnews&utm_medium=mobile,4,6,,"Eichstaedt, Smith, Merchant, Ungar, Crutchley, Preotiuc-Pietro, Asch, Schwartz",8,0,0,0,2018,"Our results show that Facebook language-based prediction models perform similarly to screening surveys in identifying patients with depression when using diagnostic codes in the EMR to identify diagnoses of depression. The profile of depressionassociated language markers is nuanced, covering emotional (sadness, depressed mood), interpersonal (hostility, loneliness), and cognitive (self-focus, rumination) processes, which previous research has established as congruent with the determinants and consequences of depression. The growth of social media and continuous improvement of machine-learning algorithms suggest that social media-based screening methods for depression may become increasingly feasible and more accurate. We chose to examine depression because it is prevalent, disabling, underdiagnosed, and treatable. As a major driver of medical morbidity and mortality, it is important to more thoroughly diagnose and treat depression across the population. Patients with depression exhibit poorer medical outcomes after acute inpatient care, increased utilization of emergency care resources, and increased all-cause mortality (25–28). Identifying patients at an earlier stage in their mental illness through novel means of detection creates opportunities for patients to be connected more readily with appropriate care resources. The present analysis suggests that social mediabased prediction of future depression status may be possible as early as 3 mo before the first documentation of depression in the medical record. In the primary care setting, a diagnosis of depression is often missed (29). The reason for such underdetection is multifactorial: depression has a broad array of possible presenting symptoms, and its severity changes across time. Primary care providers are also tasked with addressing many facets of health within a clinical visit that may be as brief as 15 min. Previous research has recommended improving detection of depression through the routine use of multistep assessment processes (30). Initial identification of individuals who may be developing depression via analysis of social media may serve as the first step in such a process (using a detection threshold favoring high true positive rates). With the increasing integration of social media platforms, smartphones, and other technologies into the lives of patients, novel avenues are becoming available to detect depression unobtrusively. These methods include the algorithmic analysis of phone sensor, usage, and GPS position data on smartphones (31), and of facial expressions in images and videos, such as those shared on social media platforms (32, 33). In principle, these different screening modalities could be combined in a way that improves overall screening to identify individuals to complete self-report inventories (34) or be assessed by a clinician. In the present study, patients permitted researchers to collect several years of retroactive social media data. These longitudinal data may allow clinicians to capture the evolution of depression severity over time with a richness unavailable to traditional clinical surveys delivered at discrete time points. The language exhibited by patients who ultimately developed depression was nuanced and varied, covering a wide array of depression-related symptoms. Changes in language patterns about specific symptoms could alert clinicians to specific depression symptoms among their consenting patients. This study illustrates how social media-based detection technologies may optimize diagnosis within one facet of health. These technologies raise important question related to patient privacy, informed consent, data protection, and data ownership. Clear guidelines are needed about access to these data, reflecting the sensitivity of content, the people accessing it, and their purpose (35). Developers and policymakers need to address the challenge that the application of an algorithm may change social media posts into protected health information, with the corresponding expectation of privacy and the right of patients to remain autonomous in their health care decisions. Similarly, those who interpret the data need to recognize that people may change what they write based on their perceptions of how that information might be observed and used. The key contribution of this study is that it links mental health diagnoses with social media content, and that it used this linkage to reveal associations between the content and symptoms of a prevalent, underdiagnosed, and treatable condition. This suggests that, one day, the analysis of social media language could serve as a scalable front-line tool for the identification of depressed individuals. Together with the growing sophistication, scalability, and efficacy of technology-supported treatments for depression (36, 37), detection and treatment of mental illness may soon meet individuals in the digital spaces they already inhabit.",2,2,2,2,2,2,0,2,4,2,2,1,1,2,0,0,2,2,7,0
https://www.pnas.org/content/115/44/11203?utm_source=yxnews&utm_medium=mobile,4,7,,"Eichstaedt, Smith, Merchant, Ungar, Crutchley, Preotiuc-Pietro, Asch, Schwartz",8,0,0,0,2018,"Participant Recruitment and Data Collection. This study was approved by the institutional review board at the University of Pennsylvania. The flow of the data collection is described in ref. 14. In total, 11,224 patients were approached in the ED over a 26-mo period. Patients were excluded if they were under 18 y old, suffered from severe trauma, were incoherent, or exhibited evidence of severe illness. Of these, 2,903 patients consented to share their social media data and their EMRs, which resulted in 2,679 (92%) unique EMRs. These EMRs were not specific to the ED but covered all patient encounters across the entire health care system. A total of 1,175 patients (44%) were able to log in to their Facebook accounts, and our Facebook app was able to retrieve any Facebook information and posts for as much as 6 y earlier, ranging from July 2008 through September 2015. These users shared a total of 949,530 Facebook statuses, which we used to model the 200 LDA topics. From the health system’s EMRs, we retrieved demographic data (age, sex, and race) and prior diagnoses (by ICD-9 codes). We considered patients as having a diagnosis of depression if their EMRs included documentation of ICD codes 296.2 (Major Depression) or 311 (Depressive Disorder, not elsewhere classified), resulting in 176 patients with any Facebook language (base rate 176/1,175 = 15.0%, or 1:5.68). Of the 176 depressed patients, 114 (63%) had at least 500 words in status updates preceding their first documentation of a diagnosis of depression. A total of 49 patients had no language preceding their first documentation, suggesting that, for 28% of the sample, their first documentation of depression preceded joining or the posting on Facebook. Notably, a depression-related ICD code could reflect self-report by the patient of a history of depression and did not necessarily imply clinical assessment or current depressive symptoms, treatment, or management [Trinh et al. (15) suggest that using ICD codes as a proxy for a diagnosis of depression is feasible with moderate accuracy]. To model the application in a medical setting and control for annual patterns in depression, for each patient with depression, we randomly selected another five patients without a history of depression who had at least 500 words in status updates preceding the same day as the first recorded diagnosis of depression. This yielded a sample of 114 + 5 × 114 = 684 patients who shared a total of 524,292 Facebook statuses in the included temporal window.§ We excluded one patient from the sample for having less than 500 words after excluding unicode tokens (such as emojis), for a final sample of 683 patients. Sample Description. Sample characteristics are shown in Table 2. Among all 683 patients, the mean age was 29.9 y (SD = 8.57); most were female (76.7%) and black (70.1%). Depressed patients were more likely to have posted more words on Facebook (difference between medians = 3,794 words; Wilcoxon W = 27,712; P = 0.014) and be female [χ2 (1, n = 583) = 7.18; P = 0.007], matching national trends in presentations to urban academic EDs (26, 38, 39). Word and Phrase Extraction. We determined the relative frequency with which users used words (unigrams) and two-word phrases (bigrams) by using our open-source Python-based language analysis infrastructure (dlatk.wwbp. org). We retained as variables the 5,381 words and phrases that were used by at least 20% of the sample across their 524,292 Facebook statuses. Topic Modeling. As the coherence of topics increases when modeled over a larger number of statuses, we modeled 200 topics from the 949,530 Facebook statuses of all patients who agreed to share their Facebook statuses by using an implementation of LDA provided by the MALLET package (40). Akin to factor analysis, LDA produces clusters of words that occur in the same context across Facebook posts, yielding semantically coherent topics. It is appropriate for the highly nonnormal frequency distributions observed in language use. After modeling, we derived the use of 200 topics (200 values per user) for every user in the sample, which summarize their language use. Temporal Feature Extraction. We split the time of the day into six bins of 4 h in length, and, for every user, calculated the fraction of statuses posted in each of these bins. Similarly, we determined the fraction of posts made on each day of the week. Metafeature Extraction. For every user, we determined how many unigrams were posted per year, the average length of the posts (in unigrams), and the average length of unigrams. Dictionary Extraction. LIWC 2015 (41) provides dictionaries (lists of words) widely used in psychological research. We matched the extracted word frequencies against these dictionaries to determine the users’ relative frequency of use of the 73 LIWC dictionaries. Prediction Models. We used machine learning to train predictive models using the unigrams, bigrams, and 200 topics, using 10-fold cross-validation to avoid overfitting (similar to ref. 42). In this cross-validation procedure, the data are randomly partitioned into 10 stratified folds, keeping depressed users and their five “control users” within the same fold. Logistic regression models with a ridge penalty and their hyperparameters were fit within 9 folds and evaluated across the remaining held-out fold. The procedure was repeated 10 times to estimate an out-of-sample probability of depression for every patient. Varying the threshold of this probability for depression classification uniquely determines a combination of true and false positive rates that form the points of a ROC curve. We summarize overall prediction performance as the area under this ROC curve (i.e., AUC), which is suitable for describing classification accuracies over unbalanced classes. Prediction in Advance of Documentation. We carried out the prediction as outlined earlier but truncated the available language data to time windows ranging from 0–6 mo before diagnosis (excluding the 24 h immediately before diagnosis) to 1–7, 3–9, 9–15, 15–21, 21–27, and 27–33 mo before the first documentation of depression in the medical records. We truncated the data analogously for control users. For this analysis, we limited the sample to those with data in each of the seven time windows, specifically thresholding at a total of 20 words total in each window. Because this lower threshold results in less stable measures of language use, we employed outlier removal, replacing feature observations that were more than 2 standard deviations from the mean with the feature’s mean. This resulted in 307 patients (56 depressed) with the same users represented in each of the time windows (average word counts for depressed and nondepressed users in these windows are shown in SI Appendix, Fig. S2). AUCs were tested for significance against the null distribution through permutation tests with 100,000 permutations. Language Associations. To determine if a language feature (topic or LIWC category) was associated with (future) depression status, we individually tested it as a predictor in an in-sample linear regression model controlling for demographic characteristics (binary variables for age quartile, ethnicity, and gender), and report its standardized regression coefficient (β) with the associated significance. We explored language correlations separately by gender but found that we had insufficient power to find language correlations among male users in the sample Controlling for Multiple Comparisons. In addition to the customary significance thresholds, we also report whether a given language feature meets a P < 0.05 significance threshold corrected with the Benjamini–Hochberg procedure (43) for multiple comparisons. Data Sharing. Medical record outcomes and the linked social media data are considered Protected Health Information and cannot be shared. However, for the main language features (200 DLA topics and 73 LIWC",2,2,2,2,2,2,0,2,4,2,2,1,1,2,0,0,2,2,7,0
https://aclanthology.org/W15-1204.pdf,5,1,,"Coppersmith, Drezde, Harman, Hollingshead, Mitchell",5,0,1,1,2015,"This paper presents a summary of the Computational Linguistics and Clinical Psychology (CLPsych) 2015 shared and unshared tasks. These tasks aimed to provide apples-to-apples comparisons of various approaches to modeling language relevant to mental health from social media. The data used for these tasks is from Twitter users who state a diagnosis of depression or post traumatic stress disorder (PTSD) and demographically-matched community controls. The unshared task was a hackathon held at Johns Hopkins University in November 2014 to explore the data, and the shared task was conducted remotely, with each participating team submitted scores for a held-back test set of users. The shared task consisted of three binary classification experiments: (1) depression versus control, (2) PTSD versus control, and (3) depression versus PTSD. Classifiers were compared primarily via their average precision, though a number of other metrics are used along with this to allow a more nuanced interpretation of the performance measures.",2,2,1,0,2,0,2,2,4,1,2,2,2,2,0,1,0,0,5,0
https://aclanthology.org/W15-1204.pdf,5,2,,"Coppersmith, Drezde, Harman, Hollingshead, Mitchell",5,0,1,1,2015,"Language is a major component of mental health assessment and treatment, and thus a useful lens for mental health analysis. The psychology literature has a long history of studying the impact of various mental health conditions on a person’s language use. More recently, the computational linguistics community has sought to develop technologies to address clinical psychology challenges. Some of this work has appeared at the Computational Linguistics and Clinical Psychology workshops (Resnik et al., 2014; Mitchell et al., 2015). The 2015 workshop hosted a shared and unshared task. These tasks focused on fundamental computational linguistics technologies that hold promise to improve mental health-related applications; in particular, detecting signals relevant to mental health in language data and associated metadata. Specifically, technologies that can demonstrably separate community controls from those with mental-health conditions are extracting signals relevant to mental health. Examining the signals those techniques extract and depend on for classification can yield insights into how aspects of mental health are manifested in language usage. To that end, the shared and unshared tasks examined Twitter users who publicly stated a diagnosis of depression or PTSD (and ageand gender-matched controls). Shared tasks are tools for fostering research communities and organizing research efforts around shared goals. They provide a forum to explore new ideas and evaluate the best-of-breed, emerging, and wild technologies. The 2015 CLPsych Shared Task consisted of three user-level binary classification tasks: PTSD vs. control, depression vs. control, and PTSD vs. depression. The first two have been addressed in a number of settings (Coppersmith et al., 2015; Coppersmith et al., 2014b; Coppersmith et al., 2014a; Resnik et al., 2013; De Choudhury et al., 2013; Rosenquist et al., 2010; Ramirez-Esparza et al., 2008), while the third task is novel. Organizing this shared task brought together many teams to consider the same problem, which had the benefit of establishing a solid foundational understanding, common standards, and a shared deep understanding of both task and data. The unshared task (affectionately the “hackathon”) was a weekend-long event in November 2014 hosted by Johns Hopkins University. The hackathon provided data similar to the shared task data and encouraged participants to explore new ideas. In addition to starting new research projects, some of which were subsequently published in the CLPsych workshop, the event laid the foundation for the shared task by refining task definitions and data setup. This paper summarizes both the shared and unshared tasks at the 2015 Computational Linguistics and Clinical Psychology workshop. We outline the data used for these tasks, and summarize the methods and common themes of the shared task participants. We also present results for system combination using the shared task submissions.",2,2,1,0,2,0,2,2,4,1,2,2,2,2,0,1,0,0,5,0
https://aclanthology.org/W15-1204.pdf,5,3,,"Coppersmith, Drezde, Harman, Hollingshead, Mitchell",5,0,1,1,2015,"Data for the shared task are comprised of public tweets collected according to the procedures of Coppersmith et al. (2014a). We briefly describe the procedure here, and refer interested readers to Coppersmith et al. (2014a). for details. Users of social media may publicly discuss their health for a variety of reasons, such as to seek treatment or health advice. More specifically to mental health, users may choose a public forum to fight the societal stigma associated with mental illness, or to explain certain behaviors to friends. Many users tweet statements of diagnosis, such as “I was just diagnosed with X and ...”, where X is a mental health condition. While this can include a large variety of mental health conditions (Coppersmith et al., 2015), the shared task considered two conditions: depression or PTSD. We chose these conditions since they are among the most common found in Twitter and have relatively high prevalence compared to other conditions. A human annotator evaluates each such statement of diagnosis to remove jokes, quotes, or any other disingenuous statements. For each user, up to their most recent 3000 public tweets were included in the dataset. Importantly, we removed the tweet in which the genuine statement of diagnosis was found, to prevent any artifact or bias created from our data sampling technique. However, some of these users do mention their condition in other tweets, and some approaches may be influenced by this phenomenon. To ensure that each included user has a sufficient amount of data, we ensured that each user has at least 25 tweets and that the majority of them are English (75% according to the Compact Language Detector1 ).",2,2,1,0,2,0,2,2,4,1,2,2,2,2,0,1,0,0,5,0
https://aclanthology.org/W15-1204.pdf,5,4,,"Coppersmith, Drezde, Harman, Hollingshead, Mitchell",5,0,1,1,2015,"A goal of the shared task is to differentiate users with a mental health diagnosis from those who do not. To that end, the shared task data included a set of randomly selected Twitter users. Age and gender play a significant role in many mental health conditions, making certain segments of the population more or less likely to be affected or diagnosed with them. When possible, demographic variables such as age and gender are controlled for when doing clinical psychology or mental health research. Few studies looking at social media and clinical psychology have done analysis with explicit matched samples, though some have done this implicitly by examining a segment of the population, (e.g., college students (Rude et al., 2004)). Some work in social media analysis has considered the effect of matched samples (Dos Reis and Culotta, 2015). To create age- and gender-matched community controls, we estimated the age and gender of each user in our sample through analysis of their language. We used the demographic classification tool from the World Well-Being Project (Sap et al., 2014)2 . For each depression and PTSD user we estimated their gender, forcing the classifier to make a binary decision as to whether the user was ‘Female’ or ‘Male’, and used the age estimate as-is (an ostensibly continuous variable). We did the same for a pool of control users who tweeted during a two week time period in early 2013 and met the criteria set out above (at least 25 Tweets and their tweets were labeled as at least 75% English). To obtain our final data set, for each user in the depression or PTSD class, we sampled (without replacement) a paired community control user of the same estimated gender with the closest estimate age. We expect (and have some anecdotal evidence) that some of the community controls suffer from depression or PTSD, and made no attempt to remove them from our dataset. If we assume that the rate of contamination in the control users is commensurate with the expected rate in the population, that would mean that this contamination makes up a small minority of the data (though a nontrivial portion of the data, especially in the case of depression).",2,2,1,0,2,0,2,2,4,1,2,2,2,2,0,1,0,0,5,0
https://aclanthology.org/W15-1204.pdf,5,5,,"Coppersmith, Drezde, Harman, Hollingshead, Mitchell",5,0,1,1,2015,"The shared task focused on three binary classification tasks. 1. Identify depression users versus control users. 2. Identify PTSD users versus control users. 3. Identify depression users versus PTSD users. Twitter users were divided into a train and test partition that was used consistently across the three tasks. The train partition consisted of 327 depression users, 246 PTSD users, and for each an ageand gender-matched control user, for a total of 1,146 users. The test data contained 150 depression users, 150 PTSD users, and an age- and gender-matched control for each, for a total of 600 users. Shared task participants were provided with user data and associated labels (depression, PTSD, or control) for the users contained in the train partition. Participants were given user data without labels for the test partition. Participants were asked to produce systems using only the training data that could provide labels for each of the three tasks for the test data. Participants used their systems to assign a numeric real-valued score for each test user for each of the three tasks. Each participating team submitted three ranked lists of the 600 test users, one list for each task. Given that machine-learning models often have a number of parameters that alter their behavior, sometimes in unexpected ways, participants were encouraged to submit multiple parameter settings of their approaches, as separate ranked lists, and the bestperforming of these for each task would be taken as the “official” figure of merit. Evaluation was conducted by the shared task organizers using the (undistributed) labels for the test users. During evaluation, irrelevant users were removed; i.e., for PTSD versus control, only 300 users were relevant for this condition: the 150 PTSD users and their demographically matched controls. The depression users and their demographically matched controls were removed from the ranked list prior to evaluation. Each submission was evaluated using several metrics. Our primary metric was average precision, which balances precision with false alarms, though this only tells a single story about the methods examined. We also evaluated precision at various false alarm rates (5%, 10%, and 20%) to provide a different view of performance. The reader will note that the highest-performing technique varied according to the evaluation measure chosen – a cautionary tale about the importance of matching evaluation measure to the envisioned task.",2,2,1,0,2,0,2,2,4,1,2,2,2,2,0,1,0,0,5,0
https://aclanthology.org/W15-1204.pdf,5,6,,"Coppersmith, Drezde, Harman, Hollingshead, Mitchell",5,0,1,1,2015,"We decided to distribute data that reflected a balanced distribution between the classes, rather than a balance that accurately reflects the user population, i.e., one that has a larger number of controls. This decision was motivated by the need for creating a dataset maximally relevant to the task, as well as limitations on data distribution from Twitter’s terms of service. A balanced dataset made some aspects of the shared task easier, such as classifier creation and interpretation. However, it also means that results need to be examined with this caveat in mind. In particular, the number of false alarms expected in the general population is much larger than in our test sample (7-15 times as frequent). In effect, this means that when examining these numbers, one must remember that each false alarm could count for 7-15 false alarms in a more realistic setting. Unfortunately, when this fact is combined with the contamination of the training data by users diagnosed (but not publicly stating a diagnosis of) depression or PTSD, it quickly becomes difficult or impossible to reliably estimate the false alarm rates in practice. A more controlled study is required to estimate these numbers more accurately. That said, the relative rankings of techniques and approaches is not subject to this particular bias: each system would be affected by the false alarm rates equally, so the relative ranking of approaches (by any of the metrics investigated) does provide a fair comparison of the techniques.",2,2,1,0,2,0,2,2,4,1,2,2,2,2,0,1,0,0,5,0
https://aclanthology.org/W15-1204.pdf,5,7,,"Coppersmith, Drezde, Harman, Hollingshead, Mitchell",5,0,1,1,2015,"We briefly describe the approaches taken by each of the participants, but encourage the reader to examine participant papers for a more thorough treatment of the approaches.",2,2,1,0,2,0,2,2,4,1,2,2,2,2,0,1,0,0,5,0
https://aclanthology.org/W15-1204.pdf,5,8,,"Coppersmith, Drezde, Harman, Hollingshead, Mitchell",5,0,1,1,2015,"UMD examined a range of supervised topic models, computed on subsets of the documents for each user. Particularly, they used a variety of supervised topic-modeling approaches to find groups of words that had maximal power to differentiate between the users for each classification task. Moreover, rather than computing topics over two (typical) extreme cases – treating each tweet as an individual document or treating each users’s tweets collectively as a single document (concatenating all tweets together) – they opted for a sensible middle ground of concatenating all tweets from a given week together as a single document (Resnik et al., 2015).",2,2,1,0,2,0,2,2,4,1,2,2,2,2,0,1,0,0,5,0
https://aclanthology.org/W15-1204.pdf,5,9,,"Coppersmith, Drezde, Harman, Hollingshead, Mitchell",5,0,1,1,2015,"The WWBP examined a wide variety of methods for inferring topics automatically, combined with binary unigram vectors (i.e., “did this user ever use this word?”), and scored using straightforward regression methods. Each of these topic-modeling techniques provided a different interpretation on modeling what groups of words belonged together, and ultimately may provide some useful insight as to which approaches are best at capturing mental health related signals (Preotiuc-Pietro et al., 2015).",2,2,1,0,2,0,2,2,4,1,2,2,2,2,0,1,0,0,5,0
https://aclanthology.org/W15-1204.pdf,5,10,,"Coppersmith, Drezde, Harman, Hollingshead, Mitchell",5,0,1,1,2015,"The Duluth submission took a well-reasoned rule based approach to these tasks, and as such provides a point to examine how powerful simple, raw language features are in this context. Importantly, the Duluth systems allow one to decouple the power of an open vocabulary approach, quite independent of any complex machine learning or complex weighting schemes applied to the open vocabulary (Pedersen, 2015).",2,2,1,0,2,0,2,2,4,1,2,2,2,2,0,1,0,0,5,0
https://aclanthology.org/W15-1204.pdf,5,11,,"Coppersmith, Drezde, Harman, Hollingshead, Mitchell",5,0,1,1,2015,"We include a small system developed by the organizers for this shared task to examine the effect of providing qualitatively different information from the other system submissions. In this system, which we will refer to as the MIQ3 (pronounced ‘Mike’) submission, we use character language models (CLMs) to assign scores to individual tweets. These scores indicate whether the user may be suffering from PTSD, depression, or neither. The general approach is to examine how likely a sequence of characters is to be generated by a given type of user (PTSD, depression, or control). This provides a score even for very short text (e.g., a tweet) and captures local information about creative spellings, abbreviations, lack of spaces, and other textual phenomena resulting from the 140-character limit of tweets (McNamee and Mayfield, 2004). At test time, we search for sequences of tweets that look “most like” the condition being tested (PTSD or depression) by comparing the condition and control probabilities estimated from the training data for all the n-grams in those tweets. In more detail, we build a CLM for each condition using the training data. For each user at test time, we score each tweet based on the character ngrams in the tweet C with the CLMs for conditions A and B as P C log p(cA)−log p(cb) |C| , where p(cA) is the probability of the given n-gram c according to the CLM model for condition A, and p(cB) is the probability according to the CLM for condition B. We then compute a set of aggregate scores from a sliding window of 10 tweets at a time, where the aggregate score is either the mean, median, or the proportion of tweets with the highest probability from the CLM for condition A (‘proppos’). To compute a single score for a single user, we take the median of the aggregate scores. This follows previous work on predicting depression and PTSD in social media (Coppersmith et al., 2014a; Coppersmith et al., 2014b). We also experimented with excluding or including tweets that heuristically may not have been authored by the Twitter account holder – specifically, this exclusion removes all tweets with URLs (as they are frequently prepopulated by the website hosting the link) and retweets (as they were authored by another Twitter user). We created 12 system submissions using: n-grams of length 5 and 6 (two approaches) crossed with the mean, median, and proppos aggregation approaches (three approaches), and with or without exclusion applied (two approaches). The top systems for Depression versus Control used 5-grams, proppos and 5-grams, mean. The top system for PTSD versus Control used 5-grams, median, no exclusion. And the top systems for Depression versus PTSD used 6-grams, mean and 6-grams, proppos.",2,2,1,0,2,0,2,2,4,1,2,2,2,2,0,1,0,0,5,0
https://aclanthology.org/W15-1204.pdf,5,12,,"Coppersmith, Drezde, Harman, Hollingshead, Mitchell",5,0,1,1,2015,"We examine only the best-performing of each of the individual system submissions for each binary classification task, but again encourage the reader to examine the individual system papers for a more detailed analysis and interpretation for what each of the teams did for their submission.",2,2,1,0,2,0,2,2,4,1,2,2,2,2,0,1,0,0,5,0
https://aclanthology.org/W15-1204.pdf,5,13,,"Coppersmith, Drezde, Harman, Hollingshead, Mitchell",5,0,1,1,2015,"The results from the four submitted systems are summarized in Figure 1. The top two rows show the performance of all the parameter settings for all the submitted systems, while the bottom two rows show receiver operating characteristic (ROC) curves for only the best-performing parameter settings from each team. Each column in the figure denotes a different task: ‘Depression versus Control’ on the left, ‘PTSD versus Control’ in the middle and ‘Depression versus PTSD’ on the right. Chance performance is noted by a black dotted line in all plots, and all systems performed better than chance (with the exception of a system with deliberately random performance submitted by Duluth). In the panels in the top two rows of Figure 1, each dot indicates a submitted parameter setting, arranged by team. From left to right, the dots represent Duluth (goldenrod), MIQ (black), UMD (red), and WWBP (blue). The best-performing system for each team is denoted by a solid horizontal line, for ease of comparison. The top row shows performance by the “official metric” of average precision, while the second row shows performance on precision at 10% false alarms. The bottom two rows of Figure 1 show the results of each team’s top-performing system (according to average-precision) across the full space of false alarms. The third row shows precision over the whole space of false alarms, while the bottom row “zooms in” to show the precision at low (0-10%) false alarm rates. These bottom two rows are shown as ROC curves, with the the false alarm rate on the x-axis and the precision on the y-axis. Performance at areas of low false alarms are particularly important to the envisioned applications, since the number of control users vastly outnumber the users with each mental health condition.",2,2,1,0,2,0,2,2,4,1,2,2,2,2,0,1,0,0,5,0
https://aclanthology.org/W15-1204.pdf,5,14,,"Coppersmith, Drezde, Harman, Hollingshead, Mitchell",5,0,1,1,2015,"As each of the submitted systems used what appeared to be very complementary feature sets, we performed several system combination experiments. However, as can be seen in Figure 2, system combination failed to outperform the best-performing system submitted for the shared task (UMD). As features for system combination, we used either system ranks or scores. For each system combination experiment, we included all scores from each of the submitted systems, for a total of 47 systems (9 from Duluth, 12 from MIQ, 16 from UMD, and 10 from WWBP), without regard for how well that system performed on the classification task; future work may examine subsetting these scores for improved combination results. Since the range of the scores output by each system varied significantly, we applied a softmax normalization sigmoid function to bring all scores for each system to range from zero to one. We explored a simple ‘voting’ scheme as well as a machine learning method, using Support Vector Machines (SVM). For the SVM, shown in Figure 2 as the lower blue ‘SVM-Combo’ curve, we experimented with using raw scores or normalized scores as features, and found the normalized scores performed much better. The SVM model is the result of training ten SVMs on system output using 10-fold cross-validation, then normalizing the SVM output prediction scores and concatenating to obtain the final result. For the voted model, which can be seen in Figure 2 as the middle green ‘Rank-Combo’ curve, we simply took the rank of each Twitter user according to each system output, and averaged the result. Future work will examine other methods for system combination and analysis.",2,2,1,0,2,0,2,2,4,1,2,2,2,2,0,1,0,0,5,0
https://aclanthology.org/W15-1204.pdf,5,15,,"Coppersmith, Drezde, Harman, Hollingshead, Mitchell",5,0,1,1,2015,"This shared task served as an opportunity for a variety of teams to come together and compare techniques and approaches for extracting linguistic signals relevant to mental health from social media data. Perhaps more importantly, though, it established a test set upon which all participating groups are now familiar, which will enable a deeper level of conversation. Two of the classification tasks examined were previously attempted, and the techniques indicate improvement over previously-published findings. Past results did differ in a number of important factors, most notably in not examining age- and gender matched controls, so direct comparisons are unfortunately not possible. From these submitted systems we can take away a few lessons about classes of techniques and their relative power. There are clear benefits to using topic modeling approaches, as demonstrated by two of the groups (UMD and WWBP) – these provide strong signals relevant to mental health, and some intuitive and interpretable groupings of words without significant manual intervention. Simple linguistic features, even without complicated machine learning techniques, provide some classification power for these tasks (as demonstrated by Duluth and MIQ). Looking forward, there is strong evidence that techniques can provide signals at a finer-grained temporal resolution than previously explored (as demonstrated by UMD and MIQ). This may open up new avenues for applying these approaches to clinical settings. Finally, the results leave open room for future work; none of these tasks were solved. This suggests both improvements to techniques as well as more work on dataset construction. However, even at this nascent stage, insight from the mental health signals these techniques extract from language is providing new directions for mental health research.",2,2,1,0,2,0,2,2,4,1,2,2,2,2,0,1,0,0,5,0
https://journals.sagepub.com/doi/full/10.1177/1178222618792860,6,1,,"Coppersmith, Leary, Crutchley, Fine",4,0,0,1,2018,"Suicide is among the 10 most common causes of death, as assessed by the World Health Organization. For every death by suicide, an estimated 138 people’s lives are meaningfully affected, and almost any other statistic around suicide deaths is equally alarming. The pervasiveness of social media—and the near-ubiquity of mobile devices used to access social media networks—offers new types of data for understanding the behavior of those who (attempt to) take their own lives and suggests new possibilities for preventive intervention. We demonstrate the feasibility of using social media data to detect those at risk for suicide. Specifically, we use natural language processing and machine learning (specifically deep learning) techniques to detect quantifiable signals around suicide attempts, and describe designs for an automated system for estimating suicide risk, usable by those without specialized mental health training (eg, a primary care doctor). We also discuss the ethical use of such technology and examine privacy implications. Currently, this technology is only used for intervention for individuals who have “opted in” for the analysis and intervention, but the technology enables scalable screening for suicide risk, potentially identifying many people who are at risk preventively and prior to any engagement with a health care system. This raises a significant cultural question about the trade-off between privacy and prevention—we have potentially life-saving technology that is currently reaching only a fraction of the possible people at risk because of respect for their privacy. Is the current trade-off between privacy and prevention the right one?",2,2,2,0,2,2,2,0,4,2,2,2,2,2,0,0,0,0,4,2
https://journals.sagepub.com/doi/full/10.1177/1178222618792860,6,2,,"Coppersmith, Leary, Crutchley, Fine",4,0,0,1,2018,"An estimated 16million suicide attempts occur each year. Of these, approximately 800000 people will die from those attempts.1 Suicide deaths have increased by 24% in the past 20years, making suicide one of the top 10 causes of death in the United States,2 a pattern that seems to be constant across geographic region within the country.3 Not only is the magnitude of the problem large and worsening, there has been little progress made over the past 50 years in understanding suicide and improving outcomes in at-risk individuals.4 The stubbornness of the problem reflects its complexity, and the densely interwoven causal factors underlying it. Here we focus on one piece of the puzzle: how can we identify those who are at risk of taking (or attempting to take) their own life, and how can this screening be used to foster effective interventions? Assessing an individual’s risk for suicidal behavior is difficult. Experienced and talented clinicians frequently struggle to correctly interpret signals in their patients’ behavior that are indicative of suicide risk. Setting aside the profound difficulties associated with understanding an individual’s personal history and its relationship to their capacity and motivations for selfharm, there are at least 2 practical reasons that assessing suicide risk is difficult: (1) the latency between the onset of acute risk for suicide and the suicide attempt itself may be too small for interventions requiring contact with health professionals, and (2) most existing methods for detecting high risk of suicide require that individuals disclose their wish to harm themselves to a health professional. In this article, we explore the possibility that digital life data—that is, the interactions that a person has with digital devices, through the daily course of their life— collected passively but with consent might at least partially address each of these difficulties. Individuals come to be at risk for suicide at different temporal intervals relative to suicide attempts. For instance, the kind of social isolation that is frequently associated with suicide can gradually accumulate over the course of a person’s life or may become acute in a very short period of time after a traumatic life event such as the loss of a loved one. Moreover, once an individual is engaged with a health care professional, standard methods of suicide intervention require both that the clinician administer a standardized risk assessment (often in the form of a questionnaire) and that the patients disclose their intention to harm themselves. Each of these presents its own challenges. First, administering a suicide screening tool may place an unreasonable burden on the health care provider. The standard for suicide screening within the health care system is Beck’s Scale for Suicide Ideation, a 5- or 19-item questionnaire examining the patient’s active and passive desire for suicide, and any specific plans they might have.5 Many patients who are at risk for suicide only interact with primary care physicians (PCPs) or emergency departments (EDs) rather than those with psychiatric specialties. Such health care providers may lack the time or the training to administer a specific questionnaire for suicide risk. Indeed, enabling PCPs and EDs to better screen for suicide risk has been posited as a method for reducing the suicide rate.6,7 Second, patients cannot always be relied upon to disclose suicidal thoughts in the clinical setting.8 These factors have the potential for a large impact if missed screening opportunities can be capitalized upon9: 24.6% of patients attempting suicide visited a mental health professional in the 1-week period prior to their attempt, with 38.3% having visited a health professional of any kind in the same period of time. Independent of the efficacy and specificity of these scales and instruments for screening for suicide risk, they are pragmatically limited in their application to times when a patient is interacting with the health care system, the health care professional they are interacting with deems administration of a suicide risk screening a worthy use of time, and the patient is willing to disclose suicidal thoughts, plans, or actions at that time.10 Many at risk are not engaged with the health care system at all11: there are strong correlations of state-based suicide rates with indicators for lack of access to health care. Ahmedani et al9 report that 26.7% of individuals attempting suicide had not seen a mental health care practitioner in the prior year, and 5.4% had not seen any health care practitioner in the prior year, with significant variation among racial and ethnic groups. Furthermore, the health care system seems to have a significant and systematic gap for helping individuals after their suicide attempt: Substance Abuse and Mental Health Services Administration (SAMHSA) survey data12 from 2011 showed that 18.3% of drug-related suicide attempt ED admissions showed no evidence of follow-up treatment. Taken together, the existing infrastructure for suicide risk detection and intervention highlights the need for some way of screening individuals outside the context of their interactions with the health care system, and for detecting signals positively associated with suicidal behavior that are less overt than explicit disclosure. The above suggests at least 2 paths to improve screening for suicide risk within the existing health care system: (1) providing evidence of risk without relying exclusively on self disclosure and (2) the pragmatic reduction of time needed with a health care worker to administer the screening tool (and thus reducing the resistance to administering the tool on the part of the health care professional). Furthermore, a model capable of identifying those at risk outside the health care system could be part of a system to funnel them toward appropriate care. Interestingly, there has been significant progress in using data outside the health care system to assess and understand mental health and well-being in recent years (which some refer to as digital phenotyping, eg, Onnela and Rauch13). In particular, signals related to a person’s mental health and well-being have been extracted from a person’s digital life. Digital life generally covers all the interactions that a person has with digital devices, through the daily course of their life, including social media data (eg, Facebook, Instagram, Twitter, Reddit), data from wearable devices (eg, Fitbit, Jawbone), geolocation, actigriphy from phone sensors,14 or interactions with smart devices (eg, Amazon’s Alexa or other Internet of Things [IoT] devices). For one point of comparison, see Figure 1, which shows one person’s interaction with the health care system (in red hashes) and posts on Facebook (in blue hashes) over the course of a few years.16 A growing body of work suggests that various facts about an individual’s mental health can be inferred from the text that a person generates, raising the intriguing possibility that social media data could be used as a screening and/or early detection tool. Social media data have been found to contain predictive signal for conditions, including major depressive disorder,17,18 post-traumatic stress disorder,19–25 schizophrenia,26 eating disorders,27,28 generalized anxiety disorder, bipolar disorder,29 self-harm,25 suicide,30–38 borderline personality disorder, and others.39,40 Computational analysis of social media data may therefore fulfill the desiderata of a screening system that (1) captures an individual’s behavior outside of their interactions with the health care system and (2) is amenable to the kind of automation and scalability that is often sorely lacking in underfunded and resource-constrained health care providers. This article’s primary contributions are as follows: • The creation of an automated model for analysis and estimation of suicide risk from social media data. • An examination of how this could be used to improve existing screening for suicide risk within the health care system. • An exploration of the ethical and privacy concerns of creating a system for suicide risk screening not currently in care.",2,2,2,0,2,2,2,0,4,2,2,2,2,2,0,0,0,0,4,2
https://journals.sagepub.com/doi/full/10.1177/1178222618792860,6,3,,"Coppersmith, Leary, Crutchley, Fine",4,0,0,1,2018,"The creation and evaluation of these machine learning algorithms depend on having social media data from people prior to a suicide attempt and a contrasting set of users who have not attempted suicide. To train the algorithms to differentiate between those who are at risk for suicide and those who are not, we also needed examples of users who are as close a match as possible to those who would attempt suicide, but did not attempt suicide (so far as we know). Social media posts from control users thus provide a baseline to which the data from those would go on to attempt suicide can be compared. We combine data from 2 sources to create this dataset—examining public self-stated data and using data donated through OurDataHelps.org.",2,2,2,0,2,2,2,0,4,2,2,2,2,2,0,0,0,0,4,2
https://journals.sagepub.com/doi/full/10.1177/1178222618792860,6,4,,"Coppersmith, Leary, Crutchley, Fine",4,0,0,1,2018,"The first data source is from a set of users who have graciously donated their data to support research in this area through OurDataHelps.org. Users of this platform sign up and authorize access to data from their digital life—social media (eg, Facebook, Twitter, Instagram, Reddit, Tumblr), wearable (eg, Fitbit, Jawbone), and other technology (eg, Strava, Runkeeper). Users also fill out questionnaires asking for basic demographic data as well as for information about their history with various mental health conditions. Specifically relevant to this study, they note the number and dates of past suicide attempts. A handful of the users’ data in OurDataHelps.org were provided by their loved ones, posthumously after their suicide. Through this authorized access, we examine the posts that the user made publicly and visible to their “friends and family”—this notably excludes private message data like Facebook Messenger or Twitter direct messages. From the users of OurDataHelps.org, we have 186 users who have attempted suicide. Many of the users who donated data at OurDataHelps.org did not attempt suicide or have any reported mental health diagnoses. For each user who attempted suicide from the OurDataHelps.org population, we find a user with the same gender and nearly the same age to serve as a control.",2,2,2,0,2,2,2,0,4,2,2,2,2,2,0,0,0,0,4,2
https://journals.sagepub.com/doi/full/10.1177/1178222618792860,6,5,,"Coppersmith, Leary, Crutchley, Fine",4,0,0,1,2018,"The second data source comes from users who publicly discuss past suicide attempts on social media, as originally described in Coppersmith et al19 and adapted to examine suicide attempts in Coppersmith et al.36 Here, we significantly increase the number of users in the dataset (4 times the size of the dataset in 2016), which increases the accuracy and generality of the machine learning algorithms. These users make posts that describe the date of a past suicide attempt, like those found in Figure 2. People may make statements like this (1) to explain past behavior, (2) as a method for fighting the stigma and discrimination associated with mental illness, or (3) as a way of offering support to those in a similar situation. From these statements, we can infer the date of the suicide attempt and examine public data prior to that attempt from this user. To find matched controls to the self-stated data, we examine a random sample of Twitter data, finding users whose posts were primarily in English, and were either human-annotated or estimated to be of the same gender and approximate age as a user who has attempted suicide. (Estimation performed via the methods described in Sap et al.42)",2,2,2,0,2,2,2,0,4,2,2,2,2,2,0,0,0,0,4,2
https://journals.sagepub.com/doi/full/10.1177/1178222618792860,6,6,,"Coppersmith, Leary, Crutchley, Fine",4,0,0,1,2018,"When the data from these 2 sources are combined, we have 547 users who have attempted suicide, 418 users of which we know the month of their suicide attempt (263 of which we know the exact date) and can access data prior to the attempt. There were 4 users present in both datasets (ie, someone who had both donated data and been found via the public self-stated methods), and these users are included only once in the combined dataset. For the analysis below, we restrict the data from these users to only the posts made in the 6months prior to their suicide attempt. This results in a final dataset of 418 users who have attempted suicide for whom we have up to 6months of posts prior to their suicide attempt (and an equal number of demographically matched controls). For comparison, Coppersmith et al36 had 125 users compared with this article’s 418. For each user, we have an average of 473 social media posts (and an equal number drawn from each matched control), for a total of 197615 posts from those who would go on to attempt suicide in the next 6months, and an additional 197615 posts from their matched controls (for a total of 395230 posts). The age and gender distribution of these data can be found in Figure 3. Most of this population is comprised of females aged 18 to 24, though there is a reasonable number of males from a similar age range as well. Less well represented, but not absent, are people older than 30 and those of a non-binary gender.",2,2,2,0,2,2,2,0,4,2,2,2,2,2,0,0,0,0,4,2
https://journals.sagepub.com/doi/full/10.1177/1178222618792860,6,7,,"Coppersmith, Leary, Crutchley, Fine",4,0,0,1,2018,"For a screening method to be highly scalable, it must minimize the time taken by humans required to provide the assessment. Thus, we design methods based on automated and computerized assessment and analysis. The bulk of the technical design work necessary here is in the creation and evaluation of machine learning methods, which examine existing data to automatically extract and determine the relevant patterns for who is at risk for suicide. The dataset described above supports this kind of analysis. Here, we describe the methods at work in some technical depth. However, the implications of the technology can be readily understood independent of the technical details in this section. The classification given to each user by these models (ie, the model’s best guess as to whether each user will go on to attempt suicide) is based on tens of thousands of small clues, too many to enumerate in any way that is accessible to human intuition. However, we can examine some text that is scored highly by the algorithm, shown in Figure 4. These illustrative examples give a feel for the validity of the technique. Similarly, Figure 5 provides a visualization of the scores of the model for each user over the last 200 tweets prior to their suicide attempt in blue (ie, all suicide attempts occur at 200 on the x-axis in this plot) and their matched controls in green. Higher is indicative of more risk for suicide. Although there is variability within each line, and portions of time when control users are above users at risk, they are generally separable—more blue on the top and more green on the bottom. The key results are (1) that there are quantifiable signals present in the language used on social media that machine learning algorithms can use to separate users who would go on to attempt suicide from those who would not with relatively high precision and (2) that the machine learning algorithms depend on a wide swath of subtle clues, rather than a few indicative phrases. The algorithm’s ability to distinguish users who would go on to attempt suicide—crucially, without input from a trained human—is good enough that it is worth considering how a tool incorporating the algorithm might fit into a clinical application. The implications of this are further discussed in the following section, and non-technical readers may safely skip to that section. The remainder of this section describes the machine learning techniques employed here in more detail.",2,2,2,0,2,2,2,0,4,2,2,2,2,2,0,0,0,0,4,2
https://journals.sagepub.com/doi/full/10.1177/1178222618792860,6,8,,"Coppersmith, Leary, Crutchley, Fine",4,0,0,1,2018,"Recent advancements in natural language processing have leveraged deep learning to improve the state of the art for language modeling43 and text classification.44,45 These techniques are loosely related to human neural architecture in that they are composed of networks of “neurons” capable of learning complex, non-linear relationships between input data (in this case, social media posts) and output data (whether or not the post was composed by someone who would go on to attempt suicide). Although the dataset here is limited in the number of attempting users, these users together contribute hundreds of thousands of individual messages. Conceptually, we train a text classification model intended, from a single post, to predict whether the author is at risk for a suicide attempt. Rather than use these individual message-level scores, we use the aggregated scores from many posts from a single user to predict the individual’s risk. With the somewhat limited amount of training data available for the task (when compared with traditional “deep learning” tasks where tens or hundreds of millions of examples are used), we leverage both supervised and unsupervised learning methods to prevent overfitting the model to the training data. The model first uses a word embedding layer to project each word into a dense vector space. This low-dimensional vector space is crafted such that semantically similar words remain a short distance from each other in Euclidean or cosine space. Our model is initialized with pretrained GloVe embeddings46 to reduce the risk of overfitting the model. The embeddings are trained by their original authors over a significantly larger dataset to learn to encode general language usage (like which words are semantically similar). This, in part, compensates for the relatively small sample of users and messages under consideration here. These embeddings are later fine-tuned during the model training process, shifting to better capture nuances of language related to mental health. Sequences of word vectors, one sequence per message, are processed via a bidirectional Long Short-Term Memory (LSTM) layer to capture contextual information between words. Next, the output of each layer is combined into a single vector using skip connections into a self-attention layer. The attention mechanism is used to apply weights to the timesteps of the sequence such that the most informative subsequences are more strongly considered in the final prediction.47,48 Finally, a linear layer with softmax output predicts a posterior probability representing the likelihood that the text was written by an author at risk for suicide.",2,2,2,0,2,2,2,0,4,2,2,2,2,2,0,0,0,0,4,2
https://journals.sagepub.com/doi/full/10.1177/1178222618792860,6,9,,"Coppersmith, Leary, Crutchley, Fine",4,0,0,1,2018,"To evaluate the efficacy of these deep learning techniques for estimating suicide risk from social media data, we evaluated the classification performance in 10-fold cross-validation across pairs of users (one user prior to their suicide attempt and their demographically matched control). Figure 6 shows the receiver operating characteristic (ROC) curve of performance for these models for separating users at risk for a suicide attempt from their matched controls. Each ROC curve shows the trade-offs between true positives and false alarms over the sensitivity of the model. For a single point of comparison, at 10% false alarm rate (0.1 on the x-axis), the models in Figure 6 range from 70% to 85% true positive rate (0.7-0.85 on the y-axis). This, to the best of our knowledge, is state of the art performance on suicide risk prediction from social media alone. Chance performance is denoted by the diagonal dashed line, and the further up and left of the dotted line a model’s curve is, the better it generally is at the task. Area under the ROC curve (AUC) is often used as a singular scalar metric of performance (chance: 0.5; perfect discrimination: 1.0); this is given in the figure legend. Each line represents an amount of data used prior to the attempt to make the prediction—the green line looks only at data from 30 to 0days prior to the suicide attempt. Similarly, the blue line uses data from 60 to 0days prior to the suicide attempt. Excluded for brevity are similarly performant experiments examining data further out from the suicide attempt. Ultimately, we found that performance was roughly comparable if we examined data a few months prior (180 to 90days prior) to the attempt and excluded data immediately preceding the attempt (90 to 0days prior). Thus, that suggests that the model is capturing trait-type information (relevant to risk for suicide at some point in time) rather than state-type information (relevant to imminent risk of harm).",2,2,2,0,2,2,2,0,4,2,2,2,2,2,0,0,0,0,4,2
https://journals.sagepub.com/doi/full/10.1177/1178222618792860,6,10,,"Coppersmith, Leary, Crutchley, Fine",4,0,0,1,2018,"Although the majority of this dataset was female users (78%), the model seems to be sufficiently expressive to also capture information for males, with a slight loss of precision. There were not a sufficient number of users of a non-binary gender to fully assess performance, but anecdotally the model performance seemed to be on par with the other genders. This suggests that the model has learned information about suicide risk that seems to be relevant across genders. This does not necessarily mean that they are governed by a single theoretic model but simply that there are common language cues across the genders and the model built is sufficiently expressive to capture many of the differences. To interpret the performance of this classifier as a screening tool, we can pick a few reasonable trade-offs between true positive and false alarm rates, and examine who would be deemed “at risk” by the algorithm. Let us assume a theoretical population of 1000 people who will get screened by this method. It is expected that 4% to 8% of them would go on to attempt suicide, so for the purposes of this illustration we use 6%.49,50 That means that 60 people from this population would go on to attempt suicide, and thus 940 people would not. If we deployed this screener against this population, allowing 1% false alarm rate (thus 24% true positive rate), we would expect 25 people to be flagged as “at risk,” 15 of which would go on to attempt suicide—that would mean that 60% of the population flagged at risk would go on to attempt suicide. Similarly, at 10% false alarms (84% true positive rate), 144 would be flagged, 35% of which would go on to attempt suicide. At 2% false alarms (35% true positive rate), 40 people would be flagged, 40% of which would go on to attempt suicide. A direct point of comparison for performance from clinicians is difficult to match exactly, but the results from Franklin et al4 suggest that the equivalent expected proportion of users identified as “at risk” by clinicians who would go on to attempt suicide would be in the 4% to 6% range—significantly lower than the 40% to 60% here. Which of these operating points are used should be determined by what the next step taken after this screening is. If, for example, a positive flag means that their healthcare provider will be reminded of the importance of the clinical screen, perhaps 10% false alarms is a reasonable operating point. If the next step is, instead, a psychology consultation, perhaps something more restrictive (eg, 2% false alarms) is more appropriate, given the cost of scheduling and having such a consultation with an indemand resource.",2,2,2,0,2,2,2,0,4,2,2,2,2,2,0,0,0,0,4,2
https://journals.sagepub.com/doi/full/10.1177/1178222618792860,6,11,,"Coppersmith, Leary, Crutchley, Fine",4,0,0,1,2018,"The methods described here demonstrate that signals exist within social media data that are quantifiable and relevant to suicide risk. Concretely, we described algorithms that are able to identify people at risk for suicide from the analysis of the language of their social media posts, at levels of precision that suggest clinical utility, and at the period early enough to permit reasonably scalable and durable interventions (ie, months preceding crisis, rather than in the moment of crisis). The model, as configured here, was optimized toward detecting trait-level risk for suicide, or how at risk a person tends to be over a long period of time as opposed to state-level information that is more transient and related to a short period of risk. This was a deliberate choice as a screening tool to find users at risk well before the point of their attempt. Other parameterizations of this model and other methods of dividing the data for training may allow more state level and information about proximal risk of suicide to be discovered, but we leave that to future work. In our estimation, the ability for a system of care to respond in the moment of crisis is more costly, more dangerous (to the patient and potentially those intervening), and ultimately of less utility than intervening months or years prior to an attempt. Importantly, these models depend on a wide variety of signals, most of which are not what a clinician would generally ascribe to association with suicide risk. This is similar in spirit to the findings of the Crisis Text Line (CTL), which found that the word “ibuprofen” was one of the most highly correlated words used by users at imminent risk of suicide, when talking to a peer providing private, anonymous, emotional support.51 The findings here are complementary to the CTL findings in many ways, though the methods used to derive them are similar. Where their data are private, anonymous conversations, these results are based on data from public social media postings. Where their data are comprised of those at imminent risk, ours reflect users’ behavior months prior to an attempt. Both cases, however, highlight the important role that language and linguistic analysis can play in understanding mental health. These results suggest that an automated screening protocol using this sort of technology would be able to detect some users at risk. In many senses, that is the easiest part of meaningful impact on the suicide rates. This technology would, of course, represent a small portion of a larger system designed to address mental health issues. Most of that system is human in nature—friends, caregivers, clinicians, and the patients themselves. Although technology enabled by algorithms such as the one reported here may be able to identify people at risk, it will only be effective at reducing the suicide rates if (1) the technology facilitates the deployment of other, existing mental health resources, and (2) the system of care (inside or outside the formal health care system) functions to divert the person from risk.",2,2,2,0,2,2,2,0,4,2,2,2,2,2,0,0,0,0,4,2
https://journals.sagepub.com/doi/full/10.1177/1178222618792860,6,12,,"Coppersmith, Leary, Crutchley, Fine",4,0,0,1,2018,"There are a few ways in which the findings reported here may fail to generalize to all segments of the population. First, these data are predominantly derived from females aged 18 to 24. Although our results indicate that this approach works reasonably well for males of a similar age (and anecdotal evidence in support of those with non-binary gender), there is not sufficient evidence to assess the efficacy for people from other age groups. One could reasonably expect that their relationship to suicide is meaningfully different from this demographic, and thus might not be as easily detected through approaches like this. To mitigate, this may require more specialized modeling for those demographics specifically, for example. Furthermore, we did not explicitly examine race or ethnicity in this work, though prior research indicates that mental illness presentation may be more reflective of deviations from cultural norms in functioning, as opposed to a consistent set of symptoms per disorder that holds cross-culturally.52 Moreover, the strength of the stigma that surrounds discussion of suicide or other forms of psychological distress varies across communities,53 which may affect the feasibility of using social media as a window onto mental health for certain types of users. It is also worth pointing out that, despite the seeming pervasiveness of social media, not everyone uses platforms like Twitter and Facebook. To the extent that there are systematic differences between those who use social media and those who do not, the findings described here simply cannot generalize to some segments of the population. Finally, almost all users in this analysis survived through their suicide attempt, at least long enough to post about it in social media. Although they may have died of a subsequent attempt, this may not be accurately reflective of the people who would go on to die by suicide or those who would die by suicide on their first attempt.",2,2,2,0,2,2,2,0,4,2,2,2,2,2,0,0,0,0,4,2
https://journals.sagepub.com/doi/full/10.1177/1178222618792860,6,13,,"Coppersmith, Leary, Crutchley, Fine",4,0,0,1,2018,"The technological results presented here demonstrate the feasibility of using automated machine learning techniques to identify people at risk for suicide, and with sufficient accuracy that it is worth examining how this might improve existing clinical systems of care. Essentially, the research presented here demonstrates that suicide screening at scale is possible, but we have not yet answered the question of how it should be used. The technology described here raises a significant cultural question about the trade-off between privacy and prevention. These algorithms can provide early warnings in life-threatening situations—potentially a piece of technology that could enable saving lives. There are a few ways this might be integrated into systems of care that prevent loss of life: 1. In the health care system with existing patients, augmenting the capabilities of PCPs or other health care professionals to screen for suicide risk: Here, the patient agrees to be assessed for risk, authorizes the analysis of the data, and authorizes the health care provider to see the results. 2. Outside the health care system, empowering a person’s support network (eg, friends or family members who are not health care professionals) to know when a person is in danger: Here, the patient authorizes the analysis of their data and authorizes their support networks to see results and alerts about their estimated risk. 3. As a screening tool to identify those at risk in the general population (and likely not receiving care): Here, public data can be screened and analyzed for risk, proactively identifying users outside the health care system. All of these systems are technically feasible to build, but there are significant ethical considerations that should be discussed publicly prior to their implementation. Here, we consider some of the central concepts to the design of such screening systems, examine prior art in this space, examine analogous systems in other sectors, and highlight some points for consideration. We will not unilaterally claim an answer to these questions, but provide discussion of relevant analogous systems, and a framework for creating and assessing the viability of a path forward.",2,2,2,0,2,2,2,0,4,2,2,2,2,2,0,0,0,0,4,2
https://journals.sagepub.com/doi/full/10.1177/1178222618792860,6,14,,"Coppersmith, Leary, Crutchley, Fine",4,0,0,1,2018,"There are many considerations in how a system using this technology might be ethically implemented, but the most central theme is whether data collection is opt-in or opt-out. An opt-in system is where a person takes a conscious action to be part of the analysis and is generally in line with the concept of informed consent. Prior to any analysis, a person is informed about what the system does, what the likely outcomes are, and what actions (if any) might be taken as part of the process. This provides the person sufficient information to make an informed decision as to whether or not they wish to partake in the study or screening. An opt-in system has the benefit that the users are willingly participating, and thus, any analysis or action is necessarily not a violation of a person’s privacy. Similarly, this means that any opt-in system has a considerably limited reach. Any screening on an opt-in basis will necessarily only find the people at risk who are already engaging with the system. In scenario (1), this would be people who are engaging with the health care system for some sort of treatment. In scenario (2), this would constitute anyone who has found this system through any means (eg, web search). This does not cover scenario (3), because the whole population has not been made aware, informed, and agreed to analysis. Thus, opt-in systems are extremely unlikely to reach all of the users who are at risk. In contrast, an opt-out system is one in which the user may not even know they are being analyzed or might be intervened with. Users in such systems may “opt out” by taking an explicit action (ie, asking to be removed). A screening system operating in an opt-out manner could examine all the public data from all the users in the world and identify those at risk, even if they have no connection to a health care system. Case (3) is an example of an opt-out system, where users have the ability to remove themselves from consideration from screening, but unless they have done so are being analyzed. For a screening system to reach and assess those not engaging in care or web searches related to the behavior in question, some sort of opt-out system is the only way to reach them.",2,2,2,0,2,2,2,0,4,2,2,2,2,2,0,0,0,0,4,2
https://journals.sagepub.com/doi/full/10.1177/1178222618792860,6,15,,"Coppersmith, Leary, Crutchley, Fine",4,0,0,1,2018,"The most prominent parallel to the use of this technology for screening is the Samaritans’ Radar App, briefly used in 2014.54 The Samaritans are a well-known and well-respected suicide prevention organization, founded in 1952. They launched an app that purported to alert users when one of their friends exhibited signs of suicide risk. Unfortunately, a few design choices garnered significant public backlash, and the app was decommissioned shortly after its launch. We will describe the core functionality of the Samaritans’ Radar and the design choices that, we suspect, were the root cause of public displeasure, then suggest methods for mitigating those concerns. Ultimately, we believe that any system architecture or process implementing automated screening tools of this kind should be guided and deeply informed by the advice of the lived experience community (those who have previously attempted suicide or have lost loved ones to suicide) as well as clinical experts involved in the daily provision of care to this population as well as to those who are at risk of suicide. Samaritans’ Radar functioned roughly as follows: a user could install the app and provide the app access to their Twitter account. The app would then continuously analyze the posts of Twitter users the app user follows. If the app detected phrases that seemed to indicate suicide risk (eg, “I just want to kill myself ”) in the posts of any of the app user’s followed accounts, it would alert the app user, so the app user could reach out and offer support. To fully understand the situation, one must also realize that any user can “follow” any other user on Twitter and thus have access to their posts (unless certain non-default privacy settings were activated to make their account “private”). The primary concern from the public was a variant of the following nightmare scenario: there was a user who downloaded and installed the app with nefarious intent. This user could follow arbitrary users (due to Twitter’s following system as above) and get Samaritans’ Radar to tell the nefarious app user when someone they followed was at risk. The nefarious app user could then take advantage of that situation and (perhaps) encourage them to take their life. Although this may be unlikely, it is worth taking seriously. Two underlying problems are (1) that the user being analyzed does not know that they are being analyzed (this is at best an opt-out scenario) and (2) anyone could have access to information indicating people at risk (without their consent or knowledge, given the previous point). In contrast, the scenarios we described above, (1) and (2), provide some monitoring aspects similar to Samaritans’ Radar but require the user being analyzed to opt in. Scenario (3) does not require an opt-in but could be restricted to providing this information only to a special class of users (eg, licensed health care staff or individuals certified in some manner). In all cases, the technology can be deployed in a way that defeats the nefarious actor above.",2,2,2,0,2,2,2,0,4,2,2,2,2,2,0,0,0,0,4,2
https://journals.sagepub.com/doi/full/10.1177/1178222618792860,6,16,,"Coppersmith, Leary, Crutchley, Fine",4,0,0,1,2018,"Perhaps the closest analogue to the sorts of systems described by scenario (3) is found in the world of marketing and advertising. The companies in this space are able to infer latent attributes about users given their online behavior, and the companies’ algorithms are designed to use these inferences to suggest products or services that the person may want. The types of personal data that the technology under consideration in this article depends on are a subset of those available to advertisers, and many of the analysis techniques are similar. Conceptually, wide-scale screening is similar in technological function to ad targeting; however, instead of steering a user’s behavior toward making a purchase, the screening would be used to steer a user’s behavior away from an adverse health event (in this case, a suicide attempt). Although the first thought of how these interventions might be done is through a pop-up ad to a person indicating they should seek medical care, this is decidedly not what we are advocating. That is a strategy unlikely to be effective and has potential to induce stress in the viewer of the ad. If we assume, however, that effective interventions can be developed and deployed within the existing advertising framework (eg, “nudges” a la Leonard [2008]55), then this may provide a reasonable framework for examining scenario (3) and also a reason parallel for framing the ethical and privacy discussions (eg, How is this different from sending an ad to buy something? How is it different from sending an ad for a prescription medication?).",2,2,2,0,2,2,2,0,4,2,2,2,2,2,0,0,0,0,4,2
https://journals.sagepub.com/doi/full/10.1177/1178222618792860,6,17,,"Coppersmith, Leary, Crutchley, Fine",4,0,0,1,2018,"Among large technology companies, Facebook in particular has been making waves51 with its announcement of an automated suicide prevention (“proactive detection”) algorithm.56 Facebook cites “pattern recognition” models trained from community flags of posts containing language indicative of suicidal ideation. Two points are of particular interest here: Facebook does not share information about the underlying model or human vetting process, and there is no way for Facebook users to opt-out of this program (without leaving the Facebook platform). Given that Facebook’s process involves contacting first responders (police, paramedics) and directing them toward the user presumed to be in danger, one would expect a high degree of confidence in basing decisions off the model, solely or in part. Even with human “Community Operations Team” members in the loop, false positives from an automated model almost necessarily lead to some false positives passed along by humans. With no details from Facebook about the process behind or performance of this automated detection algorithm (or the human moderators, for that matter), it is impossible for a third party to evaluate its utility. These facts, interestingly, have led to Facebook being unable to deploy proactive detection for suicide in the European Union (EU), due to the EU’s Data Protection Directive and General Data Protection Regulations.57",2,2,2,0,2,2,2,0,4,2,2,2,2,2,0,0,0,0,4,2
https://journals.sagepub.com/doi/full/10.1177/1178222618792860,6,18,,"Coppersmith, Leary, Crutchley, Fine",4,0,0,1,2018,"At the crux of this dilemma is the trade-off between a person’s right to privacy and the widely agreed-upon moral imperative to act on information that may save lives. For a simple and direct yet difficult example decision, consider that a hallmark feature of risk is being withdrawn, and a classic and effective way of intervention is to reach out. In many cases, the very act of reaching out can be perceived as an invasion of privacy. Concretely, if one could intervene and help a person heading toward crisis by invading their privacy, ought one do it? This critically depends on 3 premises: first, the ability to detect an individual at risk is sufficiently precise; second, that this detection is at odds with popular conceptions of privacy; and third, there is capability to effectively intervene and reduce the (risk of ) harm. The research presented here addresses the first premise, the second premise is a question that we hope is debated and discussed openly, but ultimately without also solving the third premise, no amount of careful crafting and ethical analysis will provide any measurable impact on lives saved. Anecdotally, we have found that people tend to think differently about privacy when presented in this light, and seek to find mitigating factors that would make them feel comfortable about being analyzed in an opt-out manner (ie, a system-like scenario (3)), by some trained processionals solely for the purpose of preventing suicides. To reiterate, we do not feel it is within the scope of this article—or within the bailiwick of the authors—to unilaterally suggest a specific framework for implementing algorithms of the kind described here. However, we do feel it is our responsibility to note that “the cat is out of the bag,” so to speak. That is, the widespread availability of personal digital data (which is, under current paradigms, the price that consumers collectively pay for largely free Internet content), coupled with the rapidly increasing sophistication of classification algorithms of the kind described here, suggest that the question is not if technologies like this will be deployed, but how. The best-case scenario, in our opinion, is one in which government, academia, advocates from the mental health community, and clinicians work in concert to assure that the right individuals are benefiting from such technology (ie, those who are at risk of harming themselves), and that the risks are proactively identified, discussed, and mitigated in as thoughtful and transparent a way as possible. Perhaps a silver lining of the legally questionable actions of groups such as Cambridge Analytica during and leading up to the 2016 presidential election in the United States will be a more sophisticated and vigorous public discourse surrounding the use and protection of personal digital data.",2,2,2,0,2,2,2,0,4,2,2,2,2,2,0,0,0,0,4,2
https://journals.sagepub.com/doi/full/10.1177/1178222618792860,6,19,,"Coppersmith, Leary, Crutchley, Fine",4,0,0,1,2018,"We feel that the broad question at the crux of this discussion is beyond our capability to answer: “What is the right trade-off between privacy and prevention?” However, we do see a path toward making the best possible case to the general public to enable wide-scale suicide screening (as in Parker et al58). As to whether one ought or ought not ultimately construct a system of that sort (and the eventual implications thereof ), we lay a path out for how one might credibly test the efficacy and public’s opinion on it, as we go. Using opt-in analysis of risk, with the technology detailed here, is a step that we have heard little disagreement for both in our personal discussions and in any of the literature around this sort of work (eg, Mikal et al59). Building a system based on optin principles would allow the efficacy of these methods to be tested in the real world. Building this to better empower the health care system (case 1, above) would allow for such an empirical evaluation, and convincing evidence of the technology’s efficacy. Demonstrating this efficacy and sharing it widely would allow for an informed discussion among clinical researchers and healthy policy stakeholders. If, in turn, they were convinced that it is of sufficient utility for a more widespread or opt-out-style monitoring approach, they would then have the data in hand to demonstrate efficacy and the human systems to put in place to adequately handle the increase in demand that such a system would create.",2,2,2,0,2,2,2,0,4,2,2,2,2,2,0,0,0,0,4,2
https://journals.sagepub.com/doi/full/10.1177/1178222618792860,6,20,,"Coppersmith, Leary, Crutchley, Fine",4,0,0,1,2018,"We have demonstrated state-of-the-art results for the detection of people at risk for suicide through the automatic examination of the language posted on social media. These results from what we consider to be a foundational piece of a new kind of screening system, often discussed in the crisis prevention community, but not yet implemented. These machine learning algorithms are of sufficiently high accuracy to be fruitfully used in an envisioned screening system, but the remaining parts of the system are not yet ready for implementation. We examined the ethical and privacy concerns around the use of these algorithms for screening and monitoring, concluding that there are novel ways to consider using information from these algorithms to aid intervention, but the general public has voiced opposition to related approaches. Although the design of an intervention system powered by algorithmic screening is technically possible, the cultural implications of implementation are far from settled. It is our hope that this serves as a forcing function to have the discourse about the ramifications on culture and society",2,2,2,0,2,2,2,0,4,2,2,2,2,2,0,0,0,0,4,2
https://arxiv.org/abs/1712.03538,7,1,,"Benton, Mitchell, Hovy",3,0,0,0,2017,"We introduce initial groundwork for estimating suicide risk and mental health in a deep learning framework. By modeling multiple conditions, the system learns to make predictions about suicide risk and mental health at a low false positive rate. Conditions are modeled as tasks in a multitask learning (MTL) framework, with gender prediction as an additional auxiliary task. We demonstrate the effectiveness of multi-task learning by comparison to a well-tuned single-task baseline with the same number of parameters. Our best MTL model predicts potential suicide attempt, as well as the presence of atypical mental health, with AUC > 0.8. We also find additional large improvements using multi-task learning on mental health tasks with limited training data.",2,2,1,1,2,0,0,0,0,2,2,2,2,0,0,1,2,0,5,2
https://arxiv.org/abs/1712.03538,7,2,,"Benton, Mitchell, Hovy",3,0,0,0,2017,"Suicide is one of the leading causes of death worldwide, and over 90% of individuals who die by suicide experience mental health conditions.1 However, detecting the risk of suicide, as well as monitoring the effects of related mental health conditions, is challenging. Traditional methods rely on both self-reports and impressions formed during short sessions with a clinical expert, but it is often unclear when suicide is a risk in particular.2 Consequently, conditions leading to preventable suicides are often not adequately addressed. Automated monitoring and risk assessment of patients’ language has the potential to complement traditional assessment methods, providing objective measurements to motivate further care and additional support for people with difficulties related to mental health. This paves the way towards verifying the need for additional care with insurance coverage, for example, as well as offering direct benefits to clinicians and patients. We explore some of the possibilities in the deep learning and mental health space using written social media text that people with different mental health conditions are already producing. Uncovering methods that work with such text provides the opportunity to help people with different mental health conditions by leveraging a task they are already participating in. Social media text carries implicit information about the author, which has been modeled in natural language processing (NLP) to predict author characteristics such as age (Goswami et al., 2009; Rosenthal and McKeown, 2011; Nguyen et al., 2014), gender (Sarawgi et al., 2011; Ciot et al., 2013; Liu and Ruths, 2013; Volkova et al., 2015; Hovy, 2015), personality (Schwartz et al., 2013; Volkova et al., 2014; Plank and Hovy, 2015; Park et al., 2015; Preot¸iuc-Pietro et al., 2015), and occupation (Preotiuc-Pietro et al., 2015). Similar text signals have been effectively used to predict mental health conditions such as depression (De Choudhury et al., 2013; Coppersmith et al., 2015b; Schwartz et al., 2014), suicidal ideation (Coppersmith et al., 2016; Huang et al., 2015), schizophrenia (Mitchell et al., 2015) or post-traumatic stress disorder (PTSD) (Pedersen, 2015). However, these studies typically model each condition in isolation, which misses the opportunity to model coinciding influence factors. Tasks with underlying commonalities (e.g., part of-speech tagging, parsing, and NER) have been shown to benefit from multi-task learning (MTL), as the learning implicitly leverages interactions between them (Caruana, 1993; Sutton et al., 2007; Rush et al., 2010; Collobert et al., 2011; Søgaard and Goldberg, 2016). Suicide risk and related mental health conditions are therefore good candidates for modeling in a multi-task framework. In this paper, we propose multi-task learning for detecting suicide risk and mental health conditions. The tasks of our model include neuroatypicality (i.e., atypical mental health) and suicide attempt, as well as the related mental health conditions of anxiety, depression, eating disorder, panic attacks, schizophrenia, bipolar disorder, and posttraumatic stress disorder (PTSD), and we explore the effect of task selection on model performance. We additionally include the effect of modeling gender, which has been shown to improve accuracy in tasks using social media text (Volkova et al., 2013; Hovy, 2015). Predicting suicide risk and several mental health conditions jointly opens the possibility for the model to leverage a shared representation for conditions that frequently occur together, a phenomenon known as comorbidity. Further including gender reflects the fact that gender differences are found in the patterns of mental health (WHO, 2016), which may help to sharpen the model. The MTL framework we propose allows such shared information across predictions and enables the inclusion of several loss functions with a common shared underlying representation. This approach is flexible enough to extend to factors other than the ones shown here, provided suitable data. We find that choosing tasks that are prerequisites or related to the main task is critical for learning a strong model, similar to Caruana (1996). We further find that modeling gender improves accuracy across a variety of conditions, including suicide risk. The best-performing model from our experiments demonstrates that multi-task learning is a promising new direction in automated assessment of mental health and suicide risk, with possible application to the clinical domain.",2,2,1,1,2,0,0,0,0,2,2,2,2,0,0,1,2,0,5,2
https://arxiv.org/abs/1712.03538,7,3,,"Benton, Mitchell, Hovy",3,0,0,0,2017,"We demonstrate the utility of MTL in predicting mental health conditions from social user text – a notoriously difficult task (Coppersmith et al., 2015a; Coppersmith et al., 2015b) – with potential application to detecting suicide risk. 2. We explore the influence of task selection on prediction performance, including the effect of gender. 3. We show how to model tasks with a large number of positive examples to improve the prediction accuracy of tasks with a small number of positive examples. 4. We compare the MTL model against a singletask model with the same number of parameters, which directly evaluates the multi-task learning approach. 5. The proposed MTL model increases the True Positive Rate at 10% false alarms by up to 9.7% absolute (for anxiety), a result with direct impact for clinical applications.",2,2,1,1,2,0,0,0,0,2,2,2,2,0,0,1,2,0,5,2
https://arxiv.org/abs/1712.03538,7,4,,"Benton, Mitchell, Hovy",3,0,0,0,2017,"As with any author-attribute detection, there is the danger of abusing the model to single out people (overgeneralization, see Hovy and Spruit (2016)). We are aware of this danger, and sought to minimize the risk. For this reason, we don’t provide a selection of features or representative examples. The experiments in this paper were performed with a clinical application in mind, and use carefully matched (but anonymized) data, so the distribution is not representative of the population as a whole. The results of this paper should therefore not be interpreted as a means to assess mental health conditions in social media in general, but as a test for the applicability of MTL in a well-defined clinical setting.",2,2,1,1,2,0,0,0,0,2,2,2,2,0,0,1,2,0,5,2
https://arxiv.org/abs/1712.03538,7,5,,"Benton, Mitchell, Hovy",3,0,0,0,2017,"A neural multi-task architecture opens the possibility of leveraging commonalities and differences between mental conditions. Previous work (Collobert et al., 2011; Caruana, 1996; Caruana, 1993) has indicated that such an architecture allows for sharing parameters across tasks, and can be beneficial when there is varying degrees of annotation across tasks.3 This makes MTL particularly compelling in light of mental health comorbidity, and given that different conditions have different amounts of associated data. Previous MTL approaches have shown considerable improvements over single task models, and the arguments are convincing: Predicting multiple related tasks should allow us to exploit any correlations between the predictions. However, in much of this work, an MTL model is only one possible explanation for improved accuracy. Another more salient factor has frequently been overlooked: The difference in the expressivity of the model class, i.e., neural architectures vs. discriminative or generative models, and critically, differences in the number of parameters for comparable models. Some comparisons might therefore have inadvertently compared apples to oranges. In the interest of examining the effect of multitask learning specifically, we compare the multitask predictions to models with equal expressivity. We evaluate the performance of a standard logistic regression model (a standard approach to text-classification problems), a multilayer perceptron single-task learning (STL) model, and a neural MTL model, the latter two with equal numbers of parameters. This ensures a fair comparison by isolating the unique properties of MTL from the dimensionality-reduction aspects of deep architectures in general. The neural models we evaluate come in two forms. The first, depicted in plate notation on the left in Figure 1, are the STL models. These are feedforward networks with two hidden layers, trained independently to predict each task. On the right in Figure 1 is the MTL model, where the first hidden layer from the bottom is shared between all tasks. An additional per-task hidden layer is used to give the model flexibility to map from the task-agnostic representation to a task-specific one. Each hidden layer uses a rectified linear unit as non-linearity. The output layer uses a logistic non-linearity, since all tasks are binary predictions. The MTL model can easily be extended to a stack of shared hidden layers, allowing for a more complicated mapping from input to shared space.4 As noted in Collobert et al. (2011), MTL benefits from mini-batch training, which both allows optimization to jump out of poor local optima, and more stochastic gradient steps in a fixed amount of time (Bottou, 2012). We create mini-batches by sampling from the users in our data, where each user has some subset of the conditions we are trying to predict, and may or may not be annotated with gender. At each mini-batch gradient step, we update weights for all tasks. This not only allows for randomization and faster convergence, it also provides a speed-up over the individual selection process reported in earlier work (Collobert et al., 2011). Another advantage of this setup is that we do not need complete information for every instance: Learning can proceed with asynchronous updates, dependent on what the data in each batch has been annotated for, while sharing representations throughout. This effectively learns a joint model with a common representation for several different tasks, allowing the use of several “disjoint” data sets, some with limited annotated instances.",2,2,1,1,2,0,0,0,0,2,2,2,2,0,0,1,2,0,5,2
https://arxiv.org/abs/1712.03538,7,6,,"Benton, Mitchell, Hovy",3,0,0,0,2017,"Even in a relatively simple neural model, there are a number of hyperparameters that can (and have to) be tuned to achieve good performance. We perform a line search for every model we use, sweeping over L2 regularization and hidden layer width. We select the best model based on the development loss. Figure 4 shows the performance on the corresponding test sets (plot smoothed by rolling mean of 10 for visibility). In our experiments, we sweep over the L2 regularization constant applied to all weights in {10−4 , 10−3 , 10−2 , 0.1, 0.5, 1.0, 5.0, 10.0}, and hidden layer width (same for all layers in the network) in {16, 32, 64, 128, 256, 512, 1024, 2048}. We fix the mini-batch size to 256, and 0.05 dropout on the input layer. Choosing a small minibatch size and the model with lowest development loss helps to account for overfitting. We train each model for 5,000 iterations, jointly updating all weights in our models. After this initial joint training, we select each task separately, and only update the task-specific layers of weights independently for another 1,000 iterations (selecting the set of weights achieving lowest development loss for each task individually). Weights are updated using mini-batch Adagrad (Duchi et al., 2011) – this converges more quickly than other optimization schemes we experimented with. We evaluate the tuning loss every 10 epochs, and select the model with the lowest tuning loss.",2,2,1,1,2,0,0,0,0,2,2,2,2,0,0,1,2,0,5,2
https://arxiv.org/abs/1712.03538,7,7,,"Benton, Mitchell, Hovy",3,0,0,0,2017,"We train our models on a union of multiple Twitter user datasets: 1) users identified as having anxiety, bipolar disorder, depression, panic disorder, eating disorder, PTSD, or schizophrenia (Coppersmith et al., 2015a), 2) those who had attempted suicide (Coppersmith et al., 2015c), and 3) those identified as having either depression or PTSD from the 2015 Computational Linguistics and Clinical Psychology Workshop shared task (Coppersmith et al., 2015b), along with neurotypical gendermatched controls (Twitter users not identified as having a mental condition). Users were identified as having one of these conditions if they stated explicitly they were diagnosed with this condition on Twitter (verified by a human annotator), and the data was pre-processed to remove direction indications of the condition. For a subset of 1,101 users, we also manually-annotate gender. The final dataset contains 9,611 users in total, with an average of 3521 tweets per user. The number of users with each condition is included in Table 1. Users in this joined dataset may be tagged with multiple conditions, thus the counts in this table do not sum to the total number of users. We use the entire Twitter history of each user as input to the model, and split it into character 1-to-5-grams, which have been shown to capture more information than words for many Twitter text classification tasks (Mcnamee and Mayfield, 2004; Coppersmith et al., 2015a). We compute the relative frequency of the 5,000 most frequent n-gram features for n ∈ {1, 2, 3, 4, 5} in our data, and then feed this as input to all models. This input representation is common to all models, allowing for fair comparison.",2,2,1,1,2,0,0,0,0,2,2,2,2,0,0,1,2,0,5,2
https://arxiv.org/abs/1712.03538,7,8,,"Benton, Mitchell, Hovy",3,0,0,0,2017,"Our task is to predict suicide attempt and mental conditions for each of the users in these data. We evaluate three classes of models: baseline logistic regression over character n-gram features (LR), feed-forward multilayer perceptrons trained to predict each task separately (STL), and feedforward multi-task models trained to predict a set of conditions simultaneously (MTL). We experiment with a feed-forward network against independent logistic regression models as a way to directly test the hypothesis that MTL may work well in this domain. We also perform ablation experiments to see which subsets of tasks help us learn an MTL model that predicts a particular mental condition best. For all experiments, data were divided into five equal-sized folds, three for training, one for tuning, and one for testing (we report the performance on this). All our models are implemented in Keras5 with Theano backend and GPU support. We train the models for a total of up to 15,000 epochs, using mini-batches of 256 instances. Training time on all five training folds ranged from one to eight hours on a machine with Tesla K40M.",2,2,1,1,2,0,0,0,0,2,2,2,2,0,0,1,2,0,5,2
https://arxiv.org/abs/1712.03538,7,9,,"Benton, Mitchell, Hovy",3,0,0,0,2017,"We compare the accuracy of each model at predicting each task separately. In clinical settings, we are interested in minimizing the number of false positives, i.e., incorrect diagnoses, which can cause undue stress to the patient. We are thus interested in bounding this quantity. To evaluate the performance, we plot the false positive rate (FPR) against the true positive rate (TPR). This gives us a receiver operating characteristics (ROC) curve, allowing us to inspect the performance of each model on a specific task at any level of FPR. While the ROC gives us a sense of how well a model performs at a fixed true positive rate, it makes it difficult to compare the individual tasks at a low false positive rate, which is also important for clinical application. We therefore report two more measures: the area under the ROC curve (AUC) and TPR performance at FPR=0.1 (TPR@FPR=0.1). We do not compare our models to a majority baseline model, since this model would achieve an expected AUC of 0.5 for all tasks, and F-score and TPR@FPR=0.1 of 0 for all mental conditions – users exhibiting a condition are the minority, meaning a majority baseline classifier would achieve zero recall.",2,2,1,1,2,0,0,0,0,2,2,2,2,0,0,1,2,0,5,2
https://arxiv.org/abs/1712.03538,7,10,,"Benton, Mitchell, Hovy",3,0,0,0,2017,"Figure 2 shows the AUC-score of each model for each task separately, and Figure 3 the true positive rate at a low false positive rate of 0.1. Precisionrecall curves for model/task are in Figure 5. STL is a multilayer perceptron with two hidden layers (with a similar number of parameters as the proposed MTL model). The MTL +gender and MTL models predict all tasks simultaneously, but are only evaluated on the main respective task. Both AUC and TPR (at FPR=0.1) demonstrate that single-task models models do not perform nearly as well as multi-task models or logistic regression. This is likely because the neural networks learned by STL cannot be guided by the inductive bias provided by MTL training. Note, however, that STL and MTL are often times comparable in terms of F1-score, where false positives and false negatives are equally weighted. As shown Figure 2, multi-task suicide predictions reach an AUC of 0.848, and predictions for anxiety and schizophrenia are not far behind. Interestingly however, schizophrenia stands out as being the only condition to be best predicted with a single-task model. MTL models show improvements over STL and LR models for predicting suicide, neuroatypicality, depression, anxiety, panic, bipolar disorder, and PTSD. The inclusion of gender in the MTL models leads to direct gains over an LR baseline in predicting anxiety disorders: anxiety, panic, and PTSD. In Figure 3, we illustrate the true positive rate – that is, how many cases of mental health conditions that we correctly predict – given a low false positive rate – that is, a low rate of predicting people have mental health conditions when they do not. This is particularly useful in clinical settings, where clinicians seek to minimize overdiagnosing. In this setting, MTL leads to the best performance across the board, for all tasks under consideration: Neuroatypicality, suicide, depression, anxiety, eating, panic, schizophrenia, bipolar disorder, and PTSD. Including gender in MTL further improves performance for neuroatypicality, suicide, anxiety, schizophrenia, bipolar disorder, and PTSD.",2,2,1,1,2,0,0,0,0,2,2,2,2,0,0,1,2,0,5,2
https://arxiv.org/abs/1712.03538,7,11,,"Benton, Mitchell, Hovy",3,0,0,0,2017,"We find that the prediction of the conditions with the least amount of data – bipolar disorder and PTSD – are sig- nificantly improved by having the model also predict comorbid conditions with substantially more data: depression and anxiety. We are able to increase the AUC for predicting PTSD to 0.786 by MTL, from 0.770 by LR, whereas STL fails to perform as well with an AUC of 0.667. Similarly for predicting bipolar disorder (MTL:0.723, LR:0.752, STL:0.552) and panic attack (MTL:0.724, LR:0.713, STL:0.631). These differences in AUC are significant at p = 0.05 according to bootstrap sampling tests with 5000 samples. The wide difference between MTL and STL can be explained in part by the increased feature set size – MTL training may, in this case, provide a form of regularization that STL cannot exploit. Further, modeling the common mental health conditions with the most data (depression, anxiety) helps in pulling out more rare conditions comorbid with these common health conditions. This provides evidence that an MTL model can help in predicting elusive conditions by using large data for common conditions, and a small amount of data for more rare conditions.",2,2,1,1,2,0,0,0,0,2,2,2,2,0,0,1,2,0,5,2
https://arxiv.org/abs/1712.03538,7,12,,"Benton, Mitchell, Hovy",3,0,0,0,2017,"Figures 2 and 3 both suggest that adding gender as an auxiliary task leads to more predictive models, even though the difference is not statistically significant for most tasks. This is consistent with the findings in previous work (Volkova et al., 2013; Hovy, 2015). Interestingly, though, the MTL model is worse at predicting gender itself. While this could be a direct result of data sparsity (recall that we have only a small subset annotated for gender), which could be remedied by annotating additional users for gender, this appears unlikely given the other findings of our experiments, where MTL helped in specifically these sparse scenarios. However, it has been pointed out by Caruana (1996) that not all tasks benefit from a MTL setting in the same way, and that some tasks serve purely auxiliary functions. Here, gender prediction does not benefit from including mental conditions, but helps vice versa. In other words, predicting gender is qualitatively different from predicting mental health conditions: it seems likely that the signals for anxiety ares much more similar to the ones for depression than for, say, being male, and can therefore add to detecting depression. However, the distinction between certain conditions does not add information for the distinction of gender. The effect may also be due to the fact that these data were constructed with inferred gender (used to match controls), so there might be a degree of noise in the data.",2,2,1,1,2,0,0,0,0,2,2,2,2,0,0,1,2,0,5,2
https://arxiv.org/abs/1712.03538,7,13,,"Benton, Mitchell, Hovy",3,0,0,0,2017,"Although MTL tends to dominate STL in our experiments, it is not clear whether modeling several tasks provide a beneficial bias in MTL models in general, or if there exists specific subsets of auxiliary tasks that are most beneficial for predicting suicide risk and related mental health conditions. We perform ablation experiments by training MTL models on a subset of auxiliary tasks, and prediction for a single main task. We focus on four conditions to predict well: suicide attempt, anxiety, depression, and bipolar disorder. For each main task, we vary the auxiliary tasks we train the MTL model with. Since considering all possible subsets of tasks is combinatorily unfeasible, we choose the following task subsets as auxiliary: • all: all mental conditions along with gender • all conds: all mental conditions, no gender • neuro: only neurotypicality • neuro+mood: neurotypicality, depression, and bipolar disorder (mood disorders) • neuro+anx: neurotypicality, anxiety, and panic attack (anxiety conditions) • neuro+targets: neurotypicality, anxiety, depression, suicide attempt, bipolar disorder • none: no auxiliary tasks, equivalent to STL Table 2 shows AUC for the four prediction tasks with different subsets of auxiliary tasks. Statistically significant improvements over the respective LR baselines are denoted by superscript. Restricting the auxiliary tasks to a small subset tends to hurt performance for most tasks, with exception to bipolar, which benefits from the prediction of depression and suicide attempt. All main tasks achieve their best performance using the full set of additional tasks as auxiliary. This suggests that the biases induced by predicting different kinds of mental conditions are mutually beneficial – e.g., multi-task models that predict suicide attempt may also be good at predicting anxiety. Based on these results, we find it useful to think of MTL as a framework to leverage auxiliary tasks as regularization to effectively combat data paucity and less-than-trustworthy labels. As we have demonstrated, this may be particularly useful when predicting mental health conditions and suicide risk.",2,2,1,1,2,0,0,0,0,2,2,2,2,0,0,1,2,0,5,2
https://arxiv.org/abs/1712.03538,7,14,,"Benton, Mitchell, Hovy",3,0,0,0,2017,"Our results indicate that an MTL framework can lead to significant gains over single-task models for predicting suicide risk and several mental health conditions. We find benefit from predicting related mental conditions and demographic attributes simultaneously. We experimented with all the optimizers that Keras provides, and found that Adagrad seems to converge fastest to a good optimum, although all the adaptive learning rate optimizers (such as Adam, etc.) tend to converge quickly. This indicates that the gradient is significantly steeper along certain parameters than others. Default stochastic gradient descent (SGD) was not able to converge as quickly, since it is not able to adaptively scale the learning rate for each parameter in the model – taking too small steps in directions where the gradient is shallow, and too large steps where the gradient is steep. We further note an interesting behavior: all of the adaptive learning rate optimizers yield a strange “step-wise” training loss learning curve, which hits a plateau, but then drops after about 900 iterations, only to hit another plateau, and so on. Obviously, we would prefer to have a smooth training loss curve. We can indeed achieve this using SGD, but it takes much longer to converge than, for example, Adagrad. This suggests that a well-tuned SGD would be the best optimizer for this problem, a step that would require some more experimentation and is left for future work. We also found that feature counts have a pronounced effect on the loss curves: Relative feature frequencies yield models that are much easier to train than raw feature counts. Feature representations are therefore another area of optimization, e.g., different ranges of character n-grams (e.g., n > 5) and unigrams. We used character 1-to-5-grams, since we believe that these features generalize better to a new domain (e.g., Facebook) than word unigrams. However, there is no fundamental reason not to choose longer character n-grams, other than time constraints in regenerating the data, and accounting for overfitting with proper regularization. Initialization is a decisive factor in neural models, and Goldberg (2015) recommends repeated restarts with differing initializations to find the optimal model. In an earlier experiment, we tried initializing a MTL model (without task-specific hidden layers) with pretrained word2vec embeddings of unigrams trained on the Google News n-gram corpus. However, we did not notice an improvement in F-score. This could be due to the other factors, though, such as feature sparsity. Table 3 shows parameters sweeps with hidden layer width 256, training the MTL model on the social media data with character trigrams as input features. The sweet spots in this table may be good starting points for training models in future work.",2,2,1,1,2,0,0,0,0,2,2,2,2,0,0,1,2,0,5,2
https://arxiv.org/abs/1712.03538,7,15,,"Benton, Mitchell, Hovy",3,0,0,0,2017,"MTL was introduced by Caruana (1993), based on the observation that humans rarely learn things in isolation, and that it is the similarity between related tasks that helps us get better. Some of the first works on MTL were motivated by medical risk prediction (Caruana et al., 1996), and it is now being rediscovered for this purpose (Lipton et al., 2016). The latter use a long shortterm memory (LSTM) structure to provide several medical diagnoses from health care features (yet no textual or demographic information), and find small, but probably not significant improvements over a structure similar to the STL we use here. The target in previous work was medical conditions as detected in patient records, not mental health conditions in social text. The focus in this work has been on the possibility of predicting suicide attempt and other mental health conditions using social media text that a patient may already be writing, without requiring full diagnoses. The framework proposed by Collobert et al. (2011) allows for predicting any number of NLP tasks from a convolutional neural network (CNN) representation of the input text. The model we present is much simpler: A feed-forward network with n-gram input layer, and we demonstrate how to constrain n-gram embeddings for clinical application. Comparing with further models is possible, but distracts from the question of whether MTL training can help in this domain. As we have shown, it can.",2,2,1,1,2,0,0,0,0,2,2,2,2,0,0,1,2,0,5,2
https://arxiv.org/abs/1712.03538,7,16,,"Benton, Mitchell, Hovy",3,0,0,0,2017,"In this paper, we develop neural MTL models for 10 prediction tasks (suicide, seven mental health conditions, neurotypicality, and gender). We compare their performance with STL models trained to predict each task independently. Our results show that an MTL model with all task predictions performs significantly better than other models, reaching 0.846 TPR for neuroatypicality where FPR=0.1, and AUC of 0.848, TPR of 0.559 for suicide. Due to the nature of MTL, we find additional contributions that were not the original goal of this work: Pronounced gains in detecting anxiety, PTSD, and bipolar disorder. MTL predictions for anxiety, for example, reduce the error rate from a single-task model by up to 11.9%. We also investigate the influence of model depth, comparing to progressively deeper STL feed-forward networks with the same number of parameters. We find: (1) Most of the modeling power stems from the expressivity conveyed by deep architectures. (2) Choosing the right set of auxiliary tasks for a given mental condition can yield a significantly better model. (3) The MTL model dramatically improves for conditions with the smallest amount of data. (4) Gender prediction does not follow the two previous points, but improves performance as an auxiliary task. Accuracy of the MTL approach is not yet ready to be used in isolation in the clinical setting. However, our experiments suggest this is a promising direction moving forward. There are strong gains to be made in using multi-task learning to aid clinicians in their evaluations, and with further partnerships between the clinical and machine learning community, we foresee improved suicide prevention efforts.",2,2,1,1,2,0,0,0,0,2,2,2,2,0,0,1,2,0,5,2
https://arxiv.org/abs/1709.01848,8,1,,"Yates, Cohan, Goharian",3,0,0,0,2017,"Users suffering from mental health conditions often turn to online resources for support, including specialized online support communities or general communities such as Twitter and Reddit. In this work, we present a neural framework for supporting and studying users in both types of communities. We propose methods for identifying posts in support communities that may indicate a risk of self-harm, and demonstrate that our approach outperforms strong previously proposed methods for identifying such posts. Self-harm is closely related to depression, which makes identifying depressed users on general forums a crucial related task. We introduce a large-scale general forum dataset (“RSDD”) consisting of users with self reported depression diagnoses matched with control users. We show how our method can be applied to effectively identify depressed users from their use of language alone. We demonstrate that our method outperforms strong baselines on this general forum dataset.",2,2,2,2,1,0,2,0,2,2,2,2,2,2,0,2,2,0,8,2
https://arxiv.org/abs/1709.01848,8,2,,"Yates, Cohan, Goharian",3,0,0,0,2017,"Mental health remains a major challenge in public health care. Depression is one of the most common mental disorders and 350 million people are estimated to suffer from depression worldwide (WHO, 2010). In 2014 an estimated 7% of all U.S. adults had experienced at least one major depressive disorder (2015). Suicide and selfharm are major related concerns in public mental health. Suicide is one of the leading causes of death (CDC, 2015), and each suicide case has major consequences on the physical and emotional well-being of families and on societies in general. Therefore identifying individuals at risk of selfharm and providing support to prevent it remains an important problem (Ferrari et al., 2014). Social media is often used by people with mental health problems to express their mental issues and seek support. This makes social media a significant resource for studying language related to depression, suicide, and self-harm, as well as understanding the authors’ reasons for making such posts, and identifying individuals at risk of harm (Coppersmith et al., 2014a). Depression and suicide are closely related given that depression is the psychiatric diagnosis most commonly associated with suicide. Research has demonstrated that forums are powerful platforms for selfdisclosure and social support seeking around mental health concerns (De Choudhury and De, 2014; Manikonda and De Choudhury, 2017). Such support forums are often staffed by moderators who are mental health experts, trained volunteers, or more experienced users whose role is to identify forum posts suggesting that a user is at risk of selfharm and to provide support. Studies have shown that self expression and social support are beneficial in improving the individual’s state of the mind (Turner et al., 1983; Choudhury and Kiciman, 2017) and, thus such communities and interventions are important in suicide prevention. However, there are often thousands of user posts published in such support forums daily, making it difficult to manually identify individuals at risk of self-harm. Additionally, users in acute distress need prompt attention, and any delay in responding to these users could have adverse consequences. Therefore, identifying individuals at risk of self-harm in such support forums is an important challenge. Identifying signs of depression in general social media, on the other hand, is also a difficult task that has applications for both better understanding the relationship between mental health and language use and for monitoring a specific user’s state (e.g., in the context of monitoring a user’s response to clinical care). In this work we propose and evaluate a framework for performing self-harm risk assessment and for identifying depression in online forums. We present a general neural network architecture for combining posts into a representation of a user’s activity that is used to classify the user. To address the challenge of depression risk assessment over the general forums, we introduce a large-scale novel Reddit dataset that is substantially larger than the existing data and has a much more realistic number of control users. The dataset contains over 9,000 users with selfreported depression diagnoses matched with over 107,000 control users. We apply our approach to (1) identify the users with depression on a general forum like Reddit, and to (2) estimate the risk of self-harm indicated by posts in a more specific mental-health support forum. Our methods perform significantly better on both datasets than strong existing methods, demonstrating that our approach can be used both to identify depressed users and to estimate the risk of self-harm posed by individual posts.",2,2,2,2,1,0,2,0,2,2,2,2,2,2,0,2,2,0,8,2
https://arxiv.org/abs/1709.01848,8,3,,"Yates, Cohan, Goharian",3,0,0,0,2017,"There is a growing body of related work analyzing mental health-related discourse and language usage in social media to better discover and understand mental health related concerns (Resnik et al., 2013; De Choudhury et al., 2013; Coppersmith et al., 2014b,a; Mitchell et al., 2015; Tsugawa et al., 2015; Coppersmith et al., 2015a; Althoff et al., 2016; Mowery et al., 2016; Benton et al., 2017b). To investigate NLP methods for identifying depression and PTSD users on Twitter, a shared task (Coppersmith et al., 2015b) at the 2nd Computational Linguistics and Clinical Psychology Workshop (CLPsych 2015) was introduced where the participants evaluated their methods on a dataset of about 1800 Twitter users. Other work has used data from approximately 900 Reddit.com users to support self-reported diagnosis detection (Losada and Crestani, 2016). Previous work identifying depression and other mental health problems, including the methods participating in CLPsych 2015 (e.g. works by Resnik et al. (2015) and Preot¸iuc-Pietro et al. (2015)) heavily rely on utilizing features such as LIWC (Pennebaker et al., 2015), topic modeling, manual lexicons, or other domain-dependent applicationspecific features. Aside from the effort required to design effective features, these approaches usually model the problem with respect to the selected features and ignore other indicators and signals that can improve prediction. In contrast, our model only relies on text and is not dependent on any external or domain-specific features. Existing self-reported diagnosis detection datasets contain a limited number of both control users and diagnosed users. In contrast to this, we construct a new dataset with over 9,000 depressed users matched with a realistic number of control users. In addition to general studies addressing mental health, related work has also specifically studied suicide and self-harm through social media (Jashinsky et al., 2014; Thompson et al., 2014; Gunn and Lester, 2015; De Choudhury et al., 2016; Coppersmith et al., 2016). Recently, CLPsych 2016 (Hollingshead and Ungar, 2016) investigated approaches for detecting the self-harm risk of mental health forum posts (Milne et al., 2016). Most related work in this area uses variations of linear classifiers with some sort of feature engineering; successful methods have employed: a combination of sparse (bag-of-words) and dense (doc2vec) representation of the target forum posts (Kim et al., 2016), a stack of featurerich Random Forest and linear Support Vector Machine (SVM) (Malmasi et al., 2016), an RBF SVM classifier utilizing similar sets of features (Brew, 2016), and various contextual and psycholinguistic features (Cohan et al., 2016, 2017). In contrast to the above works, our model does not use any general or domain specific feature engineering; it learns appropriate representations of documents by considering only their textual content. Our proposed models consist of a shared architecture based on a CNN, a merge layer, modelspecific loss functions, and an output layer (as we will describe in §4). While our model shares similarities with CNN-based models in prior work (Kalchbrenner et al., 2014; Kim, 2014; Xiao and Cho, 2016), it focuses on learning representations of user’s posts and combining the post representations into an overall representation of the user’s activity. In the case of self-harm risk assessment, we experiment with several loss functions to determine whether considering the ordinal nature of self-harm risk labels (i.e., green, amber, red, and crisis) can improve performance. Evaluation results suggest that the model variant using this loss function is more robust than our other variants.",2,2,2,2,1,0,2,0,2,2,2,2,2,2,0,2,2,0,8,2
https://arxiv.org/abs/1709.01848,8,4,,"Yates, Cohan, Goharian",3,0,0,0,2017,"We created a new dataset to support the task of identifying forum users with self-reported depression diagnoses. The Reddit Self-reported Depression Diagnosis (RSDD) dataset was created by annotating users from a publicly-available Reddit dataset1 . Users to annotate were selected by identifying all users who made a post between January 2006 and October 2016 matching a highprecision diagnosis pattern.2 Users with fewer than 100 posts made before their diagnosis post were discarded. Each of the remaining diagnosis posts was then viewed by three layperson annotators to decide whether the user was claiming to have been diagnosed with depression; the most common false positives included hypotheticals (e.g., “if I was diagnosed with depression”), negations (e.g., “it’s not like I’ve been diagnosed with depression”), and quotes (e.g., “my brother announced ‘I was just diagnosed with depression’ ”). Only users with at least two positive annotations were included in the final group of diagnosed users. A pool of potential control users was identified by selecting only those users who had (1) never posted in a subreddit related to mental health, and (2) never used a term related to depression or mental health. These restrictions minimize the likelihood that users with depression are included in the control group. In order to prevent the diagnosed users from being easily identified by the usage of specific keywords that are never used by the control users, we removed all posts by diagnosed users that met either one of the aforementioned conditions (i.e., that was posted in a mental health subreddit or included a depression term). For each diagnosed user and potential control user, we calculated the probability that the user would post in each subreddit (while ignoring diagnosed users’ posts made to mental health subreddits). Each diagnosed user was then greedily matched with the 12 control users who had the smallest Hellinger distance between the diagnosed user’s and the control user’s subreddit post probability distributions, excluding control users with 10% more or fewer posts than the diagnosed user. This matching approach ensures that diagnosed users are matched with control users who are interested in similar subreddits and have similar activity levels, preventing biases based on the subreddits users are involved in or based on how active the users are on Reddit. This yielded a dataset containing 9,210 diagnosed users and 107,274 control users. On average each user in the dataset has 969 posts (median 646). The mean post length is 148 tokens (median 74). The Reddit Self-reported Depression Diagnosis (RSDD) dataset differs from prior work creating self-reported diagnoses datasets in several ways: it is an order of magnitude larger, posts were annotated to confirm that they contained claims of a diagnosis, and a realistic number of control users were matched with each diagnosed user. The lists of terms related to mental health, subreddits related to mental health, high-precision depression diagnosis patterns, and further information are available3 . We note that this dataset has some (inevitable) caveats: (i) the method only captures a subpopulation of depressed people (i.e. those with self-reported diagnosis), (ii) Reddit users may not be a representative sample of the population as a whole, and (iii) there is no way to verify whether the users with self-reported diagnoses are truthful",2,2,2,2,1,0,2,0,2,2,2,2,2,2,0,2,2,0,8,2
https://arxiv.org/abs/1709.01848,8,5,,"Yates, Cohan, Goharian",3,0,0,0,2017,"For self-harm risk assessment we use data from mental health forum posts from ReachOut.com, which is a successful Australian support forum for young people. In addition to providing peersupport, ReachOut moderators and trained volunteers monitor and participate in the forum discussions. The NAACL 2016 Computational Linguistics and Clinical Psychology Workshop (Hollingshead and Ungar, 2016) released a Triage dataset containing 65,024 forum posts from ReachOut, with annotations for 1,227 posts indicating the author’s risk of self-harm (Milne et al., 2016). The annotations consist of one of four labels: green (indicating no action is required from ReachOut’s moderators), amber (non-urgent attention is required), red (urgent attention is required), and crisis (a risk that requires immediate attention).",2,2,2,2,1,0,2,0,2,2,2,2,2,2,0,2,2,0,8,2
https://arxiv.org/abs/1709.01848,8,6,,"Yates, Cohan, Goharian",3,0,0,0,2017,"Social media data are often sensitive, and even more so when the data are related to mental health. Privacy concerns and the risk to the individuals in the data should always be considered (Hovy and Spruit, 2016; Suster et al. ˇ , 2017; Benton et al., 2017a). We note that the risks associated with the data used in this work are minimal. This assessment is supported by previous work on the ReachOut dataset (Milne et al., 2016), on Twitter data (Coppersmith et al., 2015b), and on other Reddit data (Losada and Crestani, 2016). The RSDD dataset contains only publicly available Reddit posts. Annotators were shown only anonymized posts and agreed to make no attempts to deanonymize or contact them. The RSDD dataset will only be made available to researchers who agree to follow ethical guidelines, which include requirements not to contact or attempt to deanonymize any of the users. Additionally, for the ReachOut forum data that was explicitly related to mental health, the forum’s rules require the users to stay anonymous; moderators actively redact any user identifying information.",2,2,2,2,1,0,2,0,2,2,2,2,2,2,0,2,2,0,8,2
https://arxiv.org/abs/1709.01848,8,7,,"Yates, Cohan, Goharian",3,0,0,0,2017,"We describe a general neural network architecture for performing text classification over multiple input texts. We propose models based on this architecture for performing two tasks in the social media and mental health domains that we call selfharm risk classification and detecting depression. The task of self-harm risk classification is estimating a user’s current self-harm risk given the user’s post on a mental health support forum and the previous posts in the thread. The task of detecting depressions in users is identifying Reddit users with self-reported depression diagnoses given the users’ post histories (excluding posts containing mental health keywords or posted in subreddits related to mental health). While both tasks are focused on predicting a user’s mental health status, they differ in both the type of classification performed (i.e., estimating severity on a four point scale vs. boolean classification) and in the amount of data available. Our general architecture is based on a two step process: (1) identifying relevant features in each input text, and (2) combining the features observed in the model’s inputs to classify the user.",2,2,2,2,1,0,2,0,2,2,2,2,2,2,0,2,2,0,8,2
https://arxiv.org/abs/1709.01848,8,8,,"Yates, Cohan, Goharian",3,0,0,0,2017,"Our proposed models share a common architecture that takes one or more posts as input, processes the posts using a convolutional layer to identify features present in sliding windows of text, merges the features identified into a vector representation of the user’s activity, and uses a series of dense layers to perform classification on the merged vector representation. The type of merging performed and the output layers are properties of the model variant, which we describe in detail in the following section. Convolutional networks have commonly been applied to the task of text classification, such as by Kim (2014). We use categorical cross-entropy as a loss function with both methods, but also experiment with other loss functions when performing severity classification. First, the model takes one or more posts as input and processes each post with a convolutional network containing a convolutional layer and a pooling layer. This process is illustrated with a max pooling layer in Figure 2. The convolutional layer applies filters to a sliding window of k terms (a) and outputs a feature value for each sliding window region and each filter (b). The same filters are applied to each window; each filter can be viewed as a feature detector and the overall process can be conceptualized as looking for windows of terms that contain specific features. The features are not specified a priori through feature engineering, but instead are learned automatically when the model is trained. After identifying the features present in each region (i.e., sliding window), a max pooling layer considers non-overlapping regions of length n and keeps the highest feature value for each region (c). This step eliminates the regions (i.e., sliding windows) that do not contain useful features, which reduces the size of the convolutional network’s output. The same convolutional network is applied to each input post, meaning that the model learns to look for the same set of features in each. After each input post has been processed by a convolutional network, the output of each convolutional network is merged to create a representation of the user’s activity across all input posts. This representation is processed by one or more dense layers (i.e., fully connected layers) with dropout (Srivastava et al., 2014) before being processed by a final output layer to perform classification. The type of output layer is dependent on the model variant. Our shared model architecture is illustrated in Figure 1. The architecture’s hyperparameters (e.g., the sliding window size k, the number of convolutional filters used, and type of pooling) also vary among models and are described in §5. Both the convolutional and dense layers use ReLU activations (Nair and Hinton, 2010) in all model variants.",2,2,2,2,1,0,2,0,2,2,2,2,2,2,0,2,2,0,8,2
https://arxiv.org/abs/1709.01848,8,9,,"Yates, Cohan, Goharian",3,0,0,0,2017,Our model for depression detection takes a user’s posts as input and processes each post with a convolutional network. Each convolutional network performs average pooling to produce its output. These post representations are then merged with a second convolutional layer to create a user representation; we found this approach led to more stable performance than using a second average pooling or max pooling layer. The user representation created by the merge step is then passed to one or more dense layers before being passed to a dense output layer with a softmax activation function to perform classification. The number of dense layers used is a hyperparameter described in §5. Categorical cross-entropy is used as the model’s loss function.,2,2,2,2,1,0,2,0,2,2,2,2,2,2,0,2,2,0,8,2
https://arxiv.org/abs/1709.01848,8,10,,"Yates, Cohan, Goharian",3,0,0,0,2017,"Our model for self-harm risk classification takes two inputs: the target post being classified and the prior posts (if any) in the target post’s thread. The prior posts provide context and are thus useful for estimating the risk of self-harm present in the target post. The two inputs are both processed by a convolutional network as in user-level classification, but in this case the convolutional network’s outputs correspond to a representation of the target post and to a representation of the target post’s context (i.e., the prior posts in the thread). Given that these two outputs represent different aspects, they are merged by concatenating them together. This merged representation is then passed to one or more dense layers and to an output layer; the type of output layer depends on the loss function used. There are four self-harm risk assessment model variants in total: Categorical Cross Ent. uses an output layer with a softmax activation function, and categorical cross-entropy as its loss function. This mirrors the output layer and loss function used in the user level classification model. MSE uses an output layer with a linear activation function, and mean squared error as its loss function. The model’s output is thus a single value; to perform classification, this output value is rounded to the nearest integer in the interval [0, t − 1], where t is the number of target classes. The final two loss functions perform metric learning rather than performing classification directly. They learn representations of a user’s activity and of the four self-harm risk severity labels; classification is performed by comparing the euclidean distance between a representation of a user’s activity (produced by the final layer) and each of the four severity label representations. Class Metric: Let d be the size of the output layer and X be the layer’s d-dimensional output. Class Metric learns a d-dimensional representation of each class Ci such that ||X − Ci ||2 is minimized for the correct class i; this is accomplished with the loss function: where Cp is the correct (i.e., positive) class for Xi , Cn is a randomly chosen incorrect (i.e., negative) class, and α is a constant to enforce a minimum margin between classes. Classification is performed by computing the similarity between Xi and each class Cj . Class Metric (Ordinal) extends Class Metric to enforce a margin between ordinal classes as a function of the distance between classes. Given a ranked list of classes such that more similar classes have closer rankings, that is ∀i sim(Ci , Ci±1) > sim(Ci , Ci±2), we incorporate the class distance into the margin such that more distant incorrect class labels must be further away from the correct class label in the metric space. The loss function becomes where |p − n| causes the margin to scale with the distance between classes p and",2,2,2,2,1,0,2,0,2,2,2,2,2,2,0,2,2,0,8,2
https://arxiv.org/abs/1709.01848,8,11,,"Yates, Cohan, Goharian",3,0,0,0,2017,"In this section, we describe the model hyperparameters used and present our results on the depression detection and self-harm risk assessment tasks. To facilitate reproducibility we provide our code and will provide the Reddit depression dataset to researchers who sign a data usage agreement4",2,2,2,2,1,0,2,0,2,2,2,2,2,2,0,2,2,0,8,2
https://arxiv.org/abs/1709.01848,8,12,,"Yates, Cohan, Goharian",3,0,0,0,2017,"The hyperparameters used with our models are shown in Table 1. The severity risk assessment models’ hyperparameters were chosen using 10- fold cross validation on the 947 ReachOut training posts, with 15% of each fold used as validation data. The depression identification model’s hyperparameters were chosen using the Reddit validation set. The depression identification model’s second convolutional layer (i.e., the layer used to merge post representations) used filters of length 15, a stride of length 15, and the same number of filters as the first convolutional layer. All models were trained using stochastic gradient descent with the Adam optimizer (Kingma and Ba, 2014). The hyperparameters that varied across models are shown in Table 1. The convolution size, number of convolutional filters, pooling type, pooling length, and number of dense layers was similar across all post models. Class balancing was performed with Categorical Cross Ent. by weighting classes inversely proportional to their frequencies, whereas sampling an equal number of instances for each class worked best with the other methods",2,2,2,2,1,0,2,0,2,2,2,2,2,2,0,2,2,0,8,2
https://arxiv.org/abs/1709.01848,8,13,,"Yates, Cohan, Goharian",3,0,0,0,2017,". The post classification models’ input consists of skip-thought vectors (Kiros et al., 2015); each vector used is a 7200- dimensional representation of a sentence. Thus, the convolutional windows used for post classification are over sentences rather than over terms. This input representation was chosen to mitigate the effects of the ReachOut dataset’s relatively small size. The skip-thought vectors were generated from the the ReachOut forum dataset by sequentially splitting the posts in the training set into sentences, tokenizing them, and training skipthoughts using Kiros et al.’s implementation with the default parameters. Sentence boundary detection was performed using the Punkt sentence tokenizer (Kiss and Strunk, 2006) available in NLTK (Bird et al., 2009). These 2400-dimensional for post skip-thought vectors were concatenated with the 4800-dimensional book corpus skip-thought vectors available from Kiros et al.. Experiments on the training set indicated that using only the ReachOut skip-thought vectors slightly decreased performance, while using only the book corpus skip-thought vectors substantially decreased performance. As input the post models received the last 20 sentences in each target post and the last 20 sentences in the thread prior to the target post; any prior sentences are ignored.",2,2,2,2,1,0,2,0,2,2,2,2,2,2,0,2,2,0,8,2
https://arxiv.org/abs/1709.01848,8,14,,"Yates, Cohan, Goharian",3,0,0,0,2017,"The data used for depression detection was described in §3. As baselines we compare our model against the FastText classifier (Joulin et al., 2016) and MNB and SVM classifiers (Wang and Manning, 2012) using features from prior work. We tune FastText’s hyperparameters on the validation set. Specifically, we consider a maximum n-gram size ∈ [1, 2, 3, 4, 5], an embedding size ∈ [50, 100, 150], and a learning rate ∈ [0.05, 0.1, 0.25, 0.5] as suggested in the documentation. We consider two sets of features for the MNB and SVM classifiers. The first set of features is the post content itself represented as sparse bag of words features (BoW baselines). The second set of features (feature-rich baselines) comprises a large set of features including bag of words features encoded as sparse weighted vectors, external psycholinguistic features captured by LIWC5 (2015), and emotion lexicon features (Staiano and Guerini, 2014). Since our problem is identifying depression among users, psycholinguistic signals and emotional attributes in the text are potentially important features for the task. These features (as described in §2) have been also previously used by successful methods in the Twitter self-reported diagnosis detection task (Coppersmith et al., 2015b). Thus, we argue that these are strong baselines for our self-reported diagnosis detection task. We apply count based and TF-IDF based feature weighting for bag of words features. We perform standard preprocessing by removing stopwords and lowercasing the input text.6 The data is split into training, validation, and testing datasets each containing approximately 3,000 diagnosed users and their matched control users. The validation set is used for tuning development and hyperparameter tuning of our models and the baselines. The reported results are on the test set. The depression detection models’ input consisted of raw terms encoded as one-hot vectors. We used an input layer to learn 50-dimensional representation of the terms. For each target user, the CNN received up to npost posts containing up to nterm terms. In this section we present results for two values of npost. The earliest post approach (CNN-E) takes each user’s npost = 400 earliest posts as input. The random approach (CNN-R) samples npost = 1500 random posts from each user. We empirically set nterm = 100 with both approaches. We later analyze the model’s performance as npost and nterm vary in §6.1 and as the post selection strategy varies in §6.2. Results. The results of identifying depressed users for our model and baselines are shown in Table 2. Our proposed model outperforms the baselines by a large margin in terms of recall and F1 on the diagnosed users (increases of 41% and 16%, respectively), but performs worse in terms of precision. As described later in the analysis section, the CNN identifies language associated with negative sentiment across a user’s posts.",2,2,2,2,1,0,2,0,2,2,2,2,2,2,0,2,2,0,8,2
https://arxiv.org/abs/1709.01848,8,15,,"Yates, Cohan, Goharian",3,0,0,0,2017,"We train our methods to label the ReachOut posts and compare them against the top methods from CLPsych ’16. We use the same experimental protocol as was used in CLPsych ’16; our methods were trained on the 947 training posts and evaluated on the remaining 280 testing posts. We used 15% of the 947 training posts as validation data. We report results using the same metrics used in CLPsych, which were: the macro-averaged F1 for the amber, red, and crisis labels (non green posts); the macro-averaged F1 of green posts vs. amber ∪ red ∪ crisis (flagged posts); and the macro-averaged F1 of green ∪ amber vs. red∪crisis (urgent posts). The non-green F1 was used as the official CLPsych metric with the intention of placing emphasis on classification performance for the non-green categories (i.e., those that required some response). The binary flagged meta-class was chosen to measure models’ abilities to differentiate between posts that require attention and posts that do not, and the binary urgent meta-class was chosen to measure their abilities to differentiate between posts that require quick responses and posts that do not. In addition to macro-averaged F1, CLPsych also reported the accuracy for each category. We additionally report F1 macro-averaged over all classes. Results. The results on the self-harm risk assessment task for our models and for the current best-performing methods (briefly explained in §2) are shown in Table 3. We also report a baseline result which is based on a SVM classifier with bigram features. When measured by non-green F1, the official metric of the CLPsych ’16 Triage Task, our proposed models perform up to 19% better than the best existing methods. Similarly, our models perform up to 11% better when measured with an F1 macro-averaged across all categories (i.e., all column) and up to 5% better with measured accuracy across all categories. Categorical Cross Ent. performs best in all of these cases, though the difference between the performance of Categorical Cross Ent. and Class Metric with an ordinal margin is not statistically significant. We also evaluate the performance of our methods on the training set using 10-fold cross validation to better observe performance differences (Table 4). All model variants perform substantially better on the training set than on the test set. This is partially explained by the fact that the models were tuned on the training set, but the large difference in some cases (e.g., the increase in the highest non-green F1 from 0.50 to 0.87) suggest there may be qualitative differences between the datasets. The best-performing method on the test set, Categorical Cross Ent., performs the worst on the training set; worst-performing method on the test set, MSE, performs the best on the training set. Class Metric (Ordinal) performs well on both the testing and training sets, however, suggesting that it is more robust than the other methods. Furthermore, there is no statistically significant difference between Class Metric (Ordinal) and the best-performing method on either dataset.",2,2,2,2,1,0,2,0,2,2,2,2,2,2,0,2,2,0,8,2
https://arxiv.org/abs/1709.01848,8,16,,"Yates, Cohan, Goharian",3,0,0,0,2017,"In this section we consider the effects of the maximum number of posts per user (i.e., npost) and the maximum post length (i.e., nterm) on the Reddit dataset. To do so we train the CNN-R model as described in §5.1 and report F1 on the validation set. When varying npost we set nterm = 100, and when varying nterm we set npost = 1500. As shown in Figure 3, the best performance of the CNN-R model is reached when it considers 100 terms in posts and up to 1750 posts for each user. F1 increases as npost increases, up to the maximum tested value of 1750 (Figure 3a). There is relatively little change in F1 from npost = 1250 to npost = 1750, however, so we use npost = 1500 in our experiments for efficiency reasons. As shown in Figure 4a, approximately 20% of users have more than 1500 posts. The effect of the maximum post length is not consistent (Figure 3b), but performance is maximized at nterm = 100. As shown in Figure 4b, approximately 40% of posts are longer than 100 terms.",2,2,2,2,1,0,2,0,2,2,2,2,2,2,0,2,2,0,8,2
https://arxiv.org/abs/1709.01848,8,17,,"Yates, Cohan, Goharian",3,0,0,0,2017,"For users with more than the maximum number of posts npost, a post selection strategy dictates which posts are used as input to the model. Table 5 shows the effect of the post selection strategy on the Reddit dataset’s validation set. Selecting a user’s earliest posts performs the worst regardless of npost’s value, though the differences in F1 are smaller when npost = 400. Randomly selecting posts for each user performs the best across all metrics when npost = 1500, with a large increase in precision over selecting users’ earliest posts and a small increase over choosing users’ latest posts.",2,2,2,2,1,0,2,0,2,2,2,2,2,2,0,2,2,0,8,2
https://arxiv.org/abs/1709.01848,8,18,,"Yates, Cohan, Goharian",3,0,0,0,2017,"In this section we analyze the language that strongly contributed to the identification of depressed users on the Reddit dataset. Unfortunately, it is impossible to show entire Reddit posts without compromising users’ anonymity; we found that even when a post is paraphrased, enough information remains that it can easily be identified using a Web search engine. For example, one Reddit post that strongly contributed to the author’s classification as a depressed user contained the mention of a specific type of abuse and several comments vaguely related to this type of abuse. We attempted to paraphrase this post, but found that any paraphrase containing general language related to both the type of abuse and to the user’s comments was enough to identify the user. Thus, to protect the anonymity of the users in our dataset, we do not publish posts in any form. Rather than publishing posts, we identify key phrases in posts from users who were correctly identified as being depressed. Phrases from eight self-reported depressed users are shown in Table 6; to prevent these phrases from being used to identify users, we retain only the top phrase from each user. These phrases were identified by using the model’s convolutional filter weights to identify posts in the validation dataset that are strongly contributing to the model’s classification decision, and then using the convolutional filter weights to identify the phrase within each post that most strongly contributed to the post’s classification (i.e., had the highest feature values). In keeping with the design of our dataset, terms related to depression or diagnoses are not present. Instead, the model identifies phrases that often could be associated with a negative sentiment or outlook. For example, “my whole” could be part of a negative comment referring to the poster’s whole life. It should be noted that the model makes classification decisions based on the occurrence of phrases across many posts by the same user. Though one can imagine how the phrases shown here could be used to convey negative sentiment, the presence of a single such phrase is not sufficient to cause the model to classify a user as depressed.",2,2,2,2,1,0,2,0,2,2,2,2,2,2,0,2,2,0,8,2
https://arxiv.org/abs/1709.01848,8,19,,"Yates, Cohan, Goharian",3,0,0,0,2017,"In this section we report results on the 2017 CLPsych Workshop’s self-harm risk classification task.7 While CLPsych ’17 featured the same self-harm risk classification task as CLPsych ’16 (§5.3), new test data was used to conduct the evaluation. This provides an opportunity to further evaluate our model on the task of self-harm risk assessment and to conduct an error analysis. The methods were configured and evaluated in the same manner as described in §5.3. 8 Results are shown in Table 7. All methods perform substantially worse than they performed on the CLPsych ’16 test data as measured by nongreen, urgent, and overall F1. The trends across methods remain similar, however, with Categorical Cross Ent. performing the best as measured by non-green and overall F1, and with no statistically significant difference between Class Metric (Ordinal) and the best performing method. Notably, the methods’ flagged F1 scores do not see a similar decrease on the CLPsych ’17 data. This suggests that the decreased performance is being caused by an inability to distinguish between the non-green classes (i.e., amber, red, and crisis). The importance of differentiating between the red and crisis classes increased with the 2017 shared task, because the proportion of crisis labels in the data increased from 0.4% (2016 testing) and 4% (2016 training) to 11% (2017 testing). The methods rarely classify a post as crisis, however, causing an increase in the number of misclassifications on the 2017 testing data. For example, Class Metric (Ordinal) classified only four posts from the 2017 test data as crisis, and it classified no posts from the 2016 test data as crisis. We leave improving the model to better identify crisis posts as future work.",2,2,2,2,1,0,2,0,2,2,2,2,2,2,0,2,2,0,8,2
https://arxiv.org/abs/1709.01848,8,20,,"Yates, Cohan, Goharian",3,0,0,0,2017,"In this work, we argued for the close connection between social media and mental health, and described a neural network architecture for performing self-harm risk classification and depression detection on social media posts. We described the construction of the Reddit Self-reported Depression Diagnosis (RSDD) dataset9 , containing over 9,000 users with self-reported depression diagnoses matched with over 107,000 similar control users; the dataset is available under a data usage agreement. We applied our classification approach to the task of identifying depressed users on this dataset and found that it substantially outperformed strong existing methods in terms of Recall and F1. While these depression detection results are encouraging, the absolute values of the metrics illustrate that this is a challenging task and worthy of further exploration. We also applied our classification approach to the task of estimating the self-harm risk posed by posts on the ReachOut.com mental health support forum, and found that it substantially outperformed strong previously-proposed methods. Our approach and results are significant from several perspectives: they provide a strong approach to identifying posts indicating a risk of self-harm in social media; they demonstrate a means for large scale public mental health studies surrounding the state of depression; and they demonstrate the possibility of sensitive applications in the context of clinical care, where clinicians could be notified if the activities of their patients suggest they are at risk of self-harm. Furthermore, large-scale datasets such as the one presented in this paper can provide complementary information to existing data on mental health which are generally relatively smaller collections.",2,2,2,2,1,0,2,0,2,2,2,2,2,2,0,2,2,0,8,2
https://dl.acm.org/doi/abs/10.1145/2700171.2791023,9,1,,"Burnap, Colombo, Scourfield",3,0,0,0,2015,"The World Wide Web, and online social networks in particular, have increased connectivity between people such that information can spread to millions of people in a matter of minutes. This form of online collective contagion has provided many benefits to society, such as providing reassurance and emergency management in the immediate aftermath of natural disasters. However, it also poses a potential risk to vulnerable Web users who receive this information and could subsequently come to harm. One example of this would be the spread of suicidal ideation in online social networks, about which concerns have been raised. In this paper we report the results of a number of machine classifiers built with the aim of classifying text relating to suicide on Twitter. The classifier distinguishes between the more worrying content, such as suicidal ideation, and other suicide-related topics such as reporting of a suicide, memorial, campaigning and support. It also aims to identify flippant references to suicide. We built a set of baseline classifiers using lexical, structural, emotive and psychological features extracted from Twitter posts. We then improved on the baseline classifiers by building an ensemble classifier using the Rotation Forest algorithm and a Maximum Probability voting classification decision method, based on the outcome of base classifiers. This achieved an F-measure of 0.728 overall (for 7 classes, including suicidal ideation) and 0.69 for the suicidal ideation class. We summarise the results by reflecting on the most significant predictive principle components of the suicidal ideation class to provide insight into the language used on Twitter to express suicidal ideation.",2,2,2,0,0,0,0,1,1,2,2,2,2,2,0,2,2,2,10,0
https://dl.acm.org/doi/abs/10.1145/2700171.2791023,9,2,,"Burnap, Colombo, Scourfield",3,0,0,0,2015,"It is recognised that media reporting about suicide cases has been associated with suicidal behaviour [29] and concerns have been raised about how media communication may have an influence on suicidal ideation and cause a contagion effect between vulnerable subjects [14]. With the advent of open and massively popular social networking and microblogging Web sites, such as Facebook, Tumblr and Twitter (frequently referred to as social media), attention has focussed on how these new modes of communication may become a new, highly interconnected forum for collective communication and, like news media reporting, lead to contagion of suicidal ideation. Social science and medical research have investigated the impact that communication on the topic of suicide via the World Wide Web may have on vulnerable subjects, with particular attention to the younger generation [9]. [2] conducted a qualitative study by interviewing young adults who engage in suicidal behaviours and use websites dedicated to these themes. [31, 5] also conducted online searches for Web resources containing suicide-related terms and describing suicide methods. They presented a qualitative analysis of the resources they discovered and concluded that, although neutral and anti-suicide Web sites occurred most frequently, pro-suicide forums and Web sites encouraging suicidal behaviour were also present and available, suggesting that more prevention plans specifically focused on Web resources are required. Building on this, [19] have reviewed online suicide intervention and prevention literature, concluding that there is a lack of published evidence about online prevention strategies and more attention is required to develop and evaluate online preventative approaches. [33] also studied the impact of Facebook suicide notes on suicidal behaviour, reporting that it was not yet clear to what extent suicide notes on online social media actually induce copycat suicides. They note that suicide and social media effects deserve further evaluation and research. Other studies have focused on the written communication of suicide on the Web via bulletin boards [18], newsgroups [24], chat rooms [4], and web forums [22]. These are mostly qualitative analyses and where quantitative data are used in web-related suicide studies, they tend to rely solely on human classification, which is difficult to implement at scale. Computational methods have only been used in a small number of suicide communication studies. Some studies report a positive correlation between suicide rates and the volume of social media posts that may be related to suicidal ideation and intent [37, 20]. There is also a developing body of literature on the topic of identifying suicidal language on Twitter [15, 35], but very few attempts to use machine classification to automatically identify suicidal language and differentiate between this and other forms of suicide-related communication, such as awareness raising and reporting of suicides. The differentiation is a requirement for the purposes of analysing the characteristics of suicidal ideation on social media. [10, 8] study depression and other emotional states expressed via social media. Suicidal language is likely to include emotive content and possible signs of depression but we do not suggest depression and suicidal ideation are synonymous in this paper. Two very recent papers presented the results of Twitter studies aiming to classify ’risky’ language [1] and levels of ’distress’ [16] - both reporting classification performance that has potential for improvement (around 60-64%). An important step in providing support to suicidal social media users is to understand how suicidal ideation is communicated. Recent studies have shown that people are more likely to seek support from non-professional resources such as social media, rather than risk social stigmatisation by seeking formal treatment [16]. Thus, our study aims to contribute to the literature on understanding communications on the topic of suicide in social media by (i) creating a new human-annotated dataset to help identify features of suicidal ideation, (ii) creating a set of benchmark experimental results for machine learning approaches to the classification of suicidal ideation, and (iii) developing a machine classifier capable of distinguishing between worrying language such as suicidal ideation, and flippant references to suicide, awareness raising about suicide and reports of suicide. This last contribution is especially relevant to quantify actual volumes of worrying language on social media for the purposes of understanding risk to human safety, as opposed to all references to suicide. The research presented in this paper comprises an analysis of data collected from the microblogging website Twitter, the text of which has been classified into one of seven suicide-releated categories by a crowdsourced team of human annotators. We then use a range of machine learning classification methods to identify suicidal ideation in tweets and analyse the predictive features of suicidal ideation to help explain the language used by perceived suicidal social media users.",2,2,2,0,0,0,0,1,1,2,2,2,2,2,0,2,2,2,10,0
https://dl.acm.org/doi/abs/10.1145/2700171.2791023,9,3,,"Burnap, Colombo, Scourfield",3,0,0,0,2015,"The Durkheim project is aiming to mine social media data to identify markers of harmful behaviour1 . The project will study a group of US war veterans who will opt-in to share their Twitter, Facebook and LinkedIn posts over time. There are so far no publicly available results from this study but the group has recently published the results of a suicide prediction task, using text from the clinical notes of US war veterans to identify text-based statistically significant signals of suicidality, with around 60% accuracy [30]. They found clinical notes of people who had died through suicide frequently recorded behaviours indicative of fear, agitiation and delusion. Written text has also been analysed in a number of recent studies that have analysed suicide notes and developed machine classifiers to identify topics and emotions expressed by people who have taken their lives [17, 34, 27, 39, 23, 12]). Many of these papers attempt to classify text at a sentence level, which would suggest short strings much like those that would be posted to social media. However, suicide notes are written by people who have accepted suicide and then go on to harm themselves, whereas the current research is particularly interested in identifying suicidal thinking or ideation prior to self-harm, which may differ from the language used in suicide notes. Additionally, handwritten notes, even at sentence level, are not constrained by string length. Twitter posts are limited to 140 characters, which forces authors to use short, informal language that may differ from the way they would normally express feelings on paper. Finally, social media data are noisy, contain a broad range of topics, and language use varies over time. These features arguably make the task of classifying suicidal ideation more complex than it would be in a discrete recording of presuicide thoughts and feelings in a suicide note. A small number of studies have investigated the communication of suicidal ideation on social media. However, they are mainly focused on a comparison with national death rates. For example, in Korea [37] and the US [20] research has attempted to identify a positive correlation between the frequency of suicide-related posts on social media and the number of recorded suicide cases. Suicide related posts were identified using a set of keywords relating to general concepts such as suicide and depression [37] or relating to suicide methods [20]. [15] analysed the Twitter posts of a person who had recently died through suicide. They studied the posts sent in the twenty-four hours prior to death, finding an increase in positive emotions (though not statistically significant) and a change in focus from the self to others as the time of death approached. As this was only a single person study, and given the fact the person had attempted to make the posts rhyme (thereby perhaps using different language to achieve this), the authors propose larger studies of a wider range of Twitter posts. They used the Linguistic Inquiry and Word Count (LIWC) software to identify parts of speech, emotional words and cognitive processes among other concepts [26]. LIWC was also used in [16] as a sampling technique to identify ’sad’ Twitter posts that were subsequently classified using a machine learning into levels of distress on an ordinal scale, with around 64% accuracy in the best-case. Also studying linguistic features of suicidal ideation, [35] used an online panel of young (early 20s) Twitter users to examine the association between suicide-related tweets and suicidal behaviour. They identified that particular phrases such as ”want to commit suicide” were strongly associated with lifetime suicide attempts, the most powerful predictor of future suicide. They also noted that other phrases that suggest suicidal intent, such as ”want to die”, were less strongly associated. The variation here could suggest the flippant use of such phrases on social media when having a bad day - hence the additional challenges posed to classification of suicidal ideation on social media. Finally, [1] used machine learning to classify ’risky’ and ’non risky’ Tweets, as defined by human annotators, with an accuracy of around 60%. They created word lists to represent a number of topics and emotions related to suicide, finding references to insults, hurt and bullying in the ’risky’ category.",2,2,2,0,0,0,0,1,1,2,2,2,2,2,0,2,2,2,10,0
https://dl.acm.org/doi/abs/10.1145/2700171.2791023,9,4,,"Burnap, Colombo, Scourfield",3,0,0,0,2015,"Rather than manually developing a word list to represent suicidal language, we generated a lexicon of terms by collecting anonymised data from known suicide Web forums, blogs and microblogs, and asking human annotators to identify whether it contained references to suicidal ideation. First we collected user posts from four known Web sites identified by experts in the field [31, 5] as being used to discuss suicidal themes for support and prevention. The selected Web sites either had dedicated sections2 , 3 or are specifically designed for suicidal discussions4 , 5 . Then we collected data from microblogging site Tumblr 6 - specifically, content containing self-classified suicidal ideation (i.e. text posts ’tagged’ with the word ’suicide’). For each of the resulting Web sites we then collected an equal number of 200 posts, retrieved in chronological order, with a total of 800 text posts. These posts, and 1000 posts randomly selected from the Tubmlr sample, were subsequently human annotated using the crowd-sourcing online service Crowdflower7 . To avoid difficulties in the annotation of long pieces of text we discarded posts having a length greater than the five percent longer than the average post length for each of the websites considered. Human annotators were asked to identify content containing suicidal ideation using a binary criteria by answering the question ‘Is this person suicidal?’. We then applied the Term Frequency/Inverse Document frequency (TF.IDF) method to the corpus of annotated documents in order to identify terms that appear frequently in the suicidal ideation class but appear with less frequency in the non-ideation class. This process identifies terms that can be used to distinguish between the two classes. In the TF.IDF process we considered n-grams of 1 to 5 words in length, and ranked the top 500 terms. These terms were further analysed by two experienced suicide researchers to remove terms not specifically related to suicide, as well as duplicate keywords. This resulted in a final list of 62 keywords and phrases that suggested possible suicide intent. Illustrative examples are asleep and never wake, don’t want to exist and kill myself. These search terms were then used to collect data from Twitter via the Twitter Streaming Application Programming Interface (API). Twitter data were collected for a continuous period of six weeks from 1st February 2014 using the suicide-related search terms, resulting in a dataset of over four million posts. In parallel we monitored traditional media over the same period to identify the names of reported suicide cases in England. We then retrieved a second data set from Twitter using the name and surname of the deceased as search keywords. Here, the underlying idea was to collect different types of posts with a connection to suicide other than those more directly expressing suicidal ideation (which was the aim of the first dataset collection). All names were removed from the text before analysis. Following the data collection we produced a random sample of 1000 tweets from both datasets, with 80% of posts from the collection of suicide related search terms, and the remaining from the ‘names’ dataset. The human annotation task was repeated using the same crowdsourcing service. This time human annotators were asked to classify data into either one or more of the six suicide related categories listed below, or into the seventh category representing tweets that cannot be classified into any of them. This coding frame was developed with expert researchers in suicide studies to capture the best representation of how people generally communicate on the topic of suicide. Text annotation can be a subjective task, so to limit the amount of subjectivity we required at least 4 human annotations per tweet as per the convention in related research [36]. CrowdFlower provides an agreement score for each annotated unit, which is based on the majority vote of the trusted workers [21]. Because the crowdsourcing service continues to recruit workers until the task is complete, there is no guarantee that all workers will annotate the same set of units. Therefore we cannot calculate traditional interrater reliability (IRR) scores, such as Krippendorf’s Alpha or Cohen’s Kappa to determine agreement between all annotators. However, CrowdFlower has been shown to produce an agreement score that compares well to these classic measures [21]. Based on the output from our annotator task we can determine agreement on each unit. The purpose of the experiments performed in this paper are to establish the accuracy of a machine classifier when assigning tweets to a particular class of suicidal communication, and thus it is the agreement score for the unit of analysis (each tweet), and not the overall human agreement for all units that is important for validation. We removed all tweets with less than 75% agreement - again, following established methods from related research [36], and discarded any where less than three out of four annotators (75%) agreed on the dominant class for each tweet. Annotators were allowed to select multiple class labels and the majority choice was taken. The distribution of tweets to classes from c1-c7 is shown in Table 1. Note that the dominant class was flippant and improper use of suicide-related phrases and expressions, with actual suicidal intent or thinking being in the minority (about 13% of the total). The fact that four people unknown to each other and without being influenced by each other’s annotations could agree to this level would suggest that it is possible for human annotators to agree on what constitutes the language of suicidal ideation, and what is simply a flippant reference to suicide. The resulting dataset of 816 Tweets was subsequently used to train a machine learning classifier (details are provided in the next section), which is only slightly below the dataset sizes of other similar analyses of emotive content on social media (e.g. [25, 36, 3, 7]).",2,2,2,0,0,0,0,1,1,2,2,2,2,2,0,2,2,2,10,0
https://dl.acm.org/doi/abs/10.1145/2700171.2791023,9,5,,"Burnap, Colombo, Scourfield",3,0,0,0,2015,"We used the text of the tweets in order to train and test a number of machine classifiers to identify suicidal ideation and differentiate between this and other types of suiciderelated communication, including flippant references to suicide. Three features sets were derived from the text as follows: • Features representing lexical characteristics of the sentences used, such as the Parts of Speech (POS), and other language structural features, such as the most frequently used words and phrases. These are standard features used in most text mining tasks. References to self and others are also captured with POS – these terms have been identified in previous research as being evident within suicidal communication; • Features representing sentiment, affective and emotional features and levels of the terms used within the text. These were incorporated because of the particularly emotive nature of the task. Emotions such as fear, anger and general aggressiveness are particularly prominent in suicidal communication [1] • Features representing ideosyncratic language expressed in short, informal text such as social media posts within a limited number of characters. These were extracted from the annotated Tumblr posts we collected to try and incorporate the language used on social media that may not be identified using standard text mining features.",2,2,2,0,0,0,0,1,1,2,2,2,2,2,0,2,2,2,10,0
https://dl.acm.org/doi/abs/10.1145/2700171.2791023,9,6,,"Burnap, Colombo, Scourfield",3,0,0,0,2015,"For the first set of features, and part of the second set, we derived features used in [34], published within the special issue on sentiment analysis of suicide notes [28]. We will refer to this set of features as Set1. More specifically, the Set1 feature set included the following: • Parts of Speech. We used to the Stanford Part-OfSpeech (POS) Tagger8 to assign each word in a Tweet a POS label. Examples are nouns (broken down into singular, plural, proper), verbs (specifying tenses such as present, past and present participle), 1st vs 3rd person references, adjective and adverbs (comparative, superlative), pronouns (personal, possessive), as well as other tags representing conjunctions, determiners, cardinal numbers, symbols, and interjections. For each of POS we considered the frequency of each in a Tweet as a feature. • Other Structural Features. For this we considered the inclusion of negations in the sentence (total number), the specific use of a first person pronoun (either singular or plural), and external communication features such as the inclusion of a URL in a tweet or a mention symbol (indicating a retweet or reply). • General Lexical Domains. These features represent general lexical categories such as home, religion, psychology, sociology, etc. These were extracted using WordNet Domains labels,9 Affective Lexical Domains. These are a set of categories specifically related to domains representing ’affective’ concepts. These include concepts representing moods, situations eliciting emotions, or emotional responses such as joy, anger, grief, sadness, enthusiasm, surprise, love, hate, and happiness; but even more specific sub-categories such as amicability, belligerence, bad-temper, unrest, and trepidation; and opposites such as positive-negative concern, negative fear, positive-negative suspense, self-esteem, self consciousness, self-pity, and self-deprecation. These are very appropriate for the specific language we are investigating in this study. • Sentiment Score. Using SentiWordNet10 each words is assigned a score between zero and one for both positivity and negativity. The sum all words in a Tweet were used as features. • Words. The most frequently used words and n-grams in terms of (first 100) unigrams, bigrams and trigrams contained in the training set. • Keyword list. We also included each of the 62 keywords derived from the Web form text that were used for the pre-filtering search (e.g. ‘asleep and never wake’, ‘don’t want to try anymore’, ‘end it all’, ‘isn’t worth living’, ‘my life is pointless’, ‘kill myself’, ‘to live any more”, ‘want to end it’, ‘want to disappear’, ‘want to die’, etc..). Each of the search terms were included as individual features together with one global binary feature representing the inclusion of any of them in a Tweet.",2,2,2,0,0,0,0,1,1,2,2,2,2,2,0,2,2,2,10,0
https://dl.acm.org/doi/abs/10.1145/2700171.2791023,9,7,,"Burnap, Colombo, Scourfield",3,0,0,0,2015,"Given the psychological and emotional expressiveness of suicidal ideation, we then explored a second set of features by using the Linguistic Inquiry and Word Count LIWC text analysis software [26] to extract more specific labels representing affective emotions and feelings within the text. We refer to these features as Set2. These include a more extensive breakdown of categories that may be more suitable for the particular language of emotional distress that we would expect to be present in suicidal ideation. Examples are related to death, health, money, religion, occupation, and achievement, senses (e.g. feeling, hearing, seeing), and three other groups of terms related to ‘cognitive mechanisms’, ‘affect’, and ‘social words’. These can be further broken down into labels representing family, friends, humans; anxiety, anger, sadness and positive and negative emotions; and terms related to certainty, inhibition, insight, causal, inclusivity and exclusivity. A subset of these features (sadness) were used in [16], but we have incorporated a wider range of the feature set to enable us to distinguish between distress and other forms of suicide-related communication (e.g. grief, support and reporting).",2,2,2,0,0,0,0,1,1,2,2,2,2,2,0,2,2,2,10,0
https://dl.acm.org/doi/abs/10.1145/2700171.2791023,9,8,,"Burnap, Colombo, Scourfield",3,0,0,0,2015,"Next, due to the noisy nature of social media, where short, informal spelling and grammar are often used, we developed a set of regular expression (RegEx) and pattern matching rules from our collection of suicide-related posts collected from social networking website Tumblr. We refer to these features as Set3. These were annotated as part of the human annotation process conducted earlier and introduce language from short informal text related to the six suicide related categories to assist the classifier. Examples of these expressions for each class (numbered 1-6 here) include: 1 : ‘.+((\cutting |\depres|\sui)|\these|\bad|\sad).+ (\thoughts| \feel).+’ to represent phrases such as ‘suicidal / cutting / bad / these . . . thoughts / feelings’; ‘.+\wan\w.+d[ie].+’ for expressions as ‘want/wanted/wanting to die’; ‘.+\end.+ (\all|\it|\life).+’ for sentences with ‘end/ending it all’ and ‘end my life’; and ‘.+ (can.+|don.+|\take).+(\go|\live|\anymo| \cop|\alive).+’ covering a wide range of phases including ‘can’t take anymore’, ‘can’t/don’t want to live/cope anymore’, ‘don’t want to be alive’, ‘can’t take it anymore’, and ‘can’t go on’. In addition, we added a list of individual words and n-grams including ‘trigger warning’, ‘tw’, ‘eating disorder’, ‘death’, ‘selfharm’ and ‘self harm’, ‘anxiety’, and ‘pain’. 2 : ‘.+(\need|\ask|\call|\offer).+\help.+’ related to phrases as ‘call/offer for/of help’ and individual terms as ‘shut’ (e.g. website shut down) and ‘stop’ (e.g. bullying). 3 : ‘.+(\kill\hat\throw)’ for phrases including ‘kill/killing /hate myself’, ‘.+(\f**k.+’ for swearwords such as ‘f**k/ f**king’, ‘.+ (\boy\girl).+(\friend)’ for expressions with ‘boyfriend’ and ‘girlfriend’, and ‘.+(\ just)\.+(\like).+’ covering expression including ‘just’ . . . like’. In addition, some words related to general topics such as ‘work’ and ‘school’ have also been included since they are representing contexts more favourable to flippant language rather than genuine expression of distress and suicidal intent. 4 : ‘.+(\talk|\speak).+\to.+(\one|\some|\any).+’ related to phr-ases as ‘talk / speak to someone/somebody’ and words such as ‘web’, ‘blog’, ‘health’ , and ‘advice’. 5 : ‘.+miss.+(\you|\her|\him).+’ related to phrases such as ‘miss/missing you/her/him’ and ‘.+(\kill|\die|\comm).+(day| month|year).+’ to represent specific time references. 6 : ‘.+(\took|\take).+\own.+\life.+’ covering expressions including ‘took/taken his/her own life’ and words related to suicide methods such as ‘hanged’, ‘hanging’ and ‘overdose’. Note that the regular expressions included in the third class representing flippancy were also identified within those related to the first suicidal class (and vice versa). However, we decided to associate RegExs to only one of the two classes according to the nature of the annotated tweets, for example phrases as ‘hate myself’ or ‘kill myself’ were frequently associated with flippant posts whereas terms such as ‘wanted to die’ and ‘want to end it’ were more likely to be included in tweets containing evidence of suicidal thinking.",2,2,2,0,0,0,0,1,1,2,2,2,2,2,0,2,2,2,10,0
https://dl.acm.org/doi/abs/10.1145/2700171.2791023,9,9,,"Burnap, Colombo, Scourfield",3,0,0,0,2015,"We built a fourth feature set that we will refer to as the combined set, incorporating the union of all of the features in the three previous groups. Given the large number of features associated with each tweet, and potential for colinearity between features in the combined set, we applied Principal Component Analysis (PCA) as a dimension reduction procedure to convert the set of all possibly correlated variables within the combined set into a new set of linearly uncorrelated features (called principal components). The text of the tweets was also incorporated as a feature set for all experiments. We transformed each Tweet into a word vector using ngrams of size 1 to 5, and retained between 100 and 2000 words (in increments of 100, 300, 500, 1000, 1500 and 2000). The optimum performance was 1-3grams with 500 words retained, and we only present these results in this paper.",2,2,2,0,0,0,0,1,1,2,2,2,2,2,0,2,2,2,10,0
https://dl.acm.org/doi/abs/10.1145/2700171.2791023,9,10,,"Burnap, Colombo, Scourfield",3,0,0,0,2015,"We first conducted baseline experiments using the Weka machine learning libraries11. We used the four derived features sets with the most popular classifiers from the special issue on classification of suicidal topics in [27]. These were Support Vector Machine (SVM), Rule Based (we used Decision Trees (DT)), and Naive Bayes (NB). Support Vector Machines (SVM) have been shown to work very well with short informal text [25, 38], including promising results when classifying other mental health issues [11]. Feature vectors are plotted in high-dimensional space and hyperplanes (lines that separate the data points) are used to try and find the optimum way to divide the space such that the data points belonging to the different human assigned classes are separated. Multiple hyperplanes can be used and the optimal hyperplane will be the line that maximizes the separation between classes. Rule-based approaches are able to iteratively identify the feature from a set of training data that maximises information gain in a classification exercise - or put another way, it quantifies the significance of how using one feature as a rule to classify a tweet as suicidal ideation, reduces the uncertainty as to which class it belongs to. Performing this step multiple times creates a hierarchical and incremental set of rules that can be used to make classification decisions. We used a J48 decision tree (C4.5) to perform rule-based experiments. Finally, given the prevalence of individual words or short combinations of words that would be associated with suicidal ideation, it is logical to incorporate probabilistic classifiers into the experiments as they make classification decisions based on the likelihood of feature occurrence. Specific terms and phrases prevalent in each class can be identified and learned by the classifier. We implemented a Naive Bayes algorithm as a probabilistic approach.",2,2,2,0,0,0,0,1,1,2,2,2,2,2,0,2,2,2,10,0
https://dl.acm.org/doi/abs/10.1145/2700171.2791023,9,11,,"Burnap, Colombo, Scourfield",3,0,0,0,2015,"The individual baseline experiments produced a set of results that achieved a reasonable performance but clearly required refining (see Table 2). This could suggest that the sample was not large enough to allow the classifier to learn a suitable set of predictive features. It could also suggest the features themselves were either not adequate to represent the latent meaning that human annotators identified when assigning each tweet to a class, or the features were not being suitably utilised during the learning phase. Both sample size and feature set limitations led us to incorporate an ensemble classification approach, which enabled us to combine the base classifiers and different methods of feature sampling during the learning phase. There are two very popular ensemble approaches. One is Boosting [13] (e.g. AdaBoost), which aims to ’boost’ the performance of a classifier by iteratively adding a new classifier to the ensemble where each new classifier is trained on data for which the previous iteration performed poorly. An advantage of this is that, for smaller samples, the more difficult to classify instances can be focussed on to improve classifier performance. However, this approach has also been reported to reduce classifier accuracy by forcing new classifiers to focus on difficult data points at the sacrifice of other data. The second popular method is Bagging [6], which takes a bootstrap sample of data points and trains a classifier on each sample, averaging out the probabilities for each class across all classifiers in the ensemble. In [32] the authors propose an ensemble approach known as Rotation Forest (RF), which splits the feature set into a number of smaller sets before sampling from each set and running Principal Component Analysis (PCA) on each subset, creating a number of different principal components for each subset of features, and subsequently building a number of classifiers using these. This approach showed a performance improvement over Bagging and Boosting and provided a logical choice of method to refine our baseline classifiers, given the 1444 features all measuring properties of the text, possible colinearity between features, and variance of features in the training data. We hypothesised that splitting the features into a number of subsets and deriving a range of principal components from these, rather than deriving principal components from all features at once, would reduce the number of false negative results by using a wider range of principal components. We therefore repeated the experiments from the baseline phase with a RF ensemble classifier. Ensemble meta classifiers can incorporate a number of combined baseline classifiers. We experimented with incorporating all the classifiers used in the baseline experiments to determine how the principles of RF could improve these. As the initial results showed varying performance between classifiers - for example, the NB produced the lowest numbers of false positives using Set1 and Set3, but SVM produced the lowest false negatives in both cases - we chose to incorporate a second metaclassifier within the RF that used a voting principle as a mechanism to assign the label with maximum probability across all base classifiers to new instances. SVM, J48 Decision Tree and Naive Bayes classifiers were integrated within the RF classifier as an ensemble, with the classifier producing the maximum probability for new instances being selected for each classification decision. We ran two experiments with the RF approach - one with all three baseline classifiers and another with just NB and SVM classifiers. Table 3 shows the notable difference in performance when using DT to classify suicidal ideation, thus it was dropped and the ensemble approach performed much better. We have only reported the results of the NB and SVM combination.",2,2,2,0,0,0,0,1,1,2,2,2,2,2,0,2,2,2,10,0
https://dl.acm.org/doi/abs/10.1145/2700171.2791023,9,12,,"Burnap, Colombo, Scourfield",3,0,0,0,2015,"We used a 10-fold cross validation approach in the evaluation of our classification experiments. This approach iteratively trains the classifier on 90% of the training data and tests on the remaining 10%. After 10 iterations, the results are calculated by taking the mean accuracy across all models. The results are provided in this section at two levels. Tables 2 and 3 present the results for each of the baseline classifiers - Naive Bayes (NB), J48 Decision Tree (DT), and Support Vector Machine (SVM). Each row represents the results using a different set of features. The final column in the table provides the results of the Rotation Forest (RF) ensemble classifier. Table 2 provides the weighted average results across all classes, while Table 3 provides the results of the key class of interest - suicidal ideation. Evaluation followed standard classification measures of Precision measuring false positives, Recall measuring false negatives, and F-measure a harmonized mean. In the Tables we represent the best scores in bold, and the best precision and recall for each feature set in italic. The three baseline models perform similarly across all classes for feature set 1,2 and 3, with SVM slightly outperforming NB in most cases, and DT performing least well (see Table 2). In two out of 3 cases NB achieved the best precision score and SVM the best recall in all three - leading us to test an ensemble approach. It is interesting to note that combining all feature sets led to only a 0.001 improvement in precision and actually reduced recall by 0.07 when compared to Set 2. Furthermore, applying a dimension reduction method - principle component analysis - led to a further reduction in performance when applied to all features (see bottom three rows of Tables 2 and 3). However, when the training data was split into smaller samples, with principle components derived for each sample - thus broadening the diversity of components while retaining complexity - we saw a performance increase, going from a maximum performance of P=0.695 and R=0.689 to P=0.732 and R=0.729 across all classes when applying the RF approach combined with a Maximum Likelihood voting metaclassifier. When digging deeper into the key class of interest - the suicidal ideation class - we see a reduced performance for all base classifiers (see Table 3). The confusion matrix for the best performing classification model (see Table 4) shows that this is largely due to confusion between c1 (suicidal ideation) and c3 (flippant reference to suicide). This was always going to be a challenge given the subjective nature of the task and the difficulty human annotators found in agreeing on this. Sarcasm and irony are notable text classification challenges that are yet to be resolved. This is primarily due to the same language often being used in serious and flippant cases. However, the SVM baseline classifier still achieved a Precision performance of 0.657, which was in fact the best performance - even better than the RF classifier. Indeed, the baseline SVM generally outperformed the other base classifiers, and the RF ensemble, for the individual sets of features. This is in line with other existing research in this area, though we have achieved a higher performance. Yet when combining all features, and applying principle component analysis to smaller subsets of training data, the RF model performed significantly better than any other classification model for the suicidal ideation class. The maximum Recall was 0.744, which is only a slight improvement of 0.013 over the NB baseline using Set 1, but the maximum F-measure was 0.69 as compared to 0.61. These results suggest that the ensemble of multiple base classifiers with a maximum probability meta classifier offers a promising way forward for the multi-class classification of suicidal communication and ideation in ’noisy’ short informal text, such as social media posts. The ’none of the above’ confusion also suggests there may be other latent topics not present in our set of class labels. Identifying these may be a useful task for future research. Table 5 provides P, R and F results for the best performing classifier across all classes for comparison.",2,2,2,0,0,0,0,1,1,2,2,2,2,2,0,2,2,2,10,0
https://dl.acm.org/doi/abs/10.1145/2700171.2791023,9,13,,"Burnap, Colombo, Scourfield",3,0,0,0,2015,"In this section we analyse the components produced by running the Principle Component Analysis (PCA) method on the combined set that resulted in the best set of results, as shown in Tables 2 to 5. The application of PCA reduced the features set from 1444 to 255 attributes in terms of main components. For the seven suicide related classes we show in Tables 6 and 7 the most representative principal components and briefly discuss what each class represents in terms of the features in the component and the particular language used in it. Note that while the distribution of the components per class mirrors the total number of annotation per class (therefore penalising the classes less represented in our data set such as ‘memorials’) in Tables 6, 7 and in the related discussion we are giving priority to the most representative class of posts containing evidence of possible suicidal intent. We can observe the following characteristics of the features included for each class component: c1: Many of the features that appear dominant in the suicidal ideation class are those related to phrases and expressions identified in the suicide literature as being significantly associated within the language of suicide. In particular, beside a limited number of uni/bi/tri-grams generated directly from the training set, the terms derived from a number of suicide related Web sites were fundamental in classifying suicidal ideation in Twitter data. As were the regular expression features derived from Tumblr posts. Examples like ‘end it all now’ and ‘want to be dead’ and regex including expression of ‘depressive/suicidal/self harming’ ...‘thoughts /feelings’ appear strongly related to suicidal ideation and are clearly discriminating for this specific class. Other terms (such as ‘killing myself’ and the regex containing ‘die’ ... ‘my sleep’) become effective for classification when used besides other attributes such as lexical features that express surprise, exaggeration and emphasis (e.g. adverbs (‘really’), predeterminers (e.g. ‘such’ ‘rather’)), and words mapped to specific ‘affective’ domains such as ‘alarm’ and ‘misery’. Note that some other concepts and terms appear with a negative correlation as expressions of opposite affective states, such as ‘security’ and ‘admiration’. c2: For the class representing campaigning and petitions we can observe more general concepts, again expressed by regular expressions and language clues (word-lists in our terminology), such as ‘support/help’, ’blog’ as well as more specific terms (e.g ‘safety plea’) and expressions (‘put an end to this’). Some of the Wordnet domain features require further examination as they appear confusing at first - for example ’racing’ is picking up on the words ’run’ and ’running’ that are related to campaigns. c3: As the confusion matrix in Table 4 shows, the class concerning a ‘flippant’ use of suicidal language is the one presenting the major difficulties in classification, since it includes many of the same linguistic features of suicidal ideation. However, the principal components derived for this class identify certain attributes that are the opposite type of sentiment from emotional distress. These include affective states such as ‘levity’, ‘gaiety’, ‘jollity’, and ‘cheerfulness’, as well as popular conversational topics, such as casual remarks about the weather. The confusion occurs where phrases such as ‘kill myself’ are used frivolously. c4: The class representing posts related to information and support (and prevention) appear mostly represented by specific words (often unigrams and ‘tags’) directly linked to the support services (e.g. #police, #officers, internet and suicide) and/or topicality (such as sexual references (‘#lgtb), and the domains of self-harm and #suicide). c5: For the class concerning memorial messages, as may be expected, direct mentions of the name of the deceased appear highly influential as well as ‘time’ references (e.g. ’a month ago’, ’a year since’) in association with terms such as ‘killed’ and ’died’ (well captured by one of our regular expressions). In addition labels and tags as ‘rip’ and terms expressing ‘love’ ‘and ‘affection’ are also part of the components associated with this class. Again, we see some Wordnet domains appearing - ’mathematics’ and ’agriculture’ are related to specific words such as ’add’ and ’grow’. c6: The class concerning news reports related to suicide presents features such as words representing sources of information (e.g. #bbc news), types of news (research study or statistical report), and direct mentions of the name of the deceased (as well as general concepts related to the particular case, such as in the one here reported of the ‘TV’ domain). Note that the last three classes of memorial, information/support, and news reporting all share the common characteristics of including URL links within the tweets which, consequently, does not result in an effective feature for discrimination between these different classes. c7: Finally, the class of posts annotated as not related to any of the previous classes exhibits attributes such as general phrases related to self doubt (such as ‘what’s wrong with me and ‘hate myself’) and emotional states (such as ‘jitteriness’ and ‘admiration’). These are phrases that could appear in tweets relating to emotional distress but are also clearly evident in general everyday ’chatter’.",2,2,2,0,0,0,0,1,1,2,2,2,2,2,0,2,2,2,10,0
https://dl.acm.org/doi/abs/10.1145/2700171.2791023,9,14,,"Burnap, Colombo, Scourfield",3,0,0,0,2015,"In this paper we developed a number of machine classification models built with the aim of classifying text relating to communications around suicide on Twitter. The classifier distinguishes between the more worrying content, such as suicidal ideation, and other suicide-related topics such as reporting of a suicide, memorial, campaigning and support. We built a set of baseline classifiers using lexical, structural, emotive and psychological features extracted from Twitter posts. We then improved on the baseline classifiers by building an ensemble classifier using the Rotation Forest algorithm, achieving an F-measure of 0.728 overall (for 7 classes, including suicidal ideation) and 0.69 for the suicidal ideation class. We summarised and attempted to explain the results by reflecting on the most significant predictive principle components of each class to provide insight into the language used on Twitter around suicide-related communication. From this analysis we observed that word-lists and regular expressions (regex) extracted from online suicide-related discussion fora and other microblogging Web sites appear capable of capturing relevant language ‘clues’, both in terms of single words, n-grams (word-lists) and more complex patterns. These appear particularly effective for the suicidal ideation class, expressing emotional distress. Lexical and grammar features such as POSs appear mostly ineffective and scarcely present in the principal components (only some mentions as predeterminers, existential clauses and superlatives that, however, also relate to more specific ‘affective’ language features than only pure lexical ones). Affective lexical domains, appear instead very relevant (such as those represented by the WordNet library of ‘cognitive synonyms’) and able to well represent the affective and emotional states associated to this particular type of language. Concepts and labels representing broader semantic domains (also derived form the WordNet library) are, on the contrary, not effective. In fact, although they appear rather numerous as attributes within the principle components they reveal to be, on close inspection, for the majority of cases irrelevant and mostly generated by a ‘confusion’ and ‘misrepresentation’ of words (such as sentences like ‘my reason crashed’ associated to the ‘motor-racing’ domain, and ‘suicide watch’ associated to ‘numismatic’). Sentiment Scores generated by software tools for sentiment analysis appear also ineffective and either scarcely or not at all included within the principal features of each class. Note that this is true for both basic tools that only provide a binary representation of positive and negative score values (SentiWordNet) as well as more sophisticated text analysis software that generate sentiment scores over a larger range of labels representing emotional states (LIWC). A classifier for suicide-related language could potentially make an important contribution to suicide prevention. Monitoring individual social media accounts via keywords that suggest possible suicidal ideation is controversial territory, as shown by the recent withdrawal of the Samaritans Radar app in the UK 12 but there is nonetheless potential for such a lexicon to contribute to prevention in some way, as long as acceptability to social media users is thoroughly investigated. The ’real-time’ identification of aggregate levels of suicide-related communication at scale in online social networks, which could be facilitated by the ensemble classifier produced in this research, is one possible approach. This could potentially aid the identification of emerging suicide clusters and the concentration of suicidal communication following particular events such as a celebrity suicide. Our classifier goes beyond the recognition of suicidal language insofar as it also aids identification of other kinds of communication, in recognition that social media platforms can be used for multiple purposes, including the reporting of news and marshalling of campaigns. Monitoring of suicide news reporting in social media is another potential avenue where text mining and machine classification techniques could be applied. The identification of flippant use of suicidal language could be especially useful. The methods needs further development, ideally with a larger sample of social media postings, and application to platforms other than Twitter. Finally, we note that it is important to retain collaboration with domain experts in suicidology throughout the experimental and interpretation phases of future research to improve classification accuracy by incorporating prior knowledge of the characteristics of suicidal language - especially given the significance of the affective features in this paper.",2,2,2,0,0,0,0,1,1,2,2,2,2,2,0,2,2,2,10,0
https://ojs.aaai.org/index.php/ICWSM/article/view/14891,10,1,abstract,"De Choudhury, Kiciman ",2,1,0,0,2017,"Online social support is known to play a significant role in mental well-being. However, current research is limited in its ability to quantify this link. Challenges exist due to the paucity of longitudinal, pre- and post mental illness risk data, and reliable methods that can examine causality between past availability of support and future risk. In this paper, we propose a method to measure how the language of comments in Reddit mental health communities influences risk to suicidal ideation in the future. Incorporating human assessments in a stratified propensity score analysis based framework, we identify comparable subpopulations of individuals and measure the effect of online social support language. We interpret these linguistic cues with an established theoretical model of social support, and find that esteem and network support play a more prominent role in reducing forthcoming risk. We discuss the implications of our work for designing tools that can improve support provisions in online communities",2,2,2,1,0,0,2,0,2,2,2,2,0,0,0,0,2,1,3,2
https://ojs.aaai.org/index.php/ICWSM/article/view/14891,10,2,introduction,"De Choudhury, Kiciman ",2,1,0,0,2017,"Social support is an important ingredient in the attainment of improved mental well-being (Kaplan, Cassel, and Gore 1977; Leavy 1983; Billings and Moos 1984; Stroebe and Stroebe 1996). It is reported that supportive interactions can have a “buffering effect” (Cohen and Hoberman 1983); that is, they can be protective against the negative consequences of mental health. Consequently, social support is recognized to be a significant psychosocial coping resource. It, therefore, bears profound implications for health provisions and interventions that attempt to strengthen aspects of support networks for individuals prone to suicidal thoughts (Cohen, Underwood, and Gottlieb 2000). With the pervasive adoption of social media platforms, online communities have emerged to be a prime mechanism through which individuals with well-being challenges seek and obtain help, advice, and support (Huh and Ackerman 2012). Perceived support in these communities has been found to be linked to improved self-efficacy and wellbeing (Fox and Jones 2009), including facilitating recovery from health challenges (Newman et al. 2011), as well as in fostering positive behavior change (Munson et al. 2010) Despite ample evidence given by cross-sectional studies of the positive therapeutic role of online support (Rappaport 1993), researchers recognize that necessary and adequate empirical data has yet to be accumulated to substantiate this claim (Rudd 1993). Consequently, establishing a causal relationship between availability of online social support and mental health outcomes can be challenging. This is because most studies apply retrospective procedures such as regression and classification models, wherein it is difficult to identify comparable subpopulations of users who have the same set of initial symptoms. It is also plausible that certain individual traits are associated with both the access to social support and the occurrence of at-risk tendencies. Since these symptoms and traits are often the best predictors of well-being risk like suicidal ideation (Cohen and Hoberman 1983), lack of knowledge of baseline conditions can have confounding effects in interpreting the link between social support and at-risk states. A further threat to validity in cross-sectional studies of online social support is the potential bias in the retrospective measurement of social support among distressed individuals. The challenges are compounded by the difficulty in gathering a pre-morbid (or pre-risk) group of individuals who participate in these online communities. Finally, qualitative and observational studies of support also often fail to reveal conditions under which specific types of support can be nonbeneficial or even harmful (Thoits 1982). Our work seeks to address these methodological gaps in assessing the role of online social support in future risk to suicidal ideation. To do so, we examine publicly shared longitudinal post and comment data on a number of prominent mental health communities in the social media Reddit. We then identify individuals in this data who proceed to post on a Reddit suicide support forum at a later point in time. Due to the semi-anonymous nature of these communities (De Choudhury and De 2014), the content shared by individuals allows us to obtain high quality, self-reported data around mental health concerns and suicidal ideation, including data that precedes and succeeds expression of suicidal ideation in individuals. Utilizing comments received on posts in these communities as a proxy for social support, we develop a human machine hybrid statistical methodology that models and quantifies the effects of the language of these comments in individuals who do and do not post on the suicide support forum. Applying stratified propensity score matching (Rosenbaum and Rubin 1983) in a iterative fashion, we first identify linguistic features of comments that show significant effects. We obtain human assessments on the presence of suicidal ideation risk markers in posts associated with these features. Then we filter the features that correspond to comparable subpopulations. Finally, we include these assessments in computing local average treatment effect, so as to assess the effects of specific linguistic features of comments in future risk to suicidal ideation. Findings. Our findings show that not all individuals posting in the Reddit mental health communities are equally likely to be influenced by support received through comments. Those who benefit from online social support tend to show greater social and futuristic orientation, interpersonal awareness, and lower cognitive impairment. We find these observations to align with observations in the psychology literature on suicidal ideation and support (Kaplan, Cassel, and Gore 1977). Next, we qualitatively interpret the context of use of the linguistic features of the comments, that our stratified matching approach identifies to have significant effects on future risk. To do so, we adopt the social support behavioral code framework of characterizing social support (Cutrona and Suhr 1992). We find that linguistic features used to provide esteem or network support tend to reduce one’s risk to suicidal ideation in the future. Somewhat surprisingly, features associated with interpersonal acknowledgments tend to be significantly counter-beneficial. We describe how our method and findings can provide insights into the positive and negative impacts of online commentary on future mental well-being. We also discuss design and ethical implications of our work in building novel technologies for moderators and volunteers, that seek to improve social support provisions in online communities",2,2,2,1,0,0,2,0,2,2,2,2,0,0,0,0,2,1,3,2
https://ojs.aaai.org/index.php/ICWSM/article/view/14891,10,3,related work,"De Choudhury, Kiciman ",2,1,0,0,2017,"In the context of mental health, seminal work by Kaplan et al. (Kaplan, Cassel, and Gore 1977) defined social support as “the degree to which an individual’s needs for affection, approval, belonging, and security are met by significant others”. The study of social support parameters is identified to be a major investigatory tool for examining psychosocial influences upon health and disease (Kaplan, Cassel, and Gore 1977). Rappaport (Rappaport 1993) suggested that, in contrast to psychotherapy, socially shared stories form a kind of group narrative that constitutes a social identity and an avenue for social learning, growth, and behavior change. Cohen and Wills posited that social support “buffers” people from the potentially pathogenic influence of stressful events (Cohen and Hoberman 1983). Recognizing the role of social support in improved health outcomes, social scientists have developed a helpful categorization schema Social Support Behavioral Code (Cutrona and Suhr 1992). This schema was developed by evaluating the frequency of occurrence of 23 communication behaviors intended to be supportive in five categories: informational support (providing information or advice), instrumental support (expressing willingness to help in a tangible way or actually do so), esteem support (communicating respect and confidence in abilities by acts such as complimenting one), network support (communicating belonging to a group of people with similar experiences), and emotional support (communicating concern, or empathy). We adopt this characterization model of social support in our work. However, efforts to assess the validity and reliability of social support indicators are noted to be lacking in the literature (Thoits 1982). Understanding how specific individuals respond to specific support mechanisms can enable developing tailored ways that improve one’s psychological adjustment, efficacy, as well as resistance to and recovery from illness (Burleson et al. 2002). Our work attempts to explore these individual-level differences by studying online mental health support communities and understanding how linguistic attributes as well as types of support enabled through commentary, may relate to future risk or distress. Health Efficacy and Online Support. A rich body of work has examined the important role played by online communities in enabling individuals elicit and provide social support around a variety of health and well-being challenges, ranging from cancer to diabetes (Coulson 2005; Wang, Kraut, and Levine 2012; De Choudhury and De 2014; Andalibi et al. 2016). Online communities have been identified to be powerful platforms where disease specific guidance and feedback, emotional support, coping and management strategies may be sought (Greene et al. 2011; Newman et al. 2011). In the realm of mental health, a recent meta-analysis indicates that online support is effective in decreasing depression and increasing self-efficacy and quality of life (Rains and Young 2009). For instance, Oh et al. (Oh et al. 2013) surveyed Facebook users to find that a positive relationship existed between having health concerns and seeking health related social support. Andalibi et al. (Andalibi et al. studied how individuals with experience of sexual abuse sought support in different online communities on Reddit for emotional wellness. Our work extends these investigations by examining to what extent we can quantitatively discover links between support and risk to suicidal ideation in different mental health Reddit communities. Causality. The above research provides valuable insights into whether and how online support relates to and can potentially help improve one’s health and well-being. However, we note that significant gaps exist in being able to quantify and measure their influence in future health outcomes. To address this challenge, Winzelberg et al. (Winzelberg et al 2003) and Lieberman et al. (Lieberman et al. 2003), both conducted clinical trials wherein they assessed the effectiveness of online support communities for individuals with breast cancer. Following a 12 week study comparing two groups with and without access to an online support group BosomBuddies, Winzelberg et al. (Winzelberg et al. 2003) reported that the former groups showed reduced depression, perceived stress, and cancer-related trauma. These methods, however, cannot be adopted in naturalistic settings; e.g., in understanding the role of online social support in future health outcomes, based on historical observational data. Moreover, participation in online support communities is inherently self-selected in nature. This can lead to confounding effects if the effectiveness of support is measured through traditional statistical approaches like regression models (Cohen and Wills 1985). We seek to close this gap by borrowing methods from the causal inference literature, in not only examining whether social support can help or exacerbate risk, but also what specific attributes of this support are likely to be less or more beneficial to specific subpopulations.",2,2,2,1,0,0,2,0,2,2,2,2,0,0,0,0,2,1,3,2
https://ojs.aaai.org/index.php/ICWSM/article/view/14891,10,4,data,"De Choudhury, Kiciman ",2,1,0,0,2017,"Identifying At-Risk Individuals. As a starting point of our data collection, we obtained access to a Reddit dataset of mental health posts from De Choudhury et al. (De Choudhury et al 2016). This dataset included 79,833 posts from 44,262 unique users, that were shared between February 11 and November 11 2014 on 14 mental health subreddits (henceforth referred to as MH) and a prominent suicide support community on Reddit (r/SuicideWatch, henceforth referred to as SW). Example mental health subreddits include r/depression, r/mentalhealth, r/bipolarreddit, r/ptsd, r/psychoticreddit. All of these subreddits host public content, and have been examined and validated by mental health experts in prior work to be communities of mental health and suicidal ideation support (De Choudhury and De 2014). Following the method developed in (De Choudhury et al. 2016), we constructed two user classes (Table 1). We first seek to identify a set of Reddit users who initially (say time period post about mental health concerns (in the mental health subreddits, or MH) only, and would later (say time period be observed to post about suicidal ideation in the SuicideWatch community (or SW). We also identify a second set of users for whom we would not have any observation of posting in SW in or, despite their postings on the MH subreddits in. For the purposes of our investigation, we consider to span from Feb 11 2014 to Aug 11 2014, and t2 from Aug 12 2014 to Nov 11 2014, as also considered in (De Choudhury et al. 2016). The first set of users thus constitutes the class at risk of suicidal ideation in the future (henceforth referred to as MH → SW), while the latter is the class of users not at risk (henceforth referred to as MH). We were able to identify 440 users in the MH → SW group based on this approach. For the MH users, to balance our class sizes going into the ensuing propensity score matching framework, we obtained an equal number of users (440) randomly sampled from the 28,831 users who never posted in SW in t1 or t2. As also noted in (De Choudhury et al. 2016), a caveat of this approach is that some of these 28,831 users may have gone on to post on SW outside of the observable period of our dataset (i.e., after t2); however the large timespan of our data gives sufficient confidence in the purity of the classes derived. Commentary Data. Recall, the goal of this paper is to assess the role that social support, as manifested in social media, may play in an individual’s risk to suicidal ideation in the future. We consider comments made on Reddit posts of the above identified 440 MH → SW and another 440 MH users to be proxies of social support. Prior work has situated commentary in online communities to be mechanisms through which support is extended to help seekers (White and Dorman 2001). For each of the 880 users spanning both the classes presented above, we grouped their posts, and then employed the official Reddit API (http://www.reddit.com/dev/api) to obtain the entire comment thread (the last 1000 comments) of each post. The comment threads included the text of each comment associated with a post, the author (username) associated with each comment, and its timestamp in UTC. For the 440 MH → SW users, we obtained 62,024 comments that were written by 32,362 unique users, while for the 440 in MH, there were 41,894 comments written by 21,358 unique users. Figure 1 gives descriptive statistics of the commentary data corresponding to the two user classes.",2,2,2,1,0,0,2,0,2,2,2,2,0,0,0,0,2,1,3,2
https://ojs.aaai.org/index.php/ICWSM/article/view/14891,10,5,method,"De Choudhury, Kiciman ",2,1,0,0,2017,"To study how receiving social support, in the form of comments, impacts an individual’s future risk to suicidal ideation (or likelihood of being in MH → SW ), we seek to isolate the effects of comments from the influence of other factors that might confound the effect of comments on outcomes. The gold standard for this purpose is a randomized controlled trial, where individual posts are assigned to receive a specific comment, independent of other factors. Of course, randomized controlled trials are not always feasible, due to ethical or practical limitations. As an alternative, we work within the potential outcomes framework of causal analysis (Rubin 2011; Imbens and Rubin 2015) commonly used for observational studies. Specifically, we apply a high-dimensional stratified propensity score method (Rosenbaum and Rubin 1983), conditioning our analysis of the effects of comments on the content of the earlier posts and comments shared and received by users in our dataset (see Figure 2 for a schematic description). Note that, conditioned analyses generally account for observed confounding factors but, unlike randomized controlled studies, cannot account for unobserved confounding factors. Terminology and Data Preparation. Our unit of analysis is an individual Reddit user, whose experiences are characterized by their posts to the MH communities as well as other users’ comments on these posts. We featurize these posts and comments as a per-user sequence of timestamped linguistic tokens or n-grams; lower-cased and stop-word eliminated. In our analysis, every comment-token appearing in an user’s timeline is a treatment on that user. In other words, a user is said to have received a given treatment if they author a post that receives a comment containing the corresponding token. All post and comment tokens in an user’s timeline that occur before a given treatment are considered to be covariates, representing possible factors that might confound our analysis of the treatment’s effect on a user’s measured outcome (i.e., whether or not a user posts in MH → SW in the future). Correspondingly, a treatment group is the set of users who have received a given treatment, and a control group is the set of users who have not received that treatment. For a given treatment, the result of our analysis is the difference in measured outcomes between the treatment and control groups, conditioned on covariates. Stratified Propensity Score Analysis. Stratified propensity score matching attempts to isolate the effects of a particular treatment from the effects of covariates by dividing the treatment and control groups into strata where the covariates of the treatment subgroup within a strata are statistically identical to the covariates of the control subgroup within a strata. Each strata is thus, in essence, artificially approximating a randomized controlled trial where the “assignment” of a treatment is statistically uncorrelated with covariates, allowing us to better distinguish the possible causal effects of a treatment on users’ future participation in MH → SW The stratification of users is based on their estimated propensity to receive a given treatment. The estimated propensity is a machine-learned function of a user’s likelihood of receiving a treatment based on the user’s covariates (all prior post- and comment-tokens). Once our method has stratified users, we analyze strata that have common support (Caliendo and Kopeinig 2008) Within each such strata, the treatment effect is the difference between the measured outcomes of the treatment group and the control group. In our case, this is the difference between the percentage of treated users who eventually participate in MH → SW and the percentage of control users who do so. The population-weighted combination of these strata-level effects is the final local average treatment effect, where local refers to the fact that we are only estimating over strata with common support. We repeat this entire procedure, including the propensity score estimation and stratification, for each of our target treatment tokens. Implementation. In our implementation, the propensity score function is estimated using the averaged perceptron learning algorithm (Freund and Schapire 1999). Estimation is conducted based on a binary vector representation of users’ timelines, where is for a given user if the token i appears in the user’s timeline of posts and comments shared and received prior to the treatment token, and otherwise. To distinguish between the effects of words written by users themselves and the effects of words written by others in comments, we treat an n-gram in a comment as being a distinct token from the same n-gram appearing in a post. Given a learned propensity score function, we divide our dataset into 10 strata. In addition to the local average treatment effect, we report the z-score and tests of statistical significance. We perform this analysis for all target n-gram tokens that occur in the timelines of more than 10 individuals in the MH communities (11,278 tokens).",2,2,2,1,0,0,2,0,2,2,2,2,0,0,0,0,2,1,3,2
https://ojs.aaai.org/index.php/ICWSM/article/view/14891,10,6,validating comparability,"De Choudhury, Kiciman ",2,1,0,0,2017,"When assessing the effect of a comment on a user’s risk of belonging to MH → SW or MH in the future, our stratified analysis is comparing a treatment group of users who received a particular comment to a control group of people who did not. Having indistinguishable distributions of covariates in these two groups is very important to ensure that any difference in their likelihood of future posting in SW can be attributed to the fact that one group received the comment and the other group did not. However, unlike a randomized controlled trial, we have no assurance that treatment is independent of unobserved covariates (or independent of covariates incorrectly represented, from a machine learning perspective, in our covariate set). This can lead to confounded results. Therefore, ensuring that the stratified groups are correctly balanced is critical to correct estimation of outcomes i.e., the likelihood of a user being in MH → SW or MH. Towards this goal, in this section, we describe an approach to first assess, qualitatively, the comparability (or balance) between the treatment and the control groups using human judges. Human judgments of balance are valid in our analysis because it is human commenters who are replying to posts, and thus deciding whether a user gets a treatment or not. We then quantitatively measure potential differences in sociolingustic measures and language models between the two groups. Together, this approach allows us to identify those subpopulations of treatment and control users, who are more likely to be affected by comments on their posts. In this subsection, we present our qualitative approach of assessing balance between treatment and control groups belonging to different propensity strata. We first implemented our proposed propensity score matching technique among the MH posts and comments of the 880 users in our dataset. We identified tokens that had statistically significant treatment effects; negative effect would imply that receiving the comment token decreased chances of being in MH → SW, and vice versa. We randomly sampled 150 tokens with the most positive (or negative) z-scores for our ensuing qualitative assessment. Table 2 shows a sample of these tokens. Thereafter, corresponding to each of the selected comment tokens, and spanning different propensity strata, we constructed post pairs, belonging to the treatment and control user groups. For each strata (per comment token), we randomly select 10 users from the treatment and 10 users from the control group. For each of these users, we pick their most recent post and all comments they received on that post up until the point of receiving the treatment token (or ‘placebo’ in the case of the control group). Table 3 gives some paraphrased examples of post pairs thus constructed. Next we employed two raters, an expert in social media data analysis for mental health and a mental health professional, to qualitatively estimate balance in the post pairs generated above. In other words, the raters’ task was to identify risk markers of suicidal ideation in the post pairs, that may not be observable to the propensity score analysis, however aligned with known observations of behaviors of individuals at risk of suicidal ideation. In a post pair, if the markers aligned, we would infer the treatment and control groups for that particular comment token and strata to be balanced .If not, we would assume that our initial propensity score matching analysis needs further tuning to identify more accurately balanced treatment and control groups. To do so, the raters first utilized a codebook of suicidal ideation risk markers developed in (De Choudhury et al 2016) to independently rate a random sample of 100 post pairs constructed above to be balanced (very similar: rating of 1), or imbalanced (very dissimilar: rating of 0). The risk markers in the codebook were validated in (De Choudhury et al 2016) to align with the cognitive psychological integrative model of suicide (Dieserud et al . 2001). They include: mentions of hopelessness (“I feel so abandoned”), anxiety (“I am feeling panicked”), impulsiveness (“right now there’s only two ways to ending it”), lack of self-esteem (“I am too ugly to make friends”), and expressions of loneliness (“i have no one and i never felt such pain”). Following the initial rating exercise followed by resolution of differences, the raters rated a larger sample of 200 post pairs for balance and imbalance estimation. The final agreement was found to be high (Cohen’s An interesting pattern emerged out of the qualitative coding of the post pairs. Figure 3 gives a distribution of the balance and imbalance ratings over different propensity strata. We observe that the distributions are nearly mirror images of each other. That is, the balance ratings tend to be more frequent among post pairs belonging to higher propensity strata, while the imbalance ratings peak for pairs in the lower propensity strata. The distributions cross each other between stratas 3 and 4. We adopt the lower value 3 as a threshold to identify a set of strata (and corresponding subpopulation of users) for which the treatment and control groups are balanced (propensity strata). We also identify another set where the groups (and corresponding user subpopulations) are not balanced (strata Table 3 gives some example (paraphrased) tokens in our rated sample, example strata corresponding to them, and the post pair. As can be observed, the high propensity strata post pairs exhibit many of the markers of mental health challenges identified in our balance assessment codebook. These pairs are also semantically similar (“I have been alone”, “there’s no way to avoid pain 100%”, “tired of trying, and failing”, “feeling like shit but noone to talk to”). While for the lower propensity strata post pairs, only one of the posts shows these markers of risk to suicidal ideation. Summarily, the examples illustrate that our ratings that identified post pairs of these lower strata to have greater imbalance. The above balance analysis showed that the effects of getting a token in a comment may not be homogeneous. Certain users may see little effect of getting a token (low propensity strata), while others see a large effect (higher propensity strata). Essentially, in what ways are the subpopulations of users who fall in the high and low propensity strata different This is an important consideration in order to understand whether and how users with different characteristics might be less or more likely to be affected by specific tokens appearing in the comments they receive, in their likelihood of being in MH → SW in the future. To answer this and to give more credence to our qualitative balance assessment, we adopted the following quantitative approach. Specifically, we adopt the use of the LIWC lexicon (Chung and Pennebaker 2007) to quantify the extent to which a variety of sociolinguistic measures are present in the posts of the two subpopulations in the high (strata 3) and low propensity strata (strata ). Let us call them subpopulation H and subpopulation L respectively. In Table 4 we present the results of our analysis of the subpopulations H and L using the LIWC measures – our choice of these measures is motivated from prior work where they were used to examine and understand different types of mental health content on social media (De Choudhury et al. 2013). We observe significant differences across the subpopulations, as given the “Diff” metric—it is the relative percentage difference between the value of a specific measure in the mental health posts of subpopulation H and that of subpopulation L. The subpopulation H tends to express notably lower anger , sadness, and NA in their posts. Their posts also show lower inhibition and higher cognitive processing. As shown in the measures of lexical density and awareness, this subpopulation also presents more awareness of their context and environment (De Choudhury et al. 2016). Greater use of function words in their posts indicates greater logical coherency in their writing style (Chung and Pennebaker 2007). Interestingly, this subpopulation also tends to share more about their health and work, and discuss more about social and family related concerns. Finally, subpopulation H, compared to subpopulation L, tends to show lower self-focus or self-preoccupation as noted in the use of their 1st person singular pronouns. Conversely, the language of their posts tends to show higher interpersonal focus through use of 2nd and 3rd person pronouns Summarily, the above qualitative and quantitative analyses reveal significant differences between subpopulations H and L: those likely to be affected by commentary (H) manifest a type of underlying behavior and traits that put them in a position to derive greater benefit (or counter-benefit) from comments received on their mental health posts",2,2,2,1,0,0,2,0,2,2,2,2,0,0,0,0,2,1,3,2
https://ojs.aaai.org/index.php/ICWSM/article/view/14891,10,7,results,"De Choudhury, Kiciman ",2,1,0,0,2017,"Once we identify the reliably balanced strata comprising users are likely to be affected by commentary on their posts, we adapt our stratified propensity score algorithm to ignore other strata, and then computing the final outcome effect over the population of treatment and control users in the remaining strata. Being calculated only over well-balanced strata, this local average treatment effect is thus a more reliable estimate of the effect of a particular comment token, though it is also more limited in its coverage over the population of users. In Table 5 we report 40 comment tokens that give the most negative or positive zscores in distinguishing between MH → SW and MH users. Corresponding to each token, we also report the absolute number of users in our dataset (out of a total of 880) who received the token in one of their comments (treatment count), the proportion of users in our data who fell into an unclipped strata of the token (coverage), the percent increase in likelihood of belonging to MH → SW in the future based on getting the token in a comment in the past (local average treatment effect), the zscore of the token’s likelihood of appearance between the two user classes and associated statistic, and the pointwise mutual information (PMI) between the comment token and the outcome. We find that controlling for historical use of different tokens in the posts of the users as well as the tokens received by them in the comments associated with these posts, getting comments tokens such as “gently” “is helpful” “fight the” “enjoyed it” “nice i” “really fun” “totally agree” “be super’), “instructions”) significantly decrease a user’s likelihood of being in MH → SW in the future. For “gently” this decrease is 31%, for “be helpful” it is 12%, for “fight the” it is 23%, while for “instructions” it is 17%. Which are the comment tokens getting which increases the likelihood of being in MH → SW ? We show comment tokens with the most positive treatment effects and high z-scores in Table 5. Getting tokens like “proud” “am sorry” “suicide” “medication” “depressed” and “pain and” “hating” results in increasing the likelihood of being in MH → SW by Next we explore the context of usage of the above identified significant comment tokens. This analysis is meant to enable us to understand how different types of social support relate to the outcome of being in MH → SW or MH. We randomly sampled a set of 100 comments that contained the 20 tokens identified in Table 5 to decrease the likelihood of being in MH → SW, as well as another 100 comments with the 20 tokens that were found to increase future risk to being in MH → SW. We employed qualitative inductive open coding on this sampled corpus to probe into characterizing social support expressed through the tokens. To develop a codebook and a categorization scheme for the sampled comments, we followed an iterative, semi-open coding procedure. We adopted concepts and characterizations of social support given by the social support behavioral code framework (Cutrona and Suhr 1992), and in the literature on mental health, suicide, and social media (De Choudhury and De 2014). The raters included a mental health expert and a social media expert like our balance analysis. First the raters independently coded a random sub-sample of 50 comments, then discussed each comment together with assigned codes to establish a shared vocabulary. Next, they independently coded the remaining 150 comments based on the shared vocabulary and social support coding scheme thus developed. Interrater reliability Cohen’s for this task was found to be high: .86. The final set of social support codes that described the comments included emotional, esteem, informational, instrumental, and network support, as well as interpersonal acknowledgments, aligning with the types given in prior work (Cutrona and Suhr 1992). Table 6 gives example comment experts associated with these categories. The comment excerpts in Table 6 help us understand, in what ways the tokens identified to have large effects on one’s likelihood of being in MH → SW in the future, are used in social support contexts. We observe that emotional support ranges from relating to specific challenging life situations (“totally agree”), to expressing empathy over how they impact one’s life (“not alone”). Esteem support can include boosting one’s morale to fight the stigma of mental illness (“fight the”), or encouraging hope and uplifting thoughts despite distressful experiences (“misery”). Through informational support, commenters provide advice and suggestions to seek professional mental health help (“be tough”), therapy and medication (“medication”). Commenters also provide various forms of instrumental support, including self-improvement activities like involvement in pastimes (“a hobby”). Next, network support comments express solidarity, connection, and social integration (“is helpful”). Finally, acknowledgments of social support include explicit recognition of the post author’s feelings (“be super”), thoughts, or experiences (“depressed”). While the context of use of the specific tokens with significant treatment effects did not reveal notable differences between the likelihood of being in MH and MH → SW, the relative manifestation of the different social support categories in comments for the two user classes did show sharp contrast. This is observable from Figure 4, that gives the distributions of the different support categories over the 200 coded comments, associated with MH and MH → SW. A Kruskal Wallis one-way analysis of variance test indicates that the difference between these two distributions of social support types is significant For the coded comments associated with MH we find greater expression of esteem (31%) and network support (23%), followed by emotional support (16%). Informational support (9%) and acknowledgments (5%) tend to be relatively lower for comments containing tokens that decrease likelihood of posting in SW. Overall, this distribution indicates the positive impact of esteem and network support in reducing one’s future risk of suicidal ideation expression. However, the coded comments in MH → SW tend to involve largely acknowledgments (40%) and informational support (23%). Instrumental (9%) and network support (5%) constitute the smallest categories for these comments. It appears that receiving acknowledgments of one’s feelings, or advice around mental health issues do not translate to reduced future expressions of suicidal ideation. This analysis demonstrates how our propensity score analysis method can enable us understand the usefulness of various types of support. To probe further, we sought to directly examine how some of the comment tokens with large effects (negative or positive) are used in specific social support contexts. In Figure 5(a-b) we report these findings for five comment tokens with the largest negative and positive treatment effects (ref. Table 5). The relative distributions of support types reported in this figure were determined based on the 200 qualitatively coded comments above. Aligning with our above observations, we find that tokens associated with negative treatment effect appear in comments that our raters coded to be related to emotional, esteem, or network support. On the other hand, we observe that comment tokens that increase the likelihood of being in MH → SW (i.e., are counter-beneficial) occur in comments spanning interpersonal acknowledgments and those that provide information or instrumental support.",2,2,2,1,0,0,2,0,2,2,2,2,0,0,0,0,2,1,3,2
https://ojs.aaai.org/index.php/ICWSM/article/view/14891,10,8,theoretical and practical implications,"De Choudhury, Kiciman ",2,1,0,0,2017,"In this paper we provided a principled, data-driven approach to reveal in what ways social support in the form of commentary can influence mental health outcomes of individuals, specifically their risk of posting about suicidal ideation in the future. Our work bears implications for research in mental health and suicidal ideation, by providing new, previously less understood insights into the mechanics of how (online) social support may impact future risk. Despite tremendous work attempting to establish the beneficial effects of support on well-being (Kaplan, Cassel, and Gore 1977), limited work has focused on how social support can improve health (LaRocco, House, and French Jr 1980; Cohen and Hoberman 1983). Literature also indicates that there is limited but under-investigated evidence that not all sources or types of social support are equally effective in reducing distress (Thoits 1982). Our findings speak to both these aspects. By focusing on the online behaviors of a large sample of individuals spanning several months, our work also opens up promising opportunities of employing an unobtrusive data source like social media to not only understand how and what attributes of social support may promote or hinder the well-being of individuals, but also to quantify prospective risk based on historically received support. Beyond these theoretical implications, we believe our methods and these insights we gleaned may be used to create enabling tools and applications. Today, support related moderation practices in online health communities are largely non-algorithmic in nature. Moderators manually examine comments to assess whether they are could be potentially beneficial. In some social computing systems, Reddit inclusive, moderators may rely on community signals like upvotes or downvotes, or refer to comments flagged by community members to assess whether they are potentially helpful or harmful. However, since votes or user reports are accrued over a period of time, support moderation using these kind of approaches may not be prompt enough. For sensitive communities like the ones we study in this paper, judging the supportiveness of comments is time-critical, because unhelpful comments may exacerbate someone’s vulnerability. With our propensity score matching approach, interactive tools may be built for moderators, so that they are able to moderate comments in a more timely fashion. Using such tools, moderators can closely monitor (and discourage) the use of linguistic tokens in comments known to increase the likelihood of posting in SW in the future. On the other hand, the tokens that do include evidence of reducing suicidal ideation risk could be promoted dynamically, especially for those subpopulations, who our approach indicates to be at a greater likelihood of being affected by commentary.",2,2,2,1,0,0,2,0,2,2,2,2,0,0,0,0,2,1,3,2
https://ojs.aaai.org/index.php/ICWSM/article/view/14891,10,9,ethical social challenges and limitations,"De Choudhury, Kiciman ",2,1,0,0,2017,"We now discuss some ethical and social challenges of this work. The (semi)-automated systems we presented above could allow moderators and support volunteers to make improved decisions and choices based on forecasted likelihood of risk. However, what are the obligations for the moderators or the volunteers when they discover an individual to be at a higher likelihood of suicidal ideation, that might be attributed to a specific instance of support? How can online communities reap the benefits of our method, gain from the design opportunities we outline above; at the same time, protect their ethical obligations? We also envision ethical questions regarding the use of certain type of language in comments. Does preventing a well-intentioned commenter from sharing something be considered to be a impediment to free speech? Taken together, collaborations between computing researchers, mental health experts, moderators, and ethicists can help develop protocols and guidelines that facilitate the use of our work in practical contexts in the future. Discussing limitations, our method and findings do not reveal users’ intent or motivation behind the sharing of specific linguistic cues in comments. We also cannot be sure why certain forms of social support tend to be associated with reduced likelihood of suicidal ideation in the future, or why certain others tend to show converse effects. We also presume self-selection biases in our data. Individuals who post on MH or SW, despite their expression of vulnerability, are after all individuals who are seeking help and advice. Therefore, it is perhaps not surprising that certain types of comments or forms of support are beneficial to them. We acknowledge limitations to our propensity score analysis method as well. Our data does not meet the strong assumptions that are required to infer true causality: ignorability, and the stable unit treatment value assumption. There are also likely unobserved confounds, such as users’ psychological traits, offline behaviors, and history that they may not be mentioned in Reddit posts. While these limitations prevent us from making strong causal claims, in practice we find the results of our analyses provide significant insight about the role of social support in suicidal ideation",2,2,2,1,0,0,2,0,2,2,2,2,0,0,0,0,2,1,3,2
https://ojs.aaai.org/index.php/ICWSM/article/view/14891,10,10,conclusion,"De Choudhury, Kiciman ",2,1,0,0,2017,"This paper made a methodological contribution in the analysis of online social support for mental well-being. We applied stratified propensity score matching to the content of comments shared on mental health communities on Reddit. Incorporating human assessments into our stratification framework, we were able to identify subpopulations of individuals who were more likely to be affected by the comments. Finally, we qualitatively interpreted how specific linguistic cues of comments, that are associated with high or low likelihood of future suicidal ideation, were used in the context of different forms of social support. Our work bears implications for the design of tools that can improve moderation and support provisions in online support communities.",2,2,2,1,0,0,2,0,2,2,2,2,0,0,0,0,2,1,3,2
http://hcsi.cs.tsinghua.edu.cn/Paper/Paper17/IJCAI17-SHENGUANGYAO.pdf,11,1,,"Shen, Jia, Nie, Feng, Zhang, Hu, Chua, Zhu",8,0,0,0,2017,"Depression is a major contributor to the overall global burden of diseases. Traditionally, doctors diagnose depressed people face to face via referring to clinical depression criteria. However, more than 70% of the patients would not consult doctors at early stages of depression, which leads to further deterioration of their conditions. Meanwhile, people are increasingly relying on social media to disclose emotions and sharing their daily lives, thus social media have successfully been leveraged for helping detect physical and mental diseases. Inspired by these, our work aims to make timely depression detection via harvesting social media data. We construct well-labeled depression and non-depression dataset on Twitter, and extract six depression-related feature groups covering not only the clinical depression criteria, but also online behaviors on social media. With these feature groups, we propose a multimodal depressive dictionary learning model to detect the depressed users on Twitter. A series of experiments are conducted to validate this model, which outperforms (+3% to +10%) several baselines. Finally, we analyze a large-scale dataset on Twitter to reveal the underlying online behaviors between depressed and non-depressed users.",2,2,2,1,0,0,0,0,0,2,2,0,0,0,0,2,2,1,5,0
http://hcsi.cs.tsinghua.edu.cn/Paper/Paper17/IJCAI17-SHENGUANGYAO.pdf,11,2,,"Shen, Jia, Nie, Feng, Zhang, Hu, Chua, Zhu",8,0,0,0,2017,"Depression is a leading cause of disability worldwide. Globally, an estimated 350 million people of all ages suffer from depression1 . Depressed people have various depression symptoms manifested by distinguishing behaviors. In clinical diagnosis, psychological doctors often make face-to-face interviews referring to the commonly used Diagnostic and Statistical Manual of Mental Disorders criteria. Nine classes of depression symptoms are defined in the criteria, describing the distinguishing behaviors on daily lives. Although this is the most effective method for depression diagnosis, people are somehow ashamed or unaware of depression. More than 70% of people in the early stages of depression would not consult the psychological doctors, deteriorating their conditions2 . On the other hand, people are increasingly relying on social media platforms like Twitter and Facebook to disclose emotions and moods as well as share their personal statuses. The user generated contents (UGC) on social media instantly reflect not only the daily lives, but also the mental states of users. In the past decade, social media were widely used for physical and mental wellness researches, especially the mental wellness [Coppersmith et al., 2014][Lin et al., 2016][Akbari et al., 2016]. Inspired by these, some efforts have been dedicated to depression studies. Some researchers asked users to fill questionnaires or participate interviews on social media. For example, [Park et al., 2012] analyzed behaviors and the use of languages of depressed users on Twitter. These methods are effective but expensive, time-consuming and hard to get sufficient data to guarantee their findings are robust and generalizable. Besides, these questionnaires and interviews focus on the depression behaviors already defined in depression criteria. However, the symptoms of depression evolve as the world develops, especially the online behaviors, which may not be covered detailedly in the previous depression criteria. On the other hand, some work considered online behaviors on social media. [Choudhury et al., 2013] extracted several feature groups like engagement and emotion features to detect depressed users on Twitter. However, these feature groups were not regarded as different modalities, so that the relation across different feature groups can hardly be captured without a systematic multimodal framework. In this paper, we work towards timely depression detection via harvesting social media. This work is non-trival owning to the following challenges: 1) As far as we know, there is no public available large-scale benchmark datasets for depression research that are suitable to our study. 2) Users’ behaviors on social media are multi-faceted. It is hard to characterize the users from discriminant perspectives and capture the relation across different modalities. 3) Although users’ behaviors are rich and diverse, only a few are symptoms of depression, so the depressive-oriented features are sparse on social media and hard to be captured. Towards this end, we first construct well-labeled depression and non depression datasets on Twitter by rule-based heuristic methods. Second, to represent each user in our datasets, we extract depression-oriented features, inspired not only by the offline symptoms from the depression criteria, but also online behaviors on social media. These features are composed of six groups, namely, social network features, user profile features, visual features, emotional features, topic-level features, and domain-specific features. Third, taking each feature group as a single modality, we devise a multimodal depressive dictionary learning model (MDL) to learn the sparse user representations. In addition to the sparse representation, this model also considers the relation among the six modalities. We carry out extensive experiments on the labeled datasets to validate our model (+3% to +10% better than baselines in terms of F1-Measure) and analyze the contributions of features. Finally, we conduct a series of case studies to reveal the underlying behavior discrepancy between depressed and nondepressed users on social media. On account of the limited users on well-labeled depression dataset, we construct a large-scale dataset on Twitter and uncover more depressed users by our proposed method. In particular, we get some statistical findings, such as: 1) Depressed users tend to leak off more emotions on social media, especially negative emotions. Their negative emotion words are 79% more than those of non-depressed users. 2) Depressed users tend to post 44% more tweets between 23:00 and 5:00 on average. More detailed results can be found in section 6.3. Our framework is presented in Figure 1. We summarize the main contributions in three aspects: • We construct benchmark datasets for online depression detection and analysis, including the well-labeled depression and non-depression datasets as well as a largescale depression-candidate dataset. In addition, we release these datasets3 with features to facilitate wellness study for computer science and psychology. • We extract six groups of discriminant depressionoriented features to describe users from different aspects. As only few of the users’ behaviors are symptoms of depression, we present a multimodal depressive dictionary learning model to learn the sparse representation of users. Our methods can be used to timely detect depression, take proactive care to prevent the depressed condition to be deteriorated. • We analyze feature contributions and online behaviors of depression. We make our efforts to reveal the behaviors not covered in depression criteria, trying to provide more perspectives and insights for depression researches.",2,2,2,1,0,0,0,0,0,2,2,0,0,0,0,2,2,1,5,0
http://hcsi.cs.tsinghua.edu.cn/Paper/Paper17/IJCAI17-SHENGUANGYAO.pdf,11,3,,"Shen, Jia, Nie, Feng, Zhang, Hu, Chua, Zhu",8,0,0,0,2017,"Depression studies came much earlier than that of Internet. Based upon the user study or questionnaire survey, many widely-accepted scales and criteria have been developed. For example, Beck’s Depression Inventory [Beck et al., 1961] comprises of 21 questions about users’ mental and physiological state. Another example, CES-D Scale [Radloff, 1977] contains 20 questions about the mental conditions like users’ guilty feelings and sleep conditions. The questions either have several options aligned with different scores or require users to feedback the degree of their situations. The depression level is diagnosed according to the scale of the total score. On the other hand, as a standard criterion for depression diagnosis, the Diagnostic and Statistical Manual of Mental Disorders (DSM) [Whooley and Owen, 2014] introduces nine kinds of depressive indicators such as depressed mood and diminished interest. Clinicians usually check whether these symptoms have been presented during a period of time to make their final decision. Beyond all doubts, these criteria are well validated and applied to real-world cases for many years. However, as the DSM took 12 years to be evolved from DSM-IV to DSM-V, it is relatively slow for these criteria to be updated strictly, so they may not comprehensively cover the new behaviors and symptoms such as those conveyed by the timely social media.",2,2,2,1,0,0,0,0,0,2,2,0,0,0,0,2,2,1,5,0
http://hcsi.cs.tsinghua.edu.cn/Paper/Paper17/IJCAI17-SHENGUANGYAO.pdf,11,4,,"Shen, Jia, Nie, Feng, Zhang, Hu, Chua, Zhu",8,0,0,0,2017,"With the era changing, we, including the depressed users, almost cannot survive without social media. Researchers thereby started to analyze the online behaviors of depressed users. As a preliminary research, Park et al. [2012] explored the use of language in describing depressive moods utilizing real-time moods captured from Twitter users. In their followup work, Park et al. [2013] conducted face-to-face interviews with 14 active Twitter users to explore their depressive behaviors. Most recently, Xu et al. [2016] attempted to explain how web users discuss depression-related issues from the perspective of the social networks and linguistic patterns. With the aforementioned work, depression detection via social media became possible. Choudhury et al. [2013] explored the potential of using social media to detect and diagnose major depressive disorders in individual. Resnik [2015] studied the topic models in the analysis of linguistic signals for detecting depression. These depression detection efforts demonstrated that it is possible to analyze massive depressed users on social media. However, there are some limitations of the existing work: 1) Most of them conducted their experiments on a very small set of samples, so it is difficult to justify the robustness and generality of their findings on a large-scale depression group. 2) Few of them characterized the potential depressed users from multiple modalities of social media. 3) They did not systematically investigate the depression behaviors on social media, and very little work looked into the newly-presented depression symptoms as the world develops.",2,2,2,1,0,0,0,0,0,2,2,0,0,0,0,2,2,1,5,0
http://hcsi.cs.tsinghua.edu.cn/Paper/Paper17/IJCAI17-SHENGUANGYAO.pdf,11,5,,"Shen, Jia, Nie, Feng, Zhang, Hu, Chua, Zhu",8,0,0,0,2017,"To formulate our problem, we declare some notations in advance. In particular, we use bold capital letters (e.g., X) and bold lowercase letters (e.g., x) to denote matrices and vectors, respectively. We employ non-bold letters (e.g., N) to represent scalars, and denote parameters with Greek letters (e.g., λ). Besides, we use curlicue letters to represent sets (e.g.,V ). If not clarified, all vectors are in column form. Suppose that we have a set of NL labeled depression or non-depression samples, denoted as V L, and an unlabeled depression-candidate set, denoted as V U with NU samples. Let S , {1, ..., S} be a finite set of available modalities for each sample. Supposing there are M features in total, we denote Ms as the feature dimension of the s th modality, s ∈ S . For each sample vn ∈ (V L ∪ V U ), we denote xn ∈ RM as the feature vector of vn, and let x s n ∈ RMs be the feature vector for the s th modality of vn. Besides, y ∈ R NL denotes the label vector of our labeled samples. With the notations above, we can formally define our problem as: Given a set of labeled samples V L, we aim to build a model for depression detection by learning the sparse representations of V L with S modalities, and further detect more depressed users in a large-scale unlabeled set V U to analyze their common behaviors in the social networks.",2,2,2,1,0,0,0,0,0,2,2,0,0,0,0,2,2,1,5,0
http://hcsi.cs.tsinghua.edu.cn/Paper/Paper17/IJCAI17-SHENGUANGYAO.pdf,11,6,,"Shen, Jia, Nie, Feng, Zhang, Hu, Chua, Zhu",8,0,0,0,2017,"To make depression detection via social media, we constructed two datasets of depression and non-depression users on Twitter, which has mature APIs and is prevalent around the world. Given a Twitter user, we collected the profile information of the user and an anchor tweet to infer the mental state. As people should be observed for a period of time according to clinical experience, all the other tweets published within one month from the anchor tweet were also obtained. Depression Dataset D1. Based on the tweets between 2009 and 2016, we constructed a depression dataset D1. Inspired by [Coppersmith et al., 2014], users were labeled as depressed if their anchor tweets satisfied the strict pattern “(I’m/ I was/ I am/ I’ve been) diagnosed depression”. In this way we obtained 1,402 depressed users and 292,564 tweets within one month. Non-Depression Dataset D2. We constructed a nondepression dataset D2, where users were labeled as nondepressed if they had never posted any tweet containing the character string “depress”. As twitter has over 300 million active users and 10 billion tweets per month, we select the tweets on December 2016. Although D1 and D2 are well-labeled, the depressed users on D1 are too few so we constructed a larger dataset D3 for depression behaviors discovery. Depression-candidate Dataset D3. Based on the tweets on December 2016, we constructed an unlabeled depressioncandidate dataset D3, where users were obtained if their anchor tweets loosely contained the character string “depress”. Although the depression-candidate dataset contained much noise, it contained more depressed users than randomly sampling. Finally we obtained 36,993 depression-candidate users and over 35 million tweets within one month, which will be used for online behavior analysis. The statistics of the datasets are summarized in Table 1.",2,2,2,1,0,0,0,0,0,2,2,0,0,0,0,2,2,1,5,0
http://hcsi.cs.tsinghua.edu.cn/Paper/Paper17/IJCAI17-SHENGUANGYAO.pdf,11,7,,"Shen, Jia, Nie, Feng, Zhang, Hu, Chua, Zhu",8,0,0,0,2017,"Before feature extraction, we noticed that the words are flexible and variant in the raw data of social media, which causes great difficulties in word matching and semantic analysis. Therefore, we carried out the following data preprocessing procedures: 1) Emoji processing. Emoji are incompatible with many text processing algorithms. We thus removed the emoji in Tweets’ texts with an emoji library collected from Twitter, then counted them separately. 2) Stemming. As the keyword matching strategy is widely used, words must have unified representations regardless of tense and voice. For example, “married” and “marrying” should be represented as “marri” uniformly. We here adopted the Porter Stemmer [Porter, 2001] as the stemming algorithm. 3) Irregular words processing. The words on social media may be irregular because of typographical mistakes or abbreviations of common words. We thus leveraged a word2vec model trained on 400 million tweets by [Baldwin et al., 2015] to obtain the regular representations of irregular words. For each word encountered, we tried to find it in the WordNet [Miller, 1995]. Once failed, we used the word2vec model to find its most related five words with the NLTK toolbox [Bird, 2006]. Afterwards, we concatenated the preprocessed content of each user’s recent tweets in one month to a single document for extraction of text related features.",2,2,2,1,0,0,0,0,0,2,2,0,0,0,0,2,2,1,5,0
http://hcsi.cs.tsinghua.edu.cn/Paper/Paper17/IJCAI17-SHENGUANGYAO.pdf,11,8,,"Shen, Jia, Nie, Feng, Zhang, Hu, Chua, Zhu",8,0,0,0,2017,"We intended to detect and analyze depressed users with offline and online behaviors. Regarding offline behaviors, there are clear definitions in depression criteria, which have been widely used in depression diagnosis. On the other hand, we harvested the social media and found some common online behaviors. With references in computer science and psychology, we finally defined and extracted six depression-oriented feature groups to comprehensively describe each user, presented in our data-released website for more details. Social Network Feature. It was found that depressed users are less active in social networks and depressed individuals perceived Twitter more as a tool for social awareness and emotional interaction [Park et al., 2013]. Therefore, the social network features were worth considering, such as: 1) Number of tweets. We extracted the number of tweets posted historically and recently by the given user to assess the user’s activeness. 2) Social interactions. We considered the social interaction features such as the number of the user’s followings and followers to describe users’ online social behaviors. 3) Posting behaviors. We also extracted the different posting behaviors of users to reflect the state of their lives, such as the posting time distribution. User Profile Feature. The user profile features refer to users’ personal information in social networks. It was found that people having a college degree or a regular job are less likely to be depressed [Park et al., 2012]. However, there are quite few personal informations returned by Twitter APIs. Therefore, we employed the bBridge [Farseev et al., 2016], a big data platform for social multimedia analytics, to obtain the genders, ages, relationships, and education levels of users. Visual Feature. Visual features were proved effective in cross-modal problems [Zhang et al., 2016] and modeling sentiments and emotions in social networks [Wang et al., 2015]. Compared with texts, images are more vivid, freer, and pass more complex message. In our work, we considered users’ avatars in their accounts’ home pages as the first impression of users in social networks, and extracted their fivecolor combinations, brightness, saturation, cool color ratio and clear color ratio as visual features. Emotional Feature. Emotional status of depressed users differs from that of common users, so the emotional features are beneficial in depression detection. We studied: 1) Emotion words. We extracted positive and negative words count in recent tweets with LIWC [Pennebaker et al., 2001]. 2) Emoji. We invited three annotators to vote for the sentiment of the emoji in our library mentioned before. Based on the majority voting, we obtained a sentimental emoji library, with which we extracted the sentimental emoji counts. 3) VAD features. We leveraged Affective norms for English words [Bradley and Lang, 1999] to extract the VAD features, that is, valence, arousal, and dominance, which is proved effective to explain human emotions. Topic-level Feature. The topics concerned by depressed users and non-depressed users are likely to be significantly different, and topic models had been found to be effective in predicting depression on social media [Resnik et al., 2015]. In our work, we employed the unsupervised Latent Dirichlet Allocation (LDA) model to extract the topic distribution of the documents. Based on perplexity metric frequently utilized to find the optimal number of hidden topics, we ultimately obtained 25 dimensional topic features. Domain-specific Feature. From the depressed users, we gained some insightful depression-related observations. Inspired by this, we extracted: 1) Antidepressant. We established a lexicon of antidepressants from the Wikipedia page of “Antidepressant”, with which we counted the average number of antidepressant names mentioned. 2) Depression symptoms. Referring to the nine groups of symptoms in DSMIV criteria, we extracted the corresponding keywords respectively. However, different from the formal text in the criteria, the linguistic style varies widely on Twitter. Therefore, we extended the keywords with the word2vec model mentioned before to construct a lexicon of the popular words of these symptoms on Twitter. In such way, we ultimately obtained the word counts of the nine symptom categories for each user.",2,2,2,1,0,0,0,0,0,2,2,0,0,0,0,2,2,1,5,0
http://hcsi.cs.tsinghua.edu.cn/Paper/Paper17/IJCAI17-SHENGUANGYAO.pdf,11,9,,"Shen, Jia, Nie, Feng, Zhang, Hu, Chua, Zhu",8,0,0,0,2017,"Intuitively, give a sample vn, the original multimodal feature representation [x 1 n, ..., x S n] of vn has some common patterns. Besides, depression representations are sparse with regard to the depression criteria. Thus, we present a multimodal depressive dictionary learning model (MDL) to detect depressed users, with the general idea of: 1) learn the latent and sparse representation of users by dictionary learning; 2) jointly model cross modality relatedness to capture the common patterns and learn the joint sparse representations; and 3) train a classifier to detect depressed users with the learned features specifically.",2,2,2,1,0,0,0,0,0,2,2,0,0,0,0,2,2,1,5,0
http://hcsi.cs.tsinghua.edu.cn/Paper/Paper17/IJCAI17-SHENGUANGYAO.pdf,11,10,,"Shen, Jia, Nie, Feng, Zhang, Hu, Chua, Zhu",8,0,0,0,2017,"Although we extract a rich set of features from each modality, not all of them are obviously related to depressed users. In addition, as the contents on social media are usually in freestyle, some noises were extracted as well, somehow impacting the detection accuracy. We thus turned to learn the latent and sparse representation of users by dictionary learning. Given the original feature representation X = [x1, ..., xNL ] ∈ RM×NL , dictionary learning aims to learn a set of latent concepts or feature patterns, D = [d1, ..., dD] ∈ RM×D and a latent sparse representation A = [α1, ..., αNL ] ∈ R D×NL , with the following empirical cost, where the unsupervised loss l(xn, D) is defined as, where λ1 and λ2 are the regularizing parameters. The l1- norm is applied to regulate the learned representation αn to be sparse.",2,2,2,1,0,0,0,0,0,2,2,0,0,0,0,2,2,1,5,0
http://hcsi.cs.tsinghua.edu.cn/Paper/Paper17/IJCAI17-SHENGUANGYAO.pdf,11,11,,"Shen, Jia, Nie, Feng, Zhang, Hu, Chua, Zhu",8,0,0,0,2017,"In fact, different modalities are not independent of each other and share some common patterns which cannot be captured by the uni-modal dictionary learning. Therefore, dictionary learning was extended to multimodal to fuse features across modalities and learn the joint sparse representation to obtain the latent features. As our samples have S modalities, we denote Ds ∈ RMs×D as the corresponding dictionary of the s th modality and the latent feature An = [α 1 n, α2 n, ..., αS n] ∈ R D×S as the sparse representation of the n th sample vn. So the empirical cost l(xn, D) would be, where λ is a regularization parameter to balance the joint sparse and the reconstruction error. The l21-norm of An is defined as, which encourages row sparsity in An. By regularizing the l21-norm of An, we encourage collaboration cross modalities so that the same dictionary atoms from different modalities present the same concept and the sparse representations from different modalities consist with each other. The overall optimal dictionaries and joint sparse representation of each sample can be obtained by optimizing Eqn.(1) and Eqn.(3) with alternating direction method of multipliers [Parikh and Boyd, 2014] and projected stochastic gradient [Aharon and Elad, 2008], respectively.",2,2,2,1,0,0,0,0,0,2,2,0,0,0,0,2,2,1,5,0
http://hcsi.cs.tsinghua.edu.cn/Paper/Paper17/IJCAI17-SHENGUANGYAO.pdf,11,12,,"Shen, Jia, Nie, Feng, Zhang, Hu, Chua, Zhu",8,0,0,0,2017,"With the learned joint sparse representations A∗ n, n = 1, ...NL, we can train a binary classifier to detect depressed users based on cumulative loss as follows, where W = [w1 , w2 , ...wS] ∈ R D×S is the coefficient matrix, p is a regularization parameter, and lsu(yu, ws , αs ∗ v ) is a loss function that measures how the classifier, parametrized by ws , can predict yn by observing α s ∗ n . For our binary classification problem, lsu can be naturally chosen as the logistic regression loss,8 Then, Eqn.(5) can be directly solved via gradient descent.",2,2,2,1,0,0,0,0,0,2,2,0,0,0,0,2,2,1,5,0
http://hcsi.cs.tsinghua.edu.cn/Paper/Paper17/IJCAI17-SHENGUANGYAO.pdf,11,13,,"Shen, Jia, Nie, Feng, Zhang, Hu, Chua, Zhu",8,0,0,0,2017,"We validated the effectiveness of the MDL method on depression dataset D1 and non-depression dataset D2. We first tuned three key parameters of the proposed MDL and tried different scales of depressed users to obtain the best performance, then compared it with baseline methods. Next, we compared different modality combinations to validate the effectiveness of the extracted feature groups. Finally, in order to obtain large-scale depressed users, we made detection on depression-candidate D3 and compared the results to D2 to analyze online behaviors of depression. Comparison Methods We compared the following classification methods: • Naive Bayesian (NB). Naive Bayesian is a widely used classifier [Pedregosa et al., 2011]. All the features are directly inputted into the classifier and the classifier outputs whether this user is depressed. • Multiple Social Networking Learning (MSNL). MSNL [Song et al., 2015] is a novel model for multiview learning by seamlessly analyzing information from multiple sources. It was proposed for a binary classification problem, volunteerism tendency prediction. Wasserstein Dictionary Learning (WDL). WDL [Rolet et al., 2016] is a dictionary learning model using the Wasserstein distance as the fitting error between each original point and its reconstruction to leverage the similarity shared by the features. • Multimodal Depressive Dictionary Learning (MDL). The proposed method in this paper. We trained and tested these methods under 5-fold cross validation, with over 10 randomized experimental runs. Metrics We evaluated the detection performance of our method and comparison methods in terms of Accuracy (Acc.), Macroaveraged Recall (Rec.), Macro-averaged Precision (Prec.), and Macro-averaged F1-Measure (F1).",2,2,2,1,0,0,0,0,0,2,2,0,0,0,0,2,2,1,5,0
http://hcsi.cs.tsinghua.edu.cn/Paper/Paper17/IJCAI17-SHENGUANGYAO.pdf,11,14,,"Shen, Jia, Nie, Feng, Zhang, Hu, Chua, Zhu",8,0,0,0,2017,"Parameter Tuning There are three key parameters in the MDL: two regularization parameters, λ in Eqn.(3) and p in Eqn.(5), as well as an implicit parameter D. The optimal values of these parameters were carefully tuned via grid search with small but adaptive step size. The search range for λ, p, and D are [0.001, 0.04], [10−5 , 10−1 ], and [50, 200], respectively. The parameters corresponding to the best F1 measure were used to report the final results. For other competitors, the procedures to tune the parameters are the same to ensure fair comparison. We finally observed that MDL reached the optimal performance when λ = 0.007, p = 10−2.5 , and D = 130. Figures 2(a), 2(b) and 2(c) illustrate the performance of our method with respect to these three parameters, which were tuned by varying one and fixing the others with optimal values. Scalability Analysis for Depressed Users We justified the model performance on different scales of depressed users. With 1,402 depressed users in total, we fixed the capacity of our dataset to 1,500 and varied the scale of depressed users from 10% to 90% with increment of 10%. Figure 2(d) shows the trend of detection performance with different proportions of depressed users. It can be found that our method achieved an outstanding performance when depression users’ scale laid between 40% to 60%, with the best performance in 50%. However, we retrieved a decent performance under imbalanced scales. Therefore, in the following experiments, we randomly select 1,402 non-depressed users on D2 to make the scale of depressed users to be 50%. Method Comparison We compared the detection performance of MDL and baselines in terms of the four selected measures. The comparison results were summarized in Figure 2(e). From this figure, we have the following observations: 1) WDL achieved better performance than NB by 10% which shows that latent and sparse representation are effective in depression detection. 2) The MSNL and the presented MDL outperformed WDL by 5% to 8%, which verifies that modeling relation among modalities is beneficial for depressed user detection. 3) The MDL method surpassed the WDL method by 3%, which justifies the important role of modality collaboration during sparse representation learning. 4) Our MDL method achieved the best performance with 85% in F1-Measure, indicating that combining multimodal strategy and dictionary learning strategy is effective in depression detection. Modality Contribution Analysis To study the effectiveness of different modalities, we further constructed an experiment to feed our model with one modal unselected each time. Specifically, we first used all modalities, denoted as MDL. We then removed the six modalities separately and denoted them as MDL-S, MDL-U, MDL-V, MDL-E, MDL-T, and MDL-D, respectively. From the results shown in Figure 2(f) we could see that the MDL outperforms the others by 2% at least, which indicates that each modality do contribute to depression detection. Besides, it can be seen that the performance suffers the most from removing the social network modality by 16%, indicating that users’ posting behaviors are more discriminative than the contents they posted. In addition, the emotion modality also contributes much to the performance, which shows the depressed users usually have different emotion status over a long period.",2,2,2,1,0,0,0,0,0,2,2,0,0,0,0,2,2,1,5,0
http://hcsi.cs.tsinghua.edu.cn/Paper/Paper17/IJCAI17-SHENGUANGYAO.pdf,11,15,,"Shen, Jia, Nie, Feng, Zhang, Hu, Chua, Zhu",8,0,0,0,2017,"Besides analyzing the effectiveness of our MDL model, we further expected to compare some online behaviors between depressed and non-depressed users. Although the depressed users are well-labeled on D1, its size is so small that we made detection on D3 to find more depressed users. In this way, we obtained 19,233 depressed users, which were compared to the non-depressed users on D2. In this way, we have some interesting findings shown in Figure 2(g) and 2(h). 1) Posting time. The depressed users are more likely to post tweets (+44% on average) between 23:00 and 6:00, indicating that they are susceptible to insomnia. 2) Emotion catharsis. Depressed users have 0.37 positive words and 0.52 negative words per tweet, which surpasses those of non-depressed users by 0.17 and 0.23. It shows that although all users are likely to say more about their bad moods, depressed users express more emotions, especially negative emotions, on social media. Furthermore, the clear color ratio of depressed users’ avatars are 5% lower, which presented more repressed emotions to others. 3) Selfawareness. Compared to non-depressed users, nearly 200% more first personal pronouns (0.26 per tweet) are used in tweets of depressed users, which may reflect their suppressed monologues and strong senses of self-awareness. 4) Live sharing. Depressed users post antidepressant and depression symptom words 165% more (0.061 per tweet) than nondepressed users (0.023 per tweet) on average, indicating that they are willing to share what they encountered in the real life.",2,2,2,1,0,0,0,0,0,2,2,0,0,0,0,2,2,1,5,0
http://hcsi.cs.tsinghua.edu.cn/Paper/Paper17/IJCAI17-SHENGUANGYAO.pdf,11,16,,"Shen, Jia, Nie, Feng, Zhang, Hu, Chua, Zhu",8,0,0,0,2017,"This paper aims to make timely depression detection via harvesting social media. With the benchmark depression and non-depression datasets as well as well-defined discriminative depression-oriented feature groups, we proposed a multimodal depressive dictionary learning method to detect depressed users in Twitter. We then analyzed the contribution of the feature modalities and detected depressed users on a large-scale depression-candidate dataset to reveal some underlying online behaviors discrepancy between depressed users and non-depressed users on social media. Since online behaviors cannot be ignored in modern life, we expect our findings to provide more perspectives and insights for depression researches in computer science and psychology",2,2,2,1,0,0,0,0,0,2,2,0,0,0,0,2,2,1,5,0
https://aclanthology.org/W16-0307.pdf,12,1,,"Gkotsis, Oellrich, Hubbard, Dobson, Liakata, Velupillai, Dutta",7,0,0,0,2016,"Online social media, such as Reddit, has become an important resource to share personal experiences and communicate with others. Among other personal information, some social media users communicate about mental health problems they are experiencing, with the intention of getting advice, support or empathy from other users. Here, we investigate the language of Reddit posts specific to mental health, to define linguistic characteristics that could be helpful for further applications. The latter include attempting to identify posts that need urgent attention due to their nature, e.g. when someone announces their intentions of ending their life by suicide or harming others. Our results show that there are a variety of linguistic features that are discriminative across mental health user communities and that can be further exploited in subsequent classification tasks. Furthermore, while negative sentiment is almost uniformly expressed across the entire data set, we demonstrate that there are also condition-specific vocabularies used in social media to communicate about particular disorders. Source code and related materials are available from: https: // github .com/gkotsis/ reddit-mental-health.",2,2,2,2,0,0,2,0,2,2,1,2,0,0,0,0,1,2,3,0
https://aclanthology.org/W16-0307.pdf,12,2,,"Gkotsis, Oellrich, Hubbard, Dobson, Liakata, Velupillai, Dutta",7,0,0,0,2016,"Mental illnesses are estimated to account for 11% to 27% of the disability burden in Europe (Wykes et al., 2015) and mental and substance use disorders are the leading cause of years lived with disability worldwide (Whiteford et al., 2013). Our knowledge about these mental health problems is still more limited than for many physical conditions, as sufferers may relapse even after successful treatment or exhibit resistance to different treatments. Most mental health conditions begin early, disrupt education (Kessler et al., 1995) and may persist over a lifetime, causing disability when those affected would normally be at their most productive (Kessler and Frank, 1997). For example, Patel and Knapp (1997) estimated the aggregate costs of all mental disorders in the United Kingdom at 32 billion (1996/97 prices), 45% of which was due to lost productivity (Patel and Knapp, 1997). The global burden of mental and substance use disorders increased by 376% between 1990 and 2010 (Whiteford et al., 2013) which means it is an international public health priority to effectively prevent and treat mental health issues. In the UK, 17% of adults experience a subthreshold common mental disorder (McManus et al., 2009) and up to 30% of individuals with nonpsychotic common mental disorders have subthreshold psychotic symptoms (Kelleher et al., 2012) showing that a large proportion of mental illness is unrecognised, but nevertheless has a significant impact upon people’s lives. Those people with conditions that meet criteria for diagnosis are treated in primary care or by mental health professionals. Studies consistently show that between 50-60% of all individuals with a serious mental illness receive treatment for their mental health problem at any given time (Kessler et al., 2001). However, most of the pathology tracking and improvement assessment is done through questionnaires, e.g. the Personal Health Questionnaire 9 (PHQ9) for depression (Kroenke and Spitzer, 2002), and require a subjective comment by the patient, e.g. “How many days have you been bothered with little interest or pleasure in doing things in the past two weeks?”. As with every personal judgement, the responses are influenced by the environment in which the person has been asked, the relationship to the clinician and even the stigma attached to depression (Malpass et al., 2010). While there are aims to integrate real-time reporting into a patient’s life (Ibrahim et al., 2015), these are still based on set questionnaires and may not fit with the main concerns of a patient. Social media, such as Twitter1 , Facebook2 and Reddit3 , have become an accepted platform to communicate about life circumstances and experiences. A specific example of social media in the context of illness is PatientsLikeMe (Wicks et al., 2010). PatientsLikeMe has been developed to enable people suffering from an illness to exchange information with others with the same condition, e.g. to find alternative treatment opportunities. It has been shown that the support received in such online communities can be empowering by engendering self-respect and a feeling of being in control of the situation (Barak et al., 2008). Hence, social media constitutes a tremendous resource for better understanding diseases from a patient perspective. Social media data has recently been recognised as one of the resources to gather knowledge about mental illnesses (Coppersmith et al., 2015a; De Choudhury et al., 2013; Kumar et al., 2015). For example, Twitter data has been used to develop classifiers to recognise depression in users (De Choudhury et al., 2013) and to classify Twitter users who have attempted suicide from those who have not and from those who are clinically depressed (Coppersmith et al., 2015b). Furthermore, data collected from Reddit pertaining to suicidal ideation could demonstrate the existence of the Werther effect (suicide attempts and completions after media depiction of an individual’s suicide) (Kumar et al., 2015). Coppersmith and colleagues used Twitter data to determine language features that could be used to classify Twitter users into suffering from mental health problems and unaffected individuals (Coppersmith et al., 2015a). However, while the authors could identify features that allows the classification between healthy and unhealthy Twitter users, they also note that language differences in communicating about the different mental health problem remains an open question. Similarly, Mitchell et al. (2015) used Twitter data to separate users affected by schizophrenia from healthy individuals by automatically identifying characteristic language features for schizophrenia (Mitchell et al., 2015). Both the latter approaches rely on the Linguistic Inquiry and Word Count (LIWC) (Tausczik and Pennebaker, 2010), but Mitchell et al. also covers features such as Latent Dirichlet Allocation and Brown Clustering. Very recently and concurrently with our own work, De Choudhury and colleagues have shown that linguistic features can be used to predict the likelihood of individuals transitioning from posting about depression and other mental health issues on Reddit to suicidal ideation (De Choudhury et al., 2016). This work showed the ability to make causal inferences on the basis of language usage and employed a small subset of the mental health groups on Reddit. Following on from the promise that such work holds, our goal was to study language features that are characteristic for individual mental health conditions using large scale Reddit data. We anticipate that our findings can be used to assist in separating posts pertaining to different mental health problems and for various language-based applications involving the better understanding of mental health conditions. Reddit is particularly suitable for such research as it has an enormous user base4 , posts and comments are topic-specific and the data is publicly available. We focussed on subreddits (communities where users can post/comment in relation to a specific topic, e.g. suicide ideations) of the Reddit data dump5 that address the following mental health problems: Addiction, Anxiety, Asperger’s, Autism, Bipolar Disorder, Dementia, Depression, Schizophrenia, self harm and suicide ideation. These conditions are commonly encountered by mental health practitioners and contribute significantly to treatment costs. We aimed to identify linguistic characteristics that are specific to any of the mental illnesses covered and can be used for text classification tasks. The investigated characteristics include lexical as well as syntactic features, the uniqueness of vocabularies, and the expression of sentiment and happiness. Our results suggest that there are linguistic features that are discriminative of the user communities used in this study. Furthermore, applying a clustering method on subreddits, we could show that subreddits mostly contain a topic-specific vocabulary. Moreover, we could also highlight that there are differences in the way that sentiment is expressed in each of the subreddits. Source code and related materials are available from: https:/ /github .com/gkotsis/ reddit-mental-health.",2,2,2,2,0,0,2,0,2,2,1,2,0,0,0,0,1,2,3,0
https://aclanthology.org/W16-0307.pdf,12,3,,"Gkotsis, Oellrich, Hubbard, Dobson, Liakata, Velupillai, Dutta",7,0,0,0,2016,"As our aim was to define linguistic characteristics specific to mental health problems, we downloaded the Reddit data and extracted relevant posts and comments. These were then further investigated with respect to specific linguistic features, e.g. sentence structure or unique vocabularies, to determine characteristics for subsequent classification tasks. The data set as well as the methods employed are described in the following subsections.",2,2,2,2,0,0,2,0,2,2,1,2,0,0,0,0,1,2,3,0
https://aclanthology.org/W16-0307.pdf,12,4,,"Gkotsis, Oellrich, Hubbard, Dobson, Liakata, Velupillai, Dutta",7,0,0,0,2016,"Reddit is a social media network, where registered users can post requests to a broader community. Posts are hosted in topic-specific fora, so called subreddits. Subreddits can be created by users based on the subject they are interested in to communicate. All users can freely join any number of subreddits and participate in discussions. This means that the posts are sent to a community potentially knowledgeable or at least interested in the topic. We used this Reddit feature, to determine subreddits targetting specific mental health problems. For this purpose, we filtered the entire downloaded data set for subreddits targeting any of the 10 as relevant identified diseases. The entire data set as obtained was separated into posts and comments, and we preserved this separation so that analysis could be executed on either posts, comments or both combined. Posts are initial textual statements that initiate a communication with other users. Comments are replies to posts and are organised in a treelike structure. Both posts and comments can be written be anyone, and even the Reddit user that wrote the initial post can comment on it. We note here that the number of users, posts and comments varied substantially between subreddits (see Table 1). To refer to sets of both posts and comments (total also in Table 1), we use the term “communication” in the following sections. As shown in Table 1, the depression subreddit contains the largest amount of communications (1.1M), while the smallest amount is found in the BipolarSOs subreddit (5K). The number of posts is always smaller than the number of comments though the ratio of average number of comments per posts varies. The highest average rate of comments per posts can be seen on subreddit opiates, while the smallest number of replies is observed on the addiction subreddit.",2,2,2,2,0,0,2,0,2,2,1,2,0,0,0,0,1,2,3,0
https://aclanthology.org/W16-0307.pdf,12,5,,"Gkotsis, Oellrich, Hubbard, Dobson, Liakata, Velupillai, Dutta",7,0,0,0,2016,"There are many ways to model communication. Communication in the form of language use can be characterised through a variety of feature types. Our aim is to better understand the nature and depth of the communication that takes place, and one way to do this is by the analysis of linguistic features. These features are particularly relevant in the context of mental health problems, as the abilities of the sufferer to effectively communicate can be affected by such problems (Cohen and Elvevag, 2014). For ˚ example, someone suffering from Bipolar Disorder may suddenly write a lot, but not necessarily in a cohesive manner. In the Iowa Writers’ Workshop study (Andreasen, 1987) bipolar sufferers reported that they were unable to work creatively during periods of depression or mania. During depressive episodes, cognitive fluency and energy were decreased, and during manic periods they were too distractible and disorganized to work effectively, so it would be reasonable to expect this to be reflected in their prose. Understanding these features and consequently the nature and content of the posts will allow us to better design useful classification systems and predictive models. Through discussion, we determined an initial feature set of linguistic characteristics that draws on previously established measures of psychological relevance, such as LIWC and Coh-Metrix (Graesser et al., 2004). However, we note here that in order to not overload our initial feature set, we selected a subset of all the available possibilities. In our feature set, we included linguistic features introduced by Pitler and Nenkova (Pitler and Nenkova, 2008) and partially overlapping with those used in CohMetrix for predicting text quality. More specifically, we adopt features that aim at assessing the readability of textual content. Readability is a measurement that aims to assess the required education level for a reader to fully appreciate a certain content. The task of understanding textual content and assessing its quality encompasses various factors that are captured through the features that we also propose here (see supplemental material for more information about the implementation). A subset of these features have been used successfully to predict the answers to be marked as accepted in online Community-based Question Answering websites (Gkotsis et al., 2014). Our first set of features pertains to the usage of specific words in documents. For instance, we look at the usage of definite articles, since we believe that definite articles are used for specific and personal communications. Similarly, we keep track of pronouns, first-person pronouns, and the ratio between them, as indicators of the degree of first-person content. Additional features in this initial set aimed at examining text complexity. In our approach, text complexity can scale both horizontally (length, topic cohesion) and vertically (clauses, composite meanings). For the horizontal assessment, we count the number of sentences. Another set of features, which target the understanding of topic continuity and cohesion across sentences, is word overlap between adjacent sentences, either by taking into account all words, or just nouns and pronouns. For the vertical assessment, we employ the following features: a) we count the noun chunks and verb phrases (sequences of nouns and verbs, respectively) and the number of words contained within them, b) we construct the parse tree of each sentence and measure its height, and c) we count the number of subordinate conjunctions (e.g. “although”, “because” etc.). A parse tree represents the syntactic structure of a sentence, and tools such as dependency or constituency parsers are readily available for utilisation, e.g. as implemented in the Python module spaCy6 . Finally, we found that a few posts do not contain any text in their body, apart from their title. This was typically the case for posts that contained a Uniform Resource Locator (URL) to a web page of interest to the community. We believe that the ratio of the number of these posts over the total number of posts is associated with the degree of information dissemination7 , as opposed to the personal story-telling that might occur in other cases, and thus included this as an additional feature.",2,2,2,2,0,0,2,0,2,2,1,2,0,0,0,0,1,2,3,0
https://aclanthology.org/W16-0307.pdf,12,6,,"Gkotsis, Oellrich, Hubbard, Dobson, Liakata, Velupillai, Dutta",7,0,0,0,2016,"For this classification-based approach, we employed a representation based on individual words, as well as information on words that frequently co-occurred together. The aim of this task was to examine how closely aligned the vocabularies of each subreddit were, assessed via a pairwise comparison. As highlighted in Table 1, the data volume (in terms of posts and comments) differed significantly for the different subreddits. In order to compensate for the difference in size, we utilised a randomisation process by repeating the same experiment 10 times with a set of 5000 randomly drawn posts for each repeat and individually for each of the two subreddits that were compared with each other. In order to compare the vocabularies of two subreddits with each other, we built dictionaries for each pair of subreddits, by retrieving all words and sequences of words (of length 2) occurring in one or both subreddits. We then used this list of words and frequently co-occurring words to classify posts into belonging to one of the two subreddits that are being compared and recorded the performance for each of the 10 cycles for each subreddit pair. The classification performance was then averaged across all 10 cycles to obtain a representative score for each pair of subreddits. Using this classification approach, high performance scores indicate a distinctive vocabulary while low performance scores suggest a shared vocabulary across both the subreddits. The results of this pairwise comparison are illustrated in Figure 1. More details are provided in the supplementary materials, covering the algorithm and randomisation steps.",2,2,2,2,0,0,2,0,2,2,1,2,0,0,0,0,1,2,3,0
https://aclanthology.org/W16-0307.pdf,12,7,,"Gkotsis, Oellrich, Hubbard, Dobson, Liakata, Velupillai, Dutta",7,0,0,0,2016,"One additional aspect that can be assessed when looking at the linguistic aspect of communications on social media is the expression of sentiment. Sentiment has been noted as a crucial indicator of how much involved someone is in a specific event (Tausczik and Pennebaker, 2010; Murphy et al., 2015), and therefore can also play a role in the expression of mental illness. Some of the conditions investigated here may have characteristic mood patterns, e.g. it is likely that someone suffering from depression will use negative sentiment and express unhappiness, while someone suffering from Bipolar Disorder may change between positive and negative mood expressions over time. However, by assessing sentiment and happiness for a large population of individuals, novel patterns for individual mental health problems may evolve. As part of our investigation, we used two different methods, one to detect sentiment (Nielsen, 2011) and another to detect happiness (Dodds et al., 2011). Both methods, which were developed for social media studies, rely on a topic-specific dictionary. For each post in our subreddits, we determined the sentiment and happiness score by matching words against the dictionaries. We accumulated these scores on a per post basis and normalised it by the square root of the number of words in the post that were identified in the respective dictionary. Scores for happiness were further normalised to assign them to the same range as the values for sentiment: negative values are expressions of negative sentiment/unhappiness, positive values are expressions of positive sentiment and happiness, and a value of 0 can be seen as neutral. We note here that while our aim is to classify both posts and comments, we limited ourselves in this task to posts only. Comments could be considered to be a source of noise, which may mask potential sentiment and happiness coming from posts, given that our data set contains a lot more comments than posts. In future work we would like to experiment with more sophisticated linguistic methods for identifying sentiment and emotion.",2,2,2,2,0,0,2,0,2,2,1,2,0,0,0,0,1,2,3,0
https://aclanthology.org/W16-0307.pdf,12,8,,"Gkotsis, Oellrich, Hubbard, Dobson, Liakata, Velupillai, Dutta",7,0,0,0,2016,"After identifying the subreddits relevant to the mental health problems we were interested in, we determined linguistic features (related to content of communications) for each of the subreddits. The results of our investigations are presented in the following subsections.",2,2,2,2,0,0,2,0,2,2,1,2,0,0,0,0,1,2,3,0
https://aclanthology.org/W16-0307.pdf,12,9,,"Gkotsis, Oellrich, Hubbard, Dobson, Liakata, Velupillai, Dutta",7,0,0,0,2016,"Table 2 provides a summary of all the linguistic features for each of the 16 subreddits, that were assessed as part of this study. From this table, we see that two subreddits stand out in a number of the assessed criteria: BiPolarSOs and cripplingalcoholism. BiPolarSOs is a subreddit that provides support and advice to people in a relationship where either one or both partners are affected by Bipolar Disorder. Note that this means that users on this subreddit may not be affected by the disorder themselves and may result in different communications from a subreddit where only people with Bipolar Disorder are communicating. In our data set it was the smallest subreddit in terms of total number of communications (see Table 1). The subreddit cripplingalcoholism aims to facilitate communication between people addicted to alcohol. In the description of the subreddit, there is no emphasis on supporting each other and people can also share what they may consider positive experiences regarding their condition (e.g. “On day 8 of a bender that was supposed to end today because my boss was supposed to send me a bunch of work on Monday. She just emailed me and said she won’t be sending it until Wednesday! Sweet chocolate Jesus on a bicycle, I did a jig in my jammies, cracked open a new handle of rye, and am about to take the dog on a nice drunken walk. Sobriety, I’ll see you Wednesday. Maybe”). From Table 2, we see that the BipolarSOs subreddit not only has a higher number of first-person pronouns and a larger number of definite articles, but also that the average sentence seems to be more complex due to a high average height of the sentence parse trees, long verb clauses and a high number of subordinating conjunctions, while the average number of sentences per communication is comparable to those of the other subreddits. This suggests that people posting on this subreddit explain in detail their experience or advice. On the contrary, the cripplingalcoholism subreddit possesses shorter communications characterised by the lowest number of sentences per communication, the smallest maximum height of sentence trees, a low number of subordinate conjunctions and short verb clauses. Using word frequency occurrences, we also observed that here the language seems stronger than on other subreddits with the most frequently occurring word being ”fuck” (details of results not provided here8 ). The two features relating to lexical cohesion (by means of adjacent sentences using similar words, LF10 and LF11 in Table 2), show little variation across all the 16 different subreddits. Though cohesion when only taking nouns and pronouns into consideration improves, the best value obtained is 0.22, indicating a mostly low lexical cohesion across communications on each of the subreddits. One of our longer term goals is to be able to classify posts to individual subreddits, and these scores would not be sufficiently informative for this goal due to their low variation.",2,2,2,2,0,0,2,0,2,2,1,2,0,0,0,0,1,2,3,0
https://aclanthology.org/W16-0307.pdf,12,10,,"Gkotsis, Oellrich, Hubbard, Dobson, Liakata, Velupillai, Dutta",7,0,0,0,2016,"The word occurrence-based comparison of the subreddits was performed to better determine whether subreddits can be distinguished based on their lexical content (see supplementary material for more information). The results obtained are shown in Figure 1. Figure 1 shows that apart from a small number of exceptions, the language of individual subreddits is discriminable, which can be further exploited for classification purposes in later stages. For example, the subreddit OpiateRecovery shows mostly high values, which means that the language used (based on frequency of words and word pairs) on subreddit is mostly unique. OpiateRecovery shows some vocabulary overlap with the opiates and addiction subreddits, which suggests that there are some shared topics on these subreddits. One of the exceptions is the subreddit addiction. As illustrated in the heatmap the addiction subreddit shows particularly low values with other subreddits such as depression and suicidewatch. This finding is not surprising as substance addiction can lead to depression and suicidal thoughts, which is expected to be also expressed in the nature of the communication. Note that the diagonal of the matrix is suppressed to reduce the matrix dimension. Among our 16 subreddits, there are some subreddits that allude to the same mental health condition, e.g. BipolarReddit and BipolarSOs both aim to foster a community to facilitate exchange about Bipolar Disorder. While the subreddit BipolarSOs invites participation from users that are affected themselves or are in a relationship with someone affected by Bipolar Disorder, BipolarReddit is solely focussed on people suffering from this disorder. In Figure 1, we can also see that vocabularies seem to be partially shared (indicated by a lighter colour) across those subreddits addressing the same mental health problem. For example, all three subreddits relating to Bipolar Disorder (bipolar, BipolarReddit and BipolarSOs) show a pairwise score of ∼ 0.6 as opposed to ∼ 0.9 with other subreddits. Similarly, both the self-harm subreddits also share a pairwise vocabulary of ∼ 0.6. Interestingly, the subreddits autism and schizophrenia also indicate a proximity of the vocabularies and further investigations are required to assess the shared vocabularies.",2,2,2,2,0,0,2,0,2,2,1,2,0,0,0,0,1,2,3,0
https://aclanthology.org/W16-0307.pdf,12,11,,"Gkotsis, Oellrich, Hubbard, Dobson, Liakata, Velupillai, Dutta",7,0,0,0,2016,"In order to assess the emotions that Reddit users express on subreddits related to mental health problems, we used two different methods: (i) to assess sentiment and (ii) to specifically assess happiness. The results obtained by both methods are shown in Figure 2. This figure illustrates that, on average, a lot of negative sentiment is expressed across the different subreddits relating to mental health problems. We can see that posts from the subreddit SuicideWatch express the highest rate of negative sentiment, followed by posts from the Anxiety and self-harmrelated subreddits. While in the majority of cases both sentiment and happiness expression possess the same direction (i.e. either positive or negative), in a number of subreddits this is not the case. For example, the subreddit cripplingalcoholism shows expressions of happiness as well as the expression of negative sentiment. As alluded to earlier, this particular subreddit includes people that see alcoholism as a lifestyle choice. Though there may be happiness expressions related to overcoming alcoholism, there are also happiness expressions relating to the glorification of alcoholism, e.g. “[...] At the bottom of this pile of clothes is a full pint! How it came to rest there I don’t know, but thank you Taaka gods for your gift on this day. [...]”. Furthermore, Figure 2 shows a small number of subreddits, where posts seem to express positive sentiment. For example, the posts extracted from the subreddit OpiatesRecovery seem to express not only positive sentiment but also happiness. This particular subreddit aims to foster a comunity that focusses on helping each other get through opiate withdrawals and users can post their progress. While there are posts that discuss relapses, there are statements such as “[...] I’m happy to say the shivers/flashes/heebeegeebees are a lot, lot better. Not 100% gone, but gone enough. I can deal with flashes every 4-6 hours, cant deal with them every 15 minutes. [...]” to share the successes made during withdrawal. The results shown in this figure are average values, which means that subreddits that show an overall tendency to happiness and positive sentiment, may contain some posts including words of negative sentiment and unhappiness, e.g. “[...] Buying garbage from some ignorant thug to put into my fucking blood knowing how lethal it can be, but oh it couldn’t happen to me. It’s bizarre that after all this time of staying away I still can’t fully grasp how fucking close to death I was every day. [...]” from OpiateR",2,2,2,2,0,0,2,0,2,2,1,2,0,0,0,0,1,2,3,0
https://aclanthology.org/W16-0307.pdf,12,12,,"Gkotsis, Oellrich, Hubbard, Dobson, Liakata, Velupillai, Dutta",7,0,0,0,2016,"cussion In our study, we analysed 16 different subreddits covering a range of mental health problems (see supplementary material for more details). In our selection, there are subreddits with overlapping content, e.g. StopSelfHarm and selfharm. We conducted an analysis based on a selection of linguistic features and found that most of the subreddits that are topicunrelated, possess a unique vocabulary (in terms of words/word-pairs and the frequencies thereof) and discriminating lexical and syntactic features. We also observed differences in sentiment and happiness expressions, which can give further clues about the nature of a post. As symptoms are shared across conditions and more so, some of the mental health problems are cooccurring (e.g. anxiety and depression), medications and treatment strategies are shared across the different illnesses, too. This, in consequence, means that part of the vocabulary and thoughts across the different subreddits are shared, making it harder tguish between the different subreddits and, consequently, the condition in question. Given the latter, it is even more surprising that the similarity matrix shown in Figure 1 shows a good separation of topicspecific vocabularies on subreddits. With respect to the expression of sentiment and emotions, further work is needed. The methods applied here were developed based on Twitter data and further investigations are necessary to find the parts of the dictionary that are overlapping and an expertguided assessment as to whether the recognised expressions are representative and meaningful in the context of mental health problems. A previous study has investigated how support is expressed in social media (Wang et al., 2015) and can be leveraged in future work to see whether similar support models hold true for the subreddits concerning mental health conditions. Moreover, the methods we have used so far are based on lexica, which lack contextual information. In future work, we plan to add more contextualised semantic methods for determining sentiment and emotions. One limitation of the work presented here is that we did not include any subreddits that are unrelated to mental health. For example, we could have included a subreddit such as Showerthoughts into our subset to assess which of the features are unique to mental health problems only. However, this would require the definition of what is a truly unrelated subreddit and variety of topics so that the control set is not biased in itself. Furthermore, as our primary aim was to build a classifier that distinguishes several mental health problems based on the findings reported here, an implicit assumption is that a post is by default relevant to mental health conditions and does not need to be classified as such. Nevertheless, we plan to address this limitation in future work.",2,2,2,2,0,0,2,0,2,2,1,2,0,0,0,0,1,2,3,0
https://aclanthology.org/W16-0307.pdf,12,13,,"Gkotsis, Oellrich, Hubbard, Dobson, Liakata, Velupillai, Dutta",7,0,0,0,2016,"After extracting data from several subreddits pertaining to mental health problems, we investigated a subset of language features to determine discriminatory characteristics for each of the subreddits. Our results suggest that there are discriminatory linguistic features among subreddits, such as sentence complexity or vocabulary usage. We could also show that while mostly all subreddits relating to mental health problems possess highly negative sentiment, there are a number of subreddits, where positive sentiment and happiness can be observed in posts. However, in order to determine the most discriminative features between different mental health conditions, additional work is required continuing from the results shown here. In conclusion, these results pave the way for future work on classification of posts and comments concerning a mental health condition, which in turn could allow the assignment of urgency markers to address a specific communication.",2,2,2,2,0,0,2,0,2,2,1,2,0,0,0,0,1,2,3,0
https://www.nature.com/articles/srep45141,13,1,,"Gkotsis, Oellrich, Hubbard, Dobson, Liakata, Velupillai, Dutta",7,0,0,0,2017,"The number of people affected by mental illness is on the increase and with it the burden on health and social care use, as well as the loss of both productivity and quality-adjusted life-years. Natural language processing of electronic health records is increasingly used to study mental health conditions and risk behaviours on a large scale. However, narrative notes written by clinicians do not capture first-hand the patients’ own experiences, and only record cross-sectional, professional impressions at the point of care. Social media platforms have become a source of ‘in the moment’ daily exchange, with topics including well-being and mental health. In this study, we analysed posts from the social media platform Reddit and developed classifiers to recognise and classify posts related to mental illness according to 11 disorder themes. Using a neural network and deep learning approach, we could automatically recognise mental illness-related posts in our balenced dataset with an accuracy of 91.08% and select the correct theme with a weighted average accuracy of 71.37%. We believe that these results are a first step in developing methods to characterise large amounts of user-generated content that could support content curation and targeted interventions.",2,2,2,1,0,0,2,0,2,2,2,1,1,0,0,0,2,2,5,1
https://www.nature.com/articles/srep45141,13,2,,"Gkotsis, Oellrich, Hubbard, Dobson, Liakata, Velupillai, Dutta",7,0,0,0,2017,"Mental and substance use disorders are the leading cause of years lived with disability worldwide and in 2010 accounted for 7.4% of years of productive life lost due to disability1. Natural language processing of electronic health records (EHRs) is increasingly being used to study mental illness2 and risk behaviours in much closer detail than previously3. However, narrative notes are written by clinicians who record those positive findings and relevant negatives that guide their subsequent diagnosis and treatment plan for the patient4. Although EHRs allow clinicians to synthesise disparate facts making them interpretable by other clinicians, they do not “paint a full picture” of the patient experience of a mental health problem, particularly as patients may answer interview questions in a manner that they perceive will be viewed favourably by their clinician. Moreover, as patient records are only written based on meetings with their healthcare provider, critical changes in patient behaviour and wellbeing may not be recognised either immediately or at all due to a time delay in reporting, thus preventing certain real time interventions. Social media is changing the way people self-identify as suffering from a disorder and how they communicate with others with similar experiences, often asking about side effects from treatments, or sharing coping skills, and thereby feeling less isolation or stigma. Studying popular social media platforms such as Twitter and Reddit5,6 holds the key to understanding what concerns patients (rather than clinicians) most. Furthermore, this type of large-scale user-generated content offers the opportunity to understand mechanisms underlying mental health conditions at an unprecedented level. For instance, studies of children and adolescents have already shown that high daily use of social networking sites may be independently associated with poor self-rating of mental health and experiences of higher levels of psychological distress and suicidal ideation7. Following on from our initial study8, here we present the second phase of our research on classifying user-generated content from Reddit that is related to different types of mental health conditions. We aimed to study the most epidemiologically prevalent and clinically burdensome mental health conditions: depression, bipolar disorder, anxiety disorders, schizophrenia, as described in the study by Whiteford et al.1. As in this most recent global clinical study, we also studied drug use disorders: general addiction, opiate addiction and alcoholism separately. With as many as 80% of suicide deaths attributable to mental and substance use disorders it was also relevant to study both suicide and self-harm behaviour. We captured pervasive developmental disorders by studying autism and regarded it as important to also include borderline personality disorder as an important example of a group for whom sparse global epidemiological data is available1 but who form a large, and active, online community. In this paper, we address the problem of automated classification of mental health-related content on a social media platform. Our long-term goal is to not only aid clinical researchers, epidemiologists and policy makers in understanding and addressing communications on social media, but also to provide support for people suffering from mental illness. Here, we propose an approach to automatically identify Reddit posts related to mental health (binary classification) and then classify mental health-related posts according to theme-based subreddit groupings (multiclass classification) using deep learning techniques. Accompanied with manual content and topic analysis, we employ a theme-based approach to this problem, and compare results with alternative machine learning baselines. With Reddit providing monitoring and support through volunteers, being able to classify posts according to themes could enable alerting moderators by reposting posts to relevant communication threads if they have not been posted there yet. We also believe that the results and the methodology presented here is useful to gain a deeper understanding and to characterise language and content in these types of forums, and could be the first step in the development of targeted interventions using social media.",2,2,2,1,0,0,2,0,2,2,2,1,1,0,0,0,2,2,5,1
https://www.nature.com/articles/srep45141,13,3,,"Gkotsis, Oellrich, Hubbard, Dobson, Liakata, Velupillai, Dutta",7,0,0,0,2017,"The basis of this study relies on the analysis of data extracted from Reddit. Reddit is a social media platform with over 234 million users (https://about.reddit.com/advertise/, accessed November 2016) that communicate in topic-specific communities, called subreddits. A report issued by PewResearchCenter showed that 6% of adult internet users frequently visit Reddit, and men are twice as likely than women to access Reddit contents9. Posts are not character-limited and users can freely express their thoughts in as many words and in as much detail as they wish. Subreddits can be created by users and there is no restriction on this, even when a similar subreddit already exists. As a consequence, there may be several subreddits discussing a specific mental health condition. In order to assess whether or not posts are relevant to one mental health condition, and which health condition a relevant post belongs to, we developed an approach that combines manual assessment steps with automated topic detection and the application of deep learning classification of Reddit data. The general steps in our study are illustrated in Fig. 1 and further explained in the methods section. The first step includes the storage and indexing of the complete Reddit dataset (1). From this set, a semi-supervised discovery of relevant, mental health-related subreddits is applied (2). These subreddits are further analysed through a manual assessment step and the final list of subreddits is selected (3). The fourth step includes the grouping of subreddits that share the same mental health-related theme (4). The preparation of a control dataset is then performed (5), to be used for the binary classification task of posts as containing mental health-related content or not. The final steps include topic detection applied on our themes for confirmation of relevant content (6) and the two tasks of content (binary and multiclass) classification (7).",2,2,2,1,0,0,2,0,2,2,2,1,1,0,0,0,2,2,5,1
https://www.nature.com/articles/srep45141,13,4,,"Gkotsis, Oellrich, Hubbard, Dobson, Liakata, Velupillai, Dutta",7,0,0,0,2017,"A manual content analysis was performed independently by two researchers. The aim was to reach a conclusion as to whether the post content reflects the mental health condition it relates to, i.e. the name of the subreddit. Ten random samples from each of the 16 subreddits (see methods section about the selection of the subreddits) were extracted, resulting in a set (gold standard) of mental health-related themes. This manual analysis revealed that the post content in the majority of the subreddits was relevant to the mental health condition its title suggests. Most commonly, the posts were introduced by a descriptive section where the author of the post gave some context to the reason for writing, followed by a request for advice or help. The descriptive section typically contained highly relevant terms and condition-specific content. One subreddit related to neurodevelopmental disorders (Aspergers) was deemed to be mainly distantly related to the condition itself, as a large proportion of posts (6/10) were not directly condition-related (e.g. requesting advice about recently diagnosed family members without details about the condition). Therefore, this was the only subreddit where the assumption that a subreddit post content is characteristic of its mental health condition did not hold, and it was excluded from the gold standard. For the remaining 15 subreddits, the conclusion was that the post content was indeed characteristic of the intended mental health condition. Seven of these could be grouped into three mental health condition themes, resulting in a total of 11 themes (see Table 1). The language in each of the resulting mental illness themes often contained specific references related to symptoms, treatments or experiences. The depression subreddit (by far the largest in terms of size: 42% of the total mental health related posts) was most disparate. For instance, it was common to find descriptions of situations and events experienced by post authors who were not necessarily diagnosed with depression, but where the user was saddened or negatively affected by some circumstance and used the word depressed colloquially rather than in a medical sense. The themes related to self harming and suicide (selfharm and SuicideWatch) often contained explicit references to actions and thoughts, highly relevant to the condition. The drug use disorder themes (opiates, cripplingalcoholism and addiction) typically contained mentions of specific substances, medications and drugs, in combination with requests for help or advise (e.g. experiences with changing or trying specific drugs). Posts in the cripplingalcoholism subreddit were often more implicit, with a language use more indicative of being under the influence, rather than explicitly listing condition-related content. The only other subreddit related to neurodevelopmental disorders (autism) did, unlike the content in the Aspergers subreddit, predominantly contain disease-related descriptions about family members or close relatives along with questions about others’ experiences or requests for help related to the condition.",2,2,2,1,0,0,2,0,2,2,2,1,1,0,0,0,2,2,5,1
https://www.nature.com/articles/srep45141,13,5,,"Gkotsis, Oellrich, Hubbard, Dobson, Liakata, Velupillai, Dutta",7,0,0,0,2017,"As the manual characterisation was conducted only on a small set of posts for each of the subreddits, we added an additional assessment step of the generated themes using topic modelling. Topic modelling approaches use probability distributions of words to determine the topics that are represented in a collection of textual data. In this context, a topic is a collection of words that are assumed to be semantically related. This means that if we are able to extract topics that are closely related to the themes that we were hoping to represent, it is likely that the grouping of the themes is correct. Furthermore, the topics in themselves can provide some insights into what it is that patients and/or carers are most concerned about when facing a mental health condition. The topic models we obtained for the 11 themes described in the previous section showed high relevance between the content of the posts and the particular mental health condition theme they had been assigned to, in line with the conclusions of the manual assessment. The entire list of topic models can be found in our supplement. Looking across the topic models of all the themes, we can see that there are various aspects of mental illnesses covered: symptoms, medications, potential side effects, treatment alternatives and impacts/consequences on life. For example, one of the topics formed for the anxiety theme (see topic 9, supplement) contains words such as ‘panic’, ‘attack’, ‘feel’, ‘like’, and ‘heart’. Anxiety sufferers in particular often see their general practitioner (GP) because they assume they have had a heart attack rather than suffering from anxiety. Furthermore, palpitations are a common symptom of anxiety, especially when panic attacks are present10. Similarly, one of the topics related to the bipolar theme (see topic 4, supplement) contains words like ‘lamictal’, ‘seroquel’, ‘lithium’, ‘take’, ‘meds’ and ‘taking’. In addition to words indicating the existence of a prescription for the medication, all the medications mentioned are used as mood stabilisers in the treatment of bipolar disorder. In our manual analysis, two of the themes were highlighted as being less homogenous (depression) or diverging from the rest of the themes (cripplingalcoholism). However, in the topic analysis, we can still see clusters forming that are relevant to both conditions and the nature of the subreddit. For example, one of the clusters (see topic 5, supplement) for depression includes the words ‘don’t’, ‘job’, ‘money’, ‘pay’, ‘live’ and ‘life’, which could allude to the fact that people suffering from depression have difficulties keeping up with the responsibilities of a job, which may result in loss of income and the creation of obstacles in life11. The cripplingalcoholism theme has e.g. a cluster (see topic 2, supplement) containing words such as ‘like’, ‘cheers’, ‘drinking’, ‘day’, ‘happy’ and ’drink’, which suggests a glorifying aspect to being alcohol-dependent. In fact, the subreddit itself tries to involve not only those who want to defeat their addiction but also those who are content with possessing this addiction. The aforementioned cluster clearly shows evidence for the latter group of posters.",2,2,2,1,0,0,2,0,2,2,2,1,1,0,0,0,2,2,5,1
https://www.nature.com/articles/srep45141,13,6,,"Gkotsis, Oellrich, Hubbard, Dobson, Liakata, Velupillai, Dutta",7,0,0,0,2017,"The process of automatically classifying Reddit posts was separated into two individual tasks. The first task was to classify the posts as being either mental health related or not (binary classification). The second task focussed on the classification of the manually defined and automatically evaluated themes, to determine which mental health theme the post belongs to (multiclass (n = 11) classification). Results presented below predominantly concern the output of the Convolutional Neural Network (CNN) based approach, since this consistently yielded the best prediction. For the training and evaluation of all classifiers, we used identical, randomly extracted, 80–20 training-test split across the entire dataset, and we report on the results concerning the test dataset. Further details about the classifiers and settings are presented in the Methods section.",2,2,2,1,0,0,2,0,2,2,2,1,1,0,0,0,2,2,5,1
https://www.nature.com/articles/srep45141,13,7,,"Gkotsis, Oellrich, Hubbard, Dobson, Liakata, Velupillai, Dutta",7,0,0,0,2017,"The best approach (CNN) yields an accuracy of 91.08% for an almost balanced dataset: 52% of the overall posts are related to mental health (see Tables 2 and 3). The other classifiers’ results were lower, with the Feed Forward (FF) being marginally worse (90.79%). The text was preprocessed identically for all four classifiers. A linear classifier achieves 85.84% accuracy whereas an SVM-based classifier achieves 86% accuracy.",2,2,2,1,0,0,2,0,2,2,2,1,1,0,0,0,2,2,5,1
https://www.nature.com/articles/srep45141,13,8,,"Gkotsis, Oellrich, Hubbard, Dobson, Liakata, Velupillai, Dutta",7,0,0,0,2017,"The second set of results relate to the task of classifying mental health-related posts into one of the 11 themes. Figure 2 shows the resulting confusion matrix using a CNN approach. The figure shows that the left-to-right diagonal holds high values, and in the majority of the cases, the Reddit posts are classified correctly. Table 4 presents the evaluation metrics (precision, recall and F-Measure) for each individual theme as well as the weighted average when using the CNN-based approach. The table shows that all results remain high, resulting in an overall weighted average of 0.72 precision, 0.72 recall and 0.72 F-Measure (FM). Results for individual themes vary, ranging from 0.52 FM (addiction) to 0.80 (Opiates). Precision is in general higher (>0.70) than recall, except for Opiates, depression and cripplingalcoholism. Finally, Table 5 illustrates the overall accuracy for all four different classifiers. The Mean Reciprocal Rank (MRR) measure weighs in the probability distribution for each prediction and takes into account the position (in ranked order) of the true class in comparison to the actual prediction. More details on this measure are provided in the Methods section. As expected, MRR is consistently higher than accuracy for all classifiers and also shows that posts can transcend multiple themes (see Discussion section).",2,2,2,1,0,0,2,0,2,2,2,1,1,0,0,0,2,2,5,1
https://www.nature.com/articles/srep45141,13,9,,"Gkotsis, Oellrich, Hubbard, Dobson, Liakata, Velupillai, Dutta",7,0,0,0,2017,"In our study, we addressed the problem of characterising and automatically classifying user-generated content on the social media platform Reddit for the case of mental health conditions. We manually investigated data originating from several subreddits to group the posts into overarching themes. The derived grouping of posts into themes was further evaluated by applying topic detection algorithms and results suggested that the theme-based grouping was valid. We then applied two classification strategies, a binary classification to determine whether or not a post contains mental health related content, and multiclass classification to identify the mental health condition (theme) a post is referring to. Our results show that by applying a CNN approach in the binary classification task, we achieve an accuracy of 91.08% in distinguising the mental health related posts from unrelated posts. Further to that, we can identify Reddit posts as belonging to one of the 11 defined themes with an overall weighted precision of 0.72 and a recall of 0.71 (0.72 FM). Taken in conjunction, these results suggest that we can reliably identify mental health content and determine the mental health theme that is further referred to in the post, assuming that there is sufficient data for disorders to train the model on. In our approach, we assessed posts from several subreddits that were grouped into themes. Despite the manual and automated verification of the generated themes, there is an intrinsic overlap of post content across the themes due to the relatedness of mental health conditions. For example, as can be seen from the confusion matrix in Fig 2, a large number of Opiates posts are misclassified as cripplingalcoholism and vice versa. Both themes refer to a substance addiction, indicating shared language features to a larger degree than with other subreddits. Furthermore, it is worth noting that both subreddits contain strong language that is not as clearly represented in any of the other subreddits, and both subreddits allow for posts that embrace addiction. Similarly, a high rate of posts are misclassified between the selfharm and SuicideWatch subreddits, both of which feature behaviours associated with mental and emotional distress. We chose to use topic modelling as a means of automatically obtaining key characteristic words and phrases from the posts, as it has been applied successfully in similar settings in other studies (e.g. Schwartz et al. 201612). The topic models (shown in supplement; each topic is using the 10 most descriptive keywords) align with the manually created themes, and an extension of this work could be to further optimise the validation approach by scaling the number of topics retrieved with the number of available posts in the theme. The control group built for the first task (see section “Control dataset generation” in Methods for more details) utilises the author information of users who post on mental health-related subreddits. We opted for this approach due to the long computation times required for calculating age and gender estimates for individual posts and the number of posts that need to be matched. In general, age and gender matching is desired to avoid influences originating from gender or age differences in language use. While we used all users posting on mental health subreddits, only a subset of authors appears in the control dataset (around 9% of the users; 32,280 appear in the non mental health subreddits and 348,040 appear in the mental health subreddits). Moreover, this intersection of 32,280 users/authors is responsible for 62,513 posts in the mental health subreddits, which corresponds to only 13.6% of the mental health posts (see Supplement, Figure S1 for more details). This may be partially explained by users not wanting to be recognised by other Reddit users or peers and therefore disguising their identity by using so called throwaway accounts13. Hence, the results presented here concern cases where authors between the two classes are largely different. Our study does not focus on such cases, where authors post simultaneously under both classes. It has been demonstrated in the past that characteristic linguistic features are apparent in those suffering from mental illness12,14,15,16. This means that our dataset could have limitations in being applicable to assess linguistic differences between mental health and non-mental health related posts, as linguistic features are likely to be present in both the mental health and control posts. With regards to our results, this is likely to increase the difficulty of our classification task, as we are aiming to use the content to determine mental health and non-mental health posts. On the contrary, if we were to create a control group that is entirely unrelated, it is possible that our performance measures (currently best-performing: 91.08% accuracy for CNN) might increase. Concerning the classification tasks and the deep learning approaches presented, we experimented with different settings and thresholds. We considered pre-trained word vectors as input to the classifiers (e.g. using Glove’s Common Crawl containing 840 Billion tokens17), but the results did not improve. We attribute this to the size of our dataset which is adequate for representing the language within the corpus. Furthermore, we experimented with increasing the complexity of our networks in different parts, such as the dimensions of the input vectors (16), the overall topology of the network, as well as different activation and objective functions. In all the configurations experimented with, the accuracy of the classification did not improve, which is why we kept the simplest configuration. Our results for the multiclass classification are satisfactory, and provide some interesting insights. There is some variability in class-level results, for instance, for Opiates, FM is as high as 0.80, while for addiction results are lower (0.52). When studying the confusion matrix (Fig. 2), the most common misclassification is depression, for which there are two main explanations. The first reason is that depression is the most populous class, so the training of the classifier may have resulted in a bias favouring this class. The second reason is that depression is often a secondary symptom of other mental health problems and shows considerable comorbidity with other mental health disorders18. This is also confirmed through our manual survey of the content of the posts (see section “Manual characterisation of subreddits”), where it was found that posts from different themes may also concern depression, and where posts in the depression subreddit may be related to more than just depression. To further support the above, we conducted an experiment whereby we removed the most prevalent class-theme (depression, which accounts for 42% of the posts) and repeated the same multiclass classification task. Contrary to the typical behaviour in such settings, the removal of the most prevalent class resulted in an increase in the accuracy. More specifically, all classifiers improved and CNN performs the best with 79,8% accuracy (more than 8% improvement). This finding further supports the notion that the classes in our problem are neither orthogonal nor mutually exclusive in nature. When considering a less strict evaluation metric (i.e. instead of True/False as in classification tasks) such as Mean Reciprocal Rank (i.e. the position at which a theme ranked compared to the actual class), the results improve significantly (more than 12% across all 4 classifiers). This finding also highlights the fact that some of the themes are highly inter-related and not always distinguishable as separate and exclusive classes. For example, a common symptom of depression is suicidal ideation, which in consequence means that a person posting on depression may express suicidal thoughts, which are the content of communications on SuicideWatch. Concerning the binary classification task, we notice that the simple FF network achieves similar performance to the more sophisticated CNN (less than 0.3% drop in accuracy), concluding that the binary classification problem is captured sufficiently using a more simple neural network. One limitation of our approach is the number of mental health themes that are used in the multiclass classification task. In order to cover more themes, our model built for the multiclass classification would require additional data. However, the way the approach is implemented, more themes can be added over time and the only cost is that the prediction model needs to be retrained and performance re-assessed. In conclusion, our suggested method is applicable to the identification of posts relevant to a mental health subreddit as well as the identification of the actual mental health theme they relate to. Being able to classify posts in this manner is the first step in the direction of targeted interventions, e.g. by redirecting posts that seem in need of urgent moderator attention. In future work, we aim to further improve the classification into mental health themes (multiclass classification), address inter-related themes e.g. by employing hierarchical structures, increase the coverage of mental health themes and devise an “urgency scheme” to come closer to the aforementioned notification system in the frame of real-time, personalised interventions.",2,2,2,1,0,0,2,0,2,2,2,1,1,0,0,0,2,2,5,1
https://www.nature.com/articles/srep45141,13,10,,"Gkotsis, Oellrich, Hubbard, Dobson, Liakata, Velupillai, Dutta",7,0,0,0,2017,"For the purpose of this study, we downloaded a dataset from https://redd.it/3mg812 that contained all the posts made to Reddit (independent from subreddits) from 01.01.2006 to 31.08.2015. This dataset is publicly available and was generated by a Reddit user through accessing the Reddit Application Programming Interface (https://www.reddit.com/dev/api). In order to discover the relevant subreddits, we used the Elasticsearch (https://www.elastic.co/) search engine to index the complete dataset. Our analysis reports on aggregated data only, which adheres to Reddit’s published terms and conditions.",2,2,2,1,0,0,2,0,2,2,2,1,1,0,0,0,2,2,5,1
https://www.nature.com/articles/srep45141,13,11,,"Gkotsis, Oellrich, Hubbard, Dobson, Liakata, Velupillai, Dutta",7,0,0,0,2017,"As of November 2016, there are over 900,000 subreddits (http://redditmetrics.com/history). Each subreddit invites users to participate in a discussion concentrated around a specific topic. The topics can vary, from a general to a very specific topic. For instance, e.g. the subreddit Ask Me Anything (AMA) is about a single person answering questions posed by the public and has around 140,000 subscribers. Similarly, the subreddit Playstation invites users around the popular game console and has 36,000 subscribers. In order to identify the subreddits that are related to mental health, we adopted a semi-supervised approach. The first step was to leverage on a pre-existing keyword list curated by domain experts, for the purpose of identifying social media content linked to mental health19. From the obtained candidate list of subreddits, we assessed the subreddits with a high number of posts and manually selected those that corresponded to the conditions of interest for this study. All relevant subreddits and their descriptions are provided in the Supplementary material.",2,2,2,1,0,0,2,0,2,2,2,1,1,0,0,0,2,2,5,1
https://www.nature.com/articles/srep45141,13,12,,"Gkotsis, Oellrich, Hubbard, Dobson, Liakata, Velupillai, Dutta",7,0,0,0,2017,"To gain a deeper understanding of the nature of posts on these subreddits, we conducted a manual characterisation of posts contained in the selected subreddits. For this purpose,10 random posts (including their title) were collected from each of the subreddits. Two of the authors of this manuscript (AO, SV) independently read through these posts and collected notes as to whether the post was relevant to the condition the subreddit was selected from, the post concerned the poster or someone else and what the type of the post was (request for help, engaging, sharing story, etc.). A joint discussion was held to finalise the set of subreddits and themes to use for classification and identify potential differences in interpretation.",2,2,2,1,0,0,2,0,2,2,2,1,1,0,0,0,2,2,5,1
https://www.nature.com/articles/srep45141,13,13,,"Gkotsis, Oellrich, Hubbard, Dobson, Liakata, Velupillai, Dutta",7,0,0,0,2017,"In order to identify Reddit posts concerning mental health, we generated a control dataset that is non-mental health related. The first step included the collection of all authors from our mental health dataset together with the date of their first (index) posting in any one of the mental health subreddits. For each author-date pair, we retrieved all of their posts across any other subreddit and defined these as non-mental health related. In order to avoid time overlap in the generation of the control dataset, we only kept non-mental health posts by authors that were at least 180 days older than their index post in a mental health subreddit. The 180 day window was chosen both to ensure a time clinically distant from the mental state at the index post of the mental health subreddit, and to result in a balanced dataset.",2,2,2,1,0,0,2,0,2,2,2,1,1,0,0,0,2,2,5,1
https://www.nature.com/articles/srep45141,13,14,,"Gkotsis, Oellrich, Hubbard, Dobson, Liakata, Velupillai, Dutta",7,0,0,0,2017,"As the manual characterisation only covered a very small proportion of posts, we aimed to verify the grouping of the posts by building topic models across the themes. Topic models identify groups of words that are related to each other and therefore constitute a topic, presented in textual data. Here, we used the Latent Dirichlet Allocation implementation as provided by the Python package gensim20 while setting the number of topics to 10 (n = 10) and rest of the parameters to default. Data from each of the relevant themes was handled and processed individually. For this purpose, if a theme contained more than one subreddit, all the posts of the relevant subreddits were merged together before processing. We removed English language stop words as indicated in NLTK21 (frequently used words, such as “and” and “the”, that are expected to carry little characterising information; see supplement for more information) from all the posts before determining topics. The underlying assumption when using topic models in this case is that a grouping of subreddits in cohesive groups will result in topic models that are relevant to aspects of the corresponding theme (e.g. symptoms or treatments). While topic modelling can be used for validating the grouping of subreddits, it possesses the added benefit of providing insights into the contents discussed on the subreddits investigated.",2,2,2,1,0,0,2,0,2,2,2,1,1,0,0,0,2,2,5,1
https://www.nature.com/articles/srep45141,13,15,,"Gkotsis, Oellrich, Hubbard, Dobson, Liakata, Velupillai, Dutta",7,0,0,0,2017,"In our evaluation, we used the following metrics: where P is the set of classified posts and ranki refers to the rank position of the relevant theme for the i-th post.",2,2,2,1,0,0,2,0,2,2,2,1,1,0,0,0,2,2,5,1
https://www.nature.com/articles/srep45141,13,16,,"Gkotsis, Oellrich, Hubbard, Dobson, Liakata, Velupillai, Dutta",7,0,0,0,2017,"We considered two different classification problems. The first concerns the binary classification of posts as being relevant to mental health or not. The second classification problem deals with the multiclass classification of the 11 mental health themes resulting from the manual and topic assessment steps. For both tasks, we applied the same methodology and evaluation approach, which includes training four different classifiers (FF, CNN, Linear regression, SVM, explained below). The first step towards classifying the Reddit posts was to preprocess the corpus. This step included the concatenation of the post title with the body of the post. We then transformed all text into lower case. Afterwards, we prepared the text to be represented as Word Embeddings22 within the Neural Network. This was achieved by creating a word dictionary from the corpus itself, for which we kept the 5,000 most frequent words (including stopwords, which accounts for 99.999% of the total number words). The last step was to replace all words with their indices, so as to represent each post as an array of integers. For the words that were not frequent enough (lower than 0.001% of the total number of words), a single, unique integer was assigned. Finally, we only kept the first 300 words/integers for each post. The first two classifiers are based on a Neural Network (NN) architecture. Both include the same number of layers and are trained with a similar approach. The first classifier is a Feed Forward NN (see Fig. 3a). The second classifier implements a Convolutional NN, with a filter window of 3 (see Fig. 3b). The first layer takes as input the array of integers representing the content and implements an Embedding layer where each word vector has a dimension of 16. The intermediate layer is implemented using a dense structure with 64 (FF) and 256 (CNN) neurons that are used to represent the feature space. The final layer contains the number of output nodes needed for the classification task (i.e. 2 for the binary and 11 for the multiclass classification) As shown in Fig. 3, the first layer in both networks is an Embedding layer that turns an array of integers into a vector of size 16 (word embeddings). For the CNN, we added an extra, convolutional layer with a 5-length filter and max pooling with length 2. For the FF, the equivalent was a flattened layer. The next step for both networks is a dropout filter of value 0.25 (to avoid overfitting) and a dense layer of 64 and 256 neurons for the FF and CNN, respectively. We used rectified linear unit (ReLU) as activation function and the objective function was categorical cross entropy. For the training process, we set a maximum of 100 epochs with a stopping criterion of 5 epochs for no increase in the objective function. For both networks and classification problems, training stopped before 25 epochs had been reached. We also considered two more classification approaches as a baseline. Both use the same input as the NNs described above but implement a linear regression and an SVM-based classification approach. We consider these to be our baselines. The details for these approaches are provided in the supplement. All four classification approaches were trained using the same training-test split (80/20) on the dataset and the same preprocessing steps.",2,2,2,1,0,0,2,0,2,2,2,1,1,0,0,0,2,2,5,1
https://dl.acm.org/doi/abs/10.1145/3290605.3300364,14,1,,"Ernala, Birnbaum, Candan, Rizvi, Sterling, Kane, De Choudhury",7,1,0,0,2019,"A growing body of research is combining social media data with machine learning to predict mental health states of individuals. An implication of this research lies in informing evidence-based diagnosis and treatment. However, obtaining clinically valid diagnostic information from sensitive patient populations is challenging. Consequently, researchers have operationalized characteristic online behaviors as “proxy diagnostic signals” for building these models. This paper posits a challenge in using these diagnostic signals, purported to support clinical decision-making. Focusing on three commonly used proxy diagnostic signals derived from social media, we find that predictive models built on these data, although offer strong internal validity, suffer from poor external validity when tested onmental health patients.A deeper dive revealsissues of population and sampling bias, as well as of uncertainty in construct validity inherent in these proxies. We discuss the methodological and clinical implications of these gaps and provide remedial guidelines for future research.",2,2,2,0,2,2,2,2,6,2,2,2,0,2,0,2,2,1,7,0
https://dl.acm.org/doi/abs/10.1145/3290605.3300364,14,2,,"Ernala, Birnbaum, Candan, Rizvi, Sterling, Kane, De Choudhury",7,1,0,0,2019,"With rising volumes of data and pervasive use, social media has been widely adopted as a lens to provide insights into behaviors [52], mood [42], psychological traits and states [5, 53], and social interactions of individuals [56]. For mental health, a growing body of work, including that in the human computer interaction (HCI) field, is leveraging naturalistic, unobtrusive data from social media to predict mental health states of individuals [21, 25, 28, 29, 31, 34]. Parallel to HCI, in an emergent field called “digital psychiatry” [100], clinicians are exploring the efficacy of diagnostic predictions from online data for early diagnosis, evidence-based treatment, and deploying timely patient-provider interventions [40, 48]. In this line of research, on the methodological front, supervised machine learning techniques have gained prominence, providing promising predictive outcomes of mental health states [66]. The success of these techniques, however, hinges on access to ample and high-quality gold standard labels for model training. In mental health, gold standard labels often comprise diagnostic signals of people’s clinical mental health states, for instance, whether an individual might be suffering from a specific mental illness, or at the cusp of experiencing an adverse episode like a relapse or suicidal thoughts. Unlike conventional machine learning tasks in fields like computer vision and natural language processing, extensive, high quality gold standard data for predicting clinical diagnoses of mental illnesses from social media is not readily available. Literature has advocated the use of clinically validated diagnostic information collected from patient populations for building such predictive models [11, 66]. However, undertaking such efforts presents many practical and logistical challenges. These range from the difficulties in recruiting a sensitive and high risk population, to the myriad privacy and ethical concerns that accompany engaging directly with vulnerable individuals. Because of the effort- and time-consuming nature of such data acquisition approaches and the need for deep-seated cross-disciplinary partnerships, particularly with clinicians, researchers have noted such data acquisition efforts to not scale easily and quickly to large and diverse populations [21]. Consequently, researchers have operationalized a variety of online behaviors as diagnostic signals to build machine learning approaches that predict mental illness diagnoses. These “proxies” are easily accessible and inexpensively gathered from social media, without the need to directly engage with the individuals themselves.We define binary indicators of the presence or absence of these social media behaviors that might correspond to their clinical mental health state as “proxy diagnostic signals”. One notable example from literature consists of public self-reports of mental illnesses made by individuals in their social media feeds [21, 65]. This paper posits a significant challenge in using these proxy diagnostic signals revolving around their lack of clinical grounding, theoretical contextualization, and psychometric validity—concerns noted by psychiatrists and computational researchers alike [27, 48]. In other words, drawing on boyd and Crawford’s critique [13], despite gains in scale, gaps exist in our understanding of how these signals are defined, where their theoretical underpinnings are, whether they objectively and accurately measure what they claim to measure (that is, the clinical mental illness diagnosis), and whether the patterns of behaviors they exemplify are truly representative of the behaviors of patients. More generally, our position is situated in criticisms in the broader social media research area, where the use of online data, removed from the individual and the specific offline context, and collected without their direct involvement, poses natural challenges to evaluation [55, 110]. Our Contributions. Toward addressing the above methodological gaps, this paper presents a first empirical study to assess the quality of different social media-derived proxy diagnostic signals in predicting clinical diagnoses of mental illness, for treatment and patient-provider interventions. Focusing on the specific mental illness of schizophrenia and drawing upon an involved partnership between HCI and clinical researchers, we consider three proxy signals widely used in prior literature [11, 63, 65]: 1) behaviors signaling affiliation to mental health resources, 2) self-reported diagnoses, and 3) clinically appraised self-reports of diagnoses. Adopting data triangulation [35] and modern validity theory [62] as methodological foundations, we examine their predictive validity (internal and external). To do so, we design a prediction task to distinguish those with schizophrenia from control populations, and leverage a carefully-curated social media dataset of clinically diagnosed schizophrenia patients seeking treatment at a large health-care organization. We find that, although the three diagnostic signals demonstrate strong internal validity (reproducing what was established by the original works), they perform poorly on the clinically diagnosed patient data, thus suffering from poor external validity. Among the three signals, we find that the model that uses affiliation behavior as gold standard leads to the poorest performance. Our results also reveal that incorporating clinical judgment via appraisal of social media self-reports of mental illnesses leads to the best performance, among the proxy signals, when tested on clinical patient data. However, we find that all classifiers trained with the proxy diagnostic signals perform significantly poorly when compared to a model built using the data of the schizophrenia patient population. A deep dive in the performance of these classifiers via an error analysis reveals several methodological gaps in the way these diagnostic signals are conceived and employed in the predictive frameworks. These gaps range from uncertainties in the construct validity of the proxy signals, and poor theoretical grounding, to a variety of population and data sampling biases. Our findings provide remedial guidelines for researchers engaging with prediction of mental health states from social media data, and for clinicians and practitioners interested in incorporating such machine learning based diagnostic assessments in clinical decision-making.",2,2,2,0,2,2,2,2,6,2,2,2,0,2,0,2,2,1,7,0
https://dl.acm.org/doi/abs/10.1145/3290605.3300364,14,3,,"Ernala, Birnbaum, Candan, Rizvi, Sterling, Kane, De Choudhury",7,1,0,0,2019,"lives in a timely fashion, and spanning large, diverse populations, social media has emerged as an important tool in mental health. These uses, although not all encompassing, have included meeting a variety of social, technical, public health, and clinical goals. Research investigations have ranged from inferring risk to various mental illnesses [24, 29, 31, 34]—the largest body of work in this area; comparison of online and offline mental health behaviors [84]; understanding self-disclosure practices and goals [2, 39]; deciphering social support provisions to promote positive mental health outcomes [3, 33, 92]; discovering community norms and behaviors [18]; and exploring how these platforms can support intervention delivery [48]. Most relevant to the current paper is the line of research focusing on predicting mental health states and diagnoses from social media, which encompasses studies targeting different conditions [18, 22, 65], platforms [18, 32, 94, 107, 111], and disciplines [11, 14, 21, 108]. Burgeoning interest in this topic stems from the fact that social media data is readily available and archived, and can be unobtrusively gathered with low effort and cost [52]. These unique attributes help overcome many challenges in stateof-the-art clinical assessment of mental health that involves subjective recollection of historical facts—a method prone to retrospective recall bias [56]. However, appropriating social media data to inform clinical efforts around early diagnosis, tailoring treatment, or delivering interventions, suffers significant limitations. In a clinical setting, diagnostic information is available to the clinician via self-reported psycho-social signs and symptoms, theoretically and psychometrically validated clinical scales, interviews, questionnaires, and other diagnostic tools [1]. Social media data by itself, however, does not include such clinically validated signals to accurately identify and validate individuals’ mental health states. Also, collecting clinically valid diagnostic signals from social media would require engagement with an at-risk patient population, a cohort that is stigmatized, sensitive, and vulnerable. This presents logistical challenges to identification of diagnostic signals, as well as privacy and data protection issues. Such a data collection approach can be difficult to scale, and is effort- and time-consuming, requiring carefully crafted clinical and risk management protocols, and involvement of clinical experts. To circumvent these challenges, researchers have employed several online behaviors as gold standardinformation, or what we call proxy diagnostic signals to identify individuals’ mental illness diagnoses. Through a systematic literature review [58] based on a keyword search of papers on predicting mental health states from social media, we identifiedthree types of proxy diagnostic signals from the literature, which we elaborate below. Table 1 gives a taxonomy of prior worksin this area, from the perspective of the proxy diagnostic signals they use.",2,2,2,0,2,2,2,2,6,2,2,2,0,2,0,2,2,1,7,0
https://dl.acm.org/doi/abs/10.1145/3290605.3300364,14,4,,"Ernala, Birnbaum, Candan, Rizvi, Sterling, Kane, De Choudhury",7,1,0,0,2019,"Affiliation Behaviors: A first category of research represents behaviors signaling engagement or association (via hashtags, account following, community membership) with content related to mental health resources on social media, as proxy diagnostic signals of an illness [50, 63, 111]. A prominent example is McManus et al. [63] who used following a Twitter account (@schizotribe) dedicated to conversations around lived experiences of schizophrenia as a signal for gold standard information that an individual might be suffering from schizophrenia. A complementary set of papers have operationalized membership in online mental health support communities such as Reddit and Livejournal as proxies for diagnostic information [41, 68, 93, 94]. Self-reports: Next, the most popular form of proxy diagnostic signals, this category operationalizes first-hand, public self-disclosures of diagnosis of a mental illness as indicators of a clinical mental illness [8, 15, 21–23, 25, 32, 46, 47, 59– 61, 65, 69, 76, 80, 93, 97, 105–107]. A notable example, Mitchell et. al. [65] used regular expression search queries on Twitter (“I have been diagnosed with schizophrenia”) to extract selfreports of schizophrenia diagnoses and then employed them for predicting their presence/absence. External validation: Finally, this category represents humanin-the-loop, collaborative approaches that either seek selfreported information from the individual, or incorporate diagnostic scales and/or expert appraisal for identification of the proxy diagnostic signals [11, 14, 17, 32, 71, 78, 79, 81, 84, 90, 103]. Most relevantly, Birnbaum et al. [10] incorporated clinical appraisals on self-reports of schizophrenia on Twitter to build machine learning models of diagnoses.",2,2,2,0,2,2,2,2,6,2,2,2,0,2,0,2,2,1,7,0
https://dl.acm.org/doi/abs/10.1145/3290605.3300364,14,5,,"Ernala, Birnbaum, Candan, Rizvi, Sterling, Kane, De Choudhury",7,1,0,0,2019,"Appropriating these proxy diagnostic signals has overcome many challenges and barriers to gathering clinically valid diagnostic data on social media, particularly around scale and size [21], and these approaches continue to gain traction in the community. However they suffer from significant limitations, which we frame below, drawing upon the critical data literature [13, 51, 55]. Consider the case when affiliation to mental health resources is considered a proxy of a diagnosis. Alongside including genuine patients, it likely also includes other stakeholders like mental health practitioners and experts, non-profits raising awareness campaigns, caregivers etc. As another example, although the act of self-disclosing a mental illness can be an indicator of a person’s mental condition, there are gaps in understanding what an individual chooses to self-report, why, and when they decide to do so, or if they are being truthful. In other words, there is lack of evidence that these proxy signals are accurately measuring what they intend to measure, also known as construct validity [70] (whether the signals accurately identify and represent individuals at-risk). A lack of contextualization in psychiatric practice [57] or theory [7] additionally reduces confidence in their construct validity—an issue recognized in prior critiques of big data approaches [13, 55]. Although proxy signals with expert validation attempt to tackle some of these theoretical and clinical gaps, because the approach is removed from direct interaction with the individual, their veracity can be questioned, and their “claims to objectivity and accuracy can be misleading” [13]. Further, individuals with unique attributes, attitudes, and characteristics, possibly distinct from patient populations, are likely to engage in the specific types of behaviors enumerated by the proxy signals. Apart from the inclusion of “noisy” data, the unique ways in which the proxy diagnostic signals are defined and construed can lead to a variety of biases in the predictions, despite the impressive sample sizes they promise. This resonates with what boyd and Crawford noted, that “bigger data are not always better data [13]” and what Olteanu et al. discuss at length surrounding methodological pitfalls of big data [70]. In this work, we systematically examine these methodological gaps in the validity of these proxy diagnostic signals, and explore how validity issues impact their potential application in clinical decision-making.",2,2,2,0,2,2,2,2,6,2,2,2,0,2,0,2,2,1,7,0
https://dl.acm.org/doi/abs/10.1145/3290605.3300364,14,6,,"Ernala, Birnbaum, Candan, Rizvi, Sterling, Kane, De Choudhury",7,1,0,0,2019,"We use public and non-public data (gathered using appropriate protocols) from two prominent social media sites, Twitter and Facebook, for the purposes of this paper. We begin by introducing four datasets used in this paper, followed by a description of how they were collected.",2,2,2,0,2,2,2,2,6,2,2,2,0,2,0,2,2,1,7,0
https://dl.acm.org/doi/abs/10.1145/3290605.3300364,14,7,,"Ernala, Birnbaum, Candan, Rizvi, Sterling, Kane, De Choudhury",7,1,0,0,2019,"The first three datasets correspond to the three proxy diagnostic signals we adopt based on the topical focus and the existing literature, and which were introduced above. We consider them as proxies (or “proxy datasets”) of schizophrenia diagnoses in individuals. Affiliation Data. Our first dataset is motivated from prior literature that used behaviors signaling affiliation (e.g. following, hashtag usage) to mental health resources, related to schizophrenia, as diagnostic information.Adopting the approach of McManus et al. [63] (N =96), we used a Twitter account named @sardaa (Schizophrenia and Related Disorders Alliance of America), a support organization for people with schizophrenia and their caregivers, as our starting point to build this affiliation dataset. As operationalized by McManus et al. [63] and following verification of the account’s trustworthiness with our clinical coauthors, we considered all followers of the account@sardaa asindividuals with a schizophrenia diagnosis. Using the official Twitter API, we obtained the list of all followers of @sardaa (N =1847) and consistent with McManus et al. [63] collected their timeline data for the year 2014. We also collected profile information of these individuals including number of posts, chosenlanguage on Twitter(filtering for English), number of followers and number of followees, leading to a final sample of 861 Twitter users. Descriptive statistics of this data are reported in Table 2 and Figure 1(a). Self-report Data. For the second dataset, we adopt the proxy diagnostic signal ofmentalillness self-reports utilizedinmany prior works (e.g., (most prominently [65]), introduced in the previous section. Per Mitchell et al.’s approach [65] (N =174), we used a list of key phrases developed in Ernala et al. [10] to identify self-reports of schizophrenia on Twitter from 2014. Followingmanual filtering to remove noisy examples, without loss of generality, we collected the historic timeline data of all authors of these self-reports (N =412). We also collected the same metadata information as above, such as, total number of posts, chosen language on Twitter (filtering for English), total number of followers and total number of followees/friends. Descriptive statistics are reported in Table 2 and Figure 1(b). Clinically Appraised Self-report Data.Our third proxy dataset is inspired from the third body of work that used external expert appraisals on social media data to obtain diagnostic signals of mental illnesses. Following Birnbaum et al.’s approach [11] (N =146), we began with a sample of 635 individuals who had self-reported their diagnosis of schizophrenia on Twitter in 2014. A sample of each individual’s timeline, consisting of the self-report post, 10 preceding and 10 succeeding posts was then passed to two clinical coauthors1 for appraisal. The experts annotated each self-report sample on a three point scale: genuine, noisy and maybe categories and achieved high inter-rater agreement (Cohen’s κ = 0.81 between genuine and noisy). This third type of diagnostic signal provided Twitter data of 153 individuals whose self-reports were clinically appraised to be genuine. As before, we collected all metadata associated with their Twitter profiles, descriptive statistics of which are given in Table 2 and Figure 1(c).",2,2,2,0,2,2,2,2,6,2,2,2,0,2,0,2,2,1,7,0
https://dl.acm.org/doi/abs/10.1145/3290605.3300364,14,8,,"Ernala, Birnbaum, Candan, Rizvi, Sterling, Kane, De Choudhury",7,1,0,0,2019,"The predictive task of identifying individuals with schizophrenia necessitates comparisons to matched control Twitter users who do not provide an equivalent proxy diagnostic signal. Accordingly, we used the Twitter streaming API to obtain a random sample of public posts and extracted their authors. Then, we gathered their timeline data for 2014 and profile information (N =640). We filtered out any individuals who had mentions of schizophrenia in their posts. Then, we adopted a statistical matching approach [83] to ensure that the control users and the individuals in each of our proxy datasets are comparable by trait attributes. Since social media behaviors are a reliable indicator of people’s personality, psychological states, and even demographic attributes [91], we included the following covariates for the purpose of matching: total number of statuses, chosen language on Twitter, total number of followers and total number of followees. Through an iterative k-nearest-neighbor matching (k=1-15) based on the well validated Mahalanobis distance metric [82, 87], we compared the covariates of each individual’s Twitter content in each proxy dataset (affiliation, self-report, appraised self-report) with that of each of the control users obtained above, and identified a set of most similar control users based on a heuristically chosen distance threshold. For the affiliation dataset, we obtained a matched control sample of 539 users. We obtained 345 and 107 matched controls for the self-report and the clinically appraised selfreport datasets respectively. The descriptive statistics of these matched controls are given in Table 2.",2,2,2,0,2,2,2,2,6,2,2,2,0,2,0,2,2,1,7,0
https://dl.acm.org/doi/abs/10.1145/3290605.3300364,14,9,,"Ernala, Birnbaum, Candan, Rizvi, Sterling, Kane, De Choudhury",7,1,0,0,2019,"As the fourth dataset, we include social media data of patients clinically diagnosed with schizophrenia and that of clinically verified healthy controls, based on a clinical examination or DSM-5 [4] criteria. This data was collected as a part of a research study involving the paper’s authors, aimed at identifying technology-based health information to provide early identification, intervention and treatment to young adults with schizophrenia. The research protocol was approved by the Institutional Review Board (IRB) of the coordinating institutions as well as local IRBs at participating sites. Individuals between 15 and 35 years old were recruited from various inpatient and outpatient psychiatric departments at the coordinating and its partner institutions. Participants were eligible if they had a primary psychotic disorder like schizophrenia, based on clinical assessment scales (e.g., the Psychiatric Diagnostic Screening Questionnaire or PDSQ [112]) as well as a formal clinical examination facilitated by Structured Clinical Interview for DSM-5, or SCID [98]—we note that all of these diagnostic tools are backed by sound theoretical underpinnings. Healthy controls who had already been screened for psychiatric disorders and consented to prior studies were also recruited. All participants were asked to request, extract, and share entire archives of their Facebook and Twitter data. The consented participants included 88 patients who had been diagnosed with schizophrenia. Of these 88, 73 participants consented to provide their Facebook data, whereas 15 provided their Twitter data.Additionally, 55 healthy controls were recruited through the study, out of which 32 provided their Facebook data and 23 participants provided their Twitter data.We use all linguistic content from participants’ Facebook and Twitter archives i.e. status updates and comments made on Facebook, and posts shared on Twitter. As Twitter was the primary data source for all of the proxy diagnostic signals, we conjectured that the small sample of patients and healthy controls with Twitter data (N =38) could pose a challenge to building robust machine learning models. To combat this issue and to leverage the larger sample sizes with Facebook data, we conducted linguistic equivalence tests between the two data sources, a known approach in the transfer learning literature [45]. As language on Twitter cannot be directly compared to that on Facebook due to the affordances of the platforms [6], establishing linguistic equivalence was a crucial step before both data sources could be combined in building the patient and healthy control datasets. To test for linguistic equivalence, we use semantic similarity calculated using distributed word vector representations of all linguistic content contained in the Facebook and Twitter archives [75]. Cosine similarity of word vectors is often used to quantify the linguistic similarity between two datasets, and a high value indicated that the content in the two datasets was linguistically equivalent [75]. We found a high cosine similarity between the vector representations of the Facebook and Twitter data across both the schizophrenia patient (=0.98) and healthy control population (=0.84), showing the two data sources to be linguistically equivalent. Thus, our final dataset comprised either Twitter or Facebook archives of 88 schizophrenia patients and 55 healthy controls. The descriptive statistics for the combined patient and healthy control dataset are reported in Table 2. Of the total number of posts, 99% came from the Facebook archives of the patients and the healthy controls, and the remaining 1% were sourced from their Twitter archives.",2,2,2,0,2,2,2,2,6,2,2,2,0,2,0,2,2,1,7,0
https://dl.acm.org/doi/abs/10.1145/3290605.3300364,14,10,,"Ernala, Birnbaum, Candan, Rizvi, Sterling, Kane, De Choudhury",7,1,0,0,2019,"We adopt quantitative data triangulation as our methodological framework. Triangulation is an evaluation approach that uses multiple or heterogeneous methods, or data sources compiled via varied mechanisms, to develop a comprehensive understanding of a phenomenon, or to elucidate its complementary aspects [73]. Specifically, this approach is used to confirm the results of a research, and provide external validation to existing findings [35]. In essence, triangulation is an attempt to map out, or explain more fully, the richness and complexity of human behavior by studying it from more than one standpoint. Using this approach, we assess the efficacy of the three proxy diagnostic signals in identifying diagnoses of individuals with schizophrenia, both within their corresponding proxy datasets, as well in the data of schizophrenia patients. This way, we seek to establish their internal and external validity respectively. Figure 2 gives an overview of our approach.",2,2,2,0,2,2,2,2,6,2,2,2,0,2,0,2,2,1,7,0
https://dl.acm.org/doi/abs/10.1145/3290605.3300364,14,11,,"Ernala, Birnbaum, Candan, Rizvi, Sterling, Kane, De Choudhury",7,1,0,0,2019,"We set up a binary classification task to distinguish betweenindividuals with schizophrenia identified by each proxy dataset and its corresponding matched controls. We built four models: three based on the proxy datasets denoted as the Affiliation, Self-report and Appraised Self-report Models and one on the clinically validated patient data known as the Patient Model. PreparingTraining andValidationData:We use the proxy datasets and their corresponding matched control data in their entirety for training and validating the above proxy classifiers. For the Affiliation Model, the positive examples (Class 1) comprised the Twitter data of the 861 users while the negative examples (Class 0) consisted of the 539 matched control users. The positive examples for the Self-report and Appraised Self-report Models spanned the data of 412 and 153 users respectively, while the corresponding negative examples included the Twitter data of 345 and 107 matched controls. For the Patient Model, we selected a random sample of 80% of the patient dataset for model training and validation, resulting in 68 patients with schizophrenia in the positive class, and 46 healthy control participants forming the negative class. Preparing Unseen Test Data: We incorporated the heldout 20% patient data as an unseen test dataset, that could be consistently used across all models (Affiliation, Self-report, Appraised Self-report and Patient) for triangulation. This comprised 20 patients with schizophrenia and 9 healthy controls. Features: Linguistic features from text data have been widely adopted and are known to be largely successful in predicting mental health states using social media data [20, 31]. A rich body of literature in psycholinguistics has identified the association of linguistic usage to emotion and behavior, including mental health states of individuals [74]. We adopt two forms of linguistic content as features for classification. First, we build a term-frequency, inverse document-frequency based language model using the most frequent 500 n-grams (n=1-3) from the preprocessed data upon removal of stop words and URLs. Second, we use three categories of psycholinguistic measures: (1) Affective attributes, (2) Cognitive attributes and (3) Linguistic style attributes—from the well-validated psycholinguistic lexicon Linguistic Inquiry and Word Count (LIWC) [20]. Combining the two feature sets together, our overall feature space included 550 numeric features. We built classifiers for each proxy dataset to predict individuals with schizophrenia on social media from matched control users. To remove correlated features and toimprove the predictive power of the model, we employed feature selection methods [43], eliminating noisy features and identifying the most salient variables in predicting the outcome. Specifically, we use the filter method where features are selected on the basis of their scores in statistical tests for their correlation with the outcomevariable.Adopting theANOVAF -testwe reduced the feature space from 550 features tok-best features per classifier. We experimented with non-linear and ensemble classification algorithms such as Support Vector Machines, Random Forest, and Logistic Regression [36]. For each classifier, we test its performance in two steps: First, for parameter tuning and assessing internal validity, we used stratified k-fold cross validation. We varied model parameters for all classification approaches during the validation step to find the best performing model. Second, choosing this best performing model from the validation step, we evaluated its performance on the unseen test data for external validity. Across the four classifiers, for relative comparison, we report model performance using a variety of metrics: Receiver Operating Characteristic Area Under Curve (ROC AUC), accuracy and F1 scores.",2,2,2,0,2,2,2,2,6,2,2,2,0,2,0,2,2,1,7,0
https://dl.acm.org/doi/abs/10.1145/3290605.3300364,14,12,,"Ernala, Birnbaum, Candan, Rizvi, Sterling, Kane, De Choudhury",7,1,0,0,2019,"We present in Table 3 the cross validation performance of the four classifiers in distinguishing individuals with schizophrenia from matched controls. Overall, the Affiliation Model outperforms the other classifiers with the highest accuracy (Best: 0.94, Mean: 0.88, std: 0.02) and F1 (Best: 0.95, Mean: 0.91, std: 0.02) and a 27% improvement in accuracy over a ZeroR baseline (Accuracy: 0.61). Upon feature selection to top 450 features, a penalized logistic regression classifier led to high model stability. The reported accuracy of this model is close to McManus et al. [63], demonstrating that the trained model can infer distinct patterns between the two classes. Although both Self-report andAppraised Self-report models improve over their ZeroR baseline (accuracy: 0.54, 0.44 respectively), the Appraised Self-report Model performs better (Best: 0.88, Mean: 0.80, std: 0.03) than the Self-report Model (Mean: 0.72, Best:0.79, std: 0.02) across all metrics. The ROC AUC for the Self-report and Appraised Self-report Model as reported in Table 3 are 0.80 and 0.85 respectively. A penalized, logistic regression classifier again performed best on both of these datasets, based on the K-best features (=350 respectively) that we select for downstream testing. Comparing the performance of the proxy classifiers on their respective validation sets, we find that the Appraised Selfreport Model has higher precision than the Self-report Model. This was also observed by Birnbaum et al. [10]; the clinician annotation task eliminated inauthentic noisy samples leading to a high precision sample of genuine self-reports. This reveals that contextual cues picked by the experts, such as mentions of medication, mood stability, symptomatic expression provide a strong validation of the self-reports. Since both datasets were sampled from self-reports of schizophrenia in 2014, we conjecture that incorporating clinical appraisals improved performance by eliminating false positives, or ambiguous self-reports from the positive class. Finally, the Patient Model trained on patient data performs modestly, although better, compared to the proxy classifiers, with average accuracy of 0.72 (best: 0.75) and average F1 score of 0.77 (best:0.79) across 5-fold cross validation2 .",2,2,2,0,2,2,2,2,6,2,2,2,0,2,0,2,2,1,7,0
https://dl.acm.org/doi/abs/10.1145/3290605.3300364,14,13,,"Ernala, Birnbaum, Candan, Rizvi, Sterling, Kane, De Choudhury",7,1,0,0,2019,"Next, to examine their external validity on unseen patient test data, we present the performance of the proxy classifiers. Figure 3 (a-c) presents the ROC plots, per proxy classifier, showing the trade-off between true positive rate (sensitivity) against the false positive rate (1-specificity). Among the three proxy classifiers, the Affiliation Model shows poor external validity with the lowest accuracy (0.21), the lowest F1 (0.14), and the lowest AUC (0.2) on the 20% sample of unseen patient data (refer Table 3). The next best performing model is the Self-report Model outperforming the Affiliation Model with a 27% improvement in the overall accuracy (0.48), 47% improvement in F1 score (0.61) and 18% improvement (0.38) in the ROC AUC. Although this indicates that self-reports might be a better diagnostic signal than affiliation, the performance of this classifier is still weak compared to its performance during the validation step (test of internal validity). Lastly, among the three proxy classifiers, we see the strongest external validity or best performance for the Appraised Self-report Model. This classifier shows a 9% and 55% improvement in F1 (0.70), and 7% and 34% improvement in accuracy (0.55) over the Self-report and Affiliation Modelrespectively.Although theAppraised Self-reportModeldemonstrates the strongest external validity so far, there is substantive decrease in its performance compared to the validation phase. Summarily, testing the proxy classifiers on unseen patient data revealed poor external validity and that relative performance between the validation and testing steps was not preserved when tested in a clinical setting.",2,2,2,0,2,2,2,2,6,2,2,2,0,2,0,2,2,1,7,0
https://dl.acm.org/doi/abs/10.1145/3290605.3300364,14,14,,"Ernala, Birnbaum, Candan, Rizvi, Sterling, Kane, De Choudhury",7,1,0,0,2019,"Triangulating the three proxy datasets corresponding to their diagnostic signals, we compare their predictive performance with the Patient Model, again trained on the 20% sample of unseen patient test data. Through this, we establish an empirical estimate of the error incorporated by using the proxy classifiers, when applied on patient populations. First, we report the performance of the Patient Model. From Table 3, we see that this model outperforms the proxy classifiers, in distinguishing healthy controls from schizophrenia patients, giving lower false positives and false negatives. We also find that this is a highly precise model (precision: 0.93), correctly predicting schizophrenia patients as the positive class. The performance, however, is affected by low recall, and we find lower precision for the negative class due to the false negatives (=6) wherein schizophrenia patients are wrongly predicted as healthy controls. We use the performance of the Patient Model as gold standard and examine the error incorporated by each of the proxy classifiers. We use F1 and ROC AUC to situate these differences. We note the highest difference in performance exists between the Patient Model and the Affiliation Model. The Patient Model outperforms the Affiliation Model by 65% in F1 and 62% in AUC. Comparing the Patient Model with the Selfreport Model, we observe a 19% and 44% gain in F1 and ROC AUC respectively. This indicates that the online behavior of self-reporting a mental illness diagnoses might be a better diagnostic signal than the affiliation behavior. Finally, the Appraised Self-report Model shows least difference in performance when compared to the Patient Model with 10% and 31% difference in F1 and AUC respectively. This indicates that when using self-reports as a diagnostic signal, clinical appraisal leads to better predictions. In short, the triangulation step reveals variability in predictive performances of the proxy diagnostic signals when tested on unseen patient data, demonstrating trade-offs when proxy signals are used for predicting clinical mental health states, versus when information is gathered directly from patients.",2,2,2,0,2,2,2,2,6,2,2,2,0,2,0,2,2,1,7,0
https://dl.acm.org/doi/abs/10.1145/3290605.3300364,14,15,,"Ernala, Birnbaum, Candan, Rizvi, Sterling, Kane, De Choudhury",7,1,0,0,2019,"To evaluate beyond performance metrics and to reason about the poor external validity of the proxy classifiers, we present a deeper analysis of the proxy classifiers’ performance. Error Analysis. We begin by unpacking mismatches in predictions made by the proxy classifiers on unseen patient data, in terms of example false positives and false negatives. Unpacking false positive classifications: Consider an example X who is a healthy control, per a clinically validated diagnostic assessment. But, the Affiliation Model wrongly predicted them as having schizophrenia. Examining their social media timeline, we find (paraphrased) posts including excerpts such as, “mental screenshot of notes”, “are you bad for my mental health” and “use my phone in day mode because I am mentally ill”. We note that terms like ‘mental’ (β =2.17), ‘health’ (β =1.44), ‘illness’ (β =1.45) in these excerpts are highly predictive of the positive class in the Affiliation Model, leading to because the Affiliation Model simply measures engagement, association with, or interest in mental health content and resources, it missed capturing the context in which these topics were discussed by X, leading to a misclassification of X as a schizophrenia patient. Now consider a healthy control participant Y’s timeline. It includes prolific usage of terms such as ‘creepy’ (β = 0.241), ‘hell’ (β = 0.096), ‘jesus’ (β = 0.091), and ‘help’ (β = 0.401). These tokens are learned as highly predictive of the positive class by the Appraised Self-report Model, thereby leading to a misclassification of Y. Although these tokens reveal symptomatic expression, spirituality and supportseeking behaviors, notable in schizophrenia disclosures made on social media [39], the current example demonstrates varied usage of these tokens by healthy controls, in reference to pop-culture or in casual conversations. We frame these observations as the following methodological gaps: that the outcomes yielded by the proxy classifiers are not valid indicators of a clinical diagnosis of schizophrenia (poor construct validity); and that the behaviors of individuals captured by the proxy signals might not be representative of the behaviors of schizophrenia patients (sampling bias). Unpacking false negative classifications: Consider a different example A, a clinically diagnosed patient with schizophrenia. Their social media timeline data shows extensive usage of swear terms such as ‘fuck’ (β =−0.94), ‘ass’ (β =−0.63), ‘bitch’ (β =−0.67) that according to theAffiliation Model were highly predictive of the negative class, resulting in a false negative classification. Consider example B, a schizophrenia patient whose timeline largely consisted of travel and hobbies related posts with no evidence of schizophrenia experiences. The Appraised Self-report Model predicted B as a healthy control, due to lack of explicit disclosures of the illness, like symptomatic expressions and personal struggles (feature importance for LIWC categories: anger (0;0.03), body (0;0.06), swear (0;0.05) anxiety (0;0.03)). These differences reveal that the proxy signals are not measuring what they intend to measure (poor construct validity). Further, that the social media language of patients might not be very different from control users (population bias). Issues of Dataset Shift & Bias. The population and sampling biases revealed by our error analysis goes on to show that the statistical data distributions might be drastically different between the proxy datasets and the actual patient dataset— a phenomenon referred to as “dataset shift” [99]. As a next step in our deep dive, we present the following analysis to systematically examine this dataset shift and assess its effects. Specifically, to quantify dataset shift, we adopt a measure of semantic distance computation between the linguistic content of proxy and patient datasets [45]. To represent the proxy data distributions, we first identify the most frequent 500 n-grams from the positive class, per proxy classifier, and compute the word vector representation [64] for each of these n-grams. Similarly, we represent the positive class for the test distribution i.e. the data of all schizophrenia patients in the word vector space. We finally compute the cosine similarity between the proxy and patient data in the vector space. Our results bolster the findings of the error analysis, wherein we observe the farthest distance between the proxy and patient data in case of the affiliation dataset (similarity: 0.907, distance:0.092). The self-report dataset is at a closer semantic distance to the patient data distribution than the affiliation data, with a distance of 0.019 and similarity of 0.980. Finally, confirming the observations thus far, the appraised self-report dataset appears at the closest distance to the patient data with a distance of 0.017 and similarity of 0.982. Issues of Construct Validity. A second issue revealed by our error analysis was that the behavioral patterns learned by the proxy classifiers were absent in the schizophrenia patient population, raising concerns around construct validity. Therefore, next, we examine the features learned by the proxy classifiers in comparison to the features learned by the Patient Model. Table 4 shows the top features, and their feature weights for the worst and best proxy classifiers, and the Patient Model. Overlap of features: Comparing the top features of the Affiliation Model with the Patient Model, we see little overlap between the two feature spaces, prominently, in terms of use of first person pronouns and LIWC category terms about ‘social’ and ‘body’. We find that these features are predictive of one class in the Affiliation Model, whereas predictive of the opposite class in the Patient Model. Further comparing the top features of the Appraised Self-report Model with the Patient Model, we see a higher overlap than in the case of the Affiliation Model. Some of these features such as ‘feeling’, ‘help’ and use of first person pronouns are predictive of the positive class in both models, which explains the higher external validity of the Appraised Self-report Model. However, we find that there are also a number of features predictive of the positive class in the Appraised Self-report Model, that are associated with the negative class in the Patient Model, e.g., LIWC categories of present tense, body, verbs and tokens such as ‘crazy’. Although the Appraised Self-report Model is accurately learning certain patterns specific to the patient population, it misconstrues explicit mental illness disclosure behaviors (symptomatic expressions, combating stigma, and support seeking) as signals of a schizophrenia diagnosis. Mismatch of features: Finally, we observe that the most predictive features (of the positive class) in the Affiliation Model are explicit signals of mental health care and support (‘mental health’, ‘illness’, ‘depression’, ‘stigma’, ‘mhchat’), that have few occurrencesin the patient data. Similarly,in the case of the Appraised Self-report Model, content related to schizophrenia experiences (‘die’, ‘alone’, ‘sorry’, ‘creepy’, LIWC categories of negative affect and negation) are either missing or not predictive of the positive class in the Patient Model. Therefore, we argue that what these proxy classifiers actually learn is the language use of individuals actively opening up about schizophrenia experiences, seeking informational and emotional support on Twitter. In comparison, our patient population does not exhibit such disclosure or support seeking behaviors on social media.",2,2,2,0,2,2,2,2,6,2,2,2,0,2,0,2,2,1,7,0
https://dl.acm.org/doi/abs/10.1145/3290605.3300364,14,16,,"Ernala, Birnbaum, Candan, Rizvi, Sterling, Kane, De Choudhury",7,1,0,0,2019,"In this paper, we presented the first insights into some methodological gaps that exist in using social media derived diagnostic signals for predicting clinical mental health states. We found a lack of external validity when the prediction models developed using the proxy signals were tested on actual patient data. Our triangulation approach further surfaced issues of construct validity, limited theoretical underpinning, and population and sampling biases that permeate in the prediction task, through these diagnostic signals. We discuss the methodological and clinical implications of these findings.",2,2,2,0,2,2,2,2,6,2,2,2,0,2,0,2,2,1,7,0
https://dl.acm.org/doi/abs/10.1145/3290605.3300364,14,17,,"Ernala, Birnbaum, Candan, Rizvi, Sterling, Kane, De Choudhury",7,1,0,0,2019,"Uncertainty in Construct Validity. A first notable limitation of the proxy diagnostic signals we observed is the uncertainty in their construct validity. Drawing on the definition of this construct, we explore two methodological implications: 1)Do these diagnostic signals measure what they claim to measure? Our results show that the diagnostic signals are not measuring what they claim i.e. the clinical diagnosis of an individual’s mental health(schizophrenia) state. This is revealed by the considerable mismatch we observed while comparing the top predictive features of the proxy classifiers and those of the Patient Model. Unpacking the context of these features in the actual social media posts, we found that they capture support seeking behaviors, interest in others’ lived experiences of the illness, self-reported accounts of stigma and inhibition— patterns absent from the features of the Patient Model from the clinical schizophrenia population. 2) Is what is being measured by a diagnostic signal itself valid? To the latter point about construct validity, we found a lack of clinical grounding in the diagnostic information (individual’s clinical mental health state) that these signals intend to measure. Instead, what these signals presume as diagnostic information are essentially behavioral patterns associated with the appropriation of social media by a wide variety of stakeholders, not necessarily patients, in relation to the illness. These forms of appropriationincludeindividuals posting resources for mental health awareness, individuals seeking therapeutics benefits, or individuals breaking free inhibitions and mental health stigma by disclosing their illness. Although these appropriation patterns can be a valuable resource to understand the experiences of schizophrenia [86], they do not provide clinically grounded information about an individual’s diagnosis of a mental illness—thereby making them less suitable for the prediction tasks in this paper. Although the appraised self-report diagnostic signal attempts to overcome lack of clinical grounding, it suffers from other limitations that affect its construct validity. First of all, the clinical experts who appraised the self-reports of schizophrenia did not have access to the person’s clinical history, or symptoms and experiences. Collateral information— information beyond a patient’s explicit self-reports of symptoms [40]—are also critical and mainstream in any clinical mental illness diagnosis, and our clinical diagnostic signal derived from the patients factors this information through use of tools like PSDQ [112] and SCID [98]. However, for the appraised diagnostic signal, the clinicians can only gather collateral information from the content of the social media posts. This may not be sufficient for a valid diagnosis, given the limited context of what people consciously or subconsciously choose to share on social media, and vast amounts of collateral information might exist offline, which the appraising clinicians did not have access to. Theoretical Contextualization. Related to the above two issues lies another limitation, which is a lack of theoretical underpinning in the ways the diagnostic signals were identified. All of the scales and questionnaires used for clinical diagnosis, including the ones used in this paper’s patient population, draw upon theoretical frameworks, such as neurobiology, dimensional personality assessment, behavioral science, psychodynamic, and cognitive theories [77]. They undergo rigorous psychometric testing and are continually adjusted as the frameworks around mental illnesses evolve, or as the DSM [4], or more recently the National Institute of Mental Health introduced Research Domain Criteria (RDoC) framework [49] offer newer guidelines for mental health diagnostic and treatment. The proxy diagnostic signals are, however, not inspired by this theory. Instead they focus on online behaviors, which may or may not align with theoretical models, frameworks, or guidelines of mental illnesses. The other methodological gap we identify in the use of the proxy diagnostic signals for predicting clinical diagnoses relates to dataset shift [99]. In the literature, datasets shifts in supervised learning are attributed to population or data sampling biases inherent in the data [70]. We therefore discuss the foundations of this phenomenon in two ways: Population Biases. We observed that the datasets built using the proxy diagnostic signals include social media data of a unique set of individuals, who may not be representative of schizophrenia patients who are actually diagnosed with the illness and under treatment. Consequently, this population bias may manifest in several different ways: 1) The social media activities of an individual who follows online mental health resources, may be different from someone who publicly discloses their illness and experiences—and these, in turn, might be different from a clinically diagnosed patient’s social media usage and behaviors [12]; 2) The diagnostic signals capture subpopulations who may not be truthfully reporting their illnesses or may be reporting about their self-derived assessments of a mental illness experience in an exaggerated fashion, that did not involve the feedback of a clinician; and 3) The diagnostic signals consist of subpopulations who may not be mental illness patients currently under treatment, and the social media activities of those who are under formal care and those who are not, might be considerably different. Data Sampling Biases. The observed dataset shift between the proxy datasets and the schizophrenia patient datamay also be stemming from a type of sampling bias related to boundary regulation preferences of the individuals and the use of public versus private accounts. Individuals identified using these online diagnostic signals have largely public social media accounts; however, the clinically diagnosed patients we considered largely had private accounts. This difference in boundary regulation and privacy choices between the schizophrenia patients and the individuals captured via the diagnostic signals might lead to sampling biases in the proxy datasets. Identifying and quantifying the biases between the populations targeted by the diagnostic signals, alongside examining their theoretical and construct validities is, therefore, crucial before the signals are deployed to make clinical predictions. Clinical (Patient-Provider) Implications Alongside the methodological implications of making predictions of mental illness diagnoses with the proxy diagnostic signals, it is equally important to consider their impact on the key stakeholders such as clinicians and patients. To the clinician community, whose primary source of diagnostic information comprises clinically validated questionnaires, scales, interviews, and symptoms reported by the patient [1], these new forms of proxy diagnostic signals derived from social media, despite the right intentions, add complexities to the conventional psychiatric assessment method. We highlight some of these complexities in the questions below. For instance, in the absence of supplementary and accessible details of their inner workings and biases, how can clinicians trust these new forms of diagnostic signals and their validity, and thereafter act upon them? How do these new signals complement or even contradict clinicians’ mental models of reasoning, or how clinicians pursue diagnosis and treatment of their patients? Importantly, decision-making by the clinicians (for diagnosis, treatment, or patient-provider interventions) involves both high stakes and high costs. Therefore, incorrect predictions made as a result of data with poor external, construct validity, or those suffering from population and sampling biases can be dangerous and have serious consequences for the patients’ well-being, and social and professional life. While personalized patient care is touted as a strong motivation for adopting socialmedia for clinical diagnosis and treatment [66], validity and bias issues may additionally adversely impact patients trust and attitudes towards mental healthcare. When outcomes of these proxy classifiers are incorporated into clinical decisions without the patients’ awareness, poor validity can even negativelyimpact patients’ perceived agencyin treatment, or the therapeutic relationship they share with their clinicians. These issues may further conflict with patients’ preferences, needs, and values in treatment [100]. Thus bridging these methodological gaps with interactions with and involvement of the patient and clinician stakeholders is key to translating the potential of social media to support clinical diagnosis and treatment.",2,2,2,0,2,2,2,2,6,2,2,2,0,2,0,2,2,1,7,0
https://dl.acm.org/doi/abs/10.1145/3290605.3300364,14,18,,"Ernala, Birnbaum, Candan, Rizvi, Sterling, Kane, De Choudhury",7,1,0,0,2019,"In the light of the above discussion, we suggest some guidelines for researchers to bolster efforts in examining and establishing the efficacy of social media based signals for prediction of mental health states in clinical populations. □ Improving Methodological Rigor and Adopting Alternative Research Designs. A first set of guidelines center around reducing or eliminating the issues noted above. We conjecture that combining multiple proxy diagnostic signals, especially those that are complementary to each other, could provide more rigor because of their potential to target more diverse social media populations. However, this warrants empirical investigation. Alternatively, given the stigma around experiences of mental illness [26], some of the proxy diagnostic signals can be leveraged in a respondent-driven sampling framework [44]. This can be a viable mechanism to reach and recruit individuals for clinical studies that seek to collect gold standard patient data. Implementing an onlineoffline framework [48], that combines social media data with pre-existing offline longitudinal information of comparable sub-populations, can also reduce the dataset shift challenges. Further, issues of dataset shift can be overcome by adopting recent approaches from the machine learning field, such as including importance weighting of training instances based on similarity to test set [95], and employing online learning of prediction models to identify and recover from incorrect predictions [16, 54]. Crowdsourcing based data analysis and replication efforts [38, 96] can also be used to make transparent the impact of proxy dataset biases on predictive models. □BuildingandUtilizing SharedInfrastructures forDataCollection, and Data Donation Efforts. One next guideline centers around building, contributing to, and leveraging shared infrastructures and data repositories for conducting this research. Our findings showed the value of using patient data in building predictive models of mental illness diagnosis. However, we recognize that researchers without access to patient populations within large healthcare systems, or without involved collaborations in the clinical field may be at an unfortunate disadvantage. Further, patient data collection can be complex, including technological and ethical dimensions, due to the need to engage with a vulnerable population and gather sensitive (largely non-public) information, that might include HIPAA [67] protected data. Open source, HIPAA compliant infrastructures with customizable data collection functionalities can be helpful to overcome some of these technical challenges. Participatory research efforts such as the Connected and Open Research Ethics (CORE) initiative [101] can be used to develop dynamic and relevant ethical practices to guide and navigate the social and ethical complexities of patient data collection. Initiatives focusing on voluntary data donation approaches, such as the notable OurDataHelps [24] program for suicide prevention research, can be utilized to gather high quality data about people’s clinical mental health states, alongside their social media data. □HarnessingPartnerships BetweenComputationalandClinical Researchers, and Patients. Finally, this research area can benefit extensively from cross-disciplinary partnerships. Collecting patient data for building the predictivemodelsinvolves human costs, and suffers from resource and logistical constraints. In working with sensitive population such as patients with mental illnesses, it is important to have appropriate clinical risk management protocols in place [37], especially when the source of data concerns social media activities of patients monitored in a near real-time fashion [109]. Computational researchers by themselves may not be best equipped to define or implement such protocols. Moreover, clinical expertise is needed to identify and navigate the right way and the right time to approach patients for informed consent regarding data sharing, and assess how it would impact their perceptions of clinical care. Partnership of computational researchers and clinicians throughout the research pipeline—e.g., right from establishing validity of measured online behaviors, providing appraisal of the data via qualitative coding tasks, to interpreting and situating large scale data analysis, can also improve rigor and eliminate issues of construct validity and improve theoretical grounding of the approach. Moreover, directly incorporating patients’ feedback in the construction and acquisition of the clinical diagnostic signals will not only help represent their voices in the functioning of the predictive models and engage them as partners in treatment, but also support advancing the vision of participatory mental healthcare [89].",2,2,2,0,2,2,2,2,6,2,2,2,0,2,0,2,2,1,7,0
https://dl.acm.org/doi/abs/10.1145/3290605.3300364,14,19,,"Ernala, Birnbaum, Candan, Rizvi, Sterling, Kane, De Choudhury",7,1,0,0,2019,"We acknowledge some limitations in this empirical study. We note caveats in our patient data, both in terms of its limited size, as well as in terms of limited diversity—the data was collected among patients suffering from a specific mental illness and seeking treatment within a single healthcare system, albeit one of the largest in the United States.We note here that, schizophrenia is known to manifest uniformly across demographic groups (gender, ethnicity, race) and geography [9] so we conjecture such biases to be minimal. Further, our examination of validity concerns and population biases in the proxy diagnostic signals is limited to English speaking, largely Western populations. The demographics therefore, are skewed and we caution against generalization. We also acknowledge that the observations we derived are limited to largely one social media platform, Twitter. Future work can extend and attempt to replicate these findings, both by focusing on other patient populations as well as additional illnesses and social media sites. Finally, we have considered only three proxy diagnostic signals in this paper, although they are amongst the most widely used in the community. Future work can also present additional investigations on alternative proxy signals and the potential of employing the use of multiple proxy signals in a concerted fashion. Notwithstanding these limitations, there is a key takeaway in this paper: that if the broader research agenda is to use social media data toinform clinical decision-making, such as early diagnosis, treatment or patient-provider interventions, using patient data to build machine learning models is imperative. That said, the goal of this paper is not to be dismissive of the immense potential thatliesin this source of data. To quoteInkster and colleagues [48]: “While acknowledging that issues are far from settledabout the role that socialmedia shouldplayinmental health, we argue that it should no longer be a debate about whether researchers and healthcare providers engage with social networking sites, but rather how best to utilize this technology to promote positive change.” Our remedial guidelines attempt to chart some of these possible ways of using social media data for positive change in mental health—by pairing offline and online patient data as well as by meaningfully involving researchers, clinicians, and patients, it is possible to extend social media based approaches for early diagnosis, treatment, and for developing novel patient-provider interventions. In closing, we recognize that the use of proxy diagnostic signals is attractive because of scalability, as well as the low effort and minimal researcher, clinician, and patient burden they pose. However, for the limitations, both methodological and clinical, discovered in this paper, we suggest exercising caution and rigor going forward, and be cognizant of their implications for key stakeholders like clinicians and patients. Research in the HCI field has been instrumental in surfacing the many challenges of transplanting algorithms built with biased data, lacking theoretical and domain-specific validity, in real-world contexts [88, 104]. We believe the remedial guidelines we proposed will augment these efforts by starting conversations in the broader research community interested in leveraging social media data and machine learning predictive techniques to revolutionize mental health-care.",2,2,2,0,2,2,2,2,6,2,2,2,0,2,0,2,2,1,7,0
https://aclanthology.org/W18-0607.pdf,15,1,,"Ive, Gkotsis, Dutta, Stewart, Velupillai",5,0,0,0,2018,"Depression is a major public health concern in the U.S. and globally. While successful early identification and treatment can lead to many positive health and behavioral outcomes, depression, remains undiagnosed, untreated or undertreated due to several reasons, including denial of the illness as well as cultural and social stigma. With the ubiquity of social media platforms, millions of people are now sharing their online persona by expressing their thoughts, moods, emotions, and even their daily struggles with mental health on social media. Unlike traditional observational cohort studies conducted through questionnaires and self-reported surveys, we explore the reliable detection of depressive symptoms from tweets obtained, unobtrusively. Particularly, we examine and exploit multimodal big (social) data to discern depressive behaviors using a wide variety of features including individual-level demographics. By developing a multimodal framework and employing statistical techniques to fuse heterogeneous sets of features obtained through the processing of visual, textual, and user interaction data, we significantly enhance the current state-of-the-art approaches for identifying depressed individuals on Twitter (improving the average F1-Score by 5 percent) as well as facilitate demographic inferences from social media. Besides providing insights into the relationship between demographics and mental health, our research assists in the design of a new breed of demographic-aware health interventions.",2,1,2,1,0,0,0,0,0,2,1,1,0,0,0,0,1,0,1,0
https://aclanthology.org/W18-0607.pdf,15,2,,"Ive, Gkotsis, Dutta, Stewart, Velupillai",5,0,0,0,2018,"Depression is a highly prevalent public health concern and a major cause of disability worldwide. Depression affects 6.7% (i.e., about 16 million) Americans each year [1]. According to the World Mental Health Survey conducted in 17 countries, about 5% of people reported having at least one depressive episode in 2011 [2]. Untreated or undertreated depressive symptoms can lead to suicide and other chronic and risky behaviors such as drug or alcohol addiction [3]. More than 90% of people who commit suicide have a pre-existing diagnosis of depression [4]. Global efforts to curb depression involve identifying depressive symptoms through survey-based methods employing online questionnaires. These approaches suffer from under-representation as well as sampling bias. Survey data also exhibit problems due to temporal gaps between the data collection and dissemination of findings. Recent years have witnessed rapid growth in the analysis of social media for studying a wide range of health problems from detecting the influenza epidemic [5] and cardiac arrest [6] to studying mood and mental health conditions [7, 8]. The widespread adoption of social media where people voluntarily and publicly express their thoughts, moods, emotions, and feelings, and share their daily struggles with mental health has not been adequately tapped into studying mental illnesses, such as depression. Insights gleaned from social media such as Twitter can be complementary to the current survey-based methods that can assist both governmental and non-governmental organizations in policy development. The visual and textual content shared on different social media platforms like Twitter offer new opportunities for a deeper understanding of self-expressed depression both at an individual and community-level. For instance, the news headline “Twitter Fail: Teen Sent 144 Tweets Before Committing Suicide & No One Helped” highlights the need for better tools for gleaning useful insights from user generated content on social media platforms that can assist policy designers in providing resources for individuals with depressive symptoms. Recent analyses have lead to data-driven discoveries alongside the traditional hypothesis-testing social science process [9]. They have suggested that language style, sentiment, users’ activities, and engagement expressed in social media posts can predict the likelihood of depression [10, 11]. These studies often use psycholinguistic analysis, supervised and unsupervised language modeling, and expressed topics of interest. However, except for a few attempts, [12–15], these investigations have seldom studied extraction of emotional state from the visual content of posted images and profile images. Visual content can express users’ emotions more vividly, and psychologists have noted that imagery is an effective medium for communicating difficult emotions. According to eMarketer [16], photos accounted for 75% of content posted on Facebook worldwide, and are the most engaging type of content (87%). Indeed, “a picture is worth a thousand words” and now, “photos are worth a million likes.” Similarly, on Twitter, the tweets with image links get twice as much attention as those without [17], and video-linked tweets drive up engagement [18]. The ease and naturalness of expression through visual imagery can serve to glean depressive symptoms in vulnerable individuals who often seek social support through social media [19]. Further, as psychologist Carl Rogers highlights, we often pursue and promote our Ideal-Self. In this regard, the choice of profile image can be a proxy for one’s online persona [20], providing a window into an individual’s mental health status. For instance, choosing a profile image with the emaciated legs of an individual with several cuts portrays negative self-view [21]. Moreover, psychologists have argued that people use pictures to communicate messages in social media posts which represent our “Ideal Self”, or who we want to be. Indeed, we are constantly motivated to pursue behaviors that bring us closer to our Ideal Self. Inferring demographic information like gender and age can be crucial for stratifying our understanding of population-level epidemiology of mental health disorders. Relying on electronic health records data, previous studies have explored gender differences in depressive behavior from different angles including prevalence, age of onset, comorbidities, as well as biological and psychosocial factors. For instance, women have been diagnosed with depression twice as often as men, [22] and a national psychiatric morbidity survey in the UK has shown a higher risk of depression in women [23]. On the other hand, suicide rates for men are three to five times higher compared to women [24]. Women are more likely to socialize and express their dysphoria, while men tend to express their anger and show negative behaviors such as alcohol abuse and drug dependency [25]. Although depression can affect anyone at any age, the signs and risk factors for depression vary for different age groups [26]. Depression triggers for children include domestic violence, and loss of a pet, or family member. For adolescents, depression may arise from hormonal imbalances [27]. Late-life depression has caused the suicide rate in people aged 80 to 84 to be more than twice that of the general population [28]. Depression in the elderly population often occurs with other medical conditions that persist, which can increase the risk of death. Therefore, inferring demographic information while studying depressive behavior from passively sensed social data can shed better light on the population-level epidemiology of depression. The recent advancements in deep neural networks, specifically for image analysis tasks, can lead to detecting demographic features such as age and gender [29]. We aim to show that by determining and integrating a heterogeneous set of features from different modalities—aesthetic features from posted images (colorfulness, hue variance, sharpness, brightness, blurriness, naturalness), choice of profile picture (for gender, age, and facial expression), screen name, language features from both textual content and profile’s description (n-gram, emotion, sentiment), sociability from ego-network, and user engagement—we can identify individuals who are more likely to be depressed from a data set of 8,770 human-annotated Twitter users. We address the following research questions: 1) How well does the content of posted images (colors, aesthetic, and facial presentation) reflect depressive symptoms? 2) Does the choice of profile picture show any psychological traits corresponding to a depressed online persona? 3) Are profiles pictures reliable enough to represent demographic information such as age and gender, and can they be used for community-level management of depression? 4) Are there any underlying themes among depressed individuals generated using multimodal content that can be used to reliably detect depression? Our contributions include: Analysis of the content of posted images in terms of colors, aesthetic, facial presentation, and their associations with depressive symptoms; Uncovering the underlying relationships between visual and contextual content of likely depressed profiles obtained using a demographic inference process which can facilitate community-level management of depression; and Testing the performance of our interpretable heterogeneous feature set for predicting depressive symptoms.",2,1,2,1,0,0,0,0,0,2,1,1,0,0,0,0,1,0,1,0
https://aclanthology.org/W18-0607.pdf,15,3,,"Ive, Gkotsis, Dutta, Stewart, Velupillai",5,0,0,0,2018,"We have divided the related work into four subsections. First, we discuss the state-of-the-art approaches for studying depressive behavior on social data. Second, we review studies that have inferred demographic information using social media data.Then, we discuss the association between color sensitivity and mental health disorders. Finally, we cover state-of-the-art studies that have used visual imagery to study individual’s behavior.",2,1,2,1,0,0,0,0,0,2,1,1,0,0,0,0,1,0,1,0
https://aclanthology.org/W18-0607.pdf,15,4,,"Ive, Gkotsis, Dutta, Stewart, Velupillai",5,0,0,0,2018,"Several efforts have attempted to automatically detect depression from social media content utilizing machine learning, deep learning, and natural language processing approaches. From conducting a retrospective study of tweets, De Choudhury et al., (2013) characterizes depression based on factors such as language, emotion, style, ego-network, and user engagement. They built a classifier to predict the likelihood of depression from a written post [30] or an individual’s profile [31]. Moreover, there have been significant advances due to the shared task [32] focusing on methods for identifying depressed users on Twitter at the Computational Linguistics and Clinical Psychology Workshop (CLP 2015). A corpus of nearly 1,800 Twitter users was built for evaluation, and the best models employed topic modeling [33], Linguistic Inquiry and Word Count (LIWC) features, and other metadata [34]. More recently, a neural network architecture has been introduced [35] to combine Twitter posts into a representation of users’ activities for detecting depressed users. Another active line of research has focused on capturing warning signs of suicide and self-harm [36]. Through analysis of tweets posted by individuals attempting committing suicide, they indicate quantifiable signals of suicidal ideations. Moreover, the CLP 2016 [36] defined a shared task on detecting the severity of mental health from forum posts. All of these studies derive discriminative features to classify depression in user-generated content at message-level, individual-level, or community-level. The recent emergence of photo-sharing platforms such as Instagram has attracted researchers’ attention to study individual’s behavior from their visual narratives—ranging from mining their emotions [37], and happiness trend [38], to studying medical concerns [39]. Researchers have shown that people use Instagram to engage in social exchange and share their difficult experiences [13]. The role of visual imagery as a mechanism of self-disclosure by relating visual attributes to mental health disclosures on Instagram was highlighted by [14] where individual Instagram profiles were utilized to build a prediction framework for identifying markers of depression. The importance of data modality to understand user behavior on social media has been highlighted by [40]. More recently, a deep neural network sequence modeling approach that marries audio and text data modalities to analyze question-answer style interviews between an individual and an agent has been developed to study mental health [40]. Similarly, a multimodal depressive dictionary learning process was proposed to detect depressed users on Twitter [41]. They provide sparse user representations by defining a feature set consisting of social network features, user profile features, visual features, emotional features, topic-level features, and domain-specific features. Particularly, our choice to develop a multi-modal prediction framework is intended to improve upon previous work involving the use of images in multimodal depression analysis [41] and prior work on studying Instagram photos [15].",2,1,2,1,0,0,0,0,0,2,1,1,0,0,0,0,1,0,1,0
https://aclanthology.org/W18-0607.pdf,15,5,,"Ive, Gkotsis, Dutta, Stewart, Velupillai",5,0,0,0,2018,"Social media has been introduced as a critical channel to answer diverse research questions offering a wealth of data for public health research [42–44]. It can also assist in better understanding the relationship between behavioral changes and population health [45]. However, the lack of demographic indicators (e.g. age, gender, race) within the data is a major limitation for gaining deeper insights. Several research efforts have attempted to automate detection of social media users’ demographic information as summarized below. For gender inference, several studies have analyzed users’ tweets to detect gender differences reflected in linguistic patterns [46]), profile colors [47], names [48], profile images [49], social network connections [50], and user description [46]. For instance, a supervised model was developed by [51] to determine users’ gender by employing features such as screen-name, full name, profile description, and content on external resources (e.g., personal blog). Another supervised model was built to predict the user’s age group by employing features including emoticons, acronyms, slang words and phrases, punctuation, capitalization, sentence length, and included links/images, along with online behaviors such as number of friends, post time, and commenting activity [52]. To attempt to infer the age of Dutch Twitter users, a model was built that utilizes the life stage of users such as secondary school student, college student, or employee [53]. Similarly, a novel model was introduced for extracting age for Twitter users by relying on profile descriptions while devising a set of rules and patterns [54]. They also parse descriptions for occupation by consulting the SOC2010 list of occupations [55] and validating it through social surveys. A novel age inference model was developed while relying on homophily interaction information and content to predict the age of Twitter users [56]. The intuition is that people within the same age group share similar content and become friends with contemporaries. Using an extensive set of experiments, they show that their model outperformed other state-of-the-art age inference models by leveraging online interaction and content information simultaneously. The limitations of textual content for predicting age and gender was highlighted by [57]. They distinguish language use based on social gender, age identity, biological sex, and chronological age by collecting crowdsourced signals from a game in which players (crowd) guess the biological sex and age of a user based only on their tweets. Their findings indicate how linguistic markers can be misleading (e.g., a heart represented as <3 can be misinterpreted as feminine when the writer is male). Estimating age and gender from facial images by training convolutional neural networks (CNN) for face recognition is another active line of research [58].",2,1,2,1,0,0,0,0,0,2,1,1,0,0,0,0,1,0,1,0
https://aclanthology.org/W18-0607.pdf,15,6,,"Ive, Gkotsis, Dutta, Stewart, Velupillai",5,0,0,0,2018,"The strong associations between color sensitivity and mood has been highlighted by several studies [59]. In an earlier research, a strong correlation between specific color selection such as yellow and depressive behavior has been reported by [60]. With respect to color discrimination, findings based on a sample of 20 male patients, aged 18 between 45 years old with schizophrenia and manic-depressive psychosis, indicated that when their right hemisphere was depressed, the identification of color by saturation, shade, and color tone was impaired [61]. More recently, the association of color vision with bipolar disorder explored [62]. The general findings suggest that people suffering from depression are likely to reveal their mood through their choice of colors (such as preference for darker shades) in everyday life situations [63]. In this study, we leveraged the visual content shared on Twitter for studying such signals.",2,1,2,1,0,0,0,0,0,2,1,1,0,0,0,0,1,0,1,0
https://aclanthology.org/W18-0607.pdf,15,7,,"Ive, Gkotsis, Dutta, Stewart, Velupillai",5,0,0,0,2018,"The recent emergence of photo-sharing platforms such as Instagram, provides a unique opportunity to study people’s behavior through the emotions [37] with broader application in personality prediction [64] and demographic inferences. Utilizing these platforms for population-levels analysis helps to improve public health concerns [39] such as obesity [65], substance use [66], depression, and anxiety [67]. With regards to personality prediction, early efforts have shown that bag-of-visual-words and Facebook profile images could predict users’ personality [68]. Various sets of features have been obtained from the images of 11,736 Facebook users were extracted to build a computational model which has more predictive power than human raters for predicting similar personality traits [69].",2,1,2,1,0,0,0,0,0,2,1,1,0,0,0,0,1,0,1,0
https://aclanthology.org/W18-0607.pdf,15,8,,"Ive, Gkotsis, Dutta, Stewart, Velupillai",5,0,0,0,2018,"This study is focused on obtaining community-level insights about depression signs and depressive behavior. As such, even though we analyzed individual’s behavioral health information–which is considered sensitive—we utilized anonymized users in our datasets as per the approved Institutional Review Board (IRB) protocol. The study was approved and the informed consent process by Wright State University Institution review Board (SC#6258) 4.1.3. Self-disclosure refers to revealing personal and intimate information about oneself to others, which can be therapeutic for psychological well-being [70]. Previous efforts highlight diverse modes of mental health self-disclosures on social media [12]. Self-disclosure clues have been extensively utilized for creating ground-truth data for numerous social media analytic studies such as predicting users’ demographics [54], and depressive behavior [8]. For instance, vulnerable individuals may employ depressive-indicative terms in their Twitter profile descriptions. Other individuals may share their age and gender, e.g., “16 year old suicidal girl”. We employed a large dataset of 45,000 Twitter users with self-reported depressive symptoms introduced initially in [8]. All information was obtained using advanced search API [71]. To seed the search, we created a lexicon of depressive symptoms consisting of 1,500 depressive-indicative terms with the help of clinical psychologists, and employed it to collect the Twitter profiles of individuals with self-declared depressive symptoms [72]. More specifically, the dataset provides the users’ profile information including screen name, profile description, follower/followee counts, profile image, and tweet content, which can express various depression-relevant characteristics, and determine whether a user indicates any depressive behavior. Three human judges from the Department of Psychology at Wright State University assisted us in creating this annotated dataset. We reported the inter-rater agreement as K = 0.74 based on Cohen’s Kappa statistics [8]. To create a robust gold standard dataset, we discarded the instances in which at least two (out of three) of our annotators did not agree about the depressive symptoms. Our final dataset contains 8770 users with 3981 depressed users, and 4789 control users that do not express any depressive symptoms in their Twitter data. This dataset Ut contains the metadata values of each user such as profile descriptions, followers_count, created_at, and profile_image_url. Table 1 illustrates a sample of depressive-indicative phrases that appear in tweets from likely vulnerable users To further measure the robustness of our dataset, we conducted another experiment by obtaining additional annotation from our colleagues from the Department of Psychiatry at Weill Cornell Medical College. Using the following formula, we computed a statistically reliable sample size: where N is population size, Z is z-score, e denotes margin of error, and p represents standard deviation. Specifically, we employed our dataset of 8770 (population size), and confidence interval of 95% (margin of error 5%) to obtain 400 users as a concrete sample size. We then randomly selected 400 users from the dataset of 8770 users to be evaluated by two additional human judges (from the Department of Psychiatry at Weill Cornell Medical College) by manually annotating whether users’ content reflected depressive behavior or not. The average inter-rater agreement was (85% agreement, 0.77) based on Cohen’s Kappa statistics, which denotes substantial agreement and implies the robustness of our dataset.",2,1,2,1,0,0,0,0,0,2,1,1,0,0,0,0,1,0,1,0
https://aclanthology.org/W18-0607.pdf,15,9,,"Ive, Gkotsis, Dutta, Stewart, Velupillai",5,0,0,0,2018,"We extracted a user’s age by applying regular expression patterns to profile descriptions (such as “17 years old, self-harm, anxiety, depression”) [54]. We compiled “age prefixes” and “age suffixes”, and used three age-extraction rules: 1. I am X years old, 2. Born in X, and 3. X years old, where X is a “date” or age (e.g., 1994). We selected a subset of 1061 users among Ut as gold standard dataset Ua who disclosed their age. From these 1061 users, 822 belonged to the depressed class, and 239 belonged to the control class. From the 3981 depressed users, 20.6% disclosed their age in contrast with only 4% (239/4789) among the control group, suggesting that self-disclosure of age is more prevalent among vulnerable users. Fig 1 depicts the age distribution in Ua. The general trend, consistent with the results in [56, 73], is biased toward younger individuals. Indeed, according to the Pew Research Center, 47% of Twitter users are in general 30 years old or younger [74]. Similar data collection procedures with comparable distribution have been used previously [56]. We discuss our approach to mitigate the impact of the bias in Section 3. The median age is 17 for the depressed class versus 19 for the control class. This suggests that the depressed-user population is younger, or depressed adolescents are more likely to disclose their age in order to connect with peers (social homophily) [75].",2,1,2,1,0,0,0,0,0,2,1,1,0,0,0,0,1,0,1,0
https://aclanthology.org/W18-0607.pdf,15,10,,"Ive, Gkotsis, Dutta, Stewart, Velupillai",5,0,0,0,2018,"We selected a subset of 1464 users Ug from Ut who disclosed their gender in their profile description. Out of 1464 users, 64% belonged to the depressed group, and the rest (36%) belonged to the control group. 23% of the likely depressed users disclosed their gender, which is considerably higher (12%) than that of the control class. Once again, gender disclosure varies among the two gender groups. For statistical significance, we performed a chi-square test (null hypothesis: gender and depression are two independent variables). Fig 2 illustrates gender association with each of the two classes. Blue circles (positive residuals, see Fig 2A and 2D) show a positive association among corresponding row and column variables, and the red circles (negative residuals, see Fig 2B and 2C) imply a repulsion. Our findings indicate a strong association (Chi-square: 32.75, p-value:1.04e-08) between female gender, and expression of depressive symptoms on Twitter. These observations are consistent with the current literature which have shown that more women than men are diagnosed with depression [76]. In particular, the female-to-male ratio is 2:1 and 1:9 for major depressive disorder and dysthymic disorder, respectively.",2,1,2,1,0,0,0,0,0,2,1,1,0,0,0,0,1,0,1,0
https://aclanthology.org/W18-0607.pdf,15,11,,"Ive, Gkotsis, Dutta, Stewart, Velupillai",5,0,0,0,2018,We now provide an in-depth analysis of visual and textual content of vulnerable users.,2,1,2,1,0,0,0,0,0,2,1,1,0,0,0,0,1,0,1,0
https://aclanthology.org/W18-0607.pdf,15,12,,"Ive, Gkotsis, Dutta, Stewart, Velupillai",5,0,0,0,2018,"We show that the visual content in posted images and profile images provide valuable psychological cues for understanding a user’s depression status. Profile images and posted images can surface self-stigmatization [77]. As opposed to a typical computer vision framework for object recognition that relies on thousands of predetermined low-level features, emotions reflected in facial expressions are important when assessing user’s online behavior, attributes contributing to the computational aesthetics, and sentimental quotes they may subscribe to. The following sections present an in-depth analysis of visual content for both the depressed class and the control class with respect to three aspects: facial presence, facial expressions, and general image features.",2,1,2,1,0,0,0,0,0,2,1,1,0,0,0,0,1,0,1,0
https://aclanthology.org/W18-0607.pdf,15,13,,"Ive, Gkotsis, Dutta, Stewart, Velupillai",5,0,0,0,2018,"For capturing facial presence, we employed the model has been introduced in [78] where a multilevel convolutional coarse-to-fine network cascade developed to tackle facial landmark localization problem. We identified facial presentation, emotion from facial expression, and demographic features from profile images and posted images [79]. Table 2 illustrates facial presentation differences in both profile and posted images (media) for depressed users and control users in Ut. For the control class, facial presence was significantly higher in both profile images and shared media (8%, 9% respectively) compared to the depressed class. In contrast with age and gender disclosure, vulnerable users were less likely to disclose their facial identity, possibly due to lack of confidence or fear of stigma.",2,1,2,1,0,0,0,0,0,2,1,1,0,0,0,0,1,0,1,0
https://aclanthology.org/W18-0607.pdf,15,14,,"Ive, Gkotsis, Dutta, Stewart, Velupillai",5,0,0,0,2018,"Following [20]’s approach, we adopted Ekman’s model [80] of six emotions: anger, disgust, fear, joy, sadness, and surprise, and used the Face++ API [79] to automatically capture these emotions from the shared images. The positive emotions were joy and surprise, and negative emotions were anger, disgust, fear, and sadness. Foreach user u in Ut, we processed profile images and shared images for both the depressed and control groups with at least one face from the shared images (Table 3). For the images that contained multiple faces, we perform mean pooling over the frames to obtain the expected emotional features. Fig 3 illustrates the inter-correlation of these features. Additionally, we have observed that the emotions extracted from facial expressions correlated with the emotional signals captured from textual content utilizing LIWC. This indicates that visual imagery can be utilized as a complementary channel for measuring online emotional signals.",2,1,2,1,0,0,0,0,0,2,1,1,0,0,0,0,1,0,1,0
https://aclanthology.org/W18-0607.pdf,15,15,,"Ive, Gkotsis, Dutta, Stewart, Velupillai",5,0,0,0,2018,"The importance of interpretable computational aesthetic features for studying users’ online behavior has been highlighted by several efforts [81]. Color, as a pillar of the human vision system, has a strong association with conceptual ideas like emotion [82]. We measured the normalized red, green, blue, the mean of the original colors, brightness, and contrast relative to variations of luminance. We represented images in Hue-Saturation-Value color space that seems intuitive for humans, and measured the mean and variance for saturation and hue. Saturation is defined as the difference in intensity between different light wavelengths that compose the color. Although hue is not interpretable, high saturation indicates vividness and chromatic purity, which are more appealing to the human eye [20]. Colorfulness is measured as a difference against gray background [83]. Naturalness is a measure of correspondence between images and human perception of reality [83]. In color reproduction, naturalness is measured from the mental recollection of the colors of familiar objects. Additionally, there is a tendency among vulnerable users to share sentimental quotes bearing negative emotions. We performed optical character recognition (OCR) with python-tesseract [84] to extract text and their sentiment [85] score. As illustrated in Table 4, vulnerable users tend to use less colorful (higher grayscale) profile images and shared images to convey their negative feelings, and also share images that are less natural. In general, control users identified darker, grayer colors with negative mood, and generally preferred brighter, more vivid colors. By contrast, vulnerable users were found to prefer darker, grayer, and bluer colors. We found a strong positive correlation between self-declared depression and a tendency to perceive one’s surroundings as gray or lacking in color. With respect to the aesthetic quality of images (saturation, brightness, and hue), there is a significant difference between the two classes, with depressed users more frequently sharing images that are less appealing to the human eye. We employed an independent samples t-test, while adopting Bonferroni Correction as a conservative approach to adjust the confidence intervals. Overall, we had 223 features, and chose Bonferroni-corrected alpha level of 0.05/223 = 2.24e − 4 (*** p < alpha, **p < 0.05). In general, the control users identified darker, grayer colors with negative moods, and generally preferred brighter, more vivid colors. In contrast, vulnerable users preferred darker, grayer colors, and bluer images. Vulnerable users shared images that are less aesthetically pleasing with lower sharpness, and those that do not contain faces or contain only one face. On the other hand, control users tended to use sharper images with multiple faces. Additionally, vulnerable users shared images with more text content, often containing depressive quotes and negative sentiments. The desire to socialize and connect with others is also manifested in the visual imagery of vulnerable users. The images shared by vulnerable users tend to contain a single face (belonging to the user), rather than surrounded by friends and family. This further indicates the focus on the self, which is one of the most consistent markers of a mental disorder. This is also associated with an extensive usage of first person singular pronouns—which is another reliable marker of depression in content analysis of depressive behavior.",2,1,2,1,0,0,0,0,0,2,1,1,0,0,0,0,1,0,1,0
https://aclanthology.org/W18-0607.pdf,15,16,,"Ive, Gkotsis, Dutta, Stewart, Velupillai",5,0,0,0,2018,"LIWC [86] has been used extensively for examining the latent dimensions of self-expression for analyzing personality [87], depressive behavior, demographic differences [53, 57], etc. Several studies have shown that females employ more first-person singular pronouns [88], and deictic language (context-dependent words) [89], while males tend to use more articles [90] which characterize concrete thinking, and formal, informational, affirmative words [91]. For age analysis, the salient findings show that older individuals use more future tense verbs, [88] suggesting a shift in focus while aging. They also show more positive emotions [92], employ fewer self-references (i.e. ‘I’, ‘me’), and more first person plural pronouns [88]. Depressed users employ first person pronouns more frequently [93], and repeatedly use negative emotions and anger words. We analyzed psycholinguistic cues and language style to study the association between depressive behavior and demographics. Specifically, we adopted Levinson’s adult development grouping [94] that partitions users in Ua into 5 age groups: (14,19], (19,23], (23,34], (34,46], and (46,60]. Then, we applied LIWC for characterizing linguistic styles for each age group for users in Ua.",2,1,2,1,0,0,0,0,0,2,1,1,0,0,0,0,1,0,1,0
https://aclanthology.org/W18-0607.pdf,15,17,,"Ive, Gkotsis, Dutta, Stewart, Velupillai",5,0,0,0,2018,"The recent LIWC version [86] summarizes textual content in terms of language variables such as analytical thinking, clout, authenticity, and emotional tone. It also measures other linguistic dimensions such as descriptor categories (e.g., percent of target words gleaned from the dictionary, or words longer than six letters—Sixltr), informal language markers (e.g., swear words, netspeak), and other linguistic aspects (e.g., first person singular pronouns). Thinking Style: The words we use to communicate can reveal our style of thinking. There are two common approaches for extracting an individual’s thinking style. First, measuring one’s natural way of trying to understand, analyze, and organize complex events has a strong association with analytical, formal, and logical thinking. LIWC relates higher analytic thinking to more formal and logical reasoning, whereas a lower value indicates a focus on narratives. Second, cognitive processing, which measures problem solving in the mind, is captured through words such as “think,” “believe,” “realize,” and “know” and demonstrates “certainty” in communication. High values for analytical thinking implies clarity of thought. Critical thinking ability is related to education [95], and is impacted by different stages of cognitive development at different ages [96]. It has been shown that older people communicate with greater cognitive complexity while comprehending nuances and subtle differences [95]. All of these findings corroborate with our results (Table 5) We observed notable differences in raw intelligence and the ability to think analytically in depressed and control users among different age groups (see Fig 4A and 4F and Table 5). Overall, vulnerable younger users do not think as logically based on their relative analytical score and cognitive processing ability. We can also observe that the differences between age groups above 35 tend to become smaller [97]. Authenticity: Authenticity measures the degree of honesty. Authenticity is often assessed by measuring present tense verbs, first person singular pronouns (e.g., I, me, my), and by examining the linguistic manifestations of false stories [98]. People who lie use fewer self-references, and fewer complex words. Psychologists often see a child’s first successful lie as a mental milestone growth [99]. There is a decreasing trend in authenticity with age (see Fig 4B). Authenticity for depressed adolescents is strikingly higher than their control peers, and decreases with age (Fig 4B). Clout: People with high clout speak more confidently and with certainty, employing more social words with fewer negations (e.g., no, not) and swear words. In general, mid-life is relatively stable w.r.t. relationships and work. A recent study has shown that age 60 is best for self-esteem [100] as people take on managerial roles at work, and maintain satisfyinging relationships with their spouses. We see the same pattern in our data (see Fig 4C and Table 5). Unsurprisingly, lack of confidence (the 6th PHQ-9 [101] symptom) is a distinguishable characteristic of vulnerable users, leading to their lower clout scores, especially among depressed users younger than 34 years old. Self-references: First person singular words often indicate interpersonal involvement, and their high usage is associated with negative affective states such as nervousness and depression [92]. Consistent with prior studies, the frequency of first person singular words for depressed users is significantly higher compared to that of the control class. Similarly to [92], adolescents tend to use more first-person (e.g. I), and second person singular (e.g. you) pronouns (Fig 4G). The impact of the above phenomenon is reflected in significantly higher frequency of self-references for depressed adolescents. As with the control class, a downtrend suggests that as depressed individuals age, they make more distinctions and psychologically distance themselves from their topics. Informal Language Markers; Swear, Netspeak: Swear lexicon includes terms such as “fu**”, “dam*”, and “shi*”. Several studies have highlighted that the use of profanity by young adults has significantly increased over the last decade [102]. We observed the same pattern in both the depressed and the control classes (Table 5), with a higher rate for depressed users [10]. Psychologists have also shown that swearing may indicate that an individual is not a fragmented member of a society [103]. Depressed adolescents who show a higher rate of interpersonal involvement and relationships, have a higher rate of cursing (Fig 4E). Also, Netspeak lexicon measures the frequency of terms such as ‘lol’ and ‘thx’. Although the rate is higher for the depressed class, we did not find any pattern concerning adult development. Sexual, Body: The sexual lexicon contains terms like “horny”, “love”, and “incest”, and body terms like “ache”, “heart”, and “cough”. Both start with a higher rate for depressed users and decreases gradually as they age, possibly due to changes in sexual desire with age [104] (Fig 4H and 4I and Table 5).",2,1,2,1,0,0,0,0,0,2,1,1,0,0,0,0,1,0,1,0
https://aclanthology.org/W18-0607.pdf,15,18,,"Ive, Gkotsis, Dutta, Stewart, Velupillai",5,0,0,0,2018,"We employed a one-way ANOVA to compare the impact of various factors, and validate our findings above. Table 5 illustrates our findings, with a degree of freedom (df) of 1055. The null hypothesis is that the sample means for each age group are similar for each of the LIWC features.",2,1,2,1,0,0,0,0,0,2,1,1,0,0,0,0,1,0,1,0
https://aclanthology.org/W18-0607.pdf,15,19,,"Ive, Gkotsis, Dutta, Stewart, Velupillai",5,0,0,0,2018,We leveraged both the visual and textual content for predicting age and gender.,2,1,2,1,0,0,0,0,0,2,1,1,0,0,0,0,1,0,1,0
https://aclanthology.org/W18-0607.pdf,15,20,,"Ive, Gkotsis, Dutta, Stewart, Velupillai",5,0,0,0,2018,"We employed [105]’s weighted lexicon of terms that uses the dataset of 75,394 Facebook users who shared their status, age, and gender. The predictive power of this lexica was evaluated on Twitter, and Facebook, showing promising results [105]. Utilizing these two weighted lexicon of terms, we are predicting the demographic information (age or gender) of useri (denoted by Demoi) using the following equation: where Weightlex(term) is the lexicon weight of the term, and Freq(term, doc)i represents the frequency of the term in the user generated doci, and WC(doc)i measures total word count in (doc)i. As our data are biased toward younger individuals, we report age prediction performance for each age group, separately (Table 6). Moreover, to measure the average accuracy of this model, we built a balanced dataset (keeping the total number of users above 23—416), and then randomly sampled the same number of users from the age ranges (11,19] and (19,23]. The average accuracy of this model was 0.63 for depressed users, and 0.64 for the control class. Table 8 illustrates the performance of gender prediction for each class. The average accuracy was 0.82 on Ug ground-truth dataset.",2,1,2,1,0,0,0,0,0,2,1,1,0,0,0,0,1,0,1,0
https://aclanthology.org/W18-0607.pdf,15,21,,"Ive, Gkotsis, Dutta, Stewart, Velupillai",5,0,0,0,2018,"Inspired by [78]’s approach for facial landmark localization, we used their pre-trained CNN consisting of convolutional layers, including unshared and fully-connected layers, to predict gender and age from both the profile and shared images. We evaluated the performance of the gender and age prediction task on Ug and Ua, respectively, as shown in Table 6.",2,1,2,1,0,0,0,0,0,2,1,1,0,0,0,0,1,0,1,0
https://aclanthology.org/W18-0607.pdf,15,22,,"Ive, Gkotsis, Dutta, Stewart, Velupillai",5,0,0,0,2018,"We delved deeper into the benefits and drawbacks of each data modality for demographic information prediction. This is crucial as the differences between language cues between age groups above 35 tend to become smaller (see Fig 4A, 4B and 4C), making the prediction harder for older individuals [97]. In this case, the other data modality (e.g., visual content) played an integral role as a complementary source for age inference. For gender prediction, on average, the profile image-based predictor provided a more accurate prediction for both the depressed and the control class (0.92 and 0.90), compared to the content-based predictor (0.82). For age prediction (see Table 6), the textual content-based predictor (on average 0.60) outperformed both of the visual-based predictors (on average profile: 0.51, Media: 0.53). However, not every user provided facial identity on his or her account (see Table 2). We studied facial presentation for each age group to examine any association between age group, facial presentation, and depressive behavior (see Table 7). We can see youngsters in both the depressed and control classes are not likely to present their face in their profile image. Less than 3% of vulnerable users between 11-19 years revealed their facial identity. Although the content-based gender predictor was not as accurate as the image-based predictor, it is adequate for population-level analysis (see Table 8).",2,1,2,1,0,0,0,0,0,2,1,1,0,0,0,0,1,0,1,0
https://aclanthology.org/W18-0607.pdf,15,23,,"Ive, Gkotsis, Dutta, Stewart, Velupillai",5,0,0,0,2018,"We used the above findings for predicting depressive behaviors. Our model exploits an early fusion [40] technique in feature space and requires modeling each user u in Ut as vector concatenation of individual modality features. As opposed to the computationally expensive late fusion schemes, where each modality requires a separate supervised modeling, this model reduces the learning effort and has shown promising results [106]. To develop a generalizable model that avoids overfitting, we performed feature selection using statistical tests and all relevant ensemble learning models. Adding feature selection tests adds randomness to the data by creating shuffled copies of all features (shadow feature), and then trains the Random Forest classifier on the extended data. Iteratively, it checks whether the actual feature has a higher Z-score than its shadow feature (See Algorithm 1 and Fig 5) [107].",2,1,2,1,0,0,0,0,0,2,1,1,0,0,0,0,1,0,1,0
https://aclanthology.org/W18-0607.pdf,15,24,,"Ive, Gkotsis, Dutta, Stewart, Velupillai",5,0,0,0,2018,"Next, we adopted an ensemble learning method which integrated the predictive power of multiple learners with two main advantages; a high degree of interpretability with respect to the contributions of each feature, and a high predictive power. For prediction, we have where ft is a weak learner and denotes the final prediction. In particular, we optimized the loss function: where φ incorporates L1 and L2 regularization. In each iteration, the new ft(ui) is obtained by fitting the weak learner to the negative gradient of loss function. Particularly, by estimating the loss function with Taylor expansion: where its first expression is constant, the second and the third expressions are first (gi) and second order derivatives (hi) of the loss. To explore the weak learners, assume ft has k leaf nodes, Ij be subset of users from Ut belongs to the node j, and wj denotes the prediction for node j. Then, for each user i belonging to Ij, ft(ui) = wj and Next, for each leaf node j, deriving w.r.t wj: and by substituting weights: which represents the loss of fixed weak learners with k nodes. The trees are built sequentially, such that each subsequent tree aims to reduce the errors of its predecessor trees. Although, the weak learners have a higher degree of biases, the ensemble model produces a strong learner that effectively integrates the weak learners by reducing bias and variance (the ultimate goal of supervised models) [108, 109]. Table 9 illustrates how our multimodal framework outperforms the baselines for identifying depressed users in terms of average specificity, sensitivity, F-Measure, and accuracy in a 10-fold cross-validation setting on Ut dataset. Fig 6 shows how the likelihood of being classified into the depressed class varies with each feature added to the model for a sample user in the dataset. The prediction bar (the black bar) shows that the log-odds of prediction is 0.31, that is, the likelihood of this person being a depressed user is 57% (1 / (1 + exp(-0.3))). The figure also sheds light on the impact of each contributing feature. The waterfall charts represent how the probability of being depressed varies when adding each feature. For example, for our dataset, the “Analytic thinking” score measured by LIWC from the tweet content is a high value of 48.43 (Median:36.95, Mean: 40.18) and this decreases the chance of the user being classified into the depressed group by the log-odds of -1.41. This is due to the fact that depressed users have significantly lower “Analytic thinking” scores compared to the control class. Moreover, the “Clout” score of 40.46 is considered a low value (Median: 62.22, Mean: 57.17), and increases the chance of being classified as a depressed user. This is justifiable given the clear association between low self-esteem and risk for depression. With respect to the visual features, the mean and the median of “shared colorfulness” is 112.03 and 113, respectively. The value of 136.71 would be high, and decreases the chance of being depressed by log-odds of -0.54. As mentioned earlier, depressed users preferred darker, and grayer colors. The score of 0.46 as “profile naturalness” is considered high compared to 0.36 (the mean for the depressed class) which justifies pull down of the log-odds by −0.25. Using network features, for instance, the “two hop neighborhood” for depressed users (Mean: 84) are less than that of the control users (Mean: 154), and is reflected in pulling down the log-odds by -0.27.",2,1,2,1,0,0,0,0,0,2,1,1,0,0,0,0,1,0,1,0
https://aclanthology.org/W18-0607.pdf,15,25,,"Ive, Gkotsis, Dutta, Stewart, Velupillai",5,0,0,0,2018,"To test the efficacy of our multi-modal framework for detecting depressed users, we compared it against existing content, content-network, and image-based models (based on the aforementioned general image features, facial presence, and facial expressions).",2,1,2,1,0,0,0,0,0,2,1,1,0,0,0,0,1,0,1,0
https://aclanthology.org/W18-0607.pdf,15,26,,"Ive, Gkotsis, Dutta, Stewart, Velupillai",5,0,0,0,2018,"Language biases in social media posts can be a good representative of emotional state. Fig 7 illustrates the word clouds that distinguish the word usage of likely-depressed and non-depressed profiles. It is clear that depressed users often care more about their appearance. This is indicative by their usage of terms such as “pretty” and “beautiful.” They also have a tendency to talk about their family and relations using words such as family, hugs, parents, daddy, mums, sigh, grandma, maam, friendless, love, friend, mommy, people, boyf, and gf. In contrast, the control users usually talk about daily events and news such as “hurricane” and “Trump”. Such differences in word usage highlight the fact that user generated words can be distinguishable features for detecting depressed user profiles. See Table 9 for the comparative performance of our prediction framework against state-of-the-art methods used for predicting depressive behaviors—many of which employed the same feature sets and hyperparameter settings (see Models I-V). Several prior efforts have demonstrated that word embedding models can reliably enhance short text classification [115], Model VI by employing pre-trained word embeddings which have trained over 400 million tweets [116] while representing a user with retrieving word vectors for all the words a user used in tweets and profile description. We aggregate these word vectors through their means and feed it as input to a SVM classifier with a linear kernel. In Model VII, we employed [8]’s dataset of 45,000 self-reported depressed users and trained a Skip-gram model with negative sampling to learn word representations. We chose this model because it generates robust word embeddings even when the collection of training words are sparse [117]. We set dimensionality to 300 and a negative sampling rate to 10 sample words, which has shown promising results with medium-sized datasets [117]. Besides, we have observed that many vulnerable users chose specific account names, such as “Suicidal_Thoughxxx,” and “younganxietyyxxx,” which are good indicators of their depressive behavior. We used Levenshtein distance between depression indicative terms in [8]’s depression lexicon and the screen name to capture their degree of semantic similarity [118].",2,1,2,1,0,0,0,0,0,2,1,1,0,0,0,0,1,0,1,0
https://aclanthology.org/W18-0607.pdf,15,27,,"Ive, Gkotsis, Dutta, Stewart, Velupillai",5,0,0,0,2018,"We employed the aforementioned visual content features including facial presence, aesthetic features, and facial expression for depression prediction. We use three different models: Logistic Regression (Model IX), SVM (Model X), and Random Forest (Model XI). The poor performance of image-based models suggests that relying on a unique modality would not be sufficient for building a robust model due to the complexity and abstruse nature of the prediction task.",2,1,2,1,0,0,0,0,0,2,1,1,0,0,0,0,1,0,1,0
https://aclanthology.org/W18-0607.pdf,15,28,,"Ive, Gkotsis, Dutta, Stewart, Velupillai",5,0,0,0,2018,"Network-based features indicate the user’s desire to socialize and connect with others. There is a notable difference between the number of friends, followers, favorites, and status count for depressed and control users (see Table 4). For building a baseline Model VIII, we obtained egocentric network measures for each user based on the network formed using @-replies interactions among them. The egocentric social graph of a user u is an undirected graph of nodes in u’s two-hop neighborhood in our Ua dataset, where the edge between nodes u and v implies that there has been at least one @-reply exchange. Network-based features including Reciprocity, Prestige Ratio, Graph Density, Clustering Coefficient, Embeddedness, Ego components and Size of two-hop neighborhood were extracted from each user’s network [10] to reliably capture user context for depression prediction. High values for the three metrics—clustering coefficient, embeddedness, and number of ego networks—indicates that the depressed users tend to build a close network of trusted people to share their mental health issues. For both graph density and size of the two-hop neighborhood, a lower value indicates fewer interactions.",2,1,2,1,0,0,0,0,0,2,1,1,0,0,0,0,1,0,1,0
https://aclanthology.org/W18-0607.pdf,15,29,,"Ive, Gkotsis, Dutta, Stewart, Velupillai",5,0,0,0,2018,"We presented an in-depth analysis of visual and contextual content of likely depressed profiles on Twitter. We employed them for demographic (age and gender) inference processes. We developed a multimodal framework, employing statistical techniques for fusing heterogeneous sets of features obtained by processing visual, textual, and user interactions. Conducting an extensive set of experiments, we assessed the predictive power of our multimodal framework while comparing it against state-of-the-art approaches for depressed user identification on Twitter. The empirical evaluation shows that our multimodal framework is superior to them and it improved the average F1-Score by 5 percent. Effectively, visual cues gleaned from content and profile images shared on social media can further augment inferences from textual content for reliable determination of depression indicators and diagnoses. In the future, we plan to apply our approach to various data sources such as longitudinal electronic health record (EHR) systems, and private insurance reimbursement and claims data, to develop a robust “big data” platform for detecting clinical depressive behavior at the community level.",2,1,2,1,0,0,0,0,0,2,1,1,0,0,0,0,1,0,1,0
https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0226248,16,1,,"Yazdavar, Mahdavinejad, Bajaj, Romine, Sheth, Monadjemi, Thirunarayan, Meddar, Myers, Pathak, Hitzler",11,0,0,0,2020,"Depression is a major public health concern in the U.S. and globally. While successful early identification and treatment can lead to many positive health and behavioral outcomes, depression, remains undiagnosed, untreated or undertreated due to several reasons, including denial of the illness as well as cultural and social stigma. With the ubiquity of social media platforms, millions of people are now sharing their online persona by expressing their thoughts, moods, emotions, and even their daily struggles with mental health on social media. Unlike traditional observational cohort studies conducted through questionnaires and self-reported surveys, we explore the reliable detection of depressive symptoms from tweets obtained, unobtrusively. Particularly, we examine and exploit multimodal big (social) data to discern depressive behaviors using a wide variety of features including individual-level demographics. By developing a multimodal framework and employing statistical techniques to fuse heterogeneous sets of features obtained through the processing of visual, textual, and user interaction data, we significantly enhance the current state-of-the-art approaches for identifying depressed individuals on Twitter (improving the average F1-Score by 5 percent) as well as facilitate demographic inferences from social media. Besides providing insights into the relationship between demographics and mental health, our research assists in the design of a new breed of demographic-aware health interventions.",2,2,2,1,2,2,0,2,4,2,2,2,2,0,0,0,0,2,4,0
https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0226248,16,2,,"Yazdavar, Mahdavinejad, Bajaj, Romine, Sheth, Monadjemi, Thirunarayan, Meddar, Myers, Pathak, Hitzler",11,0,0,0,2020,"Depression is a highly prevalent public health concern and a major cause of disability worldwide. Depression affects 6.7% (i.e., about 16 million) Americans each year [1]. According to the World Mental Health Survey conducted in 17 countries, about 5% of people reported having at least one depressive episode in 2011 [2]. Untreated or undertreated depressive symptoms can lead to suicide and other chronic and risky behaviors such as drug or alcohol addiction [3]. More than 90% of people who commit suicide have a pre-existing diagnosis of depression [4]. Global efforts to curb depression involve identifying depressive symptoms through survey-based methods employing online questionnaires. These approaches suffer from under-representation as well as sampling bias. Survey data also exhibit problems due to temporal gaps between the data collection and dissemination of findings. Recent years have witnessed rapid growth in the analysis of social media for studying a wide range of health problems from detecting the influenza epidemic [5] and cardiac arrest [6] to studying mood and mental health conditions [7, 8]. The widespread adoption of social media where people voluntarily and publicly express their thoughts, moods, emotions, and feelings, and share their daily struggles with mental health has not been adequately tapped into studying mental illnesses, such as depression. Insights gleaned from social media such as Twitter can be complementary to the current survey-based methods that can assist both governmental and non-governmental organizations in policy development. The visual and textual content shared on different social media platforms like Twitter offer new opportunities for a deeper understanding of self-expressed depression both at an individual and community-level. For instance, the news headline “Twitter Fail: Teen Sent 144 Tweets Before Committing Suicide & No One Helped” highlights the need for better tools for gleaning useful insights from user generated content on social media platforms that can assist policy designers in providing resources for individuals with depressive symptoms. Recent analyses have lead to data-driven discoveries alongside the traditional hypothesis-testing social science process [9]. They have suggested that language style, sentiment, users’ activities, and engagement expressed in social media posts can predict the likelihood of depression [10, 11]. These studies often use psycholinguistic analysis, supervised and unsupervised language modeling, and expressed topics of interest. However, except for a few attempts, [12–15], these investigations have seldom studied extraction of emotional state from the visual content of posted images and profile images. Visual content can express users’ emotions more vividly, and psychologists have noted that imagery is an effective medium for communicating difficult emotions. According to eMarketer [16], photos accounted for 75% of content posted on Facebook worldwide, and are the most engaging type of content (87%). Indeed, “a picture is worth a thousand words” and now, “photos are worth a million likes.” Similarly, on Twitter, the tweets with image links get twice as much attention as those without [17], and video-linked tweets drive up engagement [18]. The ease and naturalness of expression through visual imagery can serve to glean depressive symptoms in vulnerable individuals who often seek social support through social media [19]. Further, as psychologist Carl Rogers highlights, we often pursue and promote our Ideal-Self. In this regard, the choice of profile image can be a proxy for one’s online persona [20], providing a window into an individual’s mental health status. For instance, choosing a profile image with the emaciated legs of an individual with several cuts portrays negative self-view [21]. Moreover, psychologists have argued that people use pictures to communicate messages in social media posts which represent our “Ideal Self”, or who we want to be. Indeed, we are constantly motivated to pursue behaviors that bring us closer to our Ideal Self. Inferring demographic information like gender and age can be crucial for stratifying our understanding of population-level epidemiology of mental health disorders. Relying on electronic health records data, previous studies have explored gender differences in depressive behavior from different angles including prevalence, age of onset, comorbidities, as well as biological and psychosocial factors. For instance, women have been diagnosed with depression twice as often as men, [22] and a national psychiatric morbidity survey in the UK has shown a higher risk of depression in women [23]. On the other hand, suicide rates for men are three to five times higher compared to women [24]. Women are more likely to socialize and express their dysphoria, while men tend to express their anger and show negative behaviors such as alcohol abuse and drug dependency [25]. Although depression can affect anyone at any age, the signs and risk factors for depression vary for different age groups [26]. Depression triggers for children include domestic violence, and loss of a pet, or family member. For adolescents, depression may arise from hormonal imbalances [27]. Late-life depression has caused the suicide rate in people aged 80 to 84 to be more than twice that of the general population [28]. Depression in the elderly population often occurs with other medical conditions that persist, which can increase the risk of death. Therefore, inferring demographic information while studying depressive behavior from passively sensed social data can shed better light on the population-level epidemiology of depression. The recent advancements in deep neural networks, specifically for image analysis tasks, can lead to detecting demographic features such as age and gender [29]. We aim to show that by determining and integrating a heterogeneous set of features from different modalities—aesthetic features from posted images (colorfulness, hue variance, sharpness, brightness, blurriness, naturalness), choice of profile picture (for gender, age, and facial expression), screen name, language features from both textual content and profile’s description (n-gram, emotion, sentiment), sociability from ego-network, and user engagement—we can identify individuals who are more likely to be depressed from a data set of 8,770 human-annotated Twitter users. We address the following research questions: 1) How well does the content of posted images (colors, aesthetic, and facial presentation) reflect depressive symptoms? 2) Does the choice of profile picture show any psychological traits corresponding to a depressed online persona? 3) Are profiles pictures reliable enough to represent demographic information such as age and gender, and can they be used for community-level management of depression? 4) Are there any underlying themes among depressed individuals generated using multimodal content that can be used to reliably detect depression? Our contributions include: Analysis of the content of posted images in terms of colors, aesthetic, facial presentation, and their associations with depressive symptoms; Uncovering the underlying relationships between visual and contextual content of likely depressed profiles obtained using a demographic inference process which can facilitate community-level management of depression; and Testing the performance of our interpretable heterogeneous feature set for predicting depressive symptoms.",2,2,2,1,2,2,0,2,4,2,2,2,2,0,0,0,0,2,4,0
https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0226248,16,3,,"Yazdavar, Mahdavinejad, Bajaj, Romine, Sheth, Monadjemi, Thirunarayan, Meddar, Myers, Pathak, Hitzler",11,0,0,0,2020,"We have divided the related work into four subsections. First, we discuss the state-of-the-art approaches for studying depressive behavior on social data. Second, we review studies that have inferred demographic information using social media data.Then, we discuss the association between color sensitivity and mental health disorders. Finally, we cover state-of-the-art studies that have used visual imagery to study individual’s behavior.",2,2,2,1,2,2,0,2,4,2,2,2,2,0,0,0,0,2,4,0
https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0226248,16,4,,"Yazdavar, Mahdavinejad, Bajaj, Romine, Sheth, Monadjemi, Thirunarayan, Meddar, Myers, Pathak, Hitzler",11,0,0,0,2020,"Several efforts have attempted to automatically detect depression from social media content utilizing machine learning, deep learning, and natural language processing approaches. From conducting a retrospective study of tweets, De Choudhury et al., (2013) characterizes depression based on factors such as language, emotion, style, ego-network, and user engagement. They built a classifier to predict the likelihood of depression from a written post [30] or an individual’s profile [31]. Moreover, there have been significant advances due to the shared task [32] focusing on methods for identifying depressed users on Twitter at the Computational Linguistics and Clinical Psychology Workshop (CLP 2015). A corpus of nearly 1,800 Twitter users was built for evaluation, and the best models employed topic modeling [33], Linguistic Inquiry and Word Count (LIWC) features, and other metadata [34]. More recently, a neural network architecture has been introduced [35] to combine Twitter posts into a representation of users’ activities for detecting depressed users. Another active line of research has focused on capturing warning signs of suicide and self-harm [36]. Through analysis of tweets posted by individuals attempting committing suicide, they indicate quantifiable signals of suicidal ideations. Moreover, the CLP 2016 [36] defined a shared task on detecting the severity of mental health from forum posts. All of these studies derive discriminative features to classify depression in user-generated content at message-level, individual-level, or community-level. The recent emergence of photo-sharing platforms such as Instagram has attracted researchers’ attention to study individual’s behavior from their visual narratives—ranging from mining their emotions [37], and happiness trend [38], to studying medical concerns [39]. Researchers have shown that people use Instagram to engage in social exchange and share their difficult experiences [13]. The role of visual imagery as a mechanism of self-disclosure by relating visual attributes to mental health disclosures on Instagram was highlighted by [14] where individual Instagram profiles were utilized to build a prediction framework for identifying markers of depression. The importance of data modality to understand user behavior on social media has been highlighted by [40]. More recently, a deep neural network sequence modeling approach that marries audio and text data modalities to analyze question-answer style interviews between an individual and an agent has been developed to study mental health [40]. Similarly, a multimodal depressive dictionary learning process was proposed to detect depressed users on Twitter [41]. They provide sparse user representations by defining a feature set consisting of social network features, user profile features, visual features, emotional features, topic-level features, and domain-specific features. Particularly, our choice to develop a multi-modal prediction framework is intended to improve upon previous work involving the use of images in multimodal depression analysis [41] and prior work on studying Instagram photos [15].",2,2,2,1,2,2,0,2,4,2,2,2,2,0,0,0,0,2,4,0
https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0226248,16,5,,"Yazdavar, Mahdavinejad, Bajaj, Romine, Sheth, Monadjemi, Thirunarayan, Meddar, Myers, Pathak, Hitzler",11,0,0,0,2020,"Social media has been introduced as a critical channel to answer diverse research questions offering a wealth of data for public health research [42–44]. It can also assist in better understanding the relationship between behavioral changes and population health [45]. However, the lack of demographic indicators (e.g. age, gender, race) within the data is a major limitation for gaining deeper insights. Several research efforts have attempted to automate detection of social media users’ demographic information as summarized below. For gender inference, several studies have analyzed users’ tweets to detect gender differences reflected in linguistic patterns [46]), profile colors [47], names [48], profile images [49], social network connections [50], and user description [46]. For instance, a supervised model was developed by [51] to determine users’ gender by employing features such as screen-name, full name, profile description, and content on external resources (e.g., personal blog). Another supervised model was built to predict the user’s age group by employing features including emoticons, acronyms, slang words and phrases, punctuation, capitalization, sentence length, and included links/images, along with online behaviors such as number of friends, post time, and commenting activity [52]. To attempt to infer the age of Dutch Twitter users, a model was built that utilizes the life stage of users such as secondary school student, college student, or employee [53]. Similarly, a novel model was introduced for extracting age for Twitter users by relying on profile descriptions while devising a set of rules and patterns [54]. They also parse descriptions for occupation by consulting the SOC2010 list of occupations [55] and validating it through social surveys. A novel age inference model was developed while relying on homophily interaction information and content to predict the age of Twitter users [56]. The intuition is that people within the same age group share similar content and become friends with contemporaries. Using an extensive set of experiments, they show that their model outperformed other state-of-the-art age inference models by leveraging online interaction and content information simultaneously. The limitations of textual content for predicting age and gender was highlighted by [57]. They distinguish language use based on social gender, age identity, biological sex, and chronological age by collecting crowdsourced signals from a game in which players (crowd) guess the biological sex and age of a user based only on their tweets. Their findings indicate how linguistic markers can be misleading (e.g., a heart represented as <3 can be misinterpreted as feminine when the writer is male). Estimating age and gender from facial images by training convolutional neural networks (CNN) for face recognition is another active line of research [58].",2,2,2,1,2,2,0,2,4,2,2,2,2,0,0,0,0,2,4,0
https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0226248,16,6,,"Yazdavar, Mahdavinejad, Bajaj, Romine, Sheth, Monadjemi, Thirunarayan, Meddar, Myers, Pathak, Hitzler",11,0,0,0,2020,"The strong associations between color sensitivity and mood has been highlighted by several studies [59]. In an earlier research, a strong correlation between specific color selection such as yellow and depressive behavior has been reported by [60]. With respect to color discrimination, findings based on a sample of 20 male patients, aged 18 between 45 years old with schizophrenia and manic-depressive psychosis, indicated that when their right hemisphere was depressed, the identification of color by saturation, shade, and color tone was impaired [61]. More recently, the association of color vision with bipolar disorder explored [62]. The general findings suggest that people suffering from depression are likely to reveal their mood through their choice of colors (such as preference for darker shades) in everyday life situations [63]. In this study, we leveraged the visual content shared on Twitter for studying such signals.",2,2,2,1,2,2,0,2,4,2,2,2,2,0,0,0,0,2,4,0
https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0226248,16,7,,"Yazdavar, Mahdavinejad, Bajaj, Romine, Sheth, Monadjemi, Thirunarayan, Meddar, Myers, Pathak, Hitzler",11,0,0,0,2020,"The recent emergence of photo-sharing platforms such as Instagram, provides a unique opportunity to study people’s behavior through the emotions [37] with broader application in personality prediction [64] and demographic inferences. Utilizing these platforms for population-levels analysis helps to improve public health concerns [39] such as obesity [65], substance use [66], depression, and anxiety [67]. With regards to personality prediction, early efforts have shown that bag-of-visual-words and Facebook profile images could predict users’ personality [68]. Various sets of features have been obtained from the images of 11,736 Facebook users were extracted to build a computational model which has more predictive power than human raters for predicting similar personality traits [69].",2,2,2,1,2,2,0,2,4,2,2,2,2,0,0,0,0,2,4,0
https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0226248,16,8,,"Yazdavar, Mahdavinejad, Bajaj, Romine, Sheth, Monadjemi, Thirunarayan, Meddar, Myers, Pathak, Hitzler",11,0,0,0,2020,"This study is focused on obtaining community-level insights about depression signs and depressive behavior. As such, even though we analyzed individual’s behavioral health information–which is considered sensitive—we utilized anonymized users in our datasets as per the approved Institutional Review Board (IRB) protocol. The study was approved and the informed consent process by Wright State University Institution review Board (SC#6258) 4.1.3. Self-disclosure refers to revealing personal and intimate information about oneself to others, which can be therapeutic for psychological well-being [70]. Previous efforts highlight diverse modes of mental health self-disclosures on social media [12]. Self-disclosure clues have been extensively utilized for creating ground-truth data for numerous social media analytic studies such as predicting users’ demographics [54], and depressive behavior [8]. For instance, vulnerable individuals may employ depressive-indicative terms in their Twitter profile descriptions. Other individuals may share their age and gender, e.g., “16 year old suicidal girl”. We employed a large dataset of 45,000 Twitter users with self-reported depressive symptoms introduced initially in [8]. All information was obtained using advanced search API [71]. To seed the search, we created a lexicon of depressive symptoms consisting of 1,500 depressive-indicative terms with the help of clinical psychologists, and employed it to collect the Twitter profiles of individuals with self-declared depressive symptoms [72]. More specifically, the dataset provides the users’ profile information including screen name, profile description, follower/followee counts, profile image, and tweet content, which can express various depression-relevant characteristics, and determine whether a user indicates any depressive behavior. Three human judges from the Department of Psychology at Wright State University assisted us in creating this annotated dataset. We reported the inter-rater agreement as K = 0.74 based on Cohen’s Kappa statistics [8]. To create a robust gold standard dataset, we discarded the instances in which at least two (out of three) of our annotators did not agree about the depressive symptoms. Our final dataset contains 8770 users with 3981 depressed users, and 4789 control users that do not express any depressive symptoms in their Twitter data. This dataset Ut contains the metadata values of each user such as profile descriptions, followers_count, created_at, and profile_image_url. Table 1 illustrates a sample of depressive-indicative phrases that appear in tweets from likely vulnerable users To further measure the robustness of our dataset, we conducted another experiment by obtaining additional annotation from our colleagues from the Department of Psychiatry at Weill Cornell Medical College. Using the following formula, we computed a statistically reliable sample size: where N is population size, Z is z-score, e denotes margin of error, and p represents standard deviation. Specifically, we employed our dataset of 8770 (population size), and confidence interval of 95% (margin of error 5%) to obtain 400 users as a concrete sample size. We then randomly selected 400 users from the dataset of 8770 users to be evaluated by two additional human judges (from the Department of Psychiatry at Weill Cornell Medical College) by manually annotating whether users’ content reflected depressive behavior or not. The average inter-rater agreement was (85% agreement, 0.77) based on Cohen’s Kappa statistics, which denotes substantial agreement and implies the robustness of our dataset.",2,2,2,1,2,2,0,2,4,2,2,2,2,0,0,0,0,2,4,0
https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0226248,16,9,,"Yazdavar, Mahdavinejad, Bajaj, Romine, Sheth, Monadjemi, Thirunarayan, Meddar, Myers, Pathak, Hitzler",11,0,0,0,2020,"We extracted a user’s age by applying regular expression patterns to profile descriptions (such as “17 years old, self-harm, anxiety, depression”) [54]. We compiled “age prefixes” and “age suffixes”, and used three age-extraction rules: 1. I am X years old, 2. Born in X, and 3. X years old, where X is a “date” or age (e.g., 1994). We selected a subset of 1061 users among Ut as gold standard dataset Ua who disclosed their age. From these 1061 users, 822 belonged to the depressed class, and 239 belonged to the control class. From the 3981 depressed users, 20.6% disclosed their age in contrast with only 4% (239/4789) among the control group, suggesting that self-disclosure of age is more prevalent among vulnerable users. Fig 1 depicts the age distribution in Ua. The general trend, consistent with the results in [56, 73], is biased toward younger individuals. Indeed, according to the Pew Research Center, 47% of Twitter users are in general 30 years old or younger [74]. Similar data collection procedures with comparable distribution have been used previously [56]. We discuss our approach to mitigate the impact of the bias in Section 3. The median age is 17 for the depressed class versus 19 for the control class. This suggests that the depressed-user population is younger, or depressed adolescents are more likely to disclose their age in order to connect with peers (social homophily) [75].",2,2,2,1,2,2,0,2,4,2,2,2,2,0,0,0,0,2,4,0
https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0226248,16,10,,"Yazdavar, Mahdavinejad, Bajaj, Romine, Sheth, Monadjemi, Thirunarayan, Meddar, Myers, Pathak, Hitzler",11,0,0,0,2020,"We selected a subset of 1464 users Ug from Ut who disclosed their gender in their profile description. Out of 1464 users, 64% belonged to the depressed group, and the rest (36%) belonged to the control group. 23% of the likely depressed users disclosed their gender, which is considerably higher (12%) than that of the control class. Once again, gender disclosure varies among the two gender groups. For statistical significance, we performed a chi-square test (null hypothesis: gender and depression are two independent variables). Fig 2 illustrates gender association with each of the two classes. Blue circles (positive residuals, see Fig 2A and 2D) show a positive association among corresponding row and column variables, and the red circles (negative residuals, see Fig 2B and 2C) imply a repulsion. Our findings indicate a strong association (Chi-square: 32.75, p-value:1.04e-08) between female gender, and expression of depressive symptoms on Twitter. These observations are consistent with the current literature which have shown that more women than men are diagnosed with depression [76]. In particular, the female-to-male ratio is 2:1 and 1:9 for major depressive disorder and dysthymic disorder, respectively.",2,2,2,1,2,2,0,2,4,2,2,2,2,0,0,0,0,2,4,0
https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0226248,16,11,,"Yazdavar, Mahdavinejad, Bajaj, Romine, Sheth, Monadjemi, Thirunarayan, Meddar, Myers, Pathak, Hitzler",11,0,0,0,2020,We now provide an in-depth analysis of visual and textual content of vulnerable users.,2,2,2,1,2,2,0,2,4,2,2,2,2,0,0,0,0,2,4,0
https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0226248,16,12,,"Yazdavar, Mahdavinejad, Bajaj, Romine, Sheth, Monadjemi, Thirunarayan, Meddar, Myers, Pathak, Hitzler",11,0,0,0,2020,"We show that the visual content in posted images and profile images provide valuable psychological cues for understanding a user’s depression status. Profile images and posted images can surface self-stigmatization [77]. As opposed to a typical computer vision framework for object recognition that relies on thousands of predetermined low-level features, emotions reflected in facial expressions are important when assessing user’s online behavior, attributes contributing to the computational aesthetics, and sentimental quotes they may subscribe to. The following sections present an in-depth analysis of visual content for both the depressed class and the control class with respect to three aspects: facial presence, facial expressions, and general image features.",2,2,2,1,2,2,0,2,4,2,2,2,2,0,0,0,0,2,4,0
https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0226248,16,13,,"Yazdavar, Mahdavinejad, Bajaj, Romine, Sheth, Monadjemi, Thirunarayan, Meddar, Myers, Pathak, Hitzler",11,0,0,0,2020,"For capturing facial presence, we employed the model has been introduced in [78] where a multilevel convolutional coarse-to-fine network cascade developed to tackle facial landmark localization problem. We identified facial presentation, emotion from facial expression, and demographic features from profile images and posted images [79]. Table 2 illustrates facial presentation differences in both profile and posted images (media) for depressed users and control users in Ut. For the control class, facial presence was significantly higher in both profile images and shared media (8%, 9% respectively) compared to the depressed class. In contrast with age and gender disclosure, vulnerable users were less likely to disclose their facial identity, possibly due to lack of confidence or fear of stigma.",2,2,2,1,2,2,0,2,4,2,2,2,2,0,0,0,0,2,4,0
https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0226248,16,14,,"Yazdavar, Mahdavinejad, Bajaj, Romine, Sheth, Monadjemi, Thirunarayan, Meddar, Myers, Pathak, Hitzler",11,0,0,0,2020,"Following [20]’s approach, we adopted Ekman’s model [80] of six emotions: anger, disgust, fear, joy, sadness, and surprise, and used the Face++ API [79] to automatically capture these emotions from the shared images. The positive emotions were joy and surprise, and negative emotions were anger, disgust, fear, and sadness. Foreach user u in Ut, we processed profile images and shared images for both the depressed and control groups with at least one face from the shared images (Table 3). For the images that contained multiple faces, we perform mean pooling over the frames to obtain the expected emotional features. Fig 3 illustrates the inter-correlation of these features. Additionally, we have observed that the emotions extracted from facial expressions correlated with the emotional signals captured from textual content utilizing LIWC. This indicates that visual imagery can be utilized as a complementary channel for measuring online emotional signals.",2,2,2,1,2,2,0,2,4,2,2,2,2,0,0,0,0,2,4,0
https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0226248,16,15,,"Yazdavar, Mahdavinejad, Bajaj, Romine, Sheth, Monadjemi, Thirunarayan, Meddar, Myers, Pathak, Hitzler",11,0,0,0,2020,"The importance of interpretable computational aesthetic features for studying users’ online behavior has been highlighted by several efforts [81]. Color, as a pillar of the human vision system, has a strong association with conceptual ideas like emotion [82]. We measured the normalized red, green, blue, the mean of the original colors, brightness, and contrast relative to variations of luminance. We represented images in Hue-Saturation-Value color space that seems intuitive for humans, and measured the mean and variance for saturation and hue. Saturation is defined as the difference in intensity between different light wavelengths that compose the color. Although hue is not interpretable, high saturation indicates vividness and chromatic purity, which are more appealing to the human eye [20]. Colorfulness is measured as a difference against gray background [83]. Naturalness is a measure of correspondence between images and human perception of reality [83]. In color reproduction, naturalness is measured from the mental recollection of the colors of familiar objects. Additionally, there is a tendency among vulnerable users to share sentimental quotes bearing negative emotions. We performed optical character recognition (OCR) with python-tesseract [84] to extract text and their sentiment [85] score. As illustrated in Table 4, vulnerable users tend to use less colorful (higher grayscale) profile images and shared images to convey their negative feelings, and also share images that are less natural. In general, control users identified darker, grayer colors with negative mood, and generally preferred brighter, more vivid colors. By contrast, vulnerable users were found to prefer darker, grayer, and bluer colors. We found a strong positive correlation between self-declared depression and a tendency to perceive one’s surroundings as gray or lacking in color. With respect to the aesthetic quality of images (saturation, brightness, and hue), there is a significant difference between the two classes, with depressed users more frequently sharing images that are less appealing to the human eye. We employed an independent samples t-test, while adopting Bonferroni Correction as a conservative approach to adjust the confidence intervals. Overall, we had 223 features, and chose Bonferroni-corrected alpha level of 0.05/223 = 2.24e − 4 (*** p < alpha, **p < 0.05). In general, the control users identified darker, grayer colors with negative moods, and generally preferred brighter, more vivid colors. In contrast, vulnerable users preferred darker, grayer colors, and bluer images. Vulnerable users shared images that are less aesthetically pleasing with lower sharpness, and those that do not contain faces or contain only one face. On the other hand, control users tended to use sharper images with multiple faces. Additionally, vulnerable users shared images with more text content, often containing depressive quotes and negative sentiments. The desire to socialize and connect with others is also manifested in the visual imagery of vulnerable users. The images shared by vulnerable users tend to contain a single face (belonging to the user), rather than surrounded by friends and family. This further indicates the focus on the self, which is one of the most consistent markers of a mental disorder. This is also associated with an extensive usage of first person singular pronouns—which is another reliable marker of depression in content analysis of depressive behavior.",2,2,2,1,2,2,0,2,4,2,2,2,2,0,0,0,0,2,4,0
https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0226248,16,16,,"Yazdavar, Mahdavinejad, Bajaj, Romine, Sheth, Monadjemi, Thirunarayan, Meddar, Myers, Pathak, Hitzler",11,0,0,0,2020,"LIWC [86] has been used extensively for examining the latent dimensions of self-expression for analyzing personality [87], depressive behavior, demographic differences [53, 57], etc. Several studies have shown that females employ more first-person singular pronouns [88], and deictic language (context-dependent words) [89], while males tend to use more articles [90] which characterize concrete thinking, and formal, informational, affirmative words [91]. For age analysis, the salient findings show that older individuals use more future tense verbs, [88] suggesting a shift in focus while aging. They also show more positive emotions [92], employ fewer self-references (i.e. ‘I’, ‘me’), and more first person plural pronouns [88]. Depressed users employ first person pronouns more frequently [93], and repeatedly use negative emotions and anger words. We analyzed psycholinguistic cues and language style to study the association between depressive behavior and demographics. Specifically, we adopted Levinson’s adult development grouping [94] that partitions users in Ua into 5 age groups: (14,19], (19,23], (23,34], (34,46], and (46,60]. Then, we applied LIWC for characterizing linguistic styles for each age group for users in Ua.",2,2,2,1,2,2,0,2,4,2,2,2,2,0,0,0,0,2,4,0
https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0226248,16,17,,"Yazdavar, Mahdavinejad, Bajaj, Romine, Sheth, Monadjemi, Thirunarayan, Meddar, Myers, Pathak, Hitzler",11,0,0,0,2020,"The recent LIWC version [86] summarizes textual content in terms of language variables such as analytical thinking, clout, authenticity, and emotional tone. It also measures other linguistic dimensions such as descriptor categories (e.g., percent of target words gleaned from the dictionary, or words longer than six letters—Sixltr), informal language markers (e.g., swear words, netspeak), and other linguistic aspects (e.g., first person singular pronouns). Thinking Style: The words we use to communicate can reveal our style of thinking. There are two common approaches for extracting an individual’s thinking style. First, measuring one’s natural way of trying to understand, analyze, and organize complex events has a strong association with analytical, formal, and logical thinking. LIWC relates higher analytic thinking to more formal and logical reasoning, whereas a lower value indicates a focus on narratives. Second, cognitive processing, which measures problem solving in the mind, is captured through words such as “think,” “believe,” “realize,” and “know” and demonstrates “certainty” in communication. High values for analytical thinking implies clarity of thought. Critical thinking ability is related to education [95], and is impacted by different stages of cognitive development at different ages [96]. It has been shown that older people communicate with greater cognitive complexity while comprehending nuances and subtle differences [95]. All of these findings corroborate with our results (Table 5). We observed notable differences in raw intelligence and the ability to think analytically in depressed and control users among different age groups (see Fig 4A and 4F and Table 5). Overall, vulnerable younger users do not think as logically based on their relative analytical score and cognitive processing ability. We can also observe that the differences between age groups above 35 tend to become smaller [97]. Authenticity: Authenticity measures the degree of honesty. Authenticity is often assessed by measuring present tense verbs, first person singular pronouns (e.g., I, me, my), and by examining the linguistic manifestations of false stories [98]. People who lie use fewer self-references, and fewer complex words. Psychologists often see a child’s first successful lie as a mental milestone growth [99]. There is a decreasing trend in authenticity with age (see Fig 4B). Authenticity for depressed adolescents is strikingly higher than their control peers, and decreases with age (Fig 4B). Clout: People with high clout speak more confidently and with certainty, employing more social words with fewer negations (e.g., no, not) and swear words. In general, mid-life is relatively stable w.r.t. relationships and work. A recent study has shown that age 60 is best for self-esteem [100] as people take on managerial roles at work, and maintain satisfyinging relationships with their spouses. We see the same pattern in our data (see Fig 4C and Table 5). Unsurprisingly, lack of confidence (the 6th PHQ-9 [101] symptom) is a distinguishable characteristic of vulnerable users, leading to their lower clout scores, especially among depressed users younger than 34 years old. Self-references: First person singular words often indicate interpersonal involvement, and their high usage is associated with negative affective states such as nervousness and depression [92]. Consistent with prior studies, the frequency of first person singular words for depressed users is significantly higher compared to that of the control class. Similarly to [92], adolescents tend to use more first-person (e.g. I), and second person singular (e.g. you) pronouns (Fig 4G). The impact of the above phenomenon is reflected in significantly higher frequency of self-references for depressed adolescents. As with the control class, a downtrend suggests that as depressed individuals age, they make more distinctions and psychologically distance themselves from their topics. Informal Language Markers; Swear, Netspeak: Swear lexicon includes terms such as “fu**”, “dam*”, and “shi*”. Several studies have highlighted that the use of profanity by young adults has significantly increased over the last decade [102]. We observed the same pattern in both the depressed and the control classes (Table 5), with a higher rate for depressed users [10]. Psychologists have also shown that swearing may indicate that an individual is not a fragmented member of a society [103]. Depressed adolescents who show a higher rate of interpersonal involvement and relationships, have a higher rate of cursing (Fig 4E). Also, Netspeak lexicon measures the frequency of terms such as ‘lol’ and ‘thx’. Although the rate is higher for the depressed class, we did not find any pattern concerning adult development. Sexual, Body: The sexual lexicon contains terms like “horny”, “love”, and “incest”, and body terms like “ache”, “heart”, and “cough”. Both start with a higher rate for depressed users and decreases gradually as they age, possibly due to changes in sexual desire with age [104] (Fig 4H and 4I and Table 5).",2,2,2,1,2,2,0,2,4,2,2,2,2,0,0,0,0,2,4,0
https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0226248,16,18,,"Yazdavar, Mahdavinejad, Bajaj, Romine, Sheth, Monadjemi, Thirunarayan, Meddar, Myers, Pathak, Hitzler",11,0,0,0,2020,"We employed a one-way ANOVA to compare the impact of various factors, and validate our findings above. Table 5 illustrates our findings, with a degree of freedom (df) of 1055. The null hypothesis is that the sample means for each age group are similar for each of the LIWC features.",2,2,2,1,2,2,0,2,4,2,2,2,2,0,0,0,0,2,4,0
https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0226248,16,19,,"Yazdavar, Mahdavinejad, Bajaj, Romine, Sheth, Monadjemi, Thirunarayan, Meddar, Myers, Pathak, Hitzler",11,0,0,0,2020,We leveraged both the visual and textual content for predicting age and gender.,2,2,2,1,2,2,0,2,4,2,2,2,2,0,0,0,0,2,4,0
https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0226248,16,20,,"Yazdavar, Mahdavinejad, Bajaj, Romine, Sheth, Monadjemi, Thirunarayan, Meddar, Myers, Pathak, Hitzler",11,0,0,0,2020,"We employed [105]’s weighted lexicon of terms that uses the dataset of 75,394 Facebook users who shared their status, age, and gender. The predictive power of this lexica was evaluated on Twitter, and Facebook, showing promising results [105]. Utilizing these two weighted lexicon of terms, we are predicting the demographic information (age or gender) of useri (denoted by Demoi) using the following equation: where Weightlex(term) is the lexicon weight of the term, and Freq(term, doc)i represents the frequency of the term in the user generated doci, and WC(doc)i measures total word count in (doc)i. As our data are biased toward younger individuals, we report age prediction performance for each age group, separately (Table 6). Moreover, to measure the average accuracy of this model, we built a balanced dataset (keeping the total number of users above 23—416), and then randomly sampled the same number of users from the age ranges (11,19] and (19,23]. The average accuracy of this model was 0.63 for depressed users, and 0.64 for the control class. Table 8 illustrates the performance of gender prediction for each class. The average accuracy was 0.82 on Ug ground-truth dataset",2,2,2,1,2,2,0,2,4,2,2,2,2,0,0,0,0,2,4,0
https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0226248,16,21,,"Yazdavar, Mahdavinejad, Bajaj, Romine, Sheth, Monadjemi, Thirunarayan, Meddar, Myers, Pathak, Hitzler",11,0,0,0,2020,"Inspired by [78]’s approach for facial landmark localization, we used their pre-trained CNN consisting of convolutional layers, including unshared and fully-connected layers, to predict gender and age from both the profile and shared images. We evaluated the performance of the gender and age prediction task on Ug and Ua, respectively, as shown in Table 6.",2,2,2,1,2,2,0,2,4,2,2,2,2,0,0,0,0,2,4,0
https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0226248,16,22,,"Yazdavar, Mahdavinejad, Bajaj, Romine, Sheth, Monadjemi, Thirunarayan, Meddar, Myers, Pathak, Hitzler",11,0,0,0,2020,"We delved deeper into the benefits and drawbacks of each data modality for demographic information prediction. This is crucial as the differences between language cues between age groups above 35 tend to become smaller (see Fig 4A, 4B and 4C), making the prediction harder for older individuals [97]. In this case, the other data modality (e.g., visual content) played an integral role as a complementary source for age inference. For gender prediction, on average, the profile image-based predictor provided a more accurate prediction for both the depressed and the control class (0.92 and 0.90), compared to the content-based predictor (0.82). For age prediction (see Table 6), the textual content-based predictor (on average 0.60) outperformed both of the visual-based predictors (on average profile: 0.51, Media: 0.53). However, not every user provided facial identity on his or her account (see Table 2). We studied facial presentation for each age group to examine any association between age group, facial presentation, and depressive behavior (see Table 7). We can see youngsters in both the depressed and control classes are not likely to present their face in their profile image. Less than 3% of vulnerable users between 11-19 years revealed their facial identity. Although the content-based gender predictor was not as accurate as the image-based predictor, it is adequate for population-level analysis (see Table 8).",2,2,2,1,2,2,0,2,4,2,2,2,2,0,0,0,0,2,4,0
https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0226248,16,23,,"Yazdavar, Mahdavinejad, Bajaj, Romine, Sheth, Monadjemi, Thirunarayan, Meddar, Myers, Pathak, Hitzler",11,0,0,0,2020,"We used the above findings for predicting depressive behaviors. Our model exploits an early fusion [40] technique in feature space and requires modeling each user u in Ut as vector concatenation of individual modality features. As opposed to the computationally expensive late fusion schemes, where each modality requires a separate supervised modeling, this model reduces the learning effort and has shown promising results [106]. To develop a generalizable model that avoids overfitting, we performed feature selection using statistical tests and all relevant ensemble learning models. Adding feature selection tests adds randomness to the data by creating shuffled copies of all features (shadow feature), and then trains the Random Forest classifier on the extended data. Iteratively, it checks whether the actual feature has a higher Z-score than its shadow feature (See Algorithm 1 and Fig 5) [107].",2,2,2,1,2,2,0,2,4,2,2,2,2,0,0,0,0,2,4,0
https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0226248,16,24,,"Yazdavar, Mahdavinejad, Bajaj, Romine, Sheth, Monadjemi, Thirunarayan, Meddar, Myers, Pathak, Hitzler",11,0,0,0,2020,"Next, we adopted an ensemble learning method which integrated the predictive power of multiple learners with two main advantages; a high degree of interpretability with respect to the contributions of each feature, and a high predictive power. For prediction, we have where ft is a weak learner and denotes the final prediction. In particular, we optimized the loss function: where φ incorporates L1 and L2 regularization. In each iteration, the new ft(ui) is obtained by fitting the weak learner to the negative gradient of loss function. Particularly, by estimating the loss function with Taylor expansion: where its first expression is constant, the second and the third expressions are first (gi) and second order derivatives (hi) of the loss. To explore the weak learners, assume ft has k leaf nodes, Ij be subset of users from Ut belongs to the node j, and wj denotes the prediction for node j. Then, for each user i belonging to Ij, ft(ui) = wj and Next, for each leaf node j, deriving w.r.t wj: and by substituting weights: which represents the loss of fixed weak learners with k nodes. The trees are built sequentially, such that each subsequent tree aims to reduce the errors of its predecessor trees. Although, the weak learners have a higher degree of biases, the ensemble model produces a strong learner that effectively integrates the weak learners by reducing bias and variance (the ultimate goal of supervised models) [108, 109]. Table 9 illustrates how our multimodal framework outperforms the baselines for identifying depressed users in terms of average specificity, sensitivity, F-Measure, and accuracy in a 10-fold cross-validation setting on Ut dataset. Fig 6 shows how the likelihood of being classified into the depressed class varies with each feature added to the model for a sample user in the dataset. The prediction bar (the black bar) shows that the log-odds of prediction is 0.31, that is, the likelihood of this person being a depressed user is 57% (1 / (1 + exp(-0.3))). The figure also sheds light on the impact of each contributing feature. The waterfall charts represent how the probability of being depressed varies when adding each feature. For example, for our dataset, the “Analytic thinking” score measured by LIWC from the tweet content is a high value of 48.43 (Median:36.95, Mean: 40.18) and this decreases the chance of the user being classified into the depressed group by the log-odds of -1.41. This is due to the fact that depressed users have significantly lower “Analytic thinking” scores compared to the control class. Moreover, the “Clout” score of 40.46 is considered a low value (Median: 62.22, Mean: 57.17), and increases the chance of being classified as a depressed user. This is justifiable given the clear association between low self-esteem and risk for depression. With respect to the visual features, the mean and the median of “shared colorfulness” is 112.03 and 113, respectively. The value of 136.71 would be high, and decreases the chance of being depressed by log-odds of -0.54. As mentioned earlier, depressed users preferred darker, and grayer colors. The score of 0.46 as “profile naturalness” is considered high compared to 0.36 (the mean for the depressed class) which justifies pull down of the log-odds by −0.25. Using network features, for instance, the “two hop neighborhood” for depressed users (Mean: 84) are less than that of the control users (Mean: 154), and is reflected in pulling down the log-odds by -0.27.",2,2,2,1,2,2,0,2,4,2,2,2,2,0,0,0,0,2,4,0
https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0226248,16,25,,"Yazdavar, Mahdavinejad, Bajaj, Romine, Sheth, Monadjemi, Thirunarayan, Meddar, Myers, Pathak, Hitzler",11,0,0,0,2020,"To test the efficacy of our multi-modal framework for detecting depressed users, we compared it against existing content, content-network, and image-based models (based on the aforementioned general image features, facial presence, and facial expressions).",2,2,2,1,2,2,0,2,4,2,2,2,2,0,0,0,0,2,4,0
https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0226248,16,26,,"Yazdavar, Mahdavinejad, Bajaj, Romine, Sheth, Monadjemi, Thirunarayan, Meddar, Myers, Pathak, Hitzler",11,0,0,0,2020,"Language biases in social media posts can be a good representative of emotional state. Fig 7 illustrates the word clouds that distinguish the word usage of likely-depressed and non-depressed profiles. It is clear that depressed users often care more about their appearance. This is indicative by their usage of terms such as “pretty” and “beautiful.” They also have a tendency to talk about their family and relations using words such as family, hugs, parents, daddy, mums, sigh, grandma, maam, friendless, love, friend, mommy, people, boyf, and gf. In contrast, the control users usually talk about daily events and news such as “hurricane” and “Trump”. Such differences in word usage highlight the fact that user generated words can be distinguishable features for detecting depressed user profiles. See Table 9 for the comparative performance of our prediction framework against state-of-the-art methods used for predicting depressive behaviors—many of which employed the same feature sets and hyperparameter settings (see Models I-V). Several prior efforts have demonstrated that word embedding models can reliably enhance short text classification [115], Model VI by employing pre-trained word embeddings which have trained over 400 million tweets [116] while representing a user with retrieving word vectors for all the words a user used in tweets and profile description. We aggregate these word vectors through their means and feed it as input to a SVM classifier with a linear kernel. In Model VII, we employed [8]’s dataset of 45,000 self-reported depressed users and trained a Skip-gram model with negative sampling to learn word representations. We chose this model because it generates robust word embeddings even when the collection of training words are sparse [117]. We set dimensionality to 300 and a negative sampling rate to 10 sample words, which has shown promising results with medium-sized datasets [117]. Besides, we have observed that many vulnerable users chose specific account names, such as “Suicidal_Thoughxxx,” and “younganxietyyxxx,” which are good indicators of their depressive behavior. We used Levenshtein distance between depression indicative terms in [8]’s depression lexicon and the screen name to capture their degree of semantic similarity [118].",2,2,2,1,2,2,0,2,4,2,2,2,2,0,0,0,0,2,4,0
https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0226248,16,27,,"Yazdavar, Mahdavinejad, Bajaj, Romine, Sheth, Monadjemi, Thirunarayan, Meddar, Myers, Pathak, Hitzler",11,0,0,0,2020,"We employed the aforementioned visual content features including facial presence, aesthetic features, and facial expression for depression prediction. We use three different models: Logistic Regression (Model IX), SVM (Model X), and Random Forest (Model XI). The poor performance of image-based models suggests that relying on a unique modality would not be sufficient for building a robust model due to the complexity and abstruse nature of the prediction task.",2,2,2,1,2,2,0,2,4,2,2,2,2,0,0,0,0,2,4,0
https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0226248,16,28,,"Yazdavar, Mahdavinejad, Bajaj, Romine, Sheth, Monadjemi, Thirunarayan, Meddar, Myers, Pathak, Hitzler",11,0,0,0,2020,"Network-based features indicate the user’s desire to socialize and connect with others. There is a notable difference between the number of friends, followers, favorites, and status count for depressed and control users (see Table 4). For building a baseline Model VIII, we obtained egocentric network measures for each user based on the network formed using @-replies interactions among them. The egocentric social graph of a user u is an undirected graph of nodes in u’s two-hop neighborhood in our Ua dataset, where the edge between nodes u and v implies that there has been at least one @-reply exchange. Network-based features including Reciprocity, Prestige Ratio, Graph Density, Clustering Coefficient, Embeddedness, Ego components and Size of two-hop neighborhood were extracted from each user’s network [10] to reliably capture user context for depression prediction. High values for the three metrics—clustering coefficient, embeddedness, and number of ego networks—indicates that the depressed users tend to build a close network of trusted people to share their mental health issues. For both graph density and size of the two-hop neighborhood, a lower value indicates fewer interactions.",2,2,2,1,2,2,0,2,4,2,2,2,2,0,0,0,0,2,4,0
https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0226248,16,29,,"Yazdavar, Mahdavinejad, Bajaj, Romine, Sheth, Monadjemi, Thirunarayan, Meddar, Myers, Pathak, Hitzler",11,0,0,0,2020,"We presented an in-depth analysis of visual and contextual content of likely depressed profiles on Twitter. We employed them for demographic (age and gender) inference processes. We developed a multimodal framework, employing statistical techniques for fusing heterogeneous sets of features obtained by processing visual, textual, and user interactions. Conducting an extensive set of experiments, we assessed the predictive power of our multimodal framework while comparing it against state-of-the-art approaches for depressed user identification on Twitter. The empirical evaluation shows that our multimodal framework is superior to them and it improved the average F1-Score by 5 percent. Effectively, visual cues gleaned from content and profile images shared on social media can further augment inferences from textual content for reliable determination of depression indicators and diagnoses. In the future, we plan to apply our approach to various data sources such as longitudinal electronic health record (EHR) systems, and private insurance reimbursement and claims data, to develop a robust “big data” platform for detecting clinical depressive behavior at the community level.",2,2,2,1,2,2,0,2,4,2,2,2,2,0,0,0,0,2,4,0
https://journals.sagepub.com/doi/full/10.1177/0886260518757756,17,1,,"Liu, Xue, Zhao, Wang, Jiao, Zhu",6,0,0,0,2018,"A great deal of research has focused on the negative consequences of domestic violence (DV) on mental health. However, current studies cannot provide direct and reliable evidence on the impacts of DV on mental health in a short term as it is not feasible to measure mental health shortly before and after an unpredictable event like DV. This study aims to explore the short-term outcomes of DV on individuals’ mental health. We collected a sample of 232 victims (77% female) and 232 nonvictims (gender and location matched with 232 victims) on Sina Weibo. In both the victim and nonvictim groups, we measured their mental health status during the 4 weeks before the first DV incident and during the 4 weeks after the DV incident. We used our proposed Online Ecological Recognition (OER) system, which is based on several predictive models to identify individuals’ mental health statuses. Mental health statuses were measured based on individuals’ Weibo profiles and messages, which included “Depression,” “Suicide Probability,” and “Satisfaction With Life.” The results showed that mental health in the victim group was impacted by DV while individuals in the nonvictim group were not. Furthermore, the victim group demonstrated an increase in depression symptoms, higher suicide risks, and decreased life satisfaction after their DV experience. In addition, the effect of DV on individuals’ mental health could appear in the conditions of child abuse, intimate partner violence, and exposure to DV. These findings inform that DV significantly impacts individuals’ mental health over the short term, as in 4 weeks. Our proposed new data collection and analyses approach, OER, has implications for employing “big data” from social networks to identify individuals’ mental health.",2,2,2,0,2,0,2,0,2,2,2,2,2,1,0,0,0,1,4,0
https://journals.sagepub.com/doi/full/10.1177/0886260518757756,17,2,,"Liu, Xue, Zhao, Wang, Jiao, Zhu",6,0,0,0,2018,"A great deal of research focuses on the negative consequences of domestic violence (DV) on mental health. Golding (1999) found that 47.6% of the battered women suffer from depression and more than 17.9% of them have attempted suicide. The experience of intimate partner violence (IPV) increases depressive symptoms (J. C. Campbell & Soeken, 1999; Hogben et al., 2001; Kennedy, Bybee, Sullivan, & Greeson, 2010; von Eye & Bogat, 2006) and suicidal ideation (Coker et al., 2002; Krishnan, Hilbert, & VanLeeuwen, 2001; Sansone, Chu, & Wiederman, 2007). Compared with nonabused women, battered women are more likely to have lower self-esteem and less life satisfaction (J. C. Campbell, 2002; Dutton et al., 2006; Zlotnick, Johnson, & Kohn, 2006). Children who are abused during childhood are found to have higher rates of depressive symptoms and suicidal ideation during adolescence (Wolfe, Scott, Wekerle, & Pittman, 2001) and adulthood (Dube et al., 2001; Widom, 1998). Furthermore, exposure to DV can result in mental health problems during young ages, adolescence, and adulthood (Fergusson, Horwood, & Lynskey, 1996; Herrenkohl, Sousa, Tajima, Herrenkohl, & Moylan, 2008; Moylan et al., 2009). However, few studies have investigated individuals’ mental health both pre and post DV (Cerdá, DiGangi, Galea, & Koenen, 2012). This muddles the effects of preexisting differences and DV itself. Existing longitudinal investigations that contain a previolence measurement usually focus on the assessment of the long-term effects of DV on mental health. However, the long-term effects were measured in the range of 1 year (Mertin & Mohr, 2001) to 10 years (Boney-McCoy & Finkelhor, 1995), which makes it difficult to distinguish the cause of mental health changes. In addition, limited studies have examined the short-term effects of DV on mental health. Given that the occurrence of DV is unpredictable, it is challenging to collect enough samples to assess the short-term effects of DV on mental health using survey methods. To explore the short-term effects of DV on mental health, it is essential to propose new data collection and analyses approach. The exponentially growing popularity of Online Social Networks (OSNs) provides a new platform for health and violence research. Currently, more than 71% of Internet users are OSN users (Statista, 2017). Sina Weibo is a leading Chinese OSN, with more than 300 million registered users in 2016. Over one third of Weibo users are active daily users. These users present detailed information in their profiles (e.g., geolocation, gender, age), tweet messages, and exchange ideas and interact with each other using Weibo functions (e.g., reply, @function, retweet). All of their online information and behaviors are publicly available and can be used to recognize individuals’ psychological traits through predictive models, such as “personality” (Qiu, Lin, Ramsay, & Yang, 2012) and “mental health status” (A. Li, Hao, Bai, & Zhu, 2015). Hao, Li, Li, and Zhu (2013) utilized machine-learning algorithms to determine individuals’ mental health from social media data. The predicted scores had correlations from 0.30 to 0.40 with self-report survey results. Hao, Li, Gao, Li, and Zhu (2014) also built reliable and valid models to predict subject well-being (SWB) using Weibo data, which had a correlation score of 0.60 with the self-report survey results. Recently, Youyou, Schwartz, Stillwell, and Kosinski (2017) used predictive models to identify the psychological indexes in their personality study. The correlations between the model-predicted scores and self-report questionnaire scores ranged from 0.30 to 0.37. These studies showed that the research findings with social media data are reliable and consistent with survey study findings. Compared with survey methods, the OSN-predicted psychological traits are more ecological. OSN predictions perform better in reflecting everyday interactions as they assess users’ behaviors indirectly and informally (Park et al., 2015). In addition, prediction models reduce the threats to internal validity of social desirability (Back et al., 2010; McCambridge, Witton, & Elbourne, 2014). We define Ecological Recognition (ER) as the approach of using ecological social media data to automatically recognize psychological traits by employing machine-learning models. We measured mental health indexes using publically available online social media data without contacting the participants, which is referred to as Online Ecological Recognition (OER). Compared with self-reports, OER functions as a more accessible, safe, and timely method to track mental health statuses following DV occurrences. This provides a significant improvement over traditional self-report measures when assessing temporary mental health status. In the present study, we used OER to recognize individuals’ mental health statuses before and after their reporting of the first DV experience on Weibo. Three indexes of mental health status, including Depression, Suicide Probability, and Life Satisfaction, were predicted from victims’ online social media behaviors, including profiles and public tweet messages. The work here revealed the relationship between individuals’ experiences of DV and their mental health status. Furthermore, this study explored the impact of different types of DV experiences on mental health, including child abuse, IPV, and exposure to DV.",2,2,2,0,2,0,2,0,2,2,2,2,2,1,0,0,0,1,4,0
https://journals.sagepub.com/doi/full/10.1177/0886260518757756,17,3,,"Liu, Xue, Zhao, Wang, Jiao, Zhu",6,0,0,0,2018,"We selected the DV cases from a pool of 1.16 million Weibo users. All Weibo users in our original data pool were active users, with more than 500 posts during the past 2 years. Approximately 92% of users had more than one update (e.g., login) every day on Weibo. We anonymized all the participants by reassigning them an ID number. From the user pool, we selected 232 users who reported their first DV experience on Weibo following three steps: (a) retrieving users by keywords; (b) manually screening for victims; (c) manually screening for victims who described their first DV experience. Step 1: Retrieving users by keywords. We collected 877,440 posts that contained DV keywords from more than 50 billion posts in our Weibo data pool. We used a total of 117 keywords. Each keyword included the combinations of at least one DV-related word (“DV,” “domestic violence,” “mental abuse,” “neglect,” “quarrel,” “scold,” “beat me,” “abuse,” and “bruise”) and one personal pronoun (“husband,” “hubby,” “wife,” “wifey,” “father,” “mother,” “mom,” “dad,” “he/she,” “you,” “child/children,” “son,” and “daughter”).1 Step 2: Manually screening for real DV experiences. This step sought to identify real DV experiences from the selected candidate posts from Step 1. We recruited 69 research assistants (RAs) and randomly divided them into 23 groups (three RAs in each group). Each group was assigned to read an average of 11,564 posts (duplicates removed). The three RAs in the same group independently worked on the same set of posts. We excluded the posts that met two criteria: (a) discussing DV news; or (b) expressing their personal opinions on DV events. The RAs checked whether a selected post was a real DV case. If so, the RAs established the types of DV as IPV, child abuse, or exposure to DV (see Appendix A). The post was labeled as a DV case when all three RAs reached an agreement following group discussions. If a user posted about his or her DV experience multiple times, we only analyzed the first post. There were a total of 644 posts, that is, 644 Weibo users who reported their DV experience on Weibo. Step 3: Second round of screening to identify the victim’s first DV experience. This step screened the posts that showed users’ first-time DV experience. Another three RAs were hired to code these 644 posts. The exclusion criteria were (a) the post contained phrases which implied long-term DV experience (e.g., “every time,” “always,” etc.); and (b) the post reported a DV case that occurred a long time before. The inclusion criteria are found in Appendix B. Each RA checked these 644 posts independently. We only kept posts that were coded as first-time DV experiences by all three RAs. We acquired 232 Weibo posts describing first-time DV experiences. The 232 users behind these posts were the victim group in this study. Matched group. We selected the nonvictim Weibo users (N = 232) from the original user pool. The users in the matched group had no DV keywords in their posts. Each user in the matched group was paired with one user in the victim group of the same gender and living in the same location according to their Weibo profiles. RA recruitment and training. We recruited 72 RAs for Step 2 and Step 3 during case selections. All of the RAs signed a Non-Disclosure Agreement (NDA) to dissuade disclosing user data. They were also forbidden from searching for the posts online and identifying post owners. Each of the RAs got 200 RMB as compensation. Among them, 69 undergraduate students were utilized at Step 2 to manually filter the real victims, and the other three were graduate students majoring in psychology at Step 3 for screening the first DV experience. We provided all RAs at Step 2 with training to recognize real victims and categorize them correctly (see Appendix A) and RAs at Step 3 training for identifying the first DV experience (see Appendix B). Both training sessions lasted for 2 hours on average with examples and exercises. The students began to rate the posts after they correctly labeled all posts during the training exercises.",2,2,2,0,2,0,2,0,2,2,2,2,2,1,0,0,0,1,4,0
https://journals.sagepub.com/doi/full/10.1177/0886260518757756,17,4,,"Liu, Xue, Zhao, Wang, Jiao, Zhu",6,0,0,0,2018,"Instead of self-reporting, we measured 464 Weibo users’ mental health status by validated prediction models. The predictive models were linear regression models that predicted user’s mental status using OSNs’ dynamic features. In this study, “dynamic features” is a term that is relative to “static features,” which refers to those features having notable changes over time (e.g., per day, L. Li, Li, Hao, Guan, & Zhu, 2014). In other words, the value of these dynamic features is expected to be different at any given point. We downloaded the 464 Weibo users’ data using the Application Programming Interface (API) provided by Sina Weibo (http://weibo.com). For each user, we extracted dynamic features from two perspectives: Linguistic Inquiry and Word Count (LIWC) features (see Gao, Hao, Li, Gao, and Zhu, 2013, and Zhao, Jiao, Bai, and Zhu, 2016, for more details) and behavioral features. LIWC is a widely used natural language processing tool mapping psychological and linguistic dimensions of written expression. It reports the degree of language usage in 88 dimensions (e.g., “positive emotion words,” “family words,” etc.). We calculated the 88 LIWC features using the Chinese version of the text analysis software, TextMind (Gao et al., 2013). TextMind divides a post into several word pieces (e.g., “I experienced DV” to “I,” “experienced,” “DV”). It then calculates the frequency of word pieces of each dimension. During the last step, TextMind outputs the ratio of the summarized frequency of each dimension to the total number of words as LIWC features. In addition to LIWC features, we also extracted 11 behavioral features: “counts of words,” “counts of words per sentence,” “counts of URLs,” “counts of @ names,” “counts of tags,” “counts of posts,” “counts of original posts,” “counts of comments,” “counts of positive emotion emoji use,” “counts of negative emotion emoji use,” and “counts of neutral emotion emoji use”. When a user posted on Weibo or updated his or her online profile, his or her online data changed and the value of dynamic features correspondingly was altered. Based on the records of users’ online behaviors in our study, some users did not update their Weibo every day (e.g., updating their profiles, sending new posts, replying to others’ posts). Thus, our prediction models were built based on weekly feature extraction rather than that daily. For each week during the research period (4 weeks before DV and 4 weeks after DV), we extracted 11 behavioral features and 88 LIWC features, that is, 99 features for each user per week. According to previous studies, the prediction model produced the best prediction results when generated by the mean value of the previous 4 weeks (Hu, 2015). We then took the mean value of 99 features across 4 weeks as the input dynamic features of each user and fed them into the prediction models. Figure 1 portrays the procedure from data extraction to mental status prediction. The models therein make use of various machinelearning algorithms to establish mappings from dynamic features to related questionnaire scores. All the prediction models have reached a moderate correlation with questionnaire scores. Predictive model of depression . We applied the prediction model to predict the degree of depression. This prediction model was developed by ( Hu, 2015 ), where they invited 10,102 Weibo users to complete the 20-item Center for Epidemiological Survey (CES-D, Radloff, 1977 ) and asked for the users’ approvals to access their Weibo data. The Pearson correlation coefficient between the predicted and self-report scores achieved 0.39 ( p < .01). Predictive models of suicide probability . We implemented prediction models of suicide probability to predict one’s facet scores of Suicide Probability Scale (SPS, Cull & Gill, 1982 ): hopelessness, suicidal ideation, negative self-evaluation, and hostility. These machine-learning models were built and validated by Zhang et al. (2014) from the data of 1,041 Weibo users. The output of the prediction model was the user’s facet score of SPS: hopelessness (12 items), suicidal ideation (eight items), negative self-evaluation (nine items), and hostility (seven items). All of the items were 4-point Likert-type scales (ranging from 0 = not at all to 3 = the most ) that describe the risk of suicide. The correlation between the predicted and self-report SPS scores reached the significant level of 0.01 across all four dimensions ( Table 1 ). Predictive model of life satisfaction . The prediction model of life satisfaction was developed by Hao et al. (2014) . Hao and colleagues invited 1,785 Weibo users to complete the Satisfaction With Life Scale (SWLS; Arrindell, Meeuwesen, & Huyse, 1991 ) and authorize their Weibo data for model training. The results indicated that stepwise regression performed best for most dimensions. The Pearson correlation coefficient between the predictions and questionnaire scores achieved 0.30 ( p < .01).",2,2,2,0,2,0,2,0,2,2,2,2,2,1,0,0,0,1,4,0
https://journals.sagepub.com/doi/full/10.1177/0886260518757756,17,5,,"Liu, Xue, Zhao, Wang, Jiao, Zhu",6,0,0,0,2018,"We recorded the timestamp of reporting the first-time DV experience as a boundary and measured mental health status twice. The first measurement employed the Weibo data of the 4 weeks before DV, and the second measurement made use of data from the 4 weeks after the DV experience. As the timestamp of the DV experience may differ from one user to another, we assessed mental health precisely 4 weeks before and after the incident for each victim individually. The nonvictims were analyzed in the same period as their paired victims—they were a one-to-one match. We first analyzed group differences between the victim group (N = 232) and the nonvictim group (independent sample t tests). We then compared the mental health change within both groups at T1 (the day before DV) and T2 (4 weeks after DV) through paired sample t tests. The significance level was α = .05 for all analyses. Effect sizes, d (with pooled standard deviations), were reported for mean differences (cf. Cohen, 1988). The same investigations were conducted on the subgroups of the three DV types: IPV, child abuse, and exposure to DV. In all these comparisons, the dependent variables were the OER-predicted scores of depression, suicide probability, and life satisfaction",2,2,2,0,2,0,2,0,2,2,2,2,2,1,0,0,0,1,4,0
https://journals.sagepub.com/doi/full/10.1177/0886260518757756,17,6,,"Liu, Xue, Zhao, Wang, Jiao, Zhu",6,0,0,0,2018,"Among the users who registered their birth date in their profile (n = 124, 53%), 77% were females, and their ages ranged from 8 to 50 years with the median age of 17 years. Among the 232 victims, 40 people suffered from IPV, 151 people suffered from child abuse, and 41 people were exposed to DV. Seventy-three percent of the victims were from Eastern China, which is the wealthiest region in China. Table 2 features the demographic profiles of these groups.",2,2,2,0,2,0,2,0,2,2,2,2,2,1,0,0,0,1,4,0
https://journals.sagepub.com/doi/full/10.1177/0886260518757756,17,7,,"Liu, Xue, Zhao, Wang, Jiao, Zhu",6,0,0,0,2018,"First, to figure out whether the victims’ mental health status had already worsened before DV, we compared the three mental health indexes of victims and nonvictims at T1 (the day before DV). We determined that there was no significant difference between groups (p < .05) regarding depression, suicide probability, and life satisfaction. The results indicated that these victims had no significantly increased risk of mental health problems before DV. To explore the impact of DV on mental health, we compared the scores of depression, suicide probability, and life satisfaction at T1 and T2 for both the victim group and the nonvictim group. In general, no significant changes were found in the nonvictim group (p > .05). In the victim group, depression and suicide probability increased after DV, and life satisfaction was reduced (see Table 3). Furthermore, the results showed that the victims scored higher on depression, t(231) = –4.080, p < .01, d = 0.27, after DV experiences. The average scores rose to 15.59 (SD = 2.22), very close to the clinical criteria for further treatments. Victims had higher scores for suicide possibility, especially in suicidal ideation, t(231) = –2.138, p < .05, d = 14, hostility, t(231) = –2.675, p < .01, d = 0.18), and hopelessness, t(231) = –2.283, p < .05, d = 0.13. Meanwhile, life satisfaction significantly decreased, t(231) = 3.087, p < .01, d = .20.",2,2,2,0,2,0,2,0,2,2,2,2,2,1,0,0,0,1,4,0
https://journals.sagepub.com/doi/full/10.1177/0886260518757756,17,8,,"Liu, Xue, Zhao, Wang, Jiao, Zhu",6,0,0,0,2018,"We also investigated the impact across three types of DV: IPV, child abuse, and exposure to DV. The estimated consequences of DV of each type were shown in Table 4. Within 4 weeks following DV, abused children experienced significantly increased depression, t(150) = –2.981, p < .01, d = 0.28. Meanwhile, child abuse was the only group that exhibited a decrease in life satisfaction, t(150) = 3.293, p < .01, d = 0.28. As the sample size of the abused children was nearly 3 times greater than the other two groups, paired sample t tests were also conducted on a randomly selected sample of 40 abused children. The results also showed that DV was associated with a decline in life satisfaction, t(40) = –2.478, p < .05. The victims of IPV had elevated depression, t(39) = –2.648, p < .01, d = 0.40, which indicated that they showed more depressive symptoms after DV. No other significant change was found. In addition, exposure to DV triggered an increase in suicide probability, especially in suicide ideation, t(40) = –2.478, p < .05, d = 0.37, and hostility, t(40) = –2.551, p < .05, d = 0.40. The heightened score suggested a higher frequency of victims feeling hostility, isolation, and impulsivity with thoughts of suicide (Valadez et al., 2009).",2,2,2,0,2,0,2,0,2,2,2,2,2,1,0,0,0,1,4,0
https://journals.sagepub.com/doi/full/10.1177/0886260518757756,17,9,,"Liu, Xue, Zhao, Wang, Jiao, Zhu",6,0,0,0,2018,"Our results demonstrate that DV has a significant impact on victims’ mental health in the short term of 4 weeks. Victims have increased levels of depression, higher suicide risks, and diminished life satisfaction after DV. In contrast, individuals in the nonvictim group do not reveal any significant change in terms of depression, suicide probability, and life satisfaction. This is the first study to identify the impacts of DV on mental health measuring both pre- and post-first-reported DV event at fixed points. Consistent with the existing literature, the findings show the negative effects of DV on mental health after considering the potential preexisting differences between the matching groups (J. C. Campbell, 2002; Dillon, Hussain, Loxton, & Rahman, 2013; Dutton et al., 2006; Ludermir, Schraiber, D’Oliveira, França-Junior, & Jansen, 2008; Resnick, Acierno, & Kilpatrick, 1997). The results on the short-term effects of different DV types are consistent with that of previous work. We found that victims of IPV exhibited higher rates of depression after the first DV experience, which is in line with Devries et al. (2013). Victims of child abuse had greater depression and less life satisfaction in the present study, which was in agreement with previous studies regarding the association between child abuse experiences and negative psychological outcomes (Herrenkohl, Klika, Herrenkohl, Russo, & Dee, 2012; Lamb & Greenbaum, 1993). Higher rates of suicide risk among witnesses of DV in the present study was consistent with Kitzmann, Gaylord, Holt, and Kenny’s (2003) findings, where exposure to DV and mental health problems were strongly linked. Our findings identified the short-term effects of DV on mental health, and we also provided evidence for their differences among three types of DV, including child abuse, IPV, and exposure to DV. Samples from previous studies were DV victims recruited from shelters or refuge houses who actively seeking help (R. Campbell, Sullivan, & Davidson, 1995; Krishnan et al., 2001; Pérez-Testor, Castillo, Davins, Salamero, & SanMartino, 2007). Existing findings regarding the effects of DV on mental health were also based on collections of victims from subsamples of longitudinal studies where very few participants reported their unpredictable DV experience (Brown, Cohen, Johnson, & Smailes, 1999; Hayatbakhsh et al., 2007). In this study, we proposed an alternative sampling method based on the publically available online records of their posts. Compared with previous work, we recruited a potential DV victim population through their postings about their DV experience through their social media messages. However, this also has its own set of limitations. The reliability of the reported DV experiences from the collected online users is one limitation of this study. Our analyses relied on Weibo users’ messages, and we assumed their self-reported DV experience was accurate. To overcome this limitation, we manually checked all the collected posts and users’ profiles. As one of the most popular OSNs, Weibo is an emerging data collection platform for social science and human behavior research. Our sample had the limitation of only including a population more likely than the general public to use social media, and specifically, post their DV experience publically. Thus, our study findings did not represent the entire DV victim population. However, although not everyone uses OSNs and not every OSN user prefers this kind of exposure, the online sampling and measurements could still offer a unique sample that could be larger, more updated, timely, and cost-beneficial than survey methods. We were able to identify the victims with relatively mild DV experiences in addition to those who actively sought help due to continual and severe DV sufferance. Our results indicated that the consequences of DV on mental health could be identified from publicly available social media data. Thus, our study informs the development of online assessment for DV victims through analyzing their social media messages. In addition, the work presented here calls for future studies to develop online social media tools for detecting social media users who search for help through posting DV experience. Herein, we put forth a new approach to investigate individuals’ mental health as well as other psychological features. Today, large machine-learning models have been built to predict an individual’s psychological profiles from OSN data (e.g., Hao et al., 2014; Qiu et al., 2012; Zhang et al., 2014). These algorithms provide a superb opportunity for psychologists to measure which could not be measured before. For example, with the help of predictive models, Youyou et al. (2017) were able to eliminate the “reference effect” in personality similarity measurement; this study showed that it was able to conduct the before/after mental health measurements perfectly matched the timing of DV incidence. More potential for OER in psychological studies can be explored in the future. Predicting users’ psychological traits can be beneficial for scientific exploration and monitoring victims’ mental health status. However, the improper use of prediction results may pose a threat to an individual’s well-being, freedom, or life no matter whether they are correct or not (Kosinski, Stillwell, & Graepel, 2013). To control the balance between the promises and perils of OER studies, extreme caution is necessary to protect victims’ privacy. We only use public data in this study, and the posts were anonymized during processing so that the user’s identity could not be paired with their mental health status. There are several additional limitations in our study that merit attention. First, owing to the limited information available from Weibo posts and user profiles, there is the possibility that the users’ information on Weibo was not consistent with their actual condition, which refers to fake profile and false reports. For example, the user may hide prior DV experience on purpose. To cope with this limitation, we utilized rigorous inclusion criteria to ensure that our results were reliable based on the users’ online reports. Second, there was unavoidable subjectivity when judging the posts, even with careful consensus by three different judges and detailed criteria. Third, we did not further analyze the demographic information as confounding variables as certain users did not provide complete demographic information on their Weibo profiles, especially for age information. In future studies, the online method can be combined with offline surveys to further enhance the authenticity and completeness of data. Fourth, most of the DV victims in this study were victims of physical violence (physical violence: n = 159; nonphysical violence: n = 73). The samples were biased probably because physical violence is more straightforward to perceive and easier to articulate compared with other types of abuse. Thus, our results suggest more of the effects of DV on physical violence victims. Fifth, our study only engages in exposure to one single type of violence. It is reported that children with dual exposure of DV (e.g., physical violence and sexual violence) may encounter increased vulnerability in mental health compared with those with exposure to a single type of DV (Moylan et al., 2009). Dose–response relationships exist between cumulative types of victimization (Ouellet-Morin et al., 2015). Future studies could explore more about it through online data. Finally, although the statistical results based on OER-predicted value could lead to new knowledge, we should be more careful when applying the prediction models to individuals or small samples that require heightened accuracy, such as assisting in diagnoses within clinical practice. Despite these limitations, our study has made several important contributions. We have demonstrated a case using OER to overcome the restrictions of traditional measurement methods. In this case, we provided reliable evidence on DV’s short-term impact on several aspects of mental health, with the before/after measurements perfectly matching the timing of DV incidents. The OER approach in our study was regarded as having much potential for future explorations of mental health and violence. It is also hopeful that OER will help reduce DV hazards through automatically monitoring the victims’ mental health and actively detecting potential victims that need help.",2,2,2,0,2,0,2,0,2,2,2,2,2,1,0,0,0,1,4,0
https://www.aaai.org/ocs/index.php/ICWSM/ICWSM18/paper/viewPaper/17884,18,1,,"Ernala, Labetoulle, Bane, Birnbaum, Rizvi, Kane, De Choudhury",7,1,0,0,2018,"Self-disclosures of mental illnesses have been identified to yield coping and therapeutic benefits. An important construct in the self-disclosure process is the audience with whom the individual interacts and shares their experiences. Mental illness self-disclosures are increasingly happening online. However, unlike online support communities where the audience comprises sympathetic peers with similar experiences, what the discloser gains from an ‘invisible’ audience on a general purpose, public social media platform is less understood. Focusing on a highly stigmatized mental illness, schizophrenia, this paper provides the first investigation characterizing the audience of disclosures of this condition on Twitter and how the audience’s engagement impacts future disclosures. Our results are based on a rich year-long temporal analysis of the data of nearly 400 disclosers and their nearly 400 thousand audiences. First, characterizing and modeling the audience engagement temporally, we find evidence of reciprocity in the disclosure process between the discloser and their audience. Then, situating our work in the Social Penetration Theory and operationalizing the disclosure process via a measure of intimacy, an auto-regressive time series model indicates that the patterns of audience engagement and content can forecast changes in the intimacy of disclosures. We discuss the implications for building socially engaging, supportive online spaces for stigmatized mental illness disclosures.",2,2,1,1,0,0,2,0,2,2,2,2,0,1,0,2,2,2,7,0
https://www.aaai.org/ocs/index.php/ICWSM/ICWSM18/paper/viewPaper/17884,18,2,,"Ernala, Labetoulle, Bane, Birnbaum, Rizvi, Kane, De Choudhury",7,1,0,0,2018,"increasingly appropriating social media sites as spaces for self-expression, spreading awareness, breaking inhibitions and stigma, finding solidarity, and building communities. Self-disclosure, the “process of making the self known to others”, is known to support this new and less expected use of social media platforms (Archer 1980). It is a precursor to online expression of identity, emotions, behaviors, and experiences. Particularly in individuals experiencing stigmatized conditions, like mental health challenges, self-disclosure is a frequent coping mechanism (Joinson 2001). A variety of motivations and intents underlie people’s decisions to self-disclose. One established reason is that people need ‘sympathetic others’, as Goffman (2009) posited: those who share the same social stigma, have had similar experiences, and those who “share with him the feeling that he is human and ‘essentially’ normal in spite of appearances and in spite of his own self doubt”. The sympathetic others in an online social platform can, however, be varied. On platforms like Reddit, where there are dedicated support communities for mental health challenges, the others are often experts and peers with similar experiences. On social networking sites like Facebook, the others are likely social ties embedded in the offline context. Yet, recent studies have uncovered “broadcasting self-disclosures”, a phenomenon that refers to sharing personal, sensitive information in a public social media context such as Twitter, to somewhat nebulous, less defined others (Bazarova and Choi 2014). Unlike online support communities, even if the disclosing individual has a mental conceptualization of their audience (Gruzd, Wellman, and Takhteyev 2011), they are likely to be ‘invisible’ and large, consisting not necessarily of experts or of peers undergoing similar experiences, but perhaps a wide variety of people with different backgrounds, interests, identity profiles, and purposes of social media use. Unlike social networking sites, the audience might also largely comprise weak ties (Kwak et al. 2010)—those that the individual might not know or ever encounter offline. Initially, disclosure of sensitive, stigmatized mental illnesses to such an invisible or even imagined audience can seem puzzling. However, the prevalence of the phenomenon, as shown in prior work (De Choudhury et al. 2017; Ernala et al. 2017), suggests that the discloser might gain certain social benefits from such an audience. How can we better understand these audience, the ways they engage with stigmatized content, and the manner they impact the disclosure process on an otherwise general purpose, social media platform? Addressing these questions will help us understand the social benefits a discloser derives over time by continuing to disclose to this audience. Building on this motivation, we present a quantitative methodology to understand audience and their engagement to stigmatized self-disclosures on Twitter. We choose the specific case of self-disclosures of schizophrenia, as it is one of the most stigmatized mental health conditions and sufferers are known to face negative stereotyping and attitudes, discriminatory and offensive behavior, and societal rejection (Dickerson et al. 2002). Specifically, we focus on the following two research questions: RQ1: What are the patterns in which social media audience are engaging with the self-disclosing individuals? RQ2: How does the audience engagement impact the future disclosure process? In other words, is audience engagement predictive to future intimacy of disclosures? Towards these research questions, employing machine learning techniques on a clinically validated dataset, we obtain a list of individuals (disclosers) who have publicly shared about their diagnosis of schizophrenia on Twitter. We define the audience of these disclosures as individuals who have interacted with the disclosers’ content using the Twitter functionalities of retweets, favorites or mentions. Then, we characterize the temporal variation in audience engagement and study its alignment with respect to what the disclosers present about themselves in their postings (RQ1). Then, drawing from the Social Penetration Theory (Altman and Taylor 1973), we model the disclosers’ behavior by operationalizing the notion of intimacy of disclosures. With this measure of intimacy and time series forecasting techniques, we assess if the engagement received from the audience is predictive of future intimacy of the disclosures made by the disclosers (RQ2). Based on our characterization of audience engagement, we find evidence of temporal and topical reciprocity in the interactions between the disclosers and their audience, as would be anticipated in an online support community. In relation to the disclosers’ data, the audience engagement includes major themes such as mental health resources, stigma, and emotional support. Our results from the time series forecasting model show that attributes of the audience engagement like number of mentions received, themes related to emotional support and personal, private life strongly predict patterns in future disclosure behavior. Through these findings, our work sheds new light into the role of the audience in public social media platforms toward supporting self-disclosures of stigmatized conditions and experiences.",2,2,1,1,0,0,2,0,2,2,2,2,0,1,0,2,2,2,7,0
https://www.aaai.org/ocs/index.php/ICWSM/ICWSM18/paper/viewPaper/17884,18,3,,"Ernala, Labetoulle, Bane, Birnbaum, Rizvi, Kane, De Choudhury",7,1,0,0,2018,"Theoretical Framework: Social Penetration Theory One of the aims of this paper has been to study the phenomenon of broadcasting self-disclosures as an interpersonal relationship between the disclosers and their audience. The Social Penetration Theory provides a relevant theoretical framework. Introduced by Altman and Taylor (1973), the theory proposes self-disclosure as a necessary precursor and a critical component to relationship development between individuals. It describes self-disclosure as the process of sharing different levels of information, varying from superficial to intimate, about oneself to others. These varying levels of disclosure (also termed as degree of social penetration) are conceptualized in terms of two dimensions: Breadth and Depth of disclosure. Of interest here is the depth, that refers to degree of intimacy in disclosures. It relates to the extent to which one comfortably opens up about a particular aspect of their personal, private life that would otherwise not be revealed publicly. Situating the social penetration theory in the context of broadcasting self-disclosures, as is the case in this paper, would mean understanding the process of relationship development between the disclosers and audience with respect to the varying levels of self disclosure. This necessitates examining the reciprocal behaviors between the disclosers and audience which motivates our discussion on RQ1. Further, contextualizing the dimensions of breadth and depth of disclosures to social media would require examining the topical content of these disclosures. Since we focus our attention on disclosures specific to one topic i.e. mental illness (schizophrenia in particular), we adopt the component of depth (or intimacy) to model disclosure in RQ2. Thus we refer to the framework of social penetration theory to situate our approach and inform our analysis.",2,2,1,1,0,0,2,0,2,2,2,2,0,1,0,2,2,2,7,0
https://www.aaai.org/ocs/index.php/ICWSM/ICWSM18/paper/viewPaper/17884,18,4,,"Ernala, Labetoulle, Bane, Birnbaum, Rizvi, Kane, De Choudhury",7,1,0,0,2018,"Self-Disclosure on Social Media A rich body of work has studied self-disclosure in the context of computer mediated communication (Joinson 2001). The findings from this literature relating disclosure to trust and group identity (Joinson and Paine 2007), reducing uncertainty and stigma (Cozby 1973; Derlaga and Berg 2013) form the building blocks of recent work on self-disclosure on social media. Supported by the affordances of anonymity (De Choudhury and De 2014; Andalibi et al. 2016) and social connectedness (Bazarova and Choi 2014), social media has been widely adopted as a space for self-disclosures. Specifically, in stigmatized conditions related to identity, health and wellbeing researchers have focused attention on characterizing and modeling self-disclosure behaviors (Haimson and Hayes 2017, Yang et al. 2017) and studying platform specific differences (De Choudhury et al. 2017). Several qualitative studies have augmented this line of research by examining the goals, motivations and challenges in online selfdisclosures (Andalibi et al. 2017). We note that work thus far has largely been around platforms where the others in the context of self-disclosures are sympathetic others, as Goffman (2009) posited it. However, the nature and impact of engaging with the audience of selfdisclosures on public social media platforms is understudied. Our work aims to fill this gap. We focus on one of the most stigmatized conditions, schizophrenia, as the psychopathology of the condition indicates that the sufferers are particularly known to benefit therapeutically from intimate self-disclosures (Shimkunas 1972).",2,2,1,1,0,0,2,0,2,2,2,2,0,1,0,2,2,2,7,0
https://www.aaai.org/ocs/index.php/ICWSM/ICWSM18/paper/viewPaper/17884,18,5,,"Ernala, Labetoulle, Bane, Birnbaum, Rizvi, Kane, De Choudhury",7,1,0,0,2018,"Online Social Capital and Support There has been a relevant line of research concerning online social capital and social support in the context of self-disclosures and wellbeing (Burke et al. 2010). Social capital allows an individual to draw on resources from other members in their social network through bonding and bridging (Coleman 1988). While online social networks have been established to support building and maintaining both kinds of social capital (Ellison et al. 2007), scholars also refer to a related concept “social support”, especially in the context of self-disclosure theories and studies of stigma. A large body of work reveals the support benefits people derive from their interpersonal relationships and social networks in relation to improved health and psychological well-being, self esteem, satisfaction with life, and reciprocity (Helliwell and Putnam 2004). Specific to our focus on stigmatized experiences around mental health, both qualitative and quantitative studies have identified social capital and social support as necessary components in self-disclosure goals and outcomes (De Choudhury and De 2014; Zhang 2017). Nevertheless, gaps still exist in our understanding of how the expectations of social support and the benefits with respect to social capital translate when the audience of self-disclosures are invisible, public, or comprise largely of weak ties. Moreover, the role that the audience of stigmatized disclosures, through support provisioning and social feedback mechanisms, plays in encouraging (or constraining) future disclosure processes, is yet to be empirically investigated. We seek to extend prior work by providing a robust data-driven study of the audience of schizophrenia disclosures on Twitter.",2,2,1,1,0,0,2,0,2,2,2,2,0,1,0,2,2,2,7,0
https://www.aaai.org/ocs/index.php/ICWSM/ICWSM18/paper/viewPaper/17884,18,6,,"Ernala, Labetoulle, Bane, Birnbaum, Rizvi, Kane, De Choudhury",7,1,0,0,2018,"Twitter Data on Schizophrenia Disclosures As a first step of our data collection, we obtained access to a clinician validated Twitter dataset of self-disclosures of schizophrenia from Ernala et al. (2017). This dataset included the public Twitter timelines (1,940,921 tweets) of 146 users who had self-disclosed regarding their diagnosis of schizophrenia for the first time in the year 2014. Example key-phrases that were used as seed search queries to identify these disclosures included first-person reports of schizophrenia experiences and diagnoses like “Diagnosed me with schizophrenia/ psychosis”1. Further, noisy data in the form of disingenuous, inappropriate statements and jokes were filtered out via manual examination and consultation with two psychiatrists who see schizophrenia patients. Next, we adopt the supervised machine learning methodology developed by Birnbaum et al. (2017). The classifier employs clinical appraisals as ground truth and linguistic (n-grams) and psycholinguistic tokens (from the LIWC dictionary (Pennebaker, Francis, and Booth 2001)) in tweets as features to successfully recognize (with 88% areaunder-curve and 80% precision) genuine self-disclosures (of schizophrenia) gathered from Twitter. Obtaining access to the clinical appraisals and adapting the technique in this classifier on a new sample of 600 Twitter users, we were able to machine label 433 of them to have genuinely disclosed their illness. Our expanded dataset (together with original 146 users) used in this paper therefore consists of 579 Twitter users who engaged in self-disclosures of schizophrenia. Recall that for the purpose of this paper, we aim to investigate the patterns of audience engagement around the Twitter content of these users and how it impacts their disclosure behavior in the future. Therefore without loss of generality, for our analysis we focus on an year long period of Twitter activity succeeding the 579 users’ self-disclosures. We found that 395 out of these 579 had an entire year’s worth of Twitter data. Over the year-long period, we found these 395 users to have shared 1,491,623 tweets with an average of 3776.26 tweets per user and 17.48 tweets per day per user. We report a summary of these descriptive statistics in Table 1.",2,2,1,1,0,0,2,0,2,2,2,2,0,1,0,2,2,2,7,0
https://www.aaai.org/ocs/index.php/ICWSM/ICWSM18/paper/viewPaper/17884,18,7,,"Ernala, Labetoulle, Bane, Birnbaum, Rizvi, Kane, De Choudhury",7,1,0,0,2018,"Definitions Next, we introduce a few definitions centering around self-disclosures of schizophrenia on Twitter and audience engagement around it. First, a ‘discloser’ is an individual who has self disclosed (revealed) their diagnosis of schizophrenia by publicly posting on Twitter, on day d, the day of disclosure. The ‘audience’ of these disclosures is the set of Twitter users who have interacted with the discloser’s Twitter posts viz-a-viz the platform’s functionalities—retweets, favorites or ‘likes’, mentions over the period of one year after day d. We operationalize ‘audience engagement’ as any instance of such an interaction between a member of the audience and the discloser. Retweets, mentions, favorites or ‘likes’ constitute the various markers of audience engagement.",2,2,1,1,0,0,2,0,2,2,2,2,0,1,0,2,2,2,7,0
https://www.aaai.org/ocs/index.php/ICWSM/ICWSM18/paper/viewPaper/17884,18,8,,"Ernala, Labetoulle, Bane, Birnbaum, Rizvi, Kane, De Choudhury",7,1,0,0,2018,"Audience and Audience Engagement Data We proceed with data collection of audience engagement, by obtaining data on the engagement markers — retweets, favorites and mentions surrounding the disclosers’ data. Retweets Data. We collected this audience engagement dataset by identifying the Twitter users who have interacted with the disclosers by retweeting their content during the one year after disclosure. First, for each tweet from the disclosers during this period of analysis, we obtained the number of retweets received by that tweet. Then, we used the official Twitter API to obtain the list of individuals who have retweeted it. Applying this method across all 395 disclosers we obtain 124,630 distinct Twitter users (retweets audience) who retweeted the disclosers’ content 2,895,118 times. Favorites Data. We identified Twitter users who interacted with the disclosers’ data through favorites (liking) during the one year after disclosure. For each tweet posted by the disclosers during the period, we first obtained the number of favorites received by the tweet. Then, we parsed the JSON object of the HTML popup that shows users who have favorited a tweet. Applying this across all disclosers, we obtained a set of 169,041 Twitter users (favorites audience) who favorited the disclosers’ content 4,592,890 times. Mentions Data. Here, we collect data on those Twitter users who have interacted with the disclosers using the mentions (or @-replies) functionality. On Twitter, when an individual replies to another (say with username B), the tweet is automatically appended with the ‘@B’ string. We used this stylistic convention of tweets to compile a list of search queries by appending an ‘@’ symbol before the username of each of our disclosers. This operation provided us with all tweets that were incoming mentions to the disclosers including Twitter users who mentioned them and the textual content of the mention tweets. This dataset finally consisted of 80,090 distinct users (mentions audience) who mentioned the disclosers in their 348,456 mention tweets. Audience Data. To compile the final set of audience, we collated the list of users in the three datasets above— 124,630 retweets audience, 169,041 favorites audience and 80,090 mentions audience, and extracted overall 373,761 users. At a discloser level, on average, the audience size was 1218.4. The distribution of audience (size) and its descriptive statistics are provided in Figure 1 and Table 1.",2,2,1,1,0,0,2,0,2,2,2,2,0,1,0,2,2,2,7,0
https://www.aaai.org/ocs/index.php/ICWSM/ICWSM18/paper/viewPaper/17884,18,9,,"Ernala, Labetoulle, Bane, Birnbaum, Rizvi, Kane, De Choudhury",7,1,0,0,2018,"Methods Per RQ1, we propose methods to characterize audience engagement around disclosers’ Twitter data based on two attributes: the content of engagement and its markers. Thematic Representation of Disclosers’ Data. First, we develop a thematic representation of the data shared by the disclosers over the year-long period following their day of disclosure d. This representation is used to examine the dynamic interaction between the disclosers and their audience in terms of content sharing. We begin by employing topic modeling on Twitter timelines of our 395 disclosers. After preprocessing the tweets to remove URLs and stopwords, we run Latent Dirichlet Allocation using MALLET: MAchine Learning for LanguagE Toolkit. We perform hyperparameter optimization over the sampling iterations to extract 30 topics. Using the topic model, we compute the topic distribution via posterior probabilities for each tweet. Next, to identify semantically interpretable, broader themes from the 30 topics, we employed qualitative labeling. Two human raters who were social media and mental health experts performed semi-open coding on the extracted topics collaboratively. Drawing from their experience studying self-disclosures of schizophrenia, the raters built a set of topical descriptors for each topic by analyzing the top contributing keywords per topic. Then, they combined the LDA topics into semantically interpretable, broader themes and also labeled whether or not each theme was related to the diagnosis and experiences of schizophrenia. Finally, to inspect the disclosers’ data over time, we used the theme annotations and computed z-scores of the average probability of each theme per day across all disclosers. Since z-scores reveal relative differences in the values of a distribution, it qualifies as a suitable metric to study variation over time. Characterizing Engagement Content. Using the same topic modeling and qualitative theme annotation approach as above, we characterized the engagement content (i.e. dataset of the mention tweets), corresponding to each discloser. First, we built an LDA model to obtain 30 topics from the linguistic content of these mention tweets. Then, we computed the topic distribution for each tweet in the mentions dataset. Next, we employed a qualitative labeling method to identify interpretable, broader themes from these LDA generated 30 topics in the engagement content. The same two raters as above were employed to analyze the top keywords per topic and come up with topical descriptors for each topic, including annotations on whether or not each theme was related to the disclosure and experiences of schizophrenia. We again used the z-scores of the average probability of each theme per day across all disclosers to identify theme-specific variation in the engagement content over time. Characterizing Engagement Markers. To characterize the engagement markers, we use the dataset of retweets, favorites and mentions received by each discloser per day during the one year period following day of disclosure. For each day d, ranging from d = 0 to d = 365, we find the average number of retweets, favorites and mentions received by all the disclosers and transform the average values into zscores. This transformation gives us the variation in engagement markers received by the disclosers as a function of time and allows relative comparison. We obtain three time series, one for retweets, favorites and mentions from this step. Discovering Patterns of Audience Engagement. To study the variations in engagement indicators (markers and content) with respect to that in disclosers’ data, we make the following categorization. Based on the thematic annotations over disclosers’ data and their corresponding engagement content, we categorize the theme labels into: themes related to the diagnosis and experiences of schizophrenia, and those unrelated. For both theme categories, we adopt time series comparison techniques (e.g., the cross correlation measure) to understand how the z-score distributions of the engagement markers and the themes of the engagement content vary with the disclosers’ theme distributions over time.",2,2,1,1,0,0,2,0,2,2,2,2,0,1,0,2,2,2,7,0
https://www.aaai.org/ocs/index.php/ICWSM/ICWSM18/paper/viewPaper/17884,18,10,,"Ernala, Labetoulle, Bane, Birnbaum, Rizvi, Kane, De Choudhury",7,1,0,0,2018,"Comparing Disclosers’ Themes and Audience’s Themes. We present results from the thematic annotations on audience’s engagement content and discuss them in the context of the themes derived from disclosers’ data (Ref. Table 2). This juxtaposition of themes helps us understand the audience response with respect to what the disclosers’ are sharing on Twitter. First, among the engagement content themes that relate to experiences of schizophrenia, we begin by considering the theme “Mental Health Support/Stigma” (MHSS) that also surfaces in the disclosers’ data. For instance, we notice the usage of words such as ‘hcsmca’, ‘pndhour’, ‘awareness’, ‘issue’ referring to online communities dedicated to exchanges around health care, mental illness and spreading awareness. The same theme also includes overlapping words like ‘depression’, ‘anxiety’, ‘meds’, ‘mental health’, ‘pain’, relating to the stigma and challenges around experiences of schizophrenia. This shows that the audience, in response to the schizophrenia content of the disclosers share their experiences and resources related to mental health care, providing solidarity. Next, we consider another common schizophrenia related theme, ‘Functioning’. We observe overlapping keywords, such as ‘people’, ‘life’, ‘good’, ‘work’, ‘money’, ‘job’, ‘love’, ‘sleep’. Relatedly, we also find the theme ‘Appearance’ (words: ‘hair’, ‘wear’, ‘red’, ‘clothes’, ‘arms’, ‘softly’) that surfaces in the tweets of both the disclosers and their audience. Taken together, these themes relate to the everyday experiences capturing behaviors around the social, emotional, physical, and cognitive aspects of life. Their cooccurrence as themes reflects the utility of engagement content as a mechanism to converse about everyday aspects of life, communicate, plan, and exchange thoughts and ideas. Next, we consider the theme ‘Emotions’ that appears in the engagement content with words like ‘love’, ‘happy’, ‘good’, ‘hope’, ‘lovely’, ‘miss’, ‘sweet’, ‘beautiful’. While this theme is also present in disclosers’ data, we note a higher prevalence of emotional content in the engagement content than that of the disclosers based on the number of topics contributing towards the theme. This particular imbalanced overlap characterizes the emotional support provisioning nature of the engagement that the disclosers gather from their audiences; a form of support found in the literature to be key to improved mental health state and in supporting therapeutic outcomes from disclosures of stigmatized conditions (De Choudhury et al. 2014). Lastly, we find an overlap between the audiences and the disclosers in the theme ‘Sexuality’ containing terms such as ‘girl’, ‘man’, ‘guy’, ‘he’s’, ‘she’s’, ‘cute’, ‘fuck’, ‘sex’. This indicates a tendency of the disclosers and in response their audiences to “open up” about deeply personal aspects of their private life that are usually not revealed publicly. Nevertheless, despite the thematic reciprocity noted above, we note a sharp distinction between the tweets of the disclosers and audience—shown by the theme ‘Symptoms’. In the case of the disclosers, this theme (‘r/paranormal’, ‘r/creepy’, ‘ufo’) reveals a predominant occurrence of words that have symptomatic relevance to schizophrenia. We do not observe such patterns in the themes extracted from the audience’s engagement content. This indicates that, although the disclosers are sharing their first person experiences of the illness, the audiences do not respond with similar personal accounts. This brings to light the distinction in broadcasting disclosures on platforms like Twitter, where, unlike support communities, the audience need not necessarily consist of peers undergoing similar experiences. By juxtaposing the thematic annotations from the disclosers and their audiences, we find evidence of reciprocal conversations around shared themes related to experiences of schizophrenia. We situate this discussion in the social penetration theory that gives a distinctive emphasis to selfdisclosing behaviors being maintained by the “gradual overlapping and exploration of their mutual selves by parties to a relationship” (Sprecher et al. 2013). Patterns of Changes in the Engagement Content. Here, we are interested in the question—how do the above (schizophrenia related and other) themes from the disclosers and the audience co-vary over time? Inspecting Figure 2(ad), we observe that there is a close temporal alignment between the disclosers’ and the audiences’ themes relating to schizophrenia experiences. Specifically, by analyzing the cross correlation between the two, we find that the highest correlation of 0.125 between the two time series occurs at a negative lag of 4. This positive correlation at a negative lag provides indications of reciprocity in the disclosure process—as the disclosers increasingly talk about their schizophrenia experiences at time t − 4 (in days), it correlates with the audience talking about similar themes related to these experiences at t. Reciprocity has been identified as a major norm in self-disclosure research (Jiang, Bazarova, and Hancock 2013). In contrast, we find that as the disclosers increasingly talk about their experiences, the audience begin limiting posts on other unrelated topics in the future (maximum correlation of -0.125 at a negative lag of 4). Patterns of Changes in the Engagement Markers. We ask the question—how does the audience, with the help of various platform functionalities, respond to disclosers, and how do different engagement markers co-vary with disclosers’ themes. Figure 3a shows the z-score distribution of these markers over time. We observe two findings. First, beginning at the day of disclosure, there is a peak in mentions indicating an increase in incoming engagement from the audience. However, there is lowered audience engagement during this early period through retweets and favorites. This could indicate that the audience find the disclosers’ content out of place and take time to modulate their engagement around it. Second, there is a very close alignment between the temporal variation in retweets and favorites received from the audience. This may be attributable to the similar functionality between both actions i.e. they both indicate some form of acknowledgement or endorsement, and have a lower barrier for content production (at the click of a button), compared to mentions which have a higher barrier to content production, requiring consciously drafted replies. Next, in Figure 2(e-j), we present an analysis of the temporal variation in the three engagement markers in relation to the disclosers’ themes—both the schizophrenia related ones as well as the rest. Upon visual inspection, we notice that the alignment between the daily measurements of engagement markers is higher with disclosers’ data related to schizophrenia experiences as compared to other unrelated content. For the time series representing thematic variation in schizophrenia related experiences, the maximum correlation with retweets and favorites is -0.09, -0.08 observed at cross correlation lags of 5, 5 respectively. The negative correlation at a positive lag denotes that as the disclosers increasingly talk about their condition and experiences, it correlates to receiving fewer retweets and favorites in the days following. This is likely explained by the perception that the actions of retweet or favorite signal information sharing intentions and do not convey an appropriate response to stigmatized disclosures. On the other hand, we observe a stronger alignment between the disclosure content related to experiences of schizophrenia and the mentions received. The correlation of disclosure related content with mentions is the strongest with a lag 0 with a positive value of 0.17. This shows that as the disclosers increasingly talk about their experiences, it correlates to receiving more mentions (on the same day). However, in the case of unrelated themes, we observe a delayed response via mentions from the audience (maximum correlation of 0.14 at lag -7). Summarily, our findings from RQ1 suggest reciprocity, temporally in the number of engagement markers received and topically, in the themes received viz-a-viz the audience engagement content.",2,2,1,1,0,0,2,0,2,2,2,2,0,1,0,2,2,2,7,0
https://www.aaai.org/ocs/index.php/ICWSM/ICWSM18/paper/viewPaper/17884,18,11,,"Ernala, Labetoulle, Bane, Birnbaum, Rizvi, Kane, De Choudhury",7,1,0,0,2018,"Methods For our second research question, we investigate whether the audience engagement as characterized by engagement markers and engagement content (RQ1), can predict future intimacy of disclosures. To begin, we describe how we operationalize intimacy of disclosures, and then propose and evaluate a time series forecasting model to predict these values accurately from the engagement markers and content. Operationalizing Intimacy of Disclosures. To operationalize the disclosure process, we refer to the Social Penetration theory that models self-disclosure as a process of building intimate interpersonal relationships. We adopt one of the measures proposed by the theory i.e. depth of disclosure or intimacy to operationalize disclosure in our work. The depth of disclosure relates to the degree of intimacy i.e. “how open or close someone can become with another person despite their anxiety over self-disclosure”. In the context of mental health related self-disclosures on Twitter, depth of disclosure would denote the extent to which the discloser continues to share information about their experiences specific to their stigmatizing condition. Given the lack of availability of ground truth data on disclosure intimacy and because discrete human judgments from a specific post may not be applicable across all users, to measure intimacy of disclosures from the textual content of disclosers’ tweets, we use the following hybrid approach leveraging topic modeling and human annotations (Chancellor et al. 2016). I. Manual annotation of disclosers’ topics: Adopting the results from topic models built over disclosers’ data as a thematic representation of their content (RQ1), we employed three human raters to analyze the top contributing keywords per topic and then label the level of intimacy disclosed via the topic. We defined the levels of intimacy to span a threepoint Likert scale—low (1), medium (2) and high (3) motivated by prior work (Taylor and Altman 1975). First, the raters manually browsed a sample of tweets by the disclosers to familiarize themselves with the content. Then, corresponding to this rating scale, they created a set of rules to annotate each topic with one of the three levels. High intimacy of disclosure (score of 3). This included topics specific to the experiences of schizophrenia, information that is rarely expressed on a public social media platform like Twitter. For example, topics around symptomatic expressions, social support and stigma related to mental illnesses were included in this category. Medium intimacy of disclosure (score of 2). This category included behavioral expressions related to functioning, so cial interactions, temporal planning that were not unusual to be shared on Twitter. Low intimacy of disclosure (score of 1). This included topics that were totally unrelated to the disclosure of schizophrenia and consisted casual social media conversations such as political issues, entertainment, etc. Following the manual annotation task, the raters had a high inter-rater reliability of 0.78 given by the Fleiss κ measure. Out of the 30 topics belonging to disclosers’ data, this annotation task yielded 8 topics with high (3) intimacy, 7 with medium (2), and 15 with low (1) intimacy score. II. Calculating tweet-level and time series measures of intimacy of disclosure. Given a tweet posted by the discloser, its posterior topic distribution given by the topic model (in RQ1), and the intimacy label (in RQ2) we calculate the intimacy of the tweet as a weighted sum of all topic probabilities by their intimacy labels to obtain a single score of intimacy of disclosure. We aggregate these tweet-level intimacy values per day and per discloser throughout our analysis period; we use z-scores of these aggregated values to capture their relative variation over time. Predicting Future Intimacy of Disclosures from Audience Engagement. Given the intimacy of disclosure expressed by the disclosers and the associated engagement markers and content of the audience over time, we describe the prediction task as a time series forecasting problem. Since historical values of intimacy can also assist in predicting future intimacy values, we adopt an auto-regressive time series forecasting model. The dependent (or response) variable that is being forecasted is the time series representing daily measurements of intimacy of disclosure (obtained above). The exogenous variables (or predictors) are the engagement markers received from the audience as characterized by the following time series—number of retweets, favorites, mentions, and theme distribution of engagement content. Note that all timeseries are expressed as z-scores of average daily measurements of the variable. Data Preparation. First, we process the data to verify stationarity assumptions of time series forecasting methods. We execute the following steps: 1) We apply a moving average transformation with a window size of 14 days to check for changes in the mean and variance over time. 2) We apply the Augmented Dickey Fuller (ADF) test, a standard test for stationarity in a series (Dickey and Fuller 1981). For the series that do not pass the ADF test, we apply a first order shift in the data and re-evaluate conditions for stationarity. Model Fitting. We propose an Auto Regressive Integrated Moving Average with Exogenous Input (ARIMAX) model to predict the dependent variable (future intimacy) from the exogenous variables (audience engagement data). Our model is meant to forecast on day t, the intimacy of disclosure based on the exogenous variables spanning n days before t. We perform grid search over a maximum lag of 20 days for the autoregressive (p) and the moving average (q) parameters to find candidate models. Applying maximum likelihood estimation, we use log-likelihood, Akaike & Bayesian information criterion (AIC, BIC) measures to assess goodness of fit. We validate the final model by performing in-sample rolling predictions and assessing model performance using metrics like the root mean squared error.",2,2,1,1,0,0,2,0,2,2,2,2,0,1,0,2,2,2,7,0
https://www.aaai.org/ocs/index.php/ICWSM/ICWSM18/paper/viewPaper/17884,18,12,,"Ernala, Labetoulle, Bane, Birnbaum, Rizvi, Kane, De Choudhury",7,1,0,0,2018,"Figure 3b shows the temporal variation in intimacy of disclosure, combined across all disclosers’ data. We observe a peak representing heightened levels of intimacy of disclosure on the day of disclosure (d = 0) and the immediately succeeding days. Since topics related to the experiences of schizophrenia were rated with an intimacy score of 3, it appears that the short period immediately following the day of disclosure continues to include high intimacy content. With this time series of intimacy of disclosure as our response variable, we proceed to results on the forecasting model. First, on testing the stationarity assumption we find that the intimacy series, despite showing minimal changes in mean and variance over time, failed to pass the ADF test for stationarity (t = -2.68, p = 0.07). Therefore, we performed first order shift (differencing) on this series and re-evaluated for stationarity by the ADF test. If Yt denotes the value of the time series Y at period t, then the first difference of Y at period t is equal to Yt - Yt−1. We find that the differenced series for intimacy successfully passes the ADF test (t = -9.17, p = 2 ×10−15). Following the same approach, we evaluate stationarity of all the exogenous variables i.e. engagement markers and content (themes). We find that all the series pass the stationarity test except for the engagement content series, specifically the following themes—“Mental Health Support/Stigma”, ‘Sexuality’, ‘Communication’ and “Temporal References”. We applied the same differencing technique and note that the stationarity assumptions are met. Next, based on the grid search results for model selection and parameter tuning, we found the best lag order for the ARIMAX process i.e. the auto-regressive and moving average parameters to be p=8 and q=3. Including the differencing parameter d = 1 we fit an ARIMAX(8,1,3) model on the time series data (intimacy of disclosure, engagement markers and content) for forecasting. The goodness of fit of this model in terms of log-likelihood, AIC and BIC were found to be -351.9, 751.9 and 845.0 respectively. Table 3 summarizes the ARIMAX model in terms of point mass estimates of the external variables, their 95% confidence intervals, and the corresponding p-values. We refer to this information, to examine the variables that provide the most explanatory power in the forecasting problem i.e.we ask what engagement markers and engagement content shared by the audience have high predictive power in forecasting future intimacy levels of the disclosers. We assess statistical significance here at the p=0.05 level. First, we observe that the number of mentions received is a significant predictor of future intimacy. This affirms our previous findings that mentions indicate a strong incoming engagement in ways of conversing, sharing experiences and resources with the disclosers. Next, we find two themes within the audience engagement content that are statistically significant to future intimacy levels. The first such theme is ‘Emotions’ with keywords such as ‘care’, ‘worry’, ‘trust’, ‘life’. Emotional support received in cases of stigmatized conditions has been shown to help with coping and provide satisfaction in online support communities by previous studies (Vlahovic et al. 2014). Prior work has also linked intimacy to satisfaction with social support received during crisis (Hobfoll, Nadler, and Leiberman 1986). This relates with our finding that emotional content received through audience engagement can be linked to intimacy and predict future disclosure behaviors. The second significant theme is ‘Sexuality’. Discussions on one’s sexuality are often considered to be sensitive in nature. When they happen on a public social media platform like Twitter, they indicate the audience’s intent to reciprocally converse with the disclosers about topics that are otherwise personal. This reciprocity might also motivate the disclosers to reveal more intimate aspects of their illness experiences to their audience. Finally, to validate the model, we compute in-sample rolling predictions for the model on an out-of-sample data over the last 30 days in our year-long period of analysis. Note that the ARIMAX model forecasts the differenced intimacy of disclosure and therefore, the predicted values are compared to the original differenced values of intimacy (Ref. Figure 3c) We observe that our model is able to closely forecast the actual intimacy levels of disclosure. Assessing model performance, we find the Root Mean Square Error, Mean Absolute Error and Symmetric Mean Absolute Percentage Error measures as 0.66, 0.52 and 6.8 respectively. These values statistically establish the satisfactory performance of the model. As a final validation step, we check the residuals of the model for absence of serial correlation. We compute the Durbin-Watson statistic which tests for the null hypothesis that there is no serial correlation (Durbin 1970). We find the test statistic (Durbin-Watson’s d) as 1.8, which is close to the ideal value of 2 in case of no serial correlation.",2,2,1,1,0,0,2,0,2,2,2,2,0,1,0,2,2,2,7,0
https://www.aaai.org/ocs/index.php/ICWSM/ICWSM18/paper/viewPaper/17884,18,13,,"Ernala, Labetoulle, Bane, Birnbaum, Rizvi, Kane, De Choudhury",7,1,0,0,2018,"Theoretical Implications We began this study questioning the puzzling nature of stigmatized self-disclosures made to an invisible audience on a public microblogging platform. By characterizing the audience engagement towards disclosures of schizophrenia on Twitter, we found evidence of reciprocity, both topically and temporally, in the interactions between the audience and disclosers. We also observed that using the functionalities of favorites, retweets, and mentions, the audience is able to engage with the disclosers in a variety of ways: providing support, advice, and solidarity, sharing personal experiences and online help resources, and conversing about everyday aspects of life. While these attributes are key characteristics of online support communities, their occurrence on Twitter is revealing as it lacks many critical components of an online community such as norms, moderation, roles etc. Similarly, strong social ties are considered to be the hallmark of quality support and psychological wellbeing (Burke, Marlow, and Lento 2010). However, despite lacking many aspects of a social network (Kwak et al. 2010) Twitter seems to be providing positive outcomes to individuals with a highly stigmatized condition, schizophrenia. Further, we examined how audience engagement impacts future disclosure behavior, to understand if the disclosers gather interpersonal and social benefits through this public disclosure process. The results from our forecasting model demonstrate that key predictors, such as number of mentions, emotional support, and discussions on personal, sensitive topics can successfully forecast future intimacy of disclosures. This finding indicates that the disclosure process supports not only bridging social capital, that is, finding new acquaintances who provide access to new information and help resources, but also over time, in bonding social capital, in the form of reciprocity, support, and companionship (Ellison, Steinfield, and Lampe 2007). Although the nature of audience providing these social capital resources is nebulous, i.e. the disclosers may not necessarily know who this audience is, even if they have an imagined mental conception of who it might be (Gruzd, Wellman, and Takhteyev 2011), the reciprocal engagement that the audience provides over time confirms observations about online social platforms facilitating formation and maintenance of social capital. Nevertheless, as argued in the literature (Steinfield, Ellison, and Lampe 2008), one might expect that disclosing about stigmatized, sensitive issues like mental illnesses to such an invisible and imagined audience might increase the likelihood of a context collapse that can hinder future disclosures. However, we find that, despite the risk of context collapse, the disclosers do not employ counteractive strategies, but rather continue to engage in schizophrenia related intimate exchanges with their audience over time.",2,2,1,1,0,0,2,0,2,2,2,2,0,1,0,2,2,2,7,0
https://www.aaai.org/ocs/index.php/ICWSM/ICWSM18/paper/viewPaper/17884,18,14,,"Ernala, Labetoulle, Bane, Birnbaum, Rizvi, Kane, De Choudhury",7,1,0,0,2018,"Practical Implications Today, technology-based therapy, counseling, and intervention tools such as 7 Cups of Tea (7cups.com) and Crisis Text Line (crisistextline.org) are being increasingly adopted, where individuals in distress can talk to trained volunteers and supportive ‘listeners’. There has also been an upsurge in the usage of similarly purposed artificial intelligence (AI) based conversational agents. While purported to be helpful, these services present unique opportunities and challenges. How can these tools accommodate stigmatized self-disclosures of mental illnesses and facilitate their expected social benefits? The methodology that we propose in this paper to study audience engagement towards stigmatized mental health disclosures provides a principled framework to examine the social interactions of disclosers and audiences in such contexts. A crucial aspect of these technology-assisted therapy tools is providing the volunteers or the AI agents adequate resources, so they can successfully engage in conversations with help seekers. To do so, there is a need to capture timely feedback, in terms of the nature and quality of engagement (of the volunteer or AI agent), and their impact on future disclosure behavior of the help seekers. With our forecasting methodology (RQ2), interactive systems can be built to enable the volunteers/agents/algorithms act on the help seekers/disclosers feedback on engagement in a timely manner. Similarly, our framework for studying patterns in audience engagement with respect to what the disclosers reveal about themselves (RQ1) can be adopted to identify specific engagement patterns signaling reciprocity. Upon identification, the usage of these markers can by promoted — either manually as guidelines to volunteers and support providers or algorithmically in the case of conversational agents. Finally, moderation efforts in online support communities and social media platforms can adopt our methodologies to similarly motivate audiences engage meaningfully with vulnerable self-disclosing individuals and to thereby create positively beneficial online therapeutic spaces.",2,2,1,1,0,0,2,0,2,2,2,2,0,1,0,2,2,2,7,0
https://www.aaai.org/ocs/index.php/ICWSM/ICWSM18/paper/viewPaper/17884,18,15,,"Ernala, Labetoulle, Bane, Birnbaum, Rizvi, Kane, De Choudhury",7,1,0,0,2018,"Limitations and Future Work We acknowledge some limitations to our work. First, our findings are limited by our data acquisition capabilities. We have not probed into the nature of the audience and questions surrounding their own social media use which remains a ripe area for future work. Further, we note that the disclosers might pursue goals other than social benefits, such as trust, impression management, and social validation that we do not disentangle in our analysis. Stemming from our interest in the invisible audience, we focused our attention on finding evidence for a general form of social benefits received by disclosure. Studying the alignment between discovered patterns of audience engagement and specific disclosure goals constitutes an interesting direction for future research. Further, the social benefits that we identify in our study (such as reciprocity) need further validation using self-reported data — for example, their impact on psychological outcomes in the discloser. Qualitative data such as interviews can be powerful in complementing this line of work. Finally, in our operationalization of intimacy of disclosures, we limit our focus to studying the impact of active, incoming audience engagement. But, future work could examine how non-responsive or non-supportive audience impacts future disclosure behaviors.",2,2,1,1,0,0,2,0,2,2,2,2,0,1,0,2,2,2,7,0
https://www.aaai.org/ocs/index.php/ICWSM/ICWSM18/paper/viewPaper/17884,18,16,,"Ernala, Labetoulle, Bane, Birnbaum, Rizvi, Kane, De Choudhury",7,1,0,0,2018,"In this paper, we provided some of the first empirical insights into the nature of audience engagement received in response to broadcasting self-disclosures of schizophrenia on Twitter. Characterizing and examining the patterns of audience engagement with respect to the disclosers’ content, we find evidence of reciprocity. Further, our results from a forecasting model demonstrate that components of audience engagement such as mentions, emotional support and discussions around personal life are strong predictors of future disclosure behaviors. Our work informs the social benefits that disclosers obtain from Twitter and has implications to technology mediated support spaces on the internet.",2,2,1,1,0,0,2,0,2,2,2,2,0,1,0,2,2,2,7,0
https://aclanthology.org/W15-1201.pdf,19,1,,"Coppersmith, Drezde, Harman, Hollingshead ",4,0,1,1,2015,"Many significant challenges exist for the mental health field, but one in particular is a lack of data available to guide research. Language provides a natural lens for studying mental health – much existing work and therapy have strong linguistic components, so the creation of a large, varied, language-centric dataset could provide significant grist for the field of mental health research. We examine a broad range of mental health conditions in Twitter data by identifying self-reported statements of diagnosis. We systematically explore language differences between ten conditions with respect to the general population, and to each other. Our aim is to provide guidance and a roadmap for where deeper exploration is likely to be fruitful.",2,2,2,1,2,0,2,2,4,2,2,2,0,2,0,1,2,0,5,0
https://aclanthology.org/W15-1201.pdf,19,2,,"Coppersmith, Drezde, Harman, Hollingshead ",4,0,1,1,2015,"A recent study commissioned by the World Economic Forum projected that mental disorders will be the single largest health cost, with global costs increasing to $6 trillion annually by 2030 (Bloom et al., 2011). Since mental health impacts the risk for chronic, non-communicable diseases, in a sense there is “no health without mental health” (Prince et al., 2007). The importance of mental health has driven the search for new and innovative methods for obtaining reliable information and evidence about mental disorders. The WHO’s Mental Health Action Plan for the next two decades calls for the strengthening of “information systems, evidence and research,” which necessitates new development and improvements in global mental health surveillance capabilities (World Health Organization, 2013). As a result, research on mental health has turned to web data sources (Ayers et al., 2013; Althouse et al., 2014; Yang et al., 2010; Hausner et al., 2008), with a particular focus on social media (De Choudhury, 2014; Schwartz et al., 2013a; De Choudhury et al., 2011). While many users discuss physical health conditions such as cancer or the flu (Paul and Dredze, 2011; Dredze, 2012; Aramaki et al., 2011; Hawn, 2009), some also discuss mental illness. There are a variety of motivations for users to share this information on social media: to offer or seek support, to fight the stigma of mental illness, or perhaps to offer an explanation for certain behaviors. Past mental health work has largely focused on depression, with some considering post-traumatic stress disorder (Coppersmith et al., 2014b), suicide (Tong et al., 2014; Jashinsky et al., 2014), seasonal affective disorder, and bipolar disorder (Coppersmith et al., 2014a). While these represent some of the most common mental disorders, it only begins to consider the range of mental health conditions for which social media could be utilized. Yet obtaining data for many conditions can be difficult, as previous techniques required the identification of affected individuals using traditional screening methods (De Choudhury, 2013; Schwartz et al., 2013b). Coppersmith et al. (2014a) proposed a novel way of obtaining mental health related Twitter data. Using the self-identification technique of Beller et al. (2014), they looked for statements such as “I was diagnosed with depression”, automatically uncovering a large number of users with mental health conditions. They demonstrated success at both surveillance and analysis of four mental health conditions. While a promising first step, the technique’s efficacy for a larger range of disorders remained untested. In this paper we employ the techniques of Coppersmith et al. (2014a) to amass a large, diverse collection of social media and associated labels of diagnosed mental health conditions. We consider the broadest range of conditions to date, many significantly less prevalent than the disorders examined previously. This tests the capacity of our approach to scale to many mental health conditions, as well as its capability to analyze relationships between conditions. In total, we present results for ten conditions, including the four considered by Coppersmith et al. (2014a). To demonstrate the presence of quantifiable signals for each condition, we build machine learning classifiers capable of separating users with each condition from control users. Furthermore, we extend previous analysis by considering approximate age- and gender-matched controls, in contrast to the randomly selected controls in most past studies. Dos Reis and Culotta (2015) found demographic controls an important baseline, as they muted the strength of the measured outcomes in social media compared to a random control group. Using demographically-matched controls allows us to clarify the analysis in conditions where age is a factor, e.g., people with PTSD tend to be older than the average user on Twitter. Using the ten conditions and control groups, we characterize a broad range of differences between the groups. We examine differences in usage patterns of categories from the Linguistic Inquiry Word Count (LIWC), a widely used psychometrically validated tool for psychology-related analysis of language (Pennebaker et al., 2007; Pennebaker et al., 2001). Depression is the only condition for which considerable previous work on social media exists for comparison, and we largely replicate those previous results. Finally, we examine relationships between the language used by people with various conditions — a task for which comparable data has never before been available. By considering multiple conditions, we can measure similarities and differences of language usage between conditions, rather than just between a condition and the general population. The paper is structured as follows: we begin with a description of how we gathered and curated the data, then present an analysis of the data’s coherence and the quantifiable signals we can extract from it, including a broad survey of observed differences in LIWC categories. Finally, we measure language correlations between pairs of conditions. We conclude with a discussion of some possible future directions suggested by this exploratory analysis.",2,2,2,1,2,0,2,2,4,2,2,2,0,2,0,1,2,0,5,0
https://aclanthology.org/W15-1201.pdf,19,3,,"Coppersmith, Drezde, Harman, Hollingshead ",4,0,1,1,2015,"There is rich literature on the interaction between mental health and language (Tausczik and Pennebaker, 2010; Ramirez-Esparza et al., 2008; Chung and Pennebaker, 2007; Pennebaker et al., 2007; Rude et al., 2004; Pennebaker et al., 2001). Social media’s emergence has renewed interest in this topic, though gathering data has been difficult. Deriving measurable signals relevant to mental health via statistical approaches requires large quantities of data that pair a person’s mental health status (e.g., diagnosed with PTSD) to their social media feed. Successful approaches towards obtaining these data have relied on three approaches: (1) Crowdsourced surveys: Some mental health conditions have self-assessment questionnaires amenable to administration over the Internet. Combining this with crowdsource platforms like Amazon’s Mechanical Turk or Crowdflower, a researcher can administer relevant mental health questionnaires and solicit the user’s public social media data for analysis. This technique has been effectively used to examine depression (De Choudhury, 2013; De Choudhury et al., 2013c; De Choudhury et al., 2013b). (2) Facebook: Researchers created an application for Facebook users that administered various personality tests, and as part of the terms of service of the application, granted the researchers access to a user’s public status updates. This corpus has been used in a wide range of questions from personality (Schwartz et al., 2013b; Park et al., In press), heart disease (Eichstaedt et al., 2015), depression (Schwartz et al., 2014), and psychological well-being (Schwartz et al., 2013a). (3) Self-Stated Diagnoses: Some social media users discuss their mental health publicly and openly, which allows researchers to create rich corpora of social media data from users who have a wide range of mental health conditions. This has been used previously to examine depression, PTSD, bipolar, and seasonal affective disorder (Coppersmith et al., 2014a; Coppersmith et al., 2014b; Hohman et al., 2014). A similar approach has been used to identify new mothers for studying the impact of major life events (De Choudhury et al., 2013a). (4) Affiliation: Some rely on a user’s affiliation to indicate a mental health condition, such as using posts from a depression forum as a sample of depression (Nguyen et al., 2014). Other work on mental health and related topics have studied questions that do not rely on an explicit diagnosis, such as measuring the moods of Twitter users (De Choudhury et al., 2011) to measure their affective states (De Choudhury et al., 2012). Outside of social media, research has demonstrated how web search queries can measure population level mental health trends (Yang et al., 2010; Ayers et al., 2013; Althouse et al., 2014).",2,2,2,1,2,0,2,2,4,2,2,2,0,2,0,1,2,0,5,0
https://aclanthology.org/W15-1201.pdf,19,4,,"Coppersmith, Drezde, Harman, Hollingshead ",4,0,1,1,2015,"We follow the Twitter data acquisition and curation process of Coppersmith et al. (2014a). This data collection method has been previously validated through replication of previous findings and showing predictive power for real-world phenomena (Coppersmith et al., 2014a; Coppersmith et al., 2014b; Hohman et al., 2014), though there likely is some ‘selection bias’ by virtue of the fact that the data is collected from social media – specifically Twitter – which may be more commonly used by a subset of the population. We summarize the main points of the data collection method here1 . We obtain messages with self-reported diagnoses using the Twitter API. Self-reported diagnoses are tweets containing statements like “I have been diagnosed with CONDITION”, where CONDITION is one of ten selected conditions (each of which has at least 100 users): Attention Deficit Hyperactivity Disorder (ADHD), Generalized Anxiety Disorder (Anx), Bipolar Disorder, Borderline Personality Disorder (Border), Depression (Dep), Eating Disorders (Eating; includes anorexia, bulimia, and eating disorders not otherwise specified [EDNOS]), obsessive compulsive disorder (OCD), post-traumatic stress disorder (PTSD), schizophrenia (Schizo; to include schizophrenia, schizotypal, schizophreniform) and seasonal affective disorder (Seasonal). We use the common names for these disorders, rather than adhering to a more formal one (e.g., DSM-IV or DSM5), for two reasons: (1) to remain agnostic to the current discussion in clinical psychology around the standards of diagnosis; and (2) our classification is based on user statements. While sometimes an obvious mapping exists for user statements to more formal definitions (e.g., “shell shock” equates to today’s “PTSD”), other times it is less obvious (e.g., “Anxiety” might refer to generalized anxiety disorder or social anxiety disorder). Each self-reported diagnosis was examined by one of the authors to verify that it was a genuine statement of a diagnosis, i.e., excluding jokes, quotes, or disingenuous statements.2 Previous work shows high inter-annotator agreement (κ = 0.77) for assessing genuine statements of diagnosis (Coppersmith et al., 2014a). For each author of a genuine diagnosis tweet we obtain a set of their public Twitter posts using the Twitter API (at least 100 posts per user, but usually more); we do not have access to private messages. All collected data was publicly posted to Twitter between 2008 and 2015.",2,2,2,1,2,0,2,2,4,2,2,2,0,2,0,1,2,0,5,0
https://aclanthology.org/W15-1201.pdf,19,5,,"Coppersmith, Drezde, Harman, Hollingshead ",4,0,1,1,2015,"Our analyses focus on user-authored content; we exclude retweets and tweets with a URL since these often quote text from the link. The text is lowercased and all non-standard characters (e.g., emoji) are converted to a systematic ASCII representation via Unidecode3 . Users were removed if their tweets were not at least 75% English, as determined by the Google Compact Language Detector4 . To avoid bias, we removed the tweets that were used to manually assess genuine statements of diagnosis. However, other tweets with a self-statement of diagnosis may remain in a user’s data. Table 1 summarizes the number of users identified and their median number of tweets for each condition.",2,2,2,1,2,0,2,2,4,2,2,2,0,2,0,1,2,0,5,0
https://aclanthology.org/W15-1201.pdf,19,6,,"Coppersmith, Drezde, Harman, Hollingshead ",4,0,1,1,2015,"Generally, control groups were formed via random selection of Twitter users. Yet physical and mental health conditions have different prevalence rates depending on age and gender. Dos Reis and Culotta (2015) demonstrated that failing to account for these can yield biased control groups that skew results, so we aim to form approximate age- and gendermatched control groups. There is a rich literature investigating the influence of age and gender on language (Pennebaker, 2011). Since Twitter does not provide demographic information for users, these insights have been broadly applied to inferring demographic information from social media (Volkova et al., 2015; Fink et al., 2012; Burger et al., 2011; Rao et al., 2011; Rao et al., 2010). We use these techniques to estimate the age and gender of each user so as to select an age- and gender-matched control group. For each user in our mental health collection we obtain age and gender estimates from the tools provided by the World Well-Being Project (Sap et al., 2014)5 . These tools use lexica derived from Facebook data to identify demographics, and have been shown successful on Twitter data. The tools provide continuous valued estimates for age and gender, so we threshold the gender values to obtain a binary label, and use the age score as is. We draw our community controls from all the Twitter users who tweeted during a two week period in early 2014 as part of Twitter’s 1% ‘spritzer’ stream. Each user who tweeted in English and whose tweets were public had an equal probability of being included in our pool of controls. From this pool, we identify the closest matching control user in terms of age and gender for each user in the mental health collection. We select controls without replacement so a control user can only be included once. In practice, differences between estimated age of paired users were miniscule.",2,2,2,1,2,0,2,2,4,2,2,2,0,2,0,1,2,0,5,0
https://aclanthology.org/W15-1201.pdf,19,7,,"Coppersmith, Drezde, Harman, Hollingshead ",4,0,1,1,2015,"We provide a comprehensive picture of differences in usage patterns of LIWC categories between users with various mental health conditions. We measure the proportion of word tokens for each user that falls into a given LIWC category, aggregate by condition, and compare across conditions. For each user, we calculate the proportion of their tokens that were part of each LIWC category. Thus for each category and each condition, we have an empirical distribution of the proportion of language attributable to that category. The violin plots in Figure 2 show an example of how this changes across conditions as compared to controls. Table 2 shows deviations for all categories and conditions as follows: ‘+++’ indicates that condition users evince this category significantly more frequently6 than control users; ‘+’ indicates that the distribution is noticeably higher for the condition population than the control population, but not outside the inter-quartile-range; ‘−’ indicates differences where condition users use this category less frequently than control users. Some interesting trends emerge from this analysis. First, some categories show differences across a broad range of mental health conditions (e.g., the ANXIETY, AUXILIARY VERBS, COGNITIVE MECHANISMS, DEATH, FUNCTION, HEALTH, and TENTATIVE categories of words). This suggests that there are a subset of changes in language that may be indicative of an underlying mental health condition (without much regard for specificity), while others seem to be very specific to the conditions they are associated with (e.g., INGEST and NEGATIONS with eating disorders). Some of the connections between LIWC categories and mental health conditions have already been substantiated in the mental health literature, while others (e.g., AUXVERB) have not and are ripe for further exploration. Second, many of the conditions show similar patterns (e.g., anxiety, bipolar, borderline, and depression), while others have distinct patterns (e.g., eating disorders and seasonal affective disorder). It is worth emphasizing that a direct mapping between these and previously-reported LIWC results (in, e.g., Coppersmith et al. (2014a) and De Choudhury et al. (2013c)) is not straightforward, since previous work did not use demographically-matched control users.",2,2,2,1,2,0,2,2,4,2,2,2,0,2,0,1,2,0,5,0
https://aclanthology.org/W15-1201.pdf,19,8,,"Coppersmith, Drezde, Harman, Hollingshead ",4,0,1,1,2015,"Validated and accepted lexicons like LIWC cover a mere fraction of the total language usage on social media. Thus, we also use an open-vocabulary approach, which has greater coverage than LIWC, and has been shown to find quantifiable signals relevant to mental health in the past (Coppersmith et al., 2014a; Coppersmith et al., 2014b). Though many open-vocabulary approaches exist, we opt for one that provides a reasonable score even for very short text, and is robust to the creative spellings, lack of spaces, and other textual faux pas common on Twitter: character n-gram language models (CLMs). In essence, rather than examining words or sequences of words, CLMs examine sequences of characters, including spaces, punctuation, and emoticons. Given a set of data from two classes (in our case, one from a given mental health condition, the other from its matched controls), the model is trained to recognize which sequences of characters are likely to be generated by either class. When these models are presented with novel text, they estimate which of the classes was more likely to have generated it. For brevity we will omit discussion of the exact score calculation and refer the interested reader to Coppersmith et al. (2014a). For all we do here, higher scores will indicate a tweet is more likely to come from a user with a given mental health condition, and lower scores are more likely to come from a control user. Since we are examining ten conditions, we have ten pairs of CLMs (for each pair, one CLM is trained from the users with a given mental health condition, and one CLM is trained from their matched controls).",2,2,2,1,2,0,2,2,4,2,2,2,0,2,0,1,2,0,5,0
https://aclanthology.org/W15-1201.pdf,19,9,,"Coppersmith, Drezde, Harman, Hollingshead ",4,0,1,1,2015,"To validate that our CLMs are capturing quantifiable differences relevant to their specific conditions, we examine their accuracy on a heldout set of users. Each condition-specific CLM produces a score that roughly equates to how much more (or less) likely it is to have come from a user with the given condition (e.g., PTSD) than a control. We aggregate these scores to compute a final score for use in classification. We score each tweet with the CLM and use the score to make a binary distinction – is this tweet more likely to have been generated by someone who has PTSD or a control? We calculate the proportion of these tweets that are classified as PTSD-like (the overall mean), which can be thought of as how PTSD-like this user looks over all time. Given that some of these symptoms change with time, we can also compute a more localized version of this mean, and derive a score according to the “most PTSD-like period the user has”. This is done by ordering these binary decisions by the time the tweet was authored, selecting a window of 50 tweets, and calculating the proportion of those tweets classified as PTSD-like. We then slide this window one tweet further (removing the oldest tweet, and adding in the next in the user’s timeline) and calculate the proportion again. The highest this rolling-window mean achieves will be referred to as the maximum local mean. We combine these scores to yield the classifier score ψ = overall mean ∗ maximum local mean, capturing how PTSD-like the user is over all time, and how PTSDlike they are at their most severe. We estimated the performance of our classifiers for each condition on distinguishing users with a mental health condition from their community controls via 10-fold cross-validation. This differs only slightly from standard cross-fold validation in that our observations are paired; we maintain this pairing when assigning folds – each mental health condition user and their matched control are in the same fold. To assess performance, we could draw a line (a threshold) in the ranked list, and classify all users above that line as having the mental health condition, and all users below that line as controls. Those with the condition above the line would be correctly classified (hits), while those controls above the line would be incorrectly classified (false alarms). Figure 3 shows performance of this classifier as Receiver Operating Characteristic (ROC) curves as we adjust this threshold, one curve per mental health condition. The x-axis shows the proportion of false alarms and the y-axis shows the proportion of true hits. All our classifiers are better than chance, but far from perfect. To aid interpretation, Table 3 shows precision at 10% false alarms. Performance for most conditions is reasonable, except seasonal affective disorder which is very difficult (as was reported by Coppersmith et al. (2014a)). Anxiety and eating disorders have much better performance than the other conditions. Most importantly, though, for all conditions (including seasonal affective disorder), we are able to identify language usage differences from control groups.",2,2,2,1,2,0,2,2,4,2,2,2,0,2,0,1,2,0,5,0
https://aclanthology.org/W15-1201.pdf,19,10,,"Coppersmith, Drezde, Harman, Hollingshead ",4,0,1,1,2015,"Given the breadth of our language data, we can compare across mental health conditions, examining relationships between the conditions under investigation, rather than only how each condition differs from controls. Previous work (Coppersmith et al., 2014a) reported preliminary findings that indicated a possible relationship between the language use from different mental health conditions: similar conditions (either in concomitance and comorbidity or symptomatology) had similar language. The story found here is related, but more complicated. For this comparison, we build new CLMs that exclude any user with a concomitant disorder (to prevent their data from making their conditions appear artificially similar). We then score a random sample of 1 million tweets that meet our earlier filters with the CLMs from each condition. We could then examine how the language in any pair of conditions is related by calculating the Pearson’s correlation (r) between the scores from these models. More interesting, though, is how all these conditions relate to one another, rather than any given pair. To that end, we use a standard clustering algorithm7 , shown in Figure 4. Here, each condition is represented by a vector of its Pearson’s r correlations, calculated as above, to each of the conditions (to include an r = 1.0 to itself). Each condition starts as its own cluster on the left side of the figure. Moving to the right, clusters are merged, most similar first, until all conditions merge into a single cluster. One particular clustering is highlighted by the colors: conditions with blue lines are in clusters of their own, so seasonal affective, ADHD, and borderline appear to be significantly different from the rest); and schizophrenia and OCD are clustered together, shown in red. While this is not the most obvious grouping of conditions, the patterns are far from random: the disorders in green (PTSD, bipolar, eating disorders, anxiety, and depression) have somewhat frequent concomitance in our data and elsewhere (Kessler et al., 2005) and recent research indicates links between OCD and schizophrenia (Meier et al., 2014). Notably, these data are not age- and gender-matched, so these variables also likely factor into the clustering. Thus, we leave this particular relationship between language and mental health as an open question, suggesting fertile grounds for more controlled future work.",2,2,2,1,2,0,2,2,4,2,2,2,0,2,0,1,2,0,5,0
https://aclanthology.org/W15-1201.pdf,19,11,,"Coppersmith, Drezde, Harman, Hollingshead ",4,0,1,1,2015,"We examined the language of social media from users with a wide range of mental health conditions, providing a roadmap for future work. We explored simple classifiers capable of distinguishing these users from their age- and gender-matched controls, based on signals quantified from the users’ language. The classifiers also allowed us to systematically compare the language used by those with the ten conditions investigated, finding some groupings of the conditions found elsewhere in the literature, but not altogether obvious. We take this as evidence that examining mental health through the lens of language is fertile ground for advances in mental health writ large. The wealth of information encoded in continually-generated social media is ripe for analysis – data scientists, computational linguists, and clinical psychologists, together, are well positioned to drive this field forward.",2,2,2,1,2,0,2,2,4,2,2,2,0,2,0,1,2,0,5,0
https://dl.acm.org/doi/abs/10.1145/2740908.2743049,20,1,,"De Choudhury, Pavalanathan",2,1,0,0,2015,"Online fora and support groups, including social media have emerged as prominent resources for individuals who are distressed by affective and behavioral challenges [10, 18, 30]. These tools act as a constantly available and conducive source of information, advice, and support [29]. An important motivation behind the use of these online resources for mental health concerns is also that they support open and honest discourse [33]. Self-disclosure can be an important therapeutic ingredient [22] and is linked to improved physical and psychological well-being [32]. The nature of online mental health discourse, however, may vary depending on the nature of identity adopted by an individual. This is likely to be particularly valid in the case of mental illness, since it is considered socially stigmatic [8]. Literature in sociology also supports this observation. In his celebrated book “Stigma” [14], Goffman examined how, individuals with a socially discredited attribute such as mental illness, tend to manage impressions of themselves in social settings—in order to protect their identities. However we note that in online settings, such as on social media, this constraint may be circumvented. This is because individuals may choose to withhold their actual identities allowing themselves to engage in more candid self-disclosure than is possible in offline settings, or through their identified online personas. Our motivation for this research is also rooted in the rich literature on online identity construction, which has been recognized as a key aspect of online communities [11, 28]. Prior work demonstrates dissociative anonymity (a resistance to attach to offline identity or to their actual account/online persona), for instance, can be the foundation of online disinhibition [24]. Online disinhibition, the ability to avoid being “visible, verifiable, and accountable”, leads people to act differently than they would in offline settings [6]. Social media naturally provides us with a rich ecosystem where we can study ways in which individuals manage their identities to engage in discourse on a stigmatized condition like mental illness. In this light, this paper focuses on a relatively underexplored area of research involving characterization of behavior around a stigmatized condition, mental illness. In doing so, we extend our own prior work on examining mental health support on social media [10]. Here we investigate identity management in social media in the context of mental health. Specifically, we focus on the social media reddit, which, in contrast to other popular social media and networking platforms like Facebook and Twitter, provisions adoption of ‘throwaway’ accounts as semi-anonymous identities for posting content. Throwaways are temporary accounts that reddit users create to dissociate from their primary reddit identity [25]. Most throwaway accounts are used exactly once [13]; thus their use disallows user behavior to be tracked historically, or through postings made from primary reddit accounts. Note that throwaways, however, do not adhere to a strict notion of anonymity [11], but research has shown that they are often used as proxies of anonymity [13, 34, 25]. We leverage this observation about throwaway accounts in our study. Our primary contribution lies in characterizing how different forms of identity on mental health subreddits are associated with distinctive affective, cognitive, linguistic style, and social attributes. Leveraging measures derived from literature in psychology, which suggest language to be a reliable way of measuring people’s internal thoughts and emotions, we also study the differences in the nature of content shared in throwaway posts and posts from regular reddit accounts on these subreddits. Our findings based on a large corpus of reddit posts indicate the presence of almost six times more throwaway posts in mental health subreddits, in contrast to other subreddits. Thus throwaways may be fulfilling a unique need for individuals seeking to use reddit for discourse around a stigmatic health concern. Moreover, we observe that throwaway postings in mental health forums exhibit increased negativity, greater cognitive bias and self-attentional focus, lowered self-esteem and greater disinhibition, even to the extent of revealing vulnerability to self-harm. Through these findings, throwaways are observed to allow individuals to be less inhibited by selfpresentation concerns, presumably due to lack of identifiability and accountability. Our work, on one hand, indicates the potential of using social media for behavioral therapy. Community moderators may encourage semi-anonymous and disinhibiting discourse around stigmatized experiences. On the other hand, since some throwaway posts manifest self harm and depressive tendencies, our findings may guide the design of in-time, privacy-secure interventions which can bring help to vulnerable populations.",2,2,2,1,0,0,2,0,2,2,2,1,0,0,0,0,1,0,1,0
https://dl.acm.org/doi/abs/10.1145/2740908.2743049,20,2,,"De Choudhury, Pavalanathan",2,1,0,0,2015,"The work presented in this paper builds on prior research around online identity and its role in the success and dynamics of online communities [11]. McKenna and Bargh [26] found that anonymity on the web leads to lowered self-presentation concerns: “under the protective cloak of anonymity users can express the way they truly feel and think”. In CMC literature, Donath [11] argued that anonymity can be “the savior of personal freedom, necessary to ensure liberty in an era of increasingly sophisticated surveillance”. In a study on identity signals on 4chan, which has majorly (90%) anonymous posts, Bernstein et al. [2] found the use of alternative mechanisms such community-specific dialect, images and fluency in posts, for establishing status in the community. Schoenebeck [35] studied posts from the anonymous website YouBeMom.com and found that anonymity provides a comfortable environment for moms to converse without constraints of social norms. Recently Leavitt [25] performed an ethnographic study to examine the context in which ‘throwaway’ accounts are used in reddit and found that the perception of anonymity shape the increased use of throwaway accounts. Our work builds on this body of research and extends our initial findings on the unique use of throwaways in mental health communities on reddit [10]. Mental illness is a stigma [8]—individuals are often uncomfortable revealing such sensitive information or seeking help in face-to-face contexts, including avoiding availing counseling for fear of getting ostracized. We examine how proxies of anonymity, such as reddit’s throwaways, may be providing users with an open and honest platform of discourse where such anxiety or trepidation are likely to be absent or minimal.",2,2,2,1,0,0,2,0,2,2,2,1,0,0,0,0,1,0,1,0
https://dl.acm.org/doi/abs/10.1145/2740908.2743049,20,3,,"De Choudhury, Pavalanathan",2,1,0,0,2015,"Our second line of motivation is rooted in another rich body of work examining the important role of disinhibition in discourse around mental health. Disinhibition may result in increased selfdisclosure [21], which is known to promote improved wellbeing and plays a positive role in psychological counseling and therapy. Jourard [22] reported that self-disclosure was a basic element in the attainment of improved mental health. Ellis [12] reported that discourse on emotionally laden traumatic experiences can be a safe way of confronting mental illness. On similar lines seminal work by Pennebaker et al. [31] found that participants assigned to a trauma-writing condition (where they wrote about a traumatic and upsetting experience) showed immune system benefits. Disclosure in this form has also been associated with reduced visits to medical centers and psychological benefits in the form of improved affective states [32]. A thorough treatment of detecting self-disclosure content in social media is not the focus of this paper. However our work relates to the above literature. We intend to examine how social media, through its ability to allow individuals to choose and regulate their online identities, might be providing new ways of candid expression amid a challenging health experience.",2,2,2,1,0,0,2,0,2,2,2,1,0,0,0,0,1,0,1,0
https://dl.acm.org/doi/abs/10.1145/2740908.2743049,20,4,,"De Choudhury, Pavalanathan",2,1,0,0,2015,"We contextualize the goals of this paper within the above body of work. We focus on the highly popular social media, curation, and news site reddit (http://www.reddit.com/). reddit allows its users to submit content in the form of links or text posts. Content entries, i.e., posts, are organized by areas of interest called “subreddits”, such as politics, programming, or science. Each subreddit share the same platform mechanics, but is essentially an entirely different community with different rules, norms, and members. Our notion of mental health communities in this paper leverages the presence of a number of mental health focused subcommunities on reddit [10]. Within these communities, for the purpose of our research, we categorize postings along two kinds of identities adopted by the post authors: the ‘throwaway’ accounts and the regular reddit accounts, which we call the ‘identified’ accounts. Throwaways are distinct from the regular reddit accounts as reddit allows an individual to create these accounts without giving out an email address, personally identifiable information or information about their regular reddit accounts. This allows discourse without drawing additional attention to a user’s regular activities [25]. While in theory no personally identifiable information is shared by reddit users even through their primary accounts, it is possible that use of reddit over a long period of time may accumulate enough information in a user’s postings. This may reveal aspects of their real lives [27], or of their behavior, personality, and attitudes. Hence for sensitive information disclosure, individuals have been known to prefer to use throwaways instead of their primary reddit accounts [13, 25]. Essentially, use of throwaways would hinder tracking behavior across postings of a user in multiple communities and prevent negative repercussions. Urbanski [34] also reported that the idea of anonymity through throwaways seems embraced in the reddit culture. Since regular reddit accounts may accrue rich context and information about an individual over long-term use of the account, we refer to them as ‘identified’ accounts. We address the following inter-related research questions: RQ 1 Are behavioral characteristics of discourse from throwaway accounts different in mental health forums versus other reddit communities? RQ 2 What are the behavioral characteristics of discourse from throwaway and identified accounts on mental health forums on reddit? RQ 3 Does the use of throwaway accounts promote sharing of more disinhibiting content on mental health forums compared to content from identified reddit users?",2,2,2,1,0,0,2,0,2,2,2,1,0,0,0,0,1,0,1,0
https://dl.acm.org/doi/abs/10.1145/2740908.2743049,20,5,,"De Choudhury, Pavalanathan",2,1,0,0,2015,"We used reddit’s official API (http://www.reddit.com/ dev/api) to collect posts, comments, and associated metadata from several mental health focused subreddits. Our data collection proceeded using the method used in [10]. We first arrived at a comprehensive list of subreddits to focus on, with the help of reddit’s native subreddit search feature (http://www.reddit.com/ reddits). Specifically, we searched for subreddits on “mental health”. Two researchers familiar with reddit employed an initial filtering step on the search results returned, so that we focus on high precision subreddits discussing mental health concerns. Thereafter, we focused on a snowball approach in which starting with a few seed subreddits (mentalhealth, depression), we compiled a second list of “related” or “similar” subreddits that are mentioned in the profile pages of the seed subreddits. The subreddits we crawled are given in Table 1. All of these subreddits host public content. Additionally, for the purposes of statistical comparison, we identified a set of subreddits, sample of which is listed in Table 2 as our control group—meaning they are unrelated to mental health topics. To compile this list, we collected set of 1000 posts from the “New” category of reddit’s home page and extracted the subreddits they were posted in— this method is likely to yield us a near-random sample of subreddits. On this candidate set of subreddits, to eliminate topical or contextual bias, we filtered a random sample of 25 to be our final control subreddit sample. Note that these control communities spanned a variety of sizes (e.g, r/Askreddit is large, r/thatHappened is smaller), and were used both for emotional (e.g., r/friendship) and objective discourse (e.g., r/505Nerds), as well as spanned multi-faceted topics. For each of these subreddits (mental health related and control), we obtained daily crawls of their posts in the New category1 , similar to [10]. Corresponding to each post we collected information on the title of the post, the body or textual content, id, timestamp when the post was made, author id, comments, and the number of upvotes and downvotes it obtained2 . The crawl of the subreddits used in this paper spanned between November 2013 and March 2014. We provide some basic descriptive statistics of the crawled dataset in Table 3 for mental health subreddits, and in Table 4 for the control. We now highlight our method of identifying throwaway posts in our dataset. We used a two step process motivated by prior work [10, 13, 25]. First, one of the authors manually inspected a sample of reddit usernames in our dataset, and recorded the various naming strategies adopted by throwaway account owners. We thus identified a set of patterns typically prevalent in the throwaway accounts: *thrw*, *throwaway*, *throw*,*thrw*,*thraway*. This is motivated from the work of Gagnon [13] and our own work [10], wherein such patterns were adopted for identifying throwaway accounts with success. We labeled all the posts written using this type of usernanames as throwaway posts. Additionally, we looked for mentions of “throwaway"" in either post titles or post text in the remaining posts and then one of the authors manually inspected each posts to label it as a throwaway post or not. This second step is motivated by the work of Leavitt [25]. In this way we compiled a high precision dataset on posts from throwaway accounts.",2,2,2,1,0,0,2,0,2,2,2,1,0,0,0,0,1,0,1,0
https://dl.acm.org/doi/abs/10.1145/2740908.2743049,20,6,,"De Choudhury, Pavalanathan",2,1,0,0,2015,"We propose four categories of attributes to characterize behavior manifested by throwaway accounts on the mental health communities we study. These are: (1) affective attributes, (2) cognitive attributes, (3) linguistic style attributes, and (4) social attributes. Measures belonging to all of these attribute categories are largely based on the psycholinguistic lexicon LIWC (http: //www.liwc.net), and were motivated from prior literature on privacy, language and intimacy, and social spheres and information management [1, 11, 20, 19]. Additionally, we leverage insights from prior literature that examine association between the behavioral expression of individuals and their responses to traumatic context and crises, including vulnerability due to mental illness [7, 10]. We consider two measures of affect: positive affect (PA), and negative affect (NA), and four other measures of emotional expression: anger, anxiety, sadness, and swear. These measures are computed using the psycholinguistic lexicon LIWC. We used LIWC to define the cognitive measures as well: (a) cognition, comprising cognitive mech, discrepancies, inhibition, negation, death, causation, certainty, and tentativeness; and (b) perception, comprising set of words in LIWC around see, hear, feel, percept, insight, and relative. Next, we consider four measures of linguistic style: (1) Lexical Density: consisting of words that are verbs, auxilliary verbs, nouns, adjectives3 , and adverbs. (2) Temporal References: consisting of past, present, and future tenses. (3) Social/Personal Concerns: included words belonging to family, friends, social, work, health, humans, religion, bio, body, money, achievement, home, and sexual. (4) Interpersonal Awareness and Focus: comprised words that are 1st person singular, 1st person plural, 2nd person, and 3rd person pronouns. Our final set of measures are the social attributes. We utilized a variety of content sharing, social interaction, and social support indicators as measures in this category. These are: post length, number of comments, vote difference (difference between upvotes and downvotes, divided by the total number of upvotes and downvotes), comment arrival rate (average time difference between any two subsequent comments in a post’s comment thread), time to first comment (time elapsed between the first comment and the time the corresponding post was shared), and median comment length.",2,2,2,1,0,0,2,0,2,2,2,1,0,0,0,0,1,0,1,0
https://dl.acm.org/doi/abs/10.1145/2740908.2743049,20,7,,"De Choudhury, Pavalanathan",2,1,0,0,2015,"Corresponding to our RQ 1, we begin by examining the discerning characteristics of posts made from throwaways in mental health (MH) subreddits compared to those in other subreddits (control subreddits as listed in Table 2). This will help us validate findings from psycholinguistics literature [7]: whether discourse from throwaway accounts in the mental health subreddits bears language markers known to be associated with psychological challenges such as mental illness. We find statistically significant differences between the two groups. We observe almost 6 times more throwaway posts in the mental health subreddits than in control subreddits. This could be because of the stigmatic nature of mental illness; likely, people prefer to post under a (semi)-anonymous identity more frequently compared to other more mundane social contexts/topics. This notable difference in the proportion of throwaway posts leads to exploring whether the throwaway posts in mental health subreddits differ from the control subreddits. We use the measures of social, affective, cognitive, and linguistic processes defined earlier for this purpose. We present statistical hypothesis testing between the cohorts using independent samples t-tests. Affective Attributes. From Table 5, we observe notable differences—throwaway MH posts tend to use more attributes of negative affective processes such as “negative emotion” (almost twice of the control group), “anger” (almost twice of the control group), “anxiety” and “sad” (more than 3 times as control group). Prior literature indicates such increased negative affect to be associated with depression symptoms such as mental instability and helplessness, loneliness, and restlessness [9]. The following post excerpts illustrate this: i know killing myself would hurt my family, but so would telling them that i’m a such a worthless failure. This tells us that reddit users find writing throwaway posts in the mental health subreddits as a forum to talk about their negative emotions in order to reduce stress and increase positive selfperceptions [12]. Cognitive Attributes. Throwaway posts in MH subreddits use more “negations” (1.5 times as control group) such as no, not, never: usage of which is associated with inhibition [24] i.e. MH posts from throwaway accounts are likely more self-conscious than users writing posts in control subreddits using throwaways: it’s a never ending cycle. therapy is not helping at all and i have tried several therapists. i have not seen my dr. since being put on the zoloft almost 2 months ago. Throwaway MH posts also use more of “feel” and “insight” words which shows their expressions of feelings. Further, throwaway MH posts use more “death” related words (4 times as control group) which indicate their vulnerability to physical harm. Linguistic Style Attributes. Throwaway MH cohort is found to be focused more on the present and the here and now (present tense) less on the past (past tense). Next, lower use of linguistic process term such as “family”, “friends”, “home”, “social”, “work”, “humans”, and “money” imply the throwaway MH redditors are less socially concerned or bothered. In fact, together with the fact that they also use greater number of first person pronouns shows that throwaway posts in mental health subreddits contain more personal stories and are in general, high in self-preoccupation [3]. Lower use of second person pronouns, first person plural pronouns and third person pronouns in throwaway MH posts implies that these redditors tend to be less socially interactive with the larger reddit audience. Increased use of “health"" words in throwaway MH posts reveals that these posts talk more about their health related issues. Social Attributes. Lastly, throwaway MH posts receive lesser social support i.e. lower number of comments (almost 6 times lower than control group). This might be due to the smaller size of the audience in the former—mental health related topics form a relatively more niche community compared to a subreddit like Askreddit. However, this cohort does receive comments at a faster rate (comment arrival rate almost 4 times than control group), as well as comments that are typically longer compared to the control group comments. This observation aligns with [10]—it is possible that the reddit audience tends to sympathize more with the throwaway MH posters, and provide more helpful and contributory feedback and opinions because of their honest confessions.",2,2,2,1,0,0,2,0,2,2,2,1,0,0,0,0,1,0,1,0
https://dl.acm.org/doi/abs/10.1145/2740908.2743049,20,8,,"De Choudhury, Pavalanathan",2,1,0,0,2015,"Findings on RQ 1 established the manner in which the throwaway redditors engaging in mental health discourse exhibit distinctive characteristics compared to those conversing about other topics. We now turn our attention to examining how the former cohort differs from those in the mental health subreddits who choose to reveal their identities (RQ 2). Cognitive Attributes. Throwaway posts in MH subreddits use significantly more “negation” words than posts made from identified reddit accounts. We note that often negation exhibits in the context of a negative expression and bears a tone of confession: “all of this is turning me into the person i don’t want to be and i cannot see an easy way passed this"", “i dont know what to do now"". Moreover, throwaway posts present more cognitive biases through use of more “certainty” (e.g. always, never) related words. Although “certainty” words are associated with emotional stability [16], throwaway users use these words in a negative context which indicates that this cohort might be suffering from lowered self-esteem and displaying self-derogatory thoughts: e.g. “i have always suffered from horrible anxiety"", “i’m always so worried"", “i have an amazing life. but i’m never happy"", whereas identified redditors do not do so: e.g.“reddit has always been the place where i feel comfortable"", “i never remember doing anything bad"", “everyone always gives me warm greetings"", Linguistic Style Attributes. Throwaway postings use more verbs (which indicate discourse around actions), but less entities, e.g., nouns and adjectives. This reveals lowered interest in objects and things around them [5]. Expressing more about actions is found to be correlated with sensitive disclosure [19]. This implies throwaway posts revealing more sensitive information that could identify a person’s routine, location or indented actions. Throwaway redditors also talk more about the “past”. However, their usage of “health” words is lower than posts from identified redditors. We conjecture this could indicate an effort to camouflage their ailment. It could also be because they are finding it difficult to come to terms with the reality that they are experiencing a direct or indirect psychological challenge. While the increased use of first person singular pronouns and lower use of second person pronouns in throwaway posts (similar to the findings in [10]) suggest that the users writing these posts are more self-focused and less interactive, the increased use of third person pronouns suggest they engage in greater discourse about others—likely about their friends and family, as we observed an increases use of “family” words. Social Attributes. Finally, significantly higher length of posts correlates with the throwaway users [16]. This is known to be a sign of verbal fluency and cognitive complexity, which tells us that they might be engaging in more candid discourse given the (semi)- anonymous identity. The comment responses to these throwaway posts are also tend to be lengthier which might be the “reciprocity effect” [22]—when one shares more information, the responders tend to exhibit support by writing lengthier comments. In summary, increased negative expression, raised cognitive bias and judgement issues, self-attentional focus, more candid discourse and lowered self-esteem in throwaway posts show that these posts bear distinctive behavioral markers in relation to stigmatic topics such as mental illness. Our findings thus are in the same vein as observed in our preliminary analysis in [10].",2,2,2,1,0,0,2,0,2,2,2,1,0,0,0,0,1,0,1,0
https://dl.acm.org/doi/abs/10.1145/2740908.2743049,20,9,,"De Choudhury, Pavalanathan",2,1,0,0,2015,"Our final research question investigates whether the use of throwaway accounts in mental health subreddits allows individuals to engage in sensitive and disinhibiting discourse about their health state and experiences. For this purpose, we look at frequently used uni-, bi-, and trigrams (top 500) in the throwaway posts as well as identified posts (Table 7, 8 list samples). We situate our findings in the light of content characterization around self-disclosure examined in prior work. We use a qualitative coding scheme to characterize n-grams from the throwaway and the identified accounts. In particular, we use a three-layer categorization scheme proposed by Altman and Taylor [1] to guide the content analysis of the depth of self-disclosure. Altman and Taylor suggest that disclosure can be categorized into either peripheral, intermediate, and core layers. The peripheral layer is concerned with disclosure around one’s biographic information (e.g. age), the intermediate layer with attitudes, values and opinions, and the core layer spans disclosure of one’s personal beliefs, needs, fears, and values. Aside from this characterization, Joinson [20] who characterized sensitive disclosure in terms of the extent of “revealed vulnerability”.",2,2,2,1,0,0,2,0,2,2,2,1,0,0,0,0,1,0,1,0
https://dl.acm.org/doi/abs/10.1145/2740908.2743049,20,10,,"De Choudhury, Pavalanathan",2,1,0,0,2015,"We find considerable selfdisclosure from the throwaway accounts along the intermediate and core layers. Corresponding to the intermedia layer of selfdisclosure, we find the posters expressing their attitudes and opinion, which generally bear a negative tone (“shitty”, “I don’t want”, “I’ve lost”, “don’t know what”, “no idea”, “the worst”): my dad would beat the living shit out of me, at times to an inch before death. i’ve been to the hospital so many times i’ve lost track. Further, several posts from the throwaway cohort hint at a sense of urgency or desire to act: (“just need to”, “to do something”, “am going to”): now i’m not crazy, i’m not a danger to any one, i just need to stay busy until i can see a new therapist in the next couple of days. Corresponding to the core layer of self-disclosure, we find throwaway posts extensively sharing posters’ personal beliefs and fear. This might reveal their vital constructs and private, sensitive informational attributes (“if I could”, “part of me”, “because I know”) [19]. The cohort also expresses a desire to avail help/need from the community (“want to talk”, “what do i”): i think about suicide at least once or twice a day but im not sure if i could go through with it. Next, many of the posts from the throwaway cohort indicate their vulnerability to physical, mental, or emotional harm with a selfloating tone; for instance, thoughts about killing themselves (“want to kill”), suicide (“want to die”) and hating themselves (“I hate myself”): i hate this. i hate myself. i don’t want to f****** be this person anymore. i’m unmotivated, unfocused [...] Finally, we also observe that throwaway posts share a great deal of information confessing posters feelings about an incident, an experience, or a thought, sometimes for self-clarification or personal expression (“what I want”, “don’t want to”, “I’ve lost”, “don’t know what”, “bring myself to”, “if I could”, “if I did”, “failed”): i know i should leave, but i can’t bring myself to because i want to be happy like before, before i found out about the drugs. so i’ve been debating ending it all. Another observation we draw is that some throwaway posts explicitly state the users’ purpose of using a throwaway account, likely to avoid the possibility of being identified by their social circles (e.g. “my girlfriend knows my regular account”; “friends know my main profile”). Some also state that they are embarrassed because of the personal nature of the information being shared (e.g. “as i am sad and embarrassed""; “i’m still not comfortable being open about taking antidepressant""). In addition, some posts mentioned that using throwaway was “obvious”(e.g. “clearly this is a throwaway”; “this is definitely a throwaway”). These observations are consistent with recent work on boundary management and adoption of temporary accounts on reddit [25].",2,2,2,1,0,0,2,0,2,2,2,1,0,0,0,0,1,0,1,0
https://dl.acm.org/doi/abs/10.1145/2740908.2743049,20,11,,"De Choudhury, Pavalanathan",2,1,0,0,2015,"redditors using their identified or regular accounts and engaging in mental health discourse, on the other hand, do not display as much intimate personal revelations, disparaging, or confessional thoughts corresponding to the intermediate or core layers of self-disclosure. Rather they focus on deriving constructive support from the reddit community. This is indicated in the usage of the frequent n-grams in Table 8 (“to help me”, “how can i”, “share”, “to find a”, “time to”): “so i really just want to find a different doctor. who isn’t rude. In fact redditors with regular/identified accounts are also found to reveal their attitudes with a positive tone in contrast to the throwaway users: “it’ll be a little more difficult to study, but i’m sure i’ll figure it out. i’ll also try to seek help [...], so i hope to never have to come here and bother you guys once more. "". We thus conclude that throwaways allows reddit users engaging in mental health discourse to exhibit considerable disinhibition. The content of their discourse is distinct from that of identified redditors, in that it relates to the core aspects of the self rather than mundane aspects of social relations typical on social media [19].",2,2,2,1,0,0,2,0,2,2,2,1,0,0,0,0,1,0,1,0
https://dl.acm.org/doi/abs/10.1145/2740908.2743049,20,12,,"De Choudhury, Pavalanathan",2,1,0,0,2015,"Theoretical and Practical Implications. The distinctive use of throwaway accounts in the mental health forums indicates that such identities are fulfilling a unique need. They are allowing expressing views and thoughts about a topic that is often considered to be sensitive or unacceptable to the mainstream [8]. Presence of almost six times more throwaway redditors in mental health communities in contrast to other subreddits in our dataset reveals the wide spread use of throwaways in forums where sensitive topics are being discussed. Note that while it is possible that certain other reddit communities of deviant behavior (e.g., pornography) may have similar or higher proportion of throwaways, however compared to communities of non-deviant behavior, we do see a larger presence of throwaway users in mental health subreddits. Broadly, our findings align with prior literature [15, 23, 2, 35], where anonymity has been found to be a desirable attribute in certain online communities. Our findings show that, in the context of mental health, semi-anonymity enabled by throwaways allowed greater disinhibition. Psychology literature indicates that such disinhibition (in the form of journaling and discourse) can be an effective healing process [4, 31]. In fact, increased self-awareness and present-orientedness, affective experiencing and cognitive processing we observe in our data, are known to be associated with candid, ingenuous and sensitive discourse [19]. Nevertheless, note that a causal claim cannot be derived from our findings. Our findings also bear implications for next generation social systems. The distinctive and candid discourse from throwaway redditors indicates that there is a need for social media designers to build appropriate recommendation, and support tools that can make such discourse fruitful for their psychological healing. We propose the following broad design considerations — (1) tools may be built to facilitate or encourage (semi)-anonymous participation as is enabled through throwaways for sensitive health topics; (2) community owners and moderators can enable better support mechanisms for such (semi)-anonymous participation; and (3) they can help direct in-time psychological services to help exceedingly vulnerable tendencies manifested in some throwaway postings, including selfderogatory and self-loathing thoughts and feelings of suicide. In summary, we note the positive benefits of candid discourse in psychological healing as enabled by the use of reddit throwaways. However, forms of identity around health discourse raise important questions about our ethical responsibilities in identifying particularly vulnerable populations. We hope this work triggers conversations and involvement with the ethics and clinician community to investigate opportunities and caution in this regard.",2,2,2,1,0,0,2,0,2,2,2,1,0,0,0,0,1,0,1,0
https://dl.acm.org/doi/abs/10.1145/2740908.2743049,20,13,,"De Choudhury, Pavalanathan",2,1,0,0,2015,"Importantly, our paper does not make any claim about attributing mental illness as a health concern to the posters of the reddit posts we study (per clinical DSM criteria). We also caution against drawing generalizations of this work. Our findings indicate the manner in which individuals might be adopting throwaway accounts on reddit to engage in discourse around a stigmatic condition like mental illness. However it is possible that the distinctive affective, stylistic, social, and cognitive attributes evident from our results are characteristic of reddit throwaways exclusively, and may not generalize to other social media that provision use of true anonymity (e.g., /b/ on 4chan). [11]. It will be fruitful to contrast reddit’s use for mental health against other online health forums, so as to explore what makes reddit a unique platform for these issues.",2,2,2,1,0,0,2,0,2,2,2,1,0,0,0,0,1,0,1,0
https://dl.acm.org/doi/abs/10.1145/2740908.2743049,20,14,,"De Choudhury, Pavalanathan",2,1,0,0,2015,"In this paper, we presented a study on identity choices that pervade mental health discourse on social media, specifically reddit. We investigated behavioral differences in terms of social, affective, cognitive, and linguistic style attributes. We observed statistically distinctive characteristics of throwaway reddit postings around mental health topics. Our findings further indicated that semi-anonymity, as enabled by throwaways, provided a candid and disinhibiting platform of discourse. We believe our work will provide new insights to improving online social systems focused on supporting mental health discourse.",2,2,2,1,0,0,2,0,2,2,2,1,0,0,0,0,1,0,1,0
https://link.springer.com/content/pdf/10.1007/s00127-018-1571-5.pdf,21,1,,"Robinson, Turk, Jilka, Cella",4,0,0,0,2018,"Background There are numerous campaigns targeting mental health stigma. However, evaluating how effective these are in changing perceptions is complex. Social media may be used to assess stigma levels and highlight new trends. This study uses a social media platform, Twitter, to investigate stigmatising and trivialising attitudes across a range of mental and physical health conditions. Methods Tweets (i.e. messages) associated with five mental and five physical health conditions were collected in ten 72-h windows over a 50-day period using automated software. A random selection of tweets per condition was considered for the analyses. Tweets were categorised according to their topic and presence of stigmatising and trivialising attitudes. Qualitative thematic analysis was performed on all stigmatising and trivialising tweets. Results A total of 1,059,258 tweets were collected, and from this sample 1300 tweets per condition were randomly selected for analysis. Overall, mental health conditions were found to be more stigmatised (12.9%) and trivialised (14.3%) compared to physical conditions (8.1 and 6.8%, respectively). Amongst mental health conditions the most stigmatised condition was schizophrenia (41%) while the most trivialised was obsessive compulsive disorder (33%). Conclusions Our findings show that mental health stigma is common on social media. Trivialisation is also common, suggesting that while society may be more open to discussing mental health problems, care should be taken to ensure this is done appropriately. This study further demonstrates the potential for social media to be used to measure the general public’s attitudes towards mental health conditions.",2,2,2,1,0,0,0,0,0,2,2,1,0,2,0,0,0,1,3,1
https://link.springer.com/content/pdf/10.1007/s00127-018-1571-5.pdf,21,2,,"Robinson, Turk, Jilka, Cella",4,0,0,0,2018,"Attitudes towards mental health are still not on equal terms with those towards physical health. Stigma is recognised as a significant barrier for the early diagnosis and treatment of various mental health conditions [1]. The World Health Organisation has highlighted the significant role of stigma in influencing mental health prognosis and has identified its reduction as a key target in its 2013–2020 action plan [2]. Stigma is thought to be more prevalent in illnesses perceived to have uncertain or complex aetiology [3]. This may partially explain why stigma towards mental health conditions is much higher than it is to physical health problems [4]. Research by Rüsch et al., supports the link between poor knowledge and stigma, and showed that increased mental health literacy is associated with a reduction in stigmatising attitudes [5]. Furthermore, poor understanding of mental health conditions has been shown to be associated with public fear and the perception that people experiencing mental health problems are dangerous [6]. One of the most significant repercussions of stigma is its effect on help-seeking behaviour. People with mental health problems are less likely to seek help if they feel their condition is stigmatised [7, 8]. Stigmatising attitudes also isolate sufferers and make many societal roles, such as finding a job, harder [9]. Trivialisation is a minimising behaviour where an illness is conceptualised as being easier to acquire, suffer with, or treat. It may also be perceived as a form of stigma and has consequences. Recent research suggests that trivialisation can arise when diagnoses are introduced into common use without education on their meaning, for example using the term OCD to describe a personal preference regarding the arrangement of their belongings (e.g. “I’m OCD about tidying my room”) [10]. This may devalue the experience of those suffering from a mental health condition [11, 12]. Studies exploring the role of trivialisation in society suggest this may reinforce social inequality and this phenomena is more prevalent in mental health conditions [10, 13, 14]. Stigma and trivialisation are not equally distributed across different mental health conditions. Research has demonstrated that schizophrenia is one of the most negatively viewed conditions due to its misperception of danger and unpredictability. Similarly, studies show that eating disorders and depression appear to be stigmatised due to a perception of greater personal controllability [15, 16]. Stigma is also common in portrayals of physical conditions, and differs between individual conditions. A study of primary care attendees found that HIV was stigmatised significantly more than diabetes and hypertension [17]. Another study comparing stigma in AIDS and cancer found that AIDS was significantly more stigmatised than cancer due to societal attitudes towards homosexuality and religion [18]. Previous studies have assessed stigma using media portrayals of mental illness. They were found to include disproportionately high levels of stigmatising references to dangerousness and violence, but these studies are limited by low response rates, a reliance on surveys and traditional media anchoring effects [19–22]. These issues have prompted the use of alternative approaches and attention has turned to social media [23]. Two recent studies have used Twitter to assess stigmatising attitudes towards mental and physical illness. Reavley and Pilkington found tweets containing #schizophrenia were significantly more stigmatised than those containing #depression. Joseph et al. showed that schizophrenia was more stigmatised than diabetes on social media [24, 25]. These studies were limited by short sampling periods and by sampling tweets containing only two specific hashtags. It is likely that searching larger phrases and key words may return views people express in conversations and this may be more representative of public opinion. A further limitation of both these studies is that only one mental and one physical health condition were compared, diminishing the opportunity to observe trends both within and between mental and physical health problems. In this study, we assess the prevalence of both stigmatising and trivialising attitudes in a great number of messages from a large social media platform (i.e. Twitter). We are aiming to contrast attitudes within different mental health conditions, as well as between different mental and physical health conditions. We also conduct a qualitative analysis on stigmatising and trivialising tweets to highlight specific content and trends.",2,2,2,1,0,0,0,0,0,2,2,1,0,2,0,0,0,1,3,1
https://link.springer.com/content/pdf/10.1007/s00127-018-1571-5.pdf,21,3,,"Robinson, Turk, Jilka, Cella",4,0,0,0,2018,"We compiled a comprehensive list (i.e. 50) of common physical and mental health conditions of varying aetiology, mode of transmission, time course, reversibility and system affected. Each term was searched using ‘Topsy’ (a real-time analytic tool for Twitter to collect tweets) over a 30-day period to ensure that each condition returned a sufficiently large number of tweets. Due to the way that our tweet-aggregation software worked, we were only able to search for conditions with single-word names or acronyms. Of the terms considered we selected the five mental and five physical health conditions with the most tweets with names that would allow us to collect tweets about them (i.e. these conditions all returned over 50,000 tweets and had single word/acronym search terms). These included schizophrenia, obsessive compulsive disorder, depression, autism, eating disorders, asthma, diabetes, HIV/AIDS, cancer and epilepsy. Search terms (see “Appendix 1”) were searched in the noun and adjectival forms of the word where possible, as described in Joseph et al. [25].",2,2,2,1,0,0,0,0,0,2,2,1,0,2,0,0,0,1,3,1
https://link.springer.com/content/pdf/10.1007/s00127-018-1571-5.pdf,21,4,,"Robinson, Turk, Jilka, Cella",4,0,0,0,2018,"Data were collected in ten 72-h-long blocks which were equally distributed throughout the 50 days between 9th December 2015 and 27th January 2016. We used the Twitter Archiver add-on for Google Sheets together with Twitter Advanced Search to automatically collect tweets containing the above terms every 15 minutes [26]. This includes references to the target conditions with or without hashtags in the tweet, ensuring a more representative sample of tweets compared to previous studies [24, 25]. By sampling every 15 min from Twitter’s application programming interface (API), we aimed to capture more than the 40% of the total tweets produced as calculated by Morstatter [27]. From this sample, we then selected a random subset of 1300 tweets per conditions to be used in the analysis. Random selection was performed with STATA ver.14.",2,2,2,1,0,0,0,0,0,2,2,1,0,2,0,0,0,1,3,1
https://link.springer.com/content/pdf/10.1007/s00127-018-1571-5.pdf,21,5,,"Robinson, Turk, Jilka, Cella",4,0,0,0,2018,"Of the tweets retrieved, some were excluded based on the following criteria: (a) part of or all the text was written in a language other than English; (b) the target word was used in a context other than that of the target medical condition (e.g. economic depression); (c) information in the tweet was limited (e.g. tweets consisting mainly of hashtags); (d) tweet content refers to the target condition in animals; (e) tweets containing the target word and pictures only. All remaining tweets were considered for analysis. The rating criteria for tweets were based on previous literature [24, 25] and refined using an iterative rating exercise on a set of 1000 random tweets between two authors (PR and DT). Raters initially created a set of definitions by examining recurring themes across tweets, then tested these on a different set of tweets and refined them in consultation with a third author (MC) until the inter-rater reliability exceeded 0.8. Tweets were also given a mutually exclusive general theme based on their purpose to identify trends in the type of tweet. We added all definitions to a coding manual (see “Appendix 3”).",2,2,2,1,0,0,0,0,0,2,2,1,0,2,0,0,0,1,3,1
https://link.springer.com/content/pdf/10.1007/s00127-018-1571-5.pdf,21,6,,"Robinson, Turk, Jilka, Cella",4,0,0,0,2018,Descriptive statistics (i.e. frequency counts) were initially used to estimate the prevalence of stigmatising and trivialising tweets per condition. A Chi-square test was used to test the difference in the proportion of stigmatising attitudes between categories. All analyses were conducted using SPSS version 22.,2,2,2,1,0,0,0,0,0,2,2,1,0,2,0,0,0,1,3,1
https://link.springer.com/content/pdf/10.1007/s00127-018-1571-5.pdf,21,7,,"Robinson, Turk, Jilka, Cella",4,0,0,0,2018,"Qualitative analysis, using a content analysis framework, was performed to add further detail on how stigmatisation and trivialisation occur on social media [28]. All tweets were coded and labelled independently by two authors (PR and DT). Tweets coded as stigmatising or trivialising were further coded in sub-categories in relation to language and emotional valence.",2,2,2,1,0,0,0,0,0,2,2,1,0,2,0,0,0,1,3,1
https://link.springer.com/content/pdf/10.1007/s00127-018-1571-5.pdf,21,8,,"Robinson, Turk, Jilka, Cella",4,0,0,0,2018,"The prevalence of stigmatising and trivialising tweets for each condition is presented in Fig. 1. The total number of tweets collected is detailed by condition in Fig. 2. Of the 1300 tweets per condition, there was some variation in the number of tweets that we could consider based on our exclusion criteria. The average stigma prevalence for the five physical health conditions considered was 8.1% whilst for the five mental health conditions was 12.9%. Analysis comparing the prevalence between categories showed that mental health conditions were 1.54 times more likely to be stigmatised, χ2 (1, N=9909)=53.95, p<.001. The prevalence of trivialisation in physical health conditions was 6.8% while in mental health conditions was 14.3%. Trivialisation was 2.10 times more prevalent in mental illness than in physical illness, χ2 (1, N=9909)=146.40, p<.001, (see Fig. 1). Both the most stigmatised (schizophrenia) and trivialised (OCD) conditions were mental health conditions. A Chi-square test performed to examine the proportion tweets by theme (see Fig. 3) showed that stigmatising tweets were more likely to occur in “Opinion” tweets, χ2 (5, N=9911)=1234.33, p<.001.",2,2,2,1,0,0,0,0,0,2,2,1,0,2,0,0,0,1,3,1
https://link.springer.com/content/pdf/10.1007/s00127-018-1571-5.pdf,21,9,,"Robinson, Turk, Jilka, Cella",4,0,0,0,2018,"Trivialisation and stigma were present across all conditions, but differed slightly in their presentation within and between physical and mental health conditions. The emerging categories across the trivialising tweets were: (A) trivialising acquisition; (B) trivialising suffering; (C) minimising recovery difficulty (‘snap out of it’ in a previous study); (D) mockery (with a negative humour element) and (E) glamourising, or using the illness as a compliment [24]. Within the mockery theme we noticed a higher prevalence of tweets associated with benefit in mental health conditions. For stigmatising tweets we identified the following subthemes: (A) negative descriptor (using the illness to describe something in a negative light); (B) wishing illness upon someone (wishing harm upon someone by way of contracting the target condition); (C) negative characteristics (associating the illness with undesirable attributes); (D) joking (demeaning the target condition by joking about it) and (E) stereotyping (associating the illness with grossly inaccurate stereotypes). Examples of tweets by category can be found in “Appendix 2”.",2,2,2,1,0,0,0,0,0,2,2,1,0,2,0,0,0,1,3,1
https://link.springer.com/content/pdf/10.1007/s00127-018-1571-5.pdf,21,10,,"Robinson, Turk, Jilka, Cella",4,0,0,0,2018,"The main aim of this study was to quantify the prevalence of stigmatising and trivialising attitudes across physical and mental health conditions on social media. The results show that mental health conditions were subject to more stigmatising and trivialising attitudes than physical health, but there was a large variation in prevalence between conditions. Our findings that schizophrenia and HIV were the most stigmatised conditions is consistent with much of the previous research on this topic using different methods [15, 17, 25]. Both conditions share a perception of being dangerous—HIV/AIDS as a highly infectious and poorly understood disease, and schizophrenia perceived as unpredictable and difficult to control. Many tweets used ‘psychotic’ as an insult, and this is likely due to a deeply entrenched culture of negativity surrounding schizophrenia reinforced by media stereotypes. The observed trends of stigma generally reflect those seen within the literature but the prevalence appears to be greater compared to previous studies [24]. Whilst this could suggest that our definition of stigma is more sensitive, this study was the first to consider whole tweets rather than just hashtags, which may have uncovered stigmatising attitudes that had not previously been assessed. Other strengths of this study, compared to previous research in this field, were the large population size, comprehensive tweet collection and the random sampling method, which ensured a representative sample of tweets for each target condition. By using automated software, we could retrieve a larger number of tweets compared to all previous studies in this area, and we also considered a wider range of target conditions. Our methodology also allowed us to build on previous research and use qualitative analysis to compare tweet themes between mental and physical illnesses. This showed that mental health conditions were more likely to be discussed through opinion rather than factual discourse and tweets in the opinion theme were more likely to be stigmatising, while physical health conditions were more likely to be discussed via informative tweets (see Fig. 3). We think this is notable as it reaffirms the idea that stigma is often driven by (misinformed) opinion, and concerted campaigns to increase the informative content in discussions of mental illness on Twitter could form the basis for future stigmareduction strategies. Although this study improves on previous research, there were still several limitations. The rating process meant there was an inherent degree of subjectivity due to differences in the perceived context and emotional tone of some tweets, and the inability to follow links and embedded pictures. This was made particularly evident by words that had dual meanings (e.g. cancer, depression). There was also a degree of selection bias as stigmatising and trivialising tweets were more likely to be lacking in context and/or grammatical correctness, rendering them less likely to be considered for analysis. We minimised the impact of these issues through our robust rating criteria and repeated inter-rater reliability testing. A binary rating system was chosen as it allowed us to rate a larger number of tweets but it may have obscured important differences in the mechanisms by which conditions are stigmatised and trivialised. Due to the information available via the API, we were unable to control for potential confounding variables such as demographic characteristics. The limitations of this study provide several opportunities for refinements in any future studies. These include a non-binary approach to rating stigma tweets, evaluating retweets (perhaps as a proxy of endorsement) and analysis of the profile that generate the tweet (e.g. activity and number of followers). It can be difficult to infer context from tweets. Systematic incorrect inference can lead to either overestimation or underestimation of stigma and trivialisation prevalence. Studies in this area should consider carefully how tweets are rated. The difficulty can be illustrated by the following two examples. The tweet: ‘I can; seizure salad’ contains little context and it is not possible to determine whether the user is trivialising the condition, or has simply misspelt Caesar. While the adjective depressed is often used to infer low mood and the sufferance of clinical depression, some tweets were ambiguous (e.g. “I have a deep love for depressed comedians”) and therefore had to be excluded. From 140 characters or less it can be difficult to unambiguously infer meaning. We have shown that stigma and trivialisation are highly prevalent on social media and that, as an ever-greater proportion of social interaction takes place online, proactive campaigns should consider assessing and addressing both on social media platforms. We believe our study can contribute to develop the knowledge necessary to build computer algorithms capable of detecting stigma on social media and give us the opportunity to target anti-stigma campaigns to those who may benefit from it most. This is the same logic used by commercial advertising where product advertising is targeted to potential consumer preference inferred by the way they use social media. Targeting anti-stigma campaigns to individuals’ profile may prove useful to educate and change attitudes towards mental health conditions.",2,2,2,1,0,0,0,0,0,2,2,1,0,2,0,0,0,1,3,1
https://arxiv.org/abs/2108.00279,22,1,,"Bucur, Podina, Dinu",3,0,0,0,2021,"In this work, we provide an extensive part-ofspeech analysis of the discourse of social media users with depression. Research in psychology revealed that depressed users tend to be self-focused, more preoccupied with themselves and ruminate more about their lives and emotions. Our work aims to make use of largescale datasets and computational methods for a quantitative exploration of discourse. We use the publicly available depression dataset from the Early Risk Prediction on the Internet Workshop (eRisk) 2018 and extract partof-speech features and several indices based on them. Our results reveal statistically significant differences between the depressed and non-depressed individuals confirming findings from the existing psychology literature. Our work provides insights regarding the way in which depressed individuals are expressing themselves on social media platforms, allowing for better-informed computational models to help monitor and prevent mental illnesses.",2,2,1,1,0,0,2,0,2,2,2,0,0,1,0,0,0,2,3,0
https://arxiv.org/abs/2108.00279,22,2,,"Bucur, Podina, Dinu",3,0,0,0,2021,"Mental health disorders are a common problem in our world. Currently, mental health issues are on the rise: there is a 13% increase in the past decade according to World Health Organization (WHO)1 , with depression being at the forefront. Many mental illnesses remain undiagnosed due to social stigma, leading people to live 1 in 5 years of disability in their lifetime. With the rise of social media websites, interdisciplinary researchers in natural language processing, psychology and network analysis have turned their attention to automatically detect and monitor mental health manifestations through users’ individual activity on social platforms (e.g. Facebook, Twitter, Reddit). The research is primarily focused on analyzing users’ texts from posts and comments and determining, through computational linguistics models, the risk for various mental conditions - self-harm, depression, addictions etc. Research is fulled through curated datasets (Yates et al., 2017; Losada and Crestani, 2016; Amir et al., 2019) with texts from primarily Reddit and Twitter. At the forefront of incentivising interdisciplinary research on monitoring mental health on social media are workshops such as the Early risk prediction on the Internet (eRisk) Workshop2 and the Workshop on Computational Linguistics and Clinical Psychology (CLPsych)3 . CLPsych and eRisk are two significant initiatives focusing on the interdisciplinary research area between computational linguistics and psychology. The eRisk Workshop, from the Conference and Labs of the Evaluation Forum (CLEF), focuses on the technologies that can be used for early risk detection of different pathologies or safety threats (Losada et al.). In five years of existence, the workshop addressed multiple mental health problems: pathological gambling, depression, self-harm and anorexia. The CLPsych Workshop was co-located with several international conferences on natural languages processing, the last edition was co-located with the Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL). Throughout the seven editions of this workshop, it hosted shared tasks on depression, post-traumatic stress disorder (PTSD) (Coppersmith et al., 2015b), labeling crisis posts from the peer-support forum ReachOut4 (Milne et al., 2016), predicting current and future psychological health from childhood essays (Lynn et al., 2018), the degree of suicide risk (no risk, low, moderate, or severe risk) (Zirikly et al., 2019) and suicide risk prediction from real data donated through OurDataHelps5 (Goharian et al., 2021). In the present study, we perform a part-of-speech analysis and contribute to the understanding of the differences in social media discourse between depressed and non-depressed individuals. We focus on the differences in part-of-speech use and ground them in the existing literature from psychology researchers. We aim to answer the following two research questions: RQ1: Are there significant differences in part-ofspeech use between individuals with self-reported depression diagnosis and control? RQ2: Can these part-of-speech features be used alone to differentiate individuals with depression from controls?",2,2,1,1,0,0,2,0,2,2,2,0,0,1,0,0,0,2,3,0
https://arxiv.org/abs/2108.00279,22,3,,"Bucur, Podina, Dinu",3,0,0,0,2021,"Detecting the manifestations of mental health disorders from social media is an interdisciplinary research problem for researchers from psychology, natural language processing and network analysis. The two main approaches used to detect cues of depression from online users are: extracting linguistic features for a quantitative analysis or using automated models for classification. The differences in language between depressed and non-depressed individuals focus on greater use of negative words, the personal pronoun ”I” (Rude et al., 2004), more words with negative polarity, frequent dichotomous expressions (e.g. always, never) (Fekete, 2002), cues of rumination (reflected in greater use of past tense verbs) (Smirnova et al., 2018) in texts from depressed individuals. Most linguistic features are extracted using the Linguistic inquiry and word count (LIWC) lexicon (Pennebaker et al., 2001). LIWC provide a list of dictionary words for more than seventy categories: part-of-speech (e.g. personal pronouns, first-person personal pronouns, nouns, present/future/past verbs, adjectives), psychological processes (e.g. social, affective, cognitive processes), personal concerns (e.g. work, money, religion, death), etc. It is used to detect cues of depression (Loveys et al., 2018; Nalabandian and Ireland, 2019; Eichstaedt et al., 2018), neuroticism (Resnik et al., 2013), to explore the language of suicide poets (Stirman and Pennebaker, 2001), etc. Other approaches to mental illnesses detection from text rely on character and word n-grams (Coppersmith et al., 2015a; Pedersen, 2015) or topic modelling techniques (Preot¸iuc-Pietro et al., 2015; Bucur and Dinu, 2020). Recent studies analyzing the online discourse of social media users with depression have focused on other particularities of language, such as offensive language. Birnbaum et al. (2020) found that depressed individuals use more swear words in their Facebook messages compared to controls. Bucur et al. (2021b) apply offensive language identification techniques and show that users with depression diagnosis use more offensive language in their Reddit posts, individuals manifesting signs of depression in their posts having a more profane language and fewer insults targeted towards other individuals or groups. Computational models used to detect the cues of depression from social media texts rely on traditional machine learning classifiers (e.g. SVM, Na¨ıve Bayes) (De Choudhury et al., 2013; Aldarwish and Ahmad, 2017; Tadesse et al., 2019), CNNs (Orabi et al., 2018; Yates et al., 2017), RNNs (Orabi et al., 2018) or transformer models (Mart´ınez-Castano et al. ˜ , 2020; Uban and Rosso, 2020; Bucur et al., 2021a). In the multimodal framework (involving text, voice and visual cues), the use of syntactic features (e.g. pronouns, adverbs usage) seems to improve the performance in depression detection, further emphasizing the relationship between linguistic features and depression (Morales et al., 2018). Recently, researchers began exploring the interpretability and explainability of the decisions made by automatic classifiers for mental illnesses detection from social media to further understand the manifestations of different mental disorders in written language (Uban et al., 2021b,a)",2,2,1,1,0,0,2,0,2,2,2,0,0,1,0,0,0,2,3,0
https://arxiv.org/abs/2108.00279,22,4,,"Bucur, Podina, Dinu",3,0,0,0,2021,"In our experiments, we use the dataset from the eRisk workshop containing posts written in English from the social media platform Reddit. The eRisk 2018 dataset (Losada and Crestani, 2016) was created for the early detection of depression task. It contains two classes of users, depression and control. Users from the depression class were annotated by their mention of diagnosis in their posts (e.g. ”I was diagnosed with depression”), but expressions such as ”I have depression”, ”I am depressed” were not taken into account. The authors removed the mentions of diagnosis from the dataset. Users from the control group are random individuals who do not have any mention of diagnosis in their post, including those active in the depression subreddit6 . The training dataset provided by the organizers contains 135 depressed users and 752 controls, while the test dataset contains 79 depressed users and 741 controls. We use both train and test splits in our exploration, consisting of a total of approximately 90,000 submissions from users with a depression diagnosis and over 985,000 in the control group.",2,2,1,1,0,0,2,0,2,2,2,0,0,1,0,0,0,2,3,0
https://arxiv.org/abs/2108.00279,22,5,,"Bucur, Podina, Dinu",3,0,0,0,2021,"Part-of-Speech Analysis For each post in our dataset, we use the spaCy7 part-of-speech tagger to extract the corresponding tags and also morphological features (e.g. person and number for pronouns) for each word. We extract the universal POS tags8 and the ones from The Penn Treebank tagset (Taylor et al., 2003). We use the latter to explore the differences in the verb tenses. We assign the tenses according to their tags: VBD and VBN corresponding to present tense, VBG, VBZ and VBP corresponding to present tense, and MD tag before a VB corresponds to the future tense (Caragea et al., 2018). From the morphological features provided by the spaCy tagger, we extract the person and the number for all the pronouns. After this analysis, we use the following features in our exploration: Universal Part-of-Speech: ADJ, ADV, NOUN, PROPN, VERB, ADP, CCONJ, DET, PART, SCONJ, AUX, PRON Verb tenses: Past, present, future Person of pronouns: First, second and thirdperson Pronoun number: Singular and plural, only for the first-person For each of these features, we compute their frequency for each post in the dataset. For the universal POS, the frequency is computed as the number of occurrences of a specific tag normalized by the total number of tags in a post. For verb tenses, the frequency of each tense is calculated as a percentage of the total number of verb occurrences For the three kinds of personal pronouns, each frequency is computed as a percentage of the number of all personal pronouns. The frequency of singular and plural first-person pronouns is computed as a percentage of all first-person pronouns. To further explore the part-of-speech usage in the social media dataset, we also use some special measurements (Havigerova et al. ´ , 2019): Pronominalisation Index (PI): reflecting the usage of pronouns, instead of another part-of-speech (e.g. nouns). It is computed as the number of pronouns divided by the number of nouns (Litvinova et al., 2016). Formality Metric (Mairesse et al., 2007): F = NOUN+ADJ+P REP +ART −P RON−V ERB−ADV −INT J+100 2 Moreover, we test the discriminatory power of both POS tags frequency usage in users’ texts and the specific computed indices. For this, we employ a Random Forest classifier on the train set of the eRisk 2018 dataset on the aforementioned features. To interpret the trained model and to estimate the importance of each feature, we employ SHapley Additive exPlanations (SHAP) (Lundberg and Lee, 2017) to measure each feature’s contribution to the classifier decision. SHAP offers a game-theoretic approach to quantify feature importance, aligned with human intuition. Classification We opted for a simple Random Forest model, trained with 50 estimators and a max depth of 15, to avoid overfitting, with balanced class weights, since the dataset is heavily imbalanced. On the test set for the eRisk 2018 dataset, we obtain a weighted F1-score of 78.37% (with balanced class weights) and a macro F1-score of 51.93%. While the classification task is difficult, we are interested in exploring the feature importances of the model, which shed light on the model behaviour and provide us with insights regarding which POS tag is most discriminatory. We further present our findings and provide interpretations and discussions based on recent findings in psychology.",2,2,1,1,0,0,2,0,2,2,2,0,0,1,0,0,0,2,3,0
https://arxiv.org/abs/2108.00279,22,6,,"Bucur, Podina, Dinu",3,0,0,0,2021,"Addressing our RQ1, we compute the Welch t-test for all our features and demonstrate that there are statistically significant differences (pvalue <0.001) in part-of-speech usage between depressed and non-depressed individuals. In this Content Part-of-speech In Figure 1, we present the usage of content words for the two classes from the eRisk 2018 dataset. Individuals diagnosed with depression tend to use fewer common and proper nouns in comparison with control users. They also use more verbs and adverbs in their posts. The discourse is focused around actions, but with fewer entities (e.g. nouns), showing a defective linguistic structure with less interest in the environment (e.g. people, objects) (De Choudhury et al., 2016). To further understand these differences, we pay a closer look at the frequencies of nouns and verbs in the social media discourse. We compute the keyness score (Kilgarriff, 2009; Gabrielatos, 2018) for verbs and nouns separately. In the keyness analysis, we compare the frequencies of nouns and verbs from the posts written by individuals with depression diagnosis (target corpus) in comparison to the posts from control users (reference corpus). In Figure 2, we present the top 20 verbs and nouns from each corpus, ordered by their log-likelihood ratio (G2 ) (Dunning, 1993). Rumination is a cognitive process focusing on past and present negative content and resulting in emotional distress (Sansone and Sansone, 2012). It is present in several mental health disorders (e.g. depression, anxiety, obsessive-compulsive disorder, post-traumatic stress disorder). In depression, rumination is defined as behaviours and thoughts that focus one’s attention on one’s depressive symptoms and on the implications of these symptoms (NolenHoeksema, 1991). The rumination, as a response to depression, focuses the person’s attention on their emotional state and inhibits the actions necessary to distract them from their mood. In Figure 3, when comparing the usage of the three verb tenses (present, past and future) between the two groups, we expected that signs of rumination would be present in our analysis of verb tenses, with texts from depressed users being shifted into the past (Smirnova et al., 2018), but this result is not found in this sample of individuals. anhedonia, people suffering from depression reporting lower anticipatory pleasure, and thus talking online less about their future plans. Anhedonia, defined as markedly diminished interest or pleasure in all, or almost all activities most of the day, nearly every day (Association et al., 2013), is a common symptom of depression. The higher frequency of cognitive verbs (e.g. feel, think, know) in the texts written by depressed individuals indicates the cognitive impairments and judgement issues specific to depression. People with depression have cognitive deficits and biases in the processing of emotional information and they are unable to adaptively regulate their emotions (De Choudhury and De, 2014). Individuals with or without depression may not differ in their initial response to an adverse event. Still, they differ in their ability to recover once they have experienced the negative emotion. Depressed individuals are not able to repair their mood. Instead, they remain in a negative state of mind, which is related to increased negative thoughts, selective attention to negative stimuli and greater accessibility of negative recollections (Joormann, 2010). In comparison, the individuals from the control group use more action verbs (e.g. vote, lead, show, begin, create. In addition, depressed individuals are more passive; they have a lower level of general activity, consistent with symptoms of depressive disorder (Hopko et al., 2003). Being an online social media platform similar to forums, Reddit is organized in subreddits with specific topics. It also has several communities dedicated to mental health problems. Compared to other social media platforms that require realname authentication (e.g. Facebook), Reddit affords users anonymity or pseudo-anonymity. Complete anonymity is almost impossible, users providing bits of information with every interaction on the platform (e.g. comments, posts). Reddit allows users to create throwaway accounts to engage temporary without revealing their identity (Kilgo et al., 2018). These types of accounts are used to discuss sensitive information or stigmatizing problems. De Choudhury and De (2014) study the mental health discourse on Reddit and show that its communities allow a high degree of information exchange related to mental health. Users use Reddit to self-disclose the challenges faced in their daily lives or in personal relationships. They also seek emotional support or specific information about mental illnesses diagnosis and treatment. Their study demonstrates that Reddit fills the gap between social media platforms like Twitter and Facebook and online health forums regarding mental health discourse. Examining the frequencies of nouns in the eRisk 2018 dataset, we show in Figure 2 that the users with self-reported depression diagnosis use their Reddit account to disclose and discuss their mental health problems (e.g. depression, anxiety, therapist) or their personal relationships (e.g. friend, boyfriend, relationship, mom, dad). The process of seeking online support is also shown in the frequency of verbs: feel, talk, diagnose, help. Even if the dataset contains control users active in the depression subreddit, the majority of control users seem to post on other themed subreddits (e.g. politics). Bucur and Dinu (2020) perform a topic modelling analysis and show that texts from control users are found in topics related to their hobbies, as opposed to depressed people, who are more focused on their feelings and life events. Our results are in line with this study, the users from the control group use more politics-related words (e.g. trump, government, president, news, vote).",2,2,1,1,0,0,2,0,2,2,2,0,0,1,0,0,0,2,3,0
https://arxiv.org/abs/2108.00279,22,7,,"Bucur, Podina, Dinu",3,0,0,0,2021,"Functional part-of-speech In Figure 4, we present the frequencies of functional part-of-speech for depressed and control users. Depressed individuals generally use fewer functional words in their texts in comparison with control users, with the exception of pronouns. Neurons involved in content words processing are equally distributed over both hemispheres, while function words are processed mainly in the left hemisphere (Pulvermuller ¨ et al., 1995). Lower preposition usage may be an outcome of deficient activation of the left hemisphere regions, responsible for producing more abstract lexical units (Litvinova et al., 2016). Function words are highly social, the capacity to use function words requires social skills (Pennebaker, 2017). The Figure 5 shows the differences in personal pronouns used by the two groups. The high frequency of first-person singular pronouns indicates a higher self-preoccupation in depressed individuals, as opposed to the first-person plural pronouns, which shows collective attention. Second and thirdperson pronouns indicate social interactivity and contain references to other people or things in the environment (De Choudhury et al., 2016). Depressed users use more first-person singular pronouns. The frequencies of first-person plural pronouns are inversely proportional to the firstperson singular pronouns frequencies. This result is in line with the self-focused attention tendency (SFA) in depressed individuals. SFA is a cognitive bias linked to depression; the high frequency of first-person singular pronouns in speech or written text is considered a linguistic marker of SFA (Brockmeyer et al., 2015). Individuals with depression have deficits in other-focused social cognitions, they are impaired in Theory of Mind reasoning and empathy. Theory of Mind enables people to make inferences on the behaviour of others and their own (Premack and Woodruff, 1978). Erle et al. (2019) show that individuals exhibiting high levels of depressive symptoms were impaired on tasks involving overcoming their egocentrism. The usage of fewer first-person plural pronouns in texts from users diagnosed with depression may be a sign of a lesser sense of belonging. The information-processing biases of depressed individuals make it hard for them to perceive cues of acceptance and belonging in social interactions, and to view ambiguous social interactions as being negative (Steger and Kashdan, 2009). Pronominalisation and Formality Indices In Figure 6 we present the results for the two metrics computed on part-of-speech tags. Depressed individuals have a higher pronominalisation index, using more pronouns in spite of nouns. This finding is also found in the language of people with self-destructive behaviour, insufficient activation of the cerebellum being associated with suicidal behaviour (Litvinova et al., 2016). Depressive individuals also use less formal language. They have a more contextualized discourse on social media, providing information about the context in order to avoid ambiguity (Heylighen and Dewaele, 2002). Addressing our RQ2, we show in Figure 7 the Shapley values for a random sample of 1500 posts from the eRisk 2018 test set. A higher Shapley value corresponds with a higher importance in the final model decision based on the feature value. We used all computed POS features and indices in our model, but show only the top 20 for clarity. It is evident that from the summary plot, the absence of proper nouns is the most discriminatory factor in the decision to classify a person as depressive. Moreover, the use of pronouns (also evident in the Pronominalisation Index) is highly correlated with positive model output. The high usage of first-person singular pronouns and low usage of first-person plural pronouns confirms both our findings in the exploratory analysis and psychology literature.",2,2,1,1,0,0,2,0,2,2,2,0,0,1,0,0,0,2,3,0
https://arxiv.org/abs/2108.00279,22,8,,"Bucur, Podina, Dinu",3,0,0,0,2021,"In this work, we provide an extensive analysis of part-of-speech usage in social media texts from depressed and non-depressed users. Our findings are confirmed by studies in psychology and show that individuals diagnosed with depression use more pronouns (especially first-person singular pronouns) and verbs, and fewer common and proper nouns. Their social media discourse revolves around their life experiences and sentiments, as opposed to control users who are not interested in discussing their problems online. Moreover, we also provided insights into the discriminatory power of POS frequencies by employing SHAP, a game-theoretic approach for model interpretation. Through this, we showed that depressive users can be characterized most easily, primarily through their usage of pronouns and proper nouns.",2,2,1,1,0,0,2,0,2,2,2,0,0,1,0,0,0,2,3,0
https://aclanthology.org/2021.louhi-1.10.pdf,23,1,,"Kulkarni, Hengle, Kulkarni, Marathe",4,0,0,0,2021,"With mental health as a problem domain in NLP, the bulk of contemporary literature revolves around building better mental illness prediction models. The research focusing on the identification of discussion clusters in online mental health communities has been relatively limited. Moreover, as the underlying methodologies used in these studies mainly conform to the traditional machine learning models and statistical methods, the scope for introducing contextualized word representations for topic and theme extraction from online mental health communities remains open. Thus, in this research, we propose topic-infused deep contextualized representations, a novel data representation technique that uses autoencoders to combine deep contextual embeddings with topical information, generating robust representations for text clustering. Investigating the Reddit discourse on Post-Traumatic Stress Disorder (PTSD) and Complex Post-Traumatic Stress Disorder (CPTSD), we elicit the thematic clusters representing the latent topics and themes discussed in the r/ptsd and r/CPTSD subreddits. Furthermore, we also present a qualitative analysis and characterization of each cluster, unraveling the prevalent discourse themes.",2,2,0,2,0,0,0,0,0,2,2,2,0,2,0,2,2,2,8,0
https://aclanthology.org/2021.louhi-1.10.pdf,23,2,,"Kulkarni, Hengle, Kulkarni, Marathe",4,0,0,0,2021,"Due to their ubiquitous nature, online health communities and social media platforms have emerged as a conducive means of information exchange and social support, especially for people with stigmatized concerns such as mental health. Consequently, these platforms provide a rich ecosystem for mental health clinicians, researchers, and practitioners to analyze the cornucopia of user-generated content and study the underlying mechanisms of different mental health conditions. With the rapid headways in artificial intelligence and computational linguistics, an increasingly large number of researchers have leveraged social media content to study various mental health illnesses and psychiatric conditions. While most of the research has focused on using the traditional machine learning models and statistical methods for predicting mental illness from social media posts, the studies addressing the discourse analysis (De Choudhury and De, 2014; Silveira Fraga et al., 2018; Loveys et al., 2018) and identification of clusters in online mental health communities (Park et al., 2018) has been relatively modest. Even with the recent surge of complex attention-based deep learning models, a large chunk of the research regarding mental health issues has focused on building better predictive systems (Benton et al., 2017; Kirinde Gamaarachchige and Inkpen, 2019; Jiang et al., 2020; Sekulic and Strube, 2019) with less emphasis on using these models for mental health related corpus analysis or information extraction. With mental health already coalesced as an appreciable public health burden, research to investigate the discourse clusters prevalent on social media is of paramount importance. Mining information from the emergent clusters provides a lens over the dominant themes of discussion, the discourse anatomy, and the dialogue structure in the online forums while also helping to comprehend the general public engagement, sentiment, ideas, and views regarding mental health. Successful research in this direction can potentially foster identification of high-risk groups, enhanced mental health patient education programs, better diagnostic and therapeutic theory building, as well as an improved understanding of the underlying design of the online mental health communities (Park et al., 2018). Post-traumatic stress disorder (PTSD) is a mental disorder resulting from traumatic experiences that leads to reliving the trauma, avoidance of certain situations, and hyper-vigilance. Similar to PTSD, complex post-traumatic stress disorder (C-PTSD) is a condition that formulates the reaction resulted from the trauma, such as uncontrollable emotions, dissociation, negative self-perception, anger, mistrust, and interpersonal difficulties. Thus, in this research, we examine the online discourse on Reddit, focussing on PTSD and C-PTSD as use cases to elicit different thematic clusters present in them. Prior research has shown that the addition of topic information to pre-trained contextualized representations yields performance improvement for semantic textual similarity (Peinelt et al., 2020). While Peinelt et al. (2020) integrated the two representations by simple concatenation, a better methodology to integrate representations from different embedding spaces is argued by Bollegala and Bao (2018). The meta-embeddings proposed by Bollegala and Bao (2018) are learned as the intermediate representations generated by various autoencoder variants. Thus, building on these two findings, we propose topic-infused deep contextualized representations, a novel data representation technique that uses a concatenated denoise autoencoder to combine deep contextual embeddings with topic information for generating robust document representations. Our methodology spawns document representations that subsume the topic information from Latent Dirichlet Allocation (Blei et al., 2003) with the contextual embeddings generated by pre-trained RoBERTa model(Liu et al., 2019). We further demonstrate that the proposed methodology achieves improvement for text clustering against the contextual embeddings generated by the pretrained RoBERTa model. In the light of the above discussion, our research makes the following contributions: • We extend the methodology of word metaembeddings to document meta-embeddings by proposing topic-infused deep contextualized representations, a data representation technique that uses a concatenated denoise autoencoder to combine deep contextual embeddings with topical information for generating robust representations for text clustering. • We carry out a qualitative analysis and characterization of each cluster from a clinical psychology perspective.",2,2,0,2,0,0,0,0,0,2,2,2,0,2,0,2,2,2,8,0
https://aclanthology.org/2021.louhi-1.10.pdf,23,3,,"Kulkarni, Hengle, Kulkarni, Marathe",4,0,0,0,2021,"Traditionally, the research in topic mining and theme extraction from online mental health communities has been focused on the use of probabilistic generative models like the Latent Dirichlet Allocation (Blei et al., 2003) and clustering techniques such as the k-means (Schutze et al. ¨ , 2008). CarronArthur et al. (2016) employed LDA to extract topics from the internet support group BlueBoard. Results showed that users engaged in discussions with a greater topical focus on experiential knowledge, disclosure, and informational support, a pattern resembling the clinical symptom-focused approach to recovery. In their study, Dao et al. (2017) used the Hierarchical Dirichlet Process (HDP) algorithm to infer latent topics from blog posts of the LiveJournal (LJ) blogging site. The authors applied the non-parametric affinity propagation algorithm to find clusters within the online communities. Toulis and Golab (2017) compared the recurring themes encountered in private journals with the ones found in the online communities of Reddit and found significant similarities in the topics discussed across both the forums. Park et al. (2018) provide an exhaustive analysis of the thematic overlap, similarity, and difference in online mental health communities of r/depression, r/anxiety, and r/ptsd. Their results show a considerable overlap of themes between the mental health groups, attesting that people engaging in such forums face common problems and comorbidity symptoms. Since their introduction, transformer-based language models such as BERT (Devlin et al., 2019) have led to impressive performance gains across multiple NLP tasks. Recent works show that these contextualized representations can also support type-level clusters, and hence, can be effectively used for modeling topics (Sia et al., 2020). The recent work by Thompson and Mimno (2020) demonstrates that running simple clustering algorithms like k-means on contextualized word representations result in word clusters that share similar properties to the ones generated by LDA. An interesting approach is followed by Peinelt et al. (2020), where the authors combine the topic models of LDA with contextualized word representations of BERT for the task of semantic similarity detection. Results depict that adding topical information improves performance, especially for examples with domain specific terms.",2,2,0,2,0,0,0,0,0,2,2,2,0,2,0,2,2,2,8,0
https://aclanthology.org/2021.louhi-1.10.pdf,23,4,,"Kulkarni, Hengle, Kulkarni, Marathe",4,0,0,0,2021,"For this experiment, we selected the r/ptsd1 and r/CPTSD 2 subreddits which have 54,000 and 97,000 active users, respectively. Using the Pushift API 3 , we crawled all the available posts from these subreddits between 1st August 2015 and 31st July 2020. To ensure that each selected post has community approval, we selected posts that have a minimum of 10 net upvotes. We further filter our dataset by eliminating posts with less than 75% English content 4 , posts with less than five words, as well as posts with [DELETED], [UPDATED], and [REMOVED] entries. We employed standard text cleaning and normalization techniques for preprocessing the posts, including removing special characters, accented words, wordplays, URLs, replacing acronyms with full forms, and expanding contractions5 . This resulted in a comprehensive dataset of 24,930 posts. The dataset statistics are provided in Table 1.",2,2,0,2,0,0,0,0,0,2,2,2,0,2,0,2,2,2,8,0
https://aclanthology.org/2021.louhi-1.10.pdf,23,5,,"Kulkarni, Hengle, Kulkarni, Marathe",4,0,0,0,2021,The proposed system consists of two key components: a robust data representation methodology and an efficient clustering algorithm. Figure 1 depicts the model architecture. Each component is elucidated in detail as follows:,2,2,0,2,0,0,0,0,0,2,2,2,0,2,0,2,2,2,8,0
https://aclanthology.org/2021.louhi-1.10.pdf,23,6,,"Kulkarni, Hengle, Kulkarni, Marathe",4,0,0,0,2021,"In this section, we posit the methodology to generate the topic-infused contextualized representations, using a multi-input concatenated denoise autoencoder. The proposed autoencoder architecture has three inputs namely: the document topic distribution of the post’s selftext 6 , contextualized document embedding of the post’s selftext, and the contextualized document embedding of the post’s title. Let S1, S2, and S3 denote the corresponding three input embedding spaces of dimensions d1, d2, and d3, respectively. Let N be the total number of posts. For each post p ∈ N, the three document embeddings are given by s1(p) ∈ R d1 , s2(p) ∈ R d2 , and s3(p) ∈ R d3 . The autoencoder model consists of three encoders E1, E2, and E3 which encode the source embeddings to a common meta-embedding space M of dimensionality dm. Each encoder independently performs dimensionality reduction and non-linear transformations on the respective embeddings, thus, learning to retain essential information from each source embedding. Dimensionalities of the encoded input embeddings are denoted respectively, by d 0 1 , d 0 2 , and d 0 3 . The concatenation of each of the encoded input source embeddings results in the document meta-embedding m(p), as given by Equation 1. Therefore, the dimensionality of document metaembedding space M is computed as the sum of dimensionalities of encoded source embeddings. It is given by Equation 2. The three decoders D1, D2, and D3, try to reconstruct the individual source embeddings from the document meta-embeddings, thereby implicitly utilizing the common and the complementary information present in the source embeddings. Equations 3, 4, and 5 represent the reconstructed versions of the source embeddings, given by sˆ1(p), sˆ2(p), and sˆ3(p). The objective loss L is calculated as the sum of the reconstruction loss for each of the three input embeddings. It is formulated in Equation 6. Thus, the proposed system jointly learns E1, E2, E3, and D1, D2, D3, such that the loss given by Equation 6 is minimized.",2,2,0,2,0,0,0,0,0,2,2,2,0,2,0,2,2,2,8,0
https://aclanthology.org/2021.louhi-1.10.pdf,23,7,,"Kulkarni, Hengle, Kulkarni, Marathe",4,0,0,0,2021,"In this study, we make use of HDBSCAN ( Hierarchical Density-Based Spatial Clustering of Applications with Noise), a hierarchical densitybased unsupervised clustering technique to congregate semantically-similar posts together in clusters (McInnes et al., 2017). Unlike partition-based clustering algorithms, HDBSCAN can find varying density clusters and is more robust to parameter selection. Moreover, HDBSCAN does not force the data points to belong to any particular cluster, making it suitable for handling outliers and noisy data. As HDBSCAN uses relative-distance measures for clustering, it often suffers from the curse of dimensionality (McInnes et al., 2017). As the dimension of the topic-infused contextualized embeddings is quite high (dm = 768), we employ UMAP (Uniform Manifold Approximation Projection), a technique for general non-linear dimensionality reduction (McInnes et al., 2018). UMAP is preferred over other dimensionality reduction algorithms as it keeps a significant portion of the high-dimensional local structure in lower dimensionality, thus, causing minimal information loss.",2,2,0,2,0,0,0,0,0,2,2,2,0,2,0,2,2,2,8,0
https://aclanthology.org/2021.louhi-1.10.pdf,23,8,,"Kulkarni, Hengle, Kulkarni, Marathe",4,0,0,0,2021,"The document topic distributions are generated using the LDA mallet python version 7 . As the topics of interest centers around entities that are mostly nouns, we follow a nouns-only approach as employed by Martin and Johnson (2015) for topic modeling. The number of topics is empirically chosen as 10 since it displayed the best topic coherence score (c v) of 0.49. The rest of the hyperparameters for LDA are kept at default. The 768-dimensional contextualized document embeddings are generated as the average of all the embeddings for each word in the document, extracted from the second last layer of the pre-trained RoBERTa-base model (Liu et al., 2019). Thus, the three input d1, d2, and d3 are of dimensions 10, 768, and 768, respectively. In our experiments, each autoencoder is implemented as a single hidden layer neural network. The dimensions d 0 1 , d 0 2 , and d 0 3 are chosen as 10, 379, and 379, respectively. The hidden dimensions are chosen as such so that the topic-infused deep contextualized representations are of dimensions 768, making them comparable with that of RoBERTa. Masking noise of 10 percent is applied to the source embeddings before encoding (Vincent et al., 2010). Leaky rectified linear (Leaky ReLU) (Maas et al., 2013) activation is applied to each layer with the default parameter setting. The model is trained end-to-end for 200 epochs, with the Adam optimizer (Kingma and Ba, 2015), a learning rate of 0.001, and a mini-batch size of 128 for minimizing the objective loss. The learning rate is reduced by a factor of 0.1 if validation loss does not decline after 10 successive epochs. The model with the best loss is used for prediction.",2,2,0,2,0,0,0,0,0,2,2,2,0,2,0,2,2,2,8,0
https://aclanthology.org/2021.louhi-1.10.pdf,23,9,,"Kulkarni, Hengle, Kulkarni, Marathe",4,0,0,0,2021,"In order to assess the clustering performance, we make use of three measures, namely the Silhouette Coefficient (SC) (Rousseeuw, 1987), the CalinskiHarabasz Index (CHI) (Calinski and Harabasz ´ , 1974), and the Davies-Bouldin Index (DBI) (Davies and Bouldin, 1979). Higher SC, and CHI, and lower DBI scores indicate a better separation of the clusters and tighter integration inside the clusters. We compare the clustering performance of three embedding sets, namely: the RoBERTa last layer embeddings, the RoBERTa second last layer embeddings, and our proposed topic-infused deep contextualized embeddings. Figures 3, 4, and 5 depict the performance comparison of the corresponding three embeddings with respect to the three metrics mentioned above. The y-axis represents the three corresponding metrics, whereas the x-axis represents the UMAP component variations. The hyperparameter values of the number of neighbours and minimum distance were chosen as 30 and 0.0, respectively, while cosine similarity was used as the metric for computing the distance in the ambient space of the input data. The random state is seeded to an integer value (42). For HDB-SCAN, the minimum cluster size was selected as 300, with the other parameters set to the default values (McInnes et al., 2017). From the comparative analysis, it is evident that the proposed topicinfused deep contextualized representations result in an improved clustering performance across all three metrics. For our study, we choose UMAP with components 10, as it empirically gives consistent performance across all three metrics. The 2D embedding space after clustering is shown in Figure 2. To reduce the cluster size, we extract the most representative posts or exemplars of each cluster 8 . In technical terms, exemplars are the data points that lie at the heart of a cluster, around which the ultimate cluster forms. Table 2 provides the statistics of the clusters and their respective exemplar sizes. To find the key-words for each cluster, we employ a class-based term-frequency inversedocument-frequency (c-TF-IDF)9 method on each of the obtained exemplars. Unlike the traditional TF-IDF, which considers each document of a corpus, c-TF-IDF is a class-based method that treats all the documents belonging to a particular class as a single document. This enables us to find only the latent topics most representative of a particular cluster and penalize the frequent words across the clusters. For each cluster, the c-TF-IDF score is calculated using the Equation 7, where each word t is extracted for each class i, and the number of documents m is divided by the total frequency of the word t across all classes n. The coherence scores for the topics generated by LDA and the clusters generated by the abovementioned embeddings are reported in Table 3. The top 15 nouns with the highest c-TF-IDF scores for each cluster are used to evaluate the topic coherence. The overall coherence of the topics generated is evaluated using extrinsic as well as intrinsic measures. As the intrinsic topic coherence is computed using word co-occurrences in documents from the corpus (Mimno et al., 2011), it is natural that LDA reports the highest intrinsic coherence (c v), as it is based on word-occurrences statistics. Moreover, as argued by Stevens et al. (2012), a good intrinsic coherence score does not necessarily guarantee that the generated topics make semantic sense or that they are interpretable by humans. Normalized pointwise mutual information (NPMI) (Bouma, 2009), on the other hand, has been shown to correlated with human judgement (Lau et al., 2014). Thus, in this study, we use NPMI as an extrinsic measure. Experiments by Sia et al. (2020) suggest that clustering contextual embeddings can result in topics with better NPMI compared to LDA. As evident from the results reported in Table 3, our results corroborate these findings, as our proposed methodology exhibits the best NPMI score. Figure 6 depicts the wordclouds of the top 50 nouns with the highest c-TF-IDF scores for each cluster.",2,2,0,2,0,0,0,0,0,2,2,2,0,2,0,2,2,2,8,0
https://aclanthology.org/2021.louhi-1.10.pdf,23,10,,"Kulkarni, Hengle, Kulkarni, Marathe",4,0,0,0,2021,"For a comprehensive qualitative evaluation, we select the top 100 posts from each clusters which exhibit the highest Jaccard Index scores 10 with that of the top 15 nouns with the highest c-TF-IDF scores for each cluster. The qualitative analysis is carried out by the subject expert in clinical psychology to draw out the major discussion themes. Total 7 themes were extracted from the 9 clusters generated. They are as follows: Abuse stories: Cluster 3 reveals self-disclosure stories of abuse that are physical, emotional, or sexual in nature. Abusers are mainly parents, siblings, close family members, co-workers, and known people. Narrations prominently highlight dysfunctional family dynamics during the victim’s childhood as well as the issues related to current family interactions. Family dynamics are characterized by alcoholic/ substance abusive parents and abusers with pathological personality traits. Posts indicate traumatic experiences, the effect of these experiences, and how they act as triggers. The victim’s exposure to the trauma was long and frequent. Flashbacks: The primary themes of cluster 6 include flashbacks of traumatic incidences and the emotional disturbances associated with them. Feelings of anxiety, panic, nightmares, fear, impulsivity, and anger are prevalent throughout the cluster. Narrations reveal the way flashbacks are impeding current life issues. Examples include an aversion to touch and sexual experiences, difficulties in romantic relationships, social relationships, daily chores, and work as well as triggering health-related symptoms. Working through flashbacks, ways of dealing with it, and help to overcome it are also shared. Advice seeking: People have vividly expressed their feelings and are seeking advice about various issues related to PTSD, according to the posts in cluster 8. Main themes revolve around advice related to job functioning impacted by PTSD symptoms, social isolation, dependency issues, management of difficult emotions, exhaustion, and frustration. Posts in this cluster are brief and direct, seeking advice, help, and support from the Reddit community. Therapy and therapist experiences: Cluster 1 and 4 mainly focuses on therapy and therapist experiences. Posts highlight the process of therapy, experiences with therapists, challenges faced in the therapeutic process, insights from therapies, transference experiences, and seeking help for such issues. Other findings include posts that seek therapeutic methods other than visiting therapists. Surprisingly, a large chunk of the posts portrays a negative connotation about therapy experiences or therapy. Difficulty in emotional regulation: Posts in cluster 5 primarily focus on different emotions associated with trauma. Posts suggest difficulty in controlling emotions, emotional neglect, panic, anxiety, feeling of emptiness, hopelessness, emotional distancing due to trauma, difficulty understanding and expressing emotions, and ambivalent emotions towards abusers. Other findings include the progress in trauma-related emotions. Most of the posts are help-seeking in nature, for validating their emotions. Working through traumatic abuse: Post in cluster 2 narrates retrospective abusive, traumatic experiences and their psychological aftereffects. The majority of the posts indicate signs of shame, dissociation, social withdrawal, anxiety, self-downing, and victimization. Confrontation, triumph while struggling with vulnerabilities, hope, and new insights about oneself are other findings. Abuse issues and general PTSD: Clusters 7 and 9 mostly contains posts about abuse stories and PTSD in general. The major themes are related to the parent-child issues, insecure parenting styles, emotional and physical abuse, the effect of childhood trauma, loss and grieving for not having healthy parental relations, the impact of insecure parenting, disclosure of trauma incidences, and sharing recovery tips. Following are a few illustrations:",2,2,0,2,0,0,0,0,0,2,2,2,0,2,0,2,2,2,8,0
https://aclanthology.org/2021.louhi-1.10.pdf,23,11,,"Kulkarni, Hengle, Kulkarni, Marathe",4,0,0,0,2021,"Cluster structure is in accordance with the formal diagnostic criteria of PTSD and C-PTSD. Abuse stories reflected in cluster 1 corresponds to the criterion-A of PTSD in DSM-5 (Association et al., 2013), that states the experience of trauma. Out of the six symptom cluster proffered by ICD-1111 for CPTSD (re-experiencing, avoidance, hypervigilance, emotional dysregulation, interpersonal difficulties, and negative self-concept), cluster 6 adequately corresponds to re-experiencing of flashbacks, while cluster 5 is consistent with emotional dysregulation. Parent-child relationships portrayed in cluster 2 corresponds to etiological factors. Dysfunctional parent-child relation in PTSD has been widely confirmed in the clinical psychology literature (Cockram et al., 2010; Cross et al., 2018; van Ee et al., 2016). On the other hand, clusters 5, 6, and 7 characterizing therapy experience, working through trauma, and advice, respectively, attribute to the treatments and interventions aspect of PTSD. In conclusion, it can be said that these clusters reveal salient features of PTSD. Although the clusters more or less convey their discrete independent themes, there exist many common topics running throughout the clusters. The recurrent expressions observed throughout the clusters pertain to personal and social life (family, parents, friends, person, relationships), daily grind (work, home, job, place), and temporal indicators (day, time, year, month, today, week). Furthermore, words encompassing cognition (thoughts, think, feel, feeling, lot, hard), emotional and affect expressions (happy, love, good, anxiety, kind, bad, hate), and inhibition expressions (avoid, deny, escape) are perpetual throughout the posts.",2,2,0,2,0,0,0,0,0,2,2,2,0,2,0,2,2,2,8,0
https://aclanthology.org/2021.louhi-1.10.pdf,23,12,,"Kulkarni, Hengle, Kulkarni, Marathe",4,0,0,0,2021,"Our work has its own drawbacks and limitations. The problem statement at hand is of fine-grained clustering rather than coarse-grained clustering, thus, making it difficult to draw out interpretable topics of discussion. This is attested by the low NPMI scores in Table 3. Furthermore, the posts’ lengthy nature (refer Table 1) makes it difficult for models like RoBERTa to capture maximum contextual information (Liu et al., 2019). Many psychiatric disorders are found to be co-morbid with PTSD (Brady et al., 2000). The research by Park and Conway (2018) indicates that people facing mental health issues often find it difficult to regulate their ideas and views. Their research further entails that posts from online mental health communities are more difficult to read and portray less lexical diversity. These factors further hinder the elicitation of interpretable topics from the corpus. Also, not all clusters exhibit independent themes of discussions and some themes are a combination of multiple clusters.",2,2,0,2,0,0,0,0,0,2,2,2,0,2,0,2,2,2,8,0
https://aclanthology.org/2021.louhi-1.10.pdf,23,13,,"Kulkarni, Hengle, Kulkarni, Marathe",4,0,0,0,2021,"In this research, we introduce topic-infused deep contextualized representations, a robust data representation methodology that successfully integrates topical information with contextualized embeddings. We collect and analyze large-scale data pertaining to the online discourse on Reddit, centered around PTSD and CPTSD, and perform density based clustering to draw out the prominent clusters and analyze the themes of discussion present in them. Despite the perpetual semantic and thematic similarities amongst the posts in the corpus, our methodology, to some extent, is able to draw out the underlying, fine-grained, latent clusters. The qualitative analysis of each cluster revealed the characteristic themes and salient features of PTSD and CPTSD, consistent with the clinical psychology literature. Through the lens of social media, this study delineates a deeper understanding of PTSD and C-PTSD, fostering further research in early detection of mental illnesses, identification of highrisk groups, enhanced mental health patient education programs, better diagnostic and therapeutic theory building, as well as an improved understanding of the underlying design of the online mental health communities. The future work of this research can take multiple directions. From an NLP standpoint, the robustness of the proposed methodology should be examined by testing them on various other benchmark NLP tasks such as semantic textual similarity, word analogy, and text classification, to name a few. Other variants of autoencoders and objective losses could be employed to facilitate tighter integration of topical information with the contextualized embeddings. From the mental health and clinical psychology perspective, such research can be easily extended to other online mental health communities to draw useful insights.",2,2,0,2,0,0,0,0,0,2,2,2,0,2,0,2,2,2,8,0
https://www.jmir.org/2018/4/e121,24,1,,"Park, Conway",2,0,0,0,2018,"Mental disorders such as depression, bipolar disorder, and schizophrenia are common, incapacitating, and have the potential to be fatal. Despite the prevalence and gravity of mental disorders, our knowledge concerning everyday challenges associated with them is relatively limited. One of the most studied deficits related to everyday challenges is language impairment, yet we do not know how mental disorders can impact common forms of written communication, for example, social media. The aims of this study were to investigate written communication challenges manifest in online mental health communities focusing on depression, bipolar disorder, and schizophrenia, as well as the impact of participating in these online mental health communities on written communication. As the control, we selected three online health communities focusing on positive emotion, exercising, and weight management. We examined lexical diversity and readability, both important features for measuring the quality of writing. We used four well-established readability metrics that consider word frequencies and syntactic complexity to measure writers’ written communication ability. We then measured the lexical diversity by calculating the percentage of unique words in posts. To compare lexical diversity and readability among communities, we first applied pairwise independent sample t tests, followed by P value adjustments using the prespecified Hommel procedure to adjust for multiple comparison. To measure the changes, we applied linear least squares regression to the readability and lexical diversity scores against the interaction sequence for each member, followed by pairwise independent sample t tests and P value adjustments. Given the large sample of members, we also report effect sizes and 95% CIs for the pairwise comparisons. On average, members of depression, bipolar disorder, and schizophrenia communities showed indications of difficulty expressing their ideas compared with three other online health communities. Our results also suggest that participating in these platforms has the potential to improve members’ written communication. For example, members of all three mental health communities showed statistically significant improvement in both lexical diversity and readability compared with members of the OHC focusing on positive emotion. We provide new insights into the written communication challenges faced by individuals suffering from depression, bipolar disorder, and schizophrenia. A comparison with three other online health communities suggests that written communication in mental health communities is significantly more difficult to read, while also consisting of a significantly less diverse lexicon. We contribute practical suggestions for utilizing our findings in Web-based communication settings to enhance members’ communicative experience. We consider these findings to be an important step toward understanding and addressing everyday written communication challenges among individuals suffering from mental disorders.",2,2,2,1,0,0,2,2,4,1,2,2,2,2,0,0,0,2,6,1
https://www.jmir.org/2018/4/e121,24,2,,"Park, Conway",2,0,0,0,2018,"Mental disorders are common, incapacitating and account for many years of lost productivity In addition, serious mental disorders such as depression bipolar disorder and schizophrenia have the potential to be fatal because of the increased risk of suicide. Despite the prevalence and gravity of mental disorders, our knowledge concerning everyday challenges associated with these conditions is relatively limited, especially when compared with many physical conditions. One of the most studied deficits related to everyday challenges for individuals suffering from depression, bipolar disorder, and schizophrenia is language impairment Researchers of these mental disorders have long suspected language impairment because of deficits in frontal lobe functioning which controls both emotion regulation and language processing. Language impairment is typically measured through one’s performance in semantic processing tasks (ie, determining semantic relationships between a word, phrase, or category or differentiating real words from pseudowords based on an individual’s semantic network and verbal fluency tasks (ie, production of words from phonemic or semantic categories Despite the importance of language in everyday life, these studies do not illustrate daily challenges associated with language impairment. Moreover, generalizability remains uncertain because of small sample size with inconsistent results regarding language impairment or frontal lobe activities Despite its potential for devastating disability, it is unclear how language impairment manifests in common forms of written communication, for example, social media communication. With increasing use of technology comes increasing opportunity to write. For instance, in 2015, 84% of American adults used the internet and one of the most frequent uses of the internet is written communication including communication on social media. Nearly two-thirds of American adults use social media, roughly a tenfold increase from a decade ago A few social media platforms and online mental health communities within Reddit, for example, have become a popular venue for individuals suffering from mental disorders Reddit supports throwaway and unidentifiable accounts, which can protect users from social discrimination surrounding mental disorders and allow honest discussions that may not be appropriate on other social media sites such as Facebook Reddit also provides contextual information that is relatively limited in other popular social media platforms (eg, Twitter), because of length limitations. It is also known that effortful tasks (ie, requiring attention) such as expressing thoughts via writing are more difficult than automatic tasks (ie, not requiring attention) for individuals suffering from depression and bipolar disorder, whereas both types of tasks are equally difficult for schizophrenia patients From previous studies on mental disorders and Reddit we can infer that individuals suffering from mental disorders also frequently engage in written communication, yet the written communication challenges faced by individuals in online mental health communities remain unknown. Examining important features of writing provides an opportunity to assess members’ written communication skills and any associated linguistic challenges. For example, a study on writing quality used linguistic features such as lexical diversity, syntactic complexity, and word frequency to predict the quality of writing In different studies, ease of reading (ie, simple and clear writing) and text cohesion with respect to text flow were suggested as some of the most determinant features of writing quality. We can examine these features to assess online mental health community members’ written communication challenges. More specifically, less lexical diversity and poor readability in posts can be a sign of language impairment. Research on language impairment has linked significantly less lexical diversity with specific language impairments Similarly, poor sentence structure and difficulties with organization and articulating ideas, which can be described as insufficient readability were also associated with language impairment Readability metrics have been long-studied or used in the field of communication education and informatics including social media writing Readability metrics provide quantitative estimates of the ease with which readers can comprehend a written text. Typically, they are given as an estimated US grade level by measuring the linguistic characteristics of a given text Moreover, readability metrics, although rudimentary, consider two of the three aforementioned features associated with writing quality: word frequencies and syntactic complexity From the perspectives of the writers and their writing quality, readability metrics can measure the writers’ ability to present ideas simply in a straightforward manner. According to one of the developers of the readability metrics, higher readability scores can indicate needless complexity or writing challenges, such as organization and articulating ideas. Language impairment can hinder writers’ ability to simply articulate ideas with ease, while using a less diverse lexicon. Moreover, one benefit of using these readability metrics is that they are computationally simple and relatively straightforward to apply. Thus, we use readability along with lexical diversity (ie, the third writing quality feature) of posts as a proxy for written communication challenges among individuals suffering from depression, bipolar disorder, and schizophrenia. Though mental health and language impairment have been studied extensively less is known about written communication challenges manifested in social media, as well as the effects of long-term participation in online mental health communities on written communication challenges among individuals suffering from depression, bipolar disorder, and schizophrenia disorder. Understanding written communication challenges among these individuals has implications for treating mental disorders, managing online mental health communities, and conducting future research. Despite the importance in clinical, practical, and public policy implications for mental health, to our knowledge, the investigation of written communication challenges utilizing communication in online mental health communities has not been the focus of previous research on mental health. We aim to fill this gap in the literature with this study and address two research questions (RQ): RQ1: To what extent do written communication challenges manifest in online mental health communities focusing on depression, bipolar disorder, and schizophrenia? As the control, we selected three online health communities (OHCs): one with less emotional challenges and two with less medical or technical terminology. RQ2: How would acts of participation (ie, posting to interact with other members) in online mental health communities impact members’ written communication?",2,2,2,1,0,0,2,2,4,1,2,2,2,2,0,0,0,2,6,1
https://www.jmir.org/2018/4/e121,24,3,,"Park, Conway",2,0,0,0,2018,"The data for this study consist of submissions and their associated comments from Reddit’s several topically focused subcommunities called subreddits. Submissions are posts that start a conversation, and comments are posts that reply to submissions or other comments. Reddit is a highly popular social media platform with more than 82.5 billion page views, 73 million submissions, and 725 million associated comments from 88,700 active subreddits in 2015 In addition to Reddit’s popularity, Reddit has features suitable for protecting mental health community members’ identity (eg, throwaway and unidentifiable accounts). Thus, we examined submissions and comments (posts from here on out to maintain clarity) from Reddit to investigate written communication challenges among individuals suffering from potentially stigmatized conditions. r/depression, r/bipolar, and r/schizophrenia, to our knowledge, are the largest and most active subreddits for their respective mental disorders In May 2017, r/depression has been active for 8 years with 178,921 subscribers r/bipolar has 24,724 subscribers and was formed 8 years ago and r/schizophrenia has 7036 subscribers and has been active for 7 years Thus, we selected r/depression, r/bipolar, and r/schizophrenia as the main communities of interest for investigating the written communication challenges faced by individuals in online mental health communities. To understand the significance of written communication among r/depression, r/bipolar, and r/schizophrenia members, we selected r/happy r/loseit and r/bodybuilding for the controls. We first selected r/happy, a subreddit that was created to share positive thoughts and happy stories. The subreddit has been active for 9 years with 116,441 subscribers as of May 2017 . Members of most OHCs experience emotional challenges from the distress of living with—or being diagnosed with—a serious condition. However, we looked for an OHC that is not directly related to mental disorders, especially depression, to help ensure that this control group’s written communication challenges are not related to mental distress even as a secondary symptom. Thus, we selected the largest and most active, positive, emotion-focused subreddit in Reddit. We selected a second OHC, r/loseit, to bolster the quality of our findings. r/loseit is a subreddit focusing on weight management and has been a community for 6 years with 425,934 subscribers We purposely selected a community without a substantial amount of medical or technical terminology because a high level of difficult medical or technical terminology can skew the readability of posts. Although it may be impossible to select OHCs without any medical or technical terminology, one study of r/loseit characterized the most-discussed topics of the community as ordinary health information and management strategies, which can be described without complex medical or technical terminology (eg, food, clothing, physical appearance, workouts, and calorie counting) Moreover, unlike r/happy, r/loseit contains a substantial amount of emotional support which can indicate that the members are facing emotional challenges similar to many OHCs. Thus, we selected r/loseit, the largest weight management community in Reddit, as a second control group. We selected a third OHC, r/bodybuilding, in which members are dedicated to passion-centric activities, exercising, and muscular development. The bodybuilding community has 259,743 subscribers and has been active for 9 years A previous study suggested that members of an online bodybuilding community exchange a considerable level of emotional support (eg, motivational support and competition preparation support) and informational support (eg, training regimes and diets) . The general discussion topics among bodybuilding community members could be relatively similar to the discussion topics among members of r/loseit; however, the two communities could consist of vastly different individuals with respect to health-related goals and habits. Thus, we include r/bodybuilding, the largest and most active muscular development community in Reddit, as the last control group.",2,2,2,1,0,0,2,2,4,1,2,2,2,2,0,0,0,2,6,1
https://www.jmir.org/2018/4/e121,24,4,,"Park, Conway",2,0,0,0,2018,"First, we used a dataset (publicly available posts from October 2007 to May 2015) that was collected and archived by a Reddit member and has been used in several previous studies Second, we extracted posts made in r/depression, r/bipolar, r/schizophrenia, r/happy, r/loseit, and r/bodybuilding. We excluded posts that were marked as [deleted] in our analyses. Third, we removed posts with less than five words to help ensure the posts have expressive content and thoughts. Many posts in online communities are short—for example, one-word answering posts (eg, “yes” and “sure”) that can be viewed as automatic tasks rather than effortful tasks. These posts can skew the results; thus, we removed posts with less than five words. Fourth, to restrict our investigation to regular members (ie, exclude throwaway accounts or infrequent members) of the communities, we confined our analysis to members (ie, unique member IDs) who have four or more meaningful posts (ie, posts with five or more words) in the specific subreddit. In a different study a similar threshold was used to determine lurkers who are not yet regularly contributing members. We used a similar threshold to identify regular members. We summarize the OHC dataset in table 1. The research reported in this study was exempted from review by the University of Utah’s institutional review board (IRB; ethics committee; IRB 00076188) under Exemption 2 as defined in US Federal Regulations 45 CFR 46.101(b).",2,2,2,1,0,0,2,2,4,1,2,2,2,2,0,0,0,2,6,1
https://www.jmir.org/2018/4/e121,24,5,,"Park, Conway",2,0,0,0,2018,"To understand how language impairment manifests in written communication, we first measure the readability of posts. Readability of posts assesses writers’ ability to simply and clearly present ideas. To assess readability, we used Flesch-Kincaid grade level Simple Measure of Gobbledygook (SMOG) index Gunning Fog index and Linsear Write formula all of which are widely used metrics in readability studies . Even though readability metrics have been shown to correlate with one another different readability metrics can still generate a range of results. To increase the reliability of our results, we calculated the mean of the four readability metrics, following the procedures of previous studies Additionally, we used min-max normalization in our analyses to give equal weight to each readability metric (readability score from here on out to maintain clarity); however, we also report the complete readability results by each readability metric and the mean before the normalization. To automatically perform the readability analysis, we used the open-source Python textstat package To calculate the mean of readability scores for each subreddit, we first calculated the mean of readability scores for individual members, then we calculated the mean for each subreddit. Next, we normalized the mean of readability scores for individual members based on minimum and maximum values of the specific communities. This two-step process is to prevent one prolific member skewing the mean of a subreddit. We then measured the lexical diversity by calculating the percent of unique words in posts (ie, the number of unique words divided by the number of total words) with the same two-step process, excluding the normalization process. To compare readability scores and lexical diversity among different subreddits, we first conducted pairwise independent sample t tests, followed by P value adjustments using the prespecified Hommel procedure to adjust for multiple comparisons. Given the large sample of members, we also reported effect sizes (d) using Cohen d , as well as 95% CI for the pairwise comparisons, following suggestions of a previous study The effect sizes were interpreted as d (.01)=very small, d (.2)=small, d (.5)=medium, d (.8)=large, d (1.2)=very large, and d (2.0)=huge . We used the open-source R lsr package to measure the effect size To bolster our findings, we manually examined the validity of using readability scores for the purpose of measuring communication challenges. Because high readability scores can also indicate sophisticated language with complex sentence structure, we manually analyzed a randomly selected sample of 120 posts (ie, 20 posts from each subreddit) after controlling for the post lengths and readability scores: 60 posts with high readability scores (ie, top 5% readability scores of a respective subreddit) and 60 posts with low readability scores (ie, bottom 5% readability scores of a respective subreddit). Furthermore, we manually assigned these posts into high and low readability groups to compare readability scores against manual judgments.",2,2,2,1,0,0,2,2,4,1,2,2,2,2,0,0,0,2,6,1
https://www.jmir.org/2018/4/e121,24,6,,"Park, Conway",2,0,0,0,2018,"To measure the change of readability and lexical diversity of posts made by each member participating in the six subreddits, we first calculated the readability scores and lexical diversity of individual posts. Then, we organized each post’s readability score and lexical diversity according to the posting time per-member basis for the subreddit. Next, we applied linear least squares regression to them against the interaction sequence (ie, determined by the posting time) for each member. We performed linear least squares regression against the interaction sequence rather than time because we are interested in the change caused by each interaction rather than time. We reported the mean of slopes for readability scores and lexical diversity to reflect the overall changes in members in each of the six subreddits. Next, we applied pairwise independent sample t tests and the Hommel procedure. We then reported effect sizes and 95% CIs as we did in RQ1. For both analyses, we also reported a comparison among r/happy, r/loseit, and r/bodybuilding to deepen our understanding of the effects of emotional challenges in language impairment.",2,2,2,1,0,0,2,2,4,1,2,2,2,2,0,0,0,2,6,1
https://www.jmir.org/2018/4/e121,24,7,,"Park, Conway",2,0,0,0,2018,"We captured the mean and SE for (1) individual readability scores measured by four different metrics, (2) mean readability scores of the four metrics, (3) normalized mean readability scores of the four metrics, (4) lexical diversity, and (5) the total number of words in posts for each of the five communities (Table 2). On average, posts from r/schizophrenia were found to be the most difficult to read (ie, highest normalized readability scores), followed by posts from r/bipolar, r/depression, r/loseit, r/happy, and then r/bodybuilding. Lexical diversity showed a similar trend. On average, posts from r/happy had the most diverse lexicon, followed by posts from r/bodybuilding, r/loseit, r/bipolar, r/schizophrenia, and then r/depression. Figure 1 presents a scatter plot of the mean readability scores and lexical diversity among six different subreddits. We then conducted pairwise independent sample t tests to compare readability scores and lexical diversity of each subreddit to understand the differences between two subreddits. Pairwise comparisons of normalized readability scores among subreddits are shown in Table 3. Posts from r/bodybuilding, r/happy, and r/loseit were statistically significantly more simply written than posts from r/depression, r/bipolar, and r/schizophrenia in terms of syntactic complexity and word frequency that were measured in readability. The effect sizes were also in between medium to huge when readability scores of r/happy, r/loseit, and r/bodybuilding were compared to readability scores of r/depression, r/bipolar, and r/schizophrenia. Table 3 summarizes these findings. Pairwise comparisons of lexical diversity showed similar results (Table 4). Posts from r/happy and r/bodybuilding used a significantly more diverse lexicon than the posts from r/depression, r/bipolar, and r/schizophrenia. The effect sizes ranged between very large to huge. Posts from r/loseit also had a significantly more diverse lexicon and had medium to large effect sizes than the posts from the three mental health subreddits. Differences in lexical diversity among posts from the three mental health subreddits had very small to small effect sizes. The lexical diversity differences between posts from r/bipolar and r/depression, as well as between r/schizophrenia and r/depression were statistically significant; however, posts from r/bipolar and r/schizophrenia were not significantly different. Interestingly, a significant difference with large to very large effect size of lexical diversity was found between the posts from r/happy and r/loseit as well. Table 4 summarizes findings on lexical diversity differences. In our manual analyses, we found that both high and low readability score posts resembled common internet communication and was void of sophisticated writing. However, we encountered several inadequately articulated posts, many in the form of run-on sentence structure. Using inadequate articulation as a guide, we manually assigned 120 posts into high or low readability groups. The manual assessment agreed with the readability score 68% of the time (82 out of 120). The readability score and manual assessment had higher agreement in posts from mental health subreddits compared with the control groups. Mental health subreddits, r/depression, r/bipolar, and r/schizophrenia, had 80%, 80%, and 90% agreement, respectively. Conversely, the control subreddits, r/happy, r/loseit, and r/bodybuilding, had 40%, 70%, and 50% agreement, respectively.",2,2,2,1,0,0,2,2,4,1,2,2,2,2,0,0,0,2,6,1
https://www.jmir.org/2018/4/e121,24,8,,"Park, Conway",2,0,0,0,2018,"To understand the effects of participating in online mental health communities with respect to their written communication, we applied linear least squares regression to readability scores and lexical diversity against the interaction sequence. Members of the three mental health subreddits showed improvement in both readability scores (ie, negative slope for improvement) and lexical diversity (ie, positive slope for improvement). Among the mental health subreddits, r/bipolar showed the most improvement, followed by r/depression and r/schizophrenia for readability scores. For lexical diversity, members improved in order of r/bipolar, r/schizophrenia, and then r/depression. Members of r/bodybuilding had the biggest improvement in readability scores, and members of r/loseit also improved in both readability scores and lexical diversity. Members of r/happy only improved in lexical diversity (Table 5). To understand the significance of the changes in readability scores and lexical diversity, we compared the changes that occurred in the three mental health subreddits against r/happy, r/bodybuilding, and r/loseit via pairwise independent sample t tests. The overall comparisons of readability scores among subreddits are shown in Table 6. Subreddit comparisons indicate that the readability of posts by members of all three mental health subreddits improved significantly more than members of r/happy. Yet, the effect sizes for those comparisons were very small to small. Moreover, only the readability of posts by members of r/bipolar improved significantly more than posts by members of r/depression and r/schizophrenia, with very small to small effects among the pairwise comparison of three mental health subreddits. Members of r/bipolar also had the most improvement in terms of lexical diversity and significantly more than members of r/depression, r/loseit, and r/happy, albeit the effect sizes were very small to small (Table 7). Furthermore, members of r/schizophrenia and r/depression improved significantly more than members of r/happy; however, no significant difference was found against r/loseit.",2,2,2,1,0,0,2,2,4,1,2,2,2,2,0,0,0,2,6,1
https://www.jmir.org/2018/4/e121,24,9,,"Park, Conway",2,0,0,0,2018,"We examined the issue of written communication challenges using readability and lexical diversity of posts from publicly accessible online mental health communities on Reddit. We found that on average, members of depression, bipolar disorder, and schizophrenia subreddits wrote posts that are significantly more difficult to read and had significantly less lexical diversity when compared with three other OHCs focusing on positive emotion, exercising, and weight management. We also found that as members of mental health communities participated more in the community, they wrote posts that were easier to read with more lexical diversity. Interestingly, members of other OHCs also improved, with the exception of readability scores of r/happy members. Only r/bipolar members showed statistically significant improvement in lexical diversity compared with members of the two other OHCs (r/happy and r/loseit), while showing statistically significant improvement compared with r/happy in terms of readability scores. Compared with r/happy members, r/depression and r/schizophrenia members also significantly improved in both examined features. Another interesting finding is readability scores and lexical diversity of r/loseit, in which members could have depressive symptoms because of the distress of being overweight. The readability scores and lexical diversity of r/loseit were in between r/happy and three mental health subreddits. Still, the posts from r/loseit were statistically significantly easier to read with more lexical diversity (medium to huge effect sizes) compared with the three mental health subreddits. However, posts from r/loseit were statistically significantly harder to read (medium to large effect size), with less lexical diversity (large to very large effect size) compared with r/happy. Members of r/bodybuilding and r/happy wrote more similar to one another than to members of r/loseit in terms of readability scores and lexical diversity. Despite the possible language impairment faced by members of mental health communities, their real-life communication challenges are unknown. To our knowledge, this is the first study to show mental health community members’ written communication challenges occurring in the real world using social media. Our analyses suggest that members of online mental health communities could encounter incoherent texts because of the language impairment of other members. Automatically correcting misspellings simplifying language and improving text coherence in posts could enhance the readability of posts and the overall experience of participating in these communities. Many online communities, including many Reddit’s subreddits, utilize moderators to regulate content and support members. A number of automated systems have been suggested to assist moderators and reduce moderator burden. Similarly, an adaptation of our automatic analysis method could be a basis for detecting individuals whose lexical diversity and readability of posts are worsening in massive scale networks. This could indicate worsening of mental disorder symptoms, and such a feature could alert and allow moderators to provide timely support. We also showed the potential for improving written communication via more frequent writing in online mental health communities. Designing features of online mental health communities for the purpose of improving written communication can enhance the everyday life of individuals suffering from mental conditions. For example, a place for expressive writing can improve their symptoms and possibly help with their written communication challenges.",2,2,2,1,0,0,2,2,4,1,2,2,2,2,0,0,0,2,6,1
https://www.jmir.org/2018/4/e121,24,10,,"Park, Conway",2,0,0,0,2018,"Research using publicly accessible social media data (such as Reddit) is typically granted exemption from review by IRBs in the US context; however, ethical considerations such as privacy remain critical In this paper, we do not report any user identifiable information to protect user privacy (eg, direct quotations and usernames).",2,2,2,1,0,0,2,2,4,1,2,2,2,2,0,0,0,2,6,1
https://www.jmir.org/2018/4/e121,24,11,,"Park, Conway",2,0,0,0,2018,"Our study has several limitations. A number of confounding factors such as individuals’ premorbid-intelligence, -verbal skill and -education level, as well as demographic and geographical characteristics could influence the writing quality other than language impairment. Other possible confounding factors associated with group dynamics and mental health conditions include the communication practices and cultures of specific subreddits, as well as medication and substance use of individuals suffering from mental health conditions. Furthermore, we assumed that high readability scores are reflecting inadequate articulation or organization by writers. Although inadequate articulation and organization can increase readability scores, high readability scores can also be because of sophisticated language and complex sentence structure. However, we did not encounter sophisticated writing in our manual assessment, and it is unlikely that such sophistication and complexity are highly prevalent in everyday communication. Similarly, we do not know how readability scores were influenced by common online communication attributes such as slang, abbreviation, community nomenclature, and misspellings or how lexical diversity was impacted by number of topics and change of topics . However, these online communication attributes are more likely to occur in all subreddits, and thus, affecting the readability scores in a similar manner. Reddit is a widely used platform more frequently used by young males [ in English-speaking nations Despite more user activities from English speaking nations (85%) it is unclear how participation by English as second language speakers is affecting the results. Additionally, members who choose to participate in r/depression, r/bipolar, and r/schizophrenia are not necessarily representative of their respective populations and are subject to selection bias. Similarly, we do not have any evidence that members of these three mental health subreddits are clinically diagnosed; the severity of their condition is unknown, and overlapping memberships could exist in these subreddits. However, one of the main limitations of previous studies were small sample sizes which could be the underlying reason for the inconsistent results Thus, given the size of r/depression, r/bipolar, and r/schizophrenia, the prevalence and gravity of mental disorders, the increasing popularity of social media, and the potential challenges associated with daily use of social media make Reddit an interesting platform to study. Although beyond the scope of this study, further investigation regarding readability metrics may be needed for more accurately determining the grade reading level We selected readability metrics based on the literature in which the metrics have been validated or used. However, we noticed a disparity among the metrics. For example, readability scores by Gunning Fog index were far greater than the other three metrics. SMOG index resulted in readability scores that were less than the other metrics. Despite the apparent differences, the scores were correlated with one another as a previous study suggested and we used the mean of normalized scores of four readability metrics to strengthen the reliability of our findings. Due to the consistent statistical results, we believe that these four metrics can measure the general difficulty of readability. We also acknowledge that our large sample size could have inflated the statistical significance levels. Thus, we reported 95% CIs and used effect sizes when interpreting the results. Another interesting future direction would be to investigate why members are improving and longitudinal changes in written communication with respect to prolonged participation in online mental health communities. In this study, we only examined the overall impact of participation in online mental health communities; however, understanding how members are improving their written communication skills could potentially inform the design of related patient education programs.",2,2,2,1,0,0,2,2,4,1,2,2,2,2,0,0,0,2,6,1
https://www.jmir.org/2018/4/e121,24,12,,"Park, Conway",2,0,0,0,2018,"We provide new insights into the written communication challenges faced by individuals suffering from depression, bipolar disorder, and schizophrenia. A comparison of mental health communities to three other OHCs suggests that writings in mental health communities were significantly more difficult to read, while consisting of a significantly less diverse lexicon. Our findings also suggest that participating in these subreddits has the potential to improve members’ written communication over time. We contribute practical suggestions for utilizing our findings in online communication settings to enhance members’ communicative experience. We consider these findings to be an important step toward understanding written communication challenges among individuals suffering from mental disorders.",2,2,2,1,0,0,2,2,4,1,2,2,2,2,0,0,0,2,6,1
https://www.jmir.org/2018/4/e121,24,13,,"Park, Conway",2,0,0,0,2018,We restricted our analysis to publicly available discussion content. The study was exempted from review by the University of Utah’s Institutional Review Board (Ethics Committee) [IRB 00076188]. AP’s contribution to this research was supported by the National Library of Medicine of the National Institutes of Health under training grant T15 LM007124. MC’s contribution to this research was supported by the National Library of Medicine of the National Institutes of Health under award numbers R00LM011393 & K99LM011393. The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health.,2,2,2,1,0,0,2,2,4,1,2,2,2,2,0,0,0,2,6,1