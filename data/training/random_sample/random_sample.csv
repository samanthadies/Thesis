Link to paper,Title,Section,Score,Text,Text 2,Text 3
https://link.springer.com/content/pdf/10.1140/epjds/s13688-017-0110-z.pdf,Instagram photos reveal predictive markers of depression,Data Collection,2,Data collection was crowdsourced using Amazon’s Mechanical Turk (MTurk) crowdwork platform. Separate surveys were created for depressed and healthy individuals. In the depressed survey participants were invited to complete a survey that involved passing a series of inclusion criteria responding to a standardized clinical depression survey answering questions related to demographics and history of depression and sharing social media history. We used the CES-D (Center for Epidemiologic Studies Depression Scale) questionnaire to screen participant depression levels []. CES-D assessment quality has been demonstrated as on-par with other depression inventories including the Beck Depression Inventory and the Kellner Symptom Questionnaire [ ]. Healthy participants were screened to ensure no history of depression and active Instagram use. See Additional file  for actual survey text. Qualified participants were asked to share their Instagram usernames and history. An app embedded in the survey allowed participants to securely log into their Instagram accounts and agree to share their data.b Upon securing consent we made a one-time collection of participants’ entire Instagram posting history. In total we collected  photographs from  Instagram users  of whom had a history of depression. We asked a different set of MTurk crowdworkers to rate the Instagram photographs collected. This new task asked participants to rate a random selection of  photos from the data we collected. Raters were asked to judge how interesting likable happy and sad each photo seemed on a continuous - scale. Each photo was rated by at least three different raters and ratings were averaged across raters. Raters were not informed that photos were from Instagram nor were they given any information about the study participants who provided the photos including mental health status. Each ratings category showed good inter-rater agreement. Only a subset of participant Instagram photos were rated (N = ). We limited ratings data to a subset because this task was time-consuming for crowdworkers and so proved a costly form of data collection. For the depressed sample ratings were only made for photos posted within a year in either direction of the date of first depression diagnosis. Within this subset for each user the nearest  posts prior to the diagnosis date were rated. For the control population the most recent  photos from each user’s date of participation in this study were rated.,,
https://dl.acm.org/doi/abs/10.1145/2858036.2858207 ,Discovering Shifts to Suicidal Ideation from Mental Health Content in Social Media,Constructing User Classes,2,We split our data into two sequential time periods (t1 from Feb 11 2014 to Aug 11 2014 and t2 from Aug 12 2014 to November 11 2014). Using these two time periods we created two sets of users. Note that since Reddit does not enforce the real name rule of having exactly one account per person our reference to “users” in this paper is equivalent to “user accounts”. First we identified those users that posted on MHs during t1 but did not post on SW during t1 or t2 (i.e. users that discuss mental health topics but not on SW; hereafter “MH”). The second class included those who posted on MHs during t1 and posted in SW during t2 (i.e. users that discuss mental health topics originally not related to suicide but eventually transition to talk about suicide; hereafter “MH → SW”). Figure 1 shows a schematic description of our user class construction. Note that by focusing on users that initiate at least one post on SW or the MHs as opposed to only commenting we can focus on those frequenting the communities for support disregarding those primarily providing help through commentary. This split yielded 440 MH → SW users; which is 1.52% of the total number of 28831 accounts who posted in MHs but never on SW during either of the periods. To construct a MH cohort of equal size who did not post on SW in either period we randomly sampled a set of 440 users from the 28831 users. Note although MH users did not post on SW during our timeframe of analysis they may have done so outside the bounds of our analysis. To support our goal of characterizing differences between the MH → SW and MH users we obtained via Reddit’s API the timeline of posts and comments authored by the 880 users (the API only provides the last 1000 public posts and comments for a user). For each post we obtained their associated metadata (e.g. vote difference or score) and comments. Our final dataset contained 4731 posts and 46949 comments from the 440 MH → SW users and 8318 posts and 54086 comments from the 440 MH users. We note an important concern: individuals may post suicidal thoughts on MHs never engaging on SW and thus “corrupting” the MHs data with discussions of suicidal ideation. We argue against this possibility. (1) SW is a prominent suicide support forum and the role of this community in suicide prevention and in acting as an inoculator of vulnerable thoughts is well-recognized [31]. (2) Most MHs (e.g. r/depression) clearly specify in their guidelines that suicidal thoughts should go to SW: “It’s usually better to post anything that specifically involves suicidal thoughts or intent in /r/SuicideWatch rather than here. If you’re concerned about someone else who may be at risk for suicide please check out their talking tips and risk assessment guide.” (3) Finally discussions with the moderators of SW confirmed that active steps are taken to move all suicidal ideation related content to SW. Given these considerations we expect that few suicidal ideation posts appear on subreddits outside of SW.,,
https://dl.acm.org/doi/abs/10.1145/2702613.2732733,Detecting and Characterizing Mental Health Related Self-Disclosure in Social Media,Generating Ground Truth on Self-Disclosure Data Collection,2,Automatic detection of self-disclosure levels in posts necessitates obtaining gold standard labels on self-disclosure in essence “ground truth”. For the purpose two raters familiar with Reddit and its mental health communities in particular independently rated a small random sample (50 posts) with equal proportions from mental health and control subreddits for three levels of self-disclosure – no selfdisclosure low and high self-disclosure. These three classes of self-disclosure were chosen based on categorization by Bak et al in [10]. The raters mutually discussed their labels thereafter and thus came up with a set of rules for rating. The rules were further aligned with observations in prior work [9 10 11]. Per the rules: Posts that either reveal personal information (e.g. age location gender etc.) or divulge sensitive or vulnerable thoughts beliefs or embarrassing/confessional experiences were to be considered to be indicative of high self-disclosure. Joinson [10] characterized sensitive disclosure in terms of the extent of “revealed vulnerability”. Posts about self but not disclosing any personal or emotionally vulnerable content was to be considered of low self-disclosure. No self-disclosure posts were those which were about people or things other than the posting author and which divulged information unrelated to the self. Following these mutually agreed upon rules the previous two raters and an additional rater familiar with Reddit independently coded a larger sample of 800 posts to create a training set for the purposes of classification. The raters had good agreement in their ratings: Fleiss’ Kappa was found to be .73. However given the subjective nature of characterization of self-disclosure we considered only those posts in the training set for which we had agreement across all three raters – this gave us 627 posts. Across the three categories the coded set consisted of 38% posts of high selfdisclosure 35% posts of low self-disclosure and 27% with no self-disclosure. Table 3 gives examples of mental health post excerpts with high self-disclosure while Table 4 and 5 provide examples of low and no self-disclosure posts. Note that including non-mental health posts in the training set was essential so as to let the detector learn on posts of low and no self-disclosure and on those not mental health related.,We used Reddit's official API to collect posts comments on posts and associated metadata from several mental health focused subreddits. We build on the data collection methodology we used in [4]. In order to arrive at a comprehensive list of subreddits to focus on we utilized Reddit's native subreddit search feature (http://www.reddit.com/reddits). We searched for subreddits on “mental health”. Two researchers familiar with Reddit employed an initial filtering step on the search results returned so that we “seed” on high precision subreddits discussing mental health concerns. Thereafter we focused on a snowball approach to compile a second list of “related” or “similar” subreddits that are mentioned in the profile pages of the seed subreddits. Sample of subreddits (31 in all) we crawled are given in Table 1. Note all of these subreddits host public content. For the purposes of self-disclosure detection we also identified subreddits (sample listed in Table 2) as our control group (total of 12 subreddits) – meaning they are unrelated to mental health topics. For sanity check we randomly sampled a set of 200 posts from the control subreddits and two researchers familiar with Reddit manually checked their content for presence of any mental health content. We found that 97% of subreddit content in our sample were not about any mental health concern (Cohen’s Kappa for inter-rater agreement was .84). In all our dataset had 32509 posts from 23807 users in the mental health subreddits and 15383 posts from 13216 users in the control forums. For each of the unique users in the mental health forums we further collected all of their Reddit post/comment histories (last 1000 posts/comments per Reddit API limits) if their number of posts and comments in our dataset was five or more – this gave us 7248 users and 4.1M posts/comments,
https://dl.acm.org/doi/abs/10.1145/2702123.2702280,Recognizing Depression from Twitter Activity,Data Gathering,0,In this study we gathered information on depression levels of Twitter users and their activity histories. To do this we published a website to administer a questionnaire and disseminated information about the website over Twitter1. In contrast to De Choudhury et al. [14] who collected data from Englishspeaking users through crowdsourcing this study collected data from Japanese-speaking volunteers. This approach was used to investigate the extent to which depression risk can be estimated for a population different from the population considered by the prior research [14]. Figure 1 shows a screenshot of our website. The website collected the responses to a questionnaire to evaluate the degree of depression of the Twitter users who participated (hereinafter the participants) and to collect the histories of participants activities on Twitter. The activity histories of participants were collected through the Twitter application programming interface (API)2 and the questionnaires to determine degree of depression were completed by participants through their web browsers. Before data collection visitors to the website were presented with a written explanation of the aims of the experiment the information that would be collected and how that information would be handled. Those who consented to become participants after receiving the explanation logged into their individual Twitter accounts through the OAuth authorization process. Next participants were surveyed on gender age occupation and history of depression following which they answered a questionnaire designed to evaluate degree of depression. A message called the “kokoro score” (“kokoro” is a Japanese word meaning “heart”) determined on the basis of answers to the questionnaire and information in the collected tweets was displayed to participants after completion of the questionnaire (Fig. 2). Experiment participants were able to tweet the message displayed which made it possible to promote the website over Twitter by word-of-mouth in a type of snowball sampling. The CES-D questionnaire was used to evaluate the degree of depression [30]. In the CES-D test participants answered 20 questions on a Likert-type 4-point scale. Each answer was assigned a score of 0-3 points with the sum of the points from all answers used as the score to estimate likelihood of depression. Several standards exist by which to determine the appropriate cutoff score for identifying depression. In this research we regarded a score of 22 points or higher as indicating active depression and a score of 21 points or lower as indicating no active depression; these are the same values as used in [14] and give a cutoff score of 22. In addition answers to BDI [2] a depression scale used with characteristics similar to CESD were collected to ensure the reliability of data. For each participant scores were calculated on both scales with poor correlation regarded as indicating unreliable answers. The time taken to answer the questionnaires was also recorded and those completed in too brief a time were excluded. After each participant answered the questionnaire the activity history of that participant on Twitter was collected from Twitter by using the API. At most 3200 tweets were collected for each participant and the number of users following the participant and being followed by the participant were recorded. Tweets published after the questionnaire was taken were discarded. The website was opened to the public on 4 December 2013 at which time the authors publicized it on their Twitter accounts. Between 4 December 2013 and 8 February 2014 219 people participated in the experiment. After eliminating participants who did not tweet and participants who answered the questionnaire in fewer than 30 seconds (as previously mentioned to ensure the reliability of the questionnaire answers) 214 sets of answers remained. Only the first set of answers was used for participants who completed the questionnaire more than once. As a result data about 209 experiment participants (male: 121; female: 88) aged 16 to 55 (mean: 28.8 years; standard deviation: 8.2 years) were analyzed. The correlations between CES-D score and BDI score for these participants were high 0.87 and there were no participants with uncorrelated scores so the data for all 209 participants were used; excluded datasets are not discussed any further. Figure 3 shows the histogram of CES-D scores of 209 participants. Among the participants 81 (resp. 128) were estimated to have (resp. not have) active depression for an incidence of approximately 39%. This incidence is similar to that found by De Choudhury et al. [14] who identified depression in approximately 36% of participants. Table 1 gives statistics on the activity histories of participants.,,
https://www.nature.com/articles/s41598-017-12961-9,Forecasting the onset and course of mental illness with Twitter data,Machine Learning Models,1,We trained supervised machine learning classifiers to discriminate between affected and healthy sample members’ observations. Classifiers were trained on a randomly-selected 70% of total observations and tested on the remaining 30%. Out of several candidate algorithms a 1200-tree Random Forests classifier demonstrated best performance. Stratified five-fold cross-validation was used to optimize Random Forests hyperparameters and final accuracy scores were averaged over five separate randomized runs. See Supplementary Information section III for optimization details. Precision recall specificity negative predictive value and F1 accuracy scores are reported and general practitioners’ unassisted diagnostic accuracy rates as reported in Mitchell Vaze and Rao28 (MVR) and Taubman-Ben-Ari et al.22 (TBA) are used as informal benchmarks for depression and PTSD respectively. In addition to the fact that our results are drawn from different samples using different observational units than our chosen comparisons comparing point estimates of accuracy metrics is not a statistically formal means of model comparison. We felt it was more meaningful to frame our findings in a realistic context however informal rather than to benchmark against a naive statistical model that simply predicted the majority class for all observations.,,
https://aclanthology.org/W14-3214.pdf,Towards Assessing Changes in Degree of Depression through Facebook,Dataset,2,We used a dataset of 28749 nonclinical users who opted into a Facebook application (“MyPersonality”; Kosinski and Stillwell 2012) between June 2009 and March 2011 completed a 100-item personality questionnaire (an International Personality Item Pool (IPIP) proxy to the NEO-PI-R (Goldberg 1999) and shared access to their status updates containing at least 500 words. Users wrote on average of 4236 words (69917624 total word instances) and a subset of 16507 users provided gender and age in which 57.0% were female and the mean age was 24.8. The dataset was divided into training and testing samples. In particular the testing sample consisted of a random set of 1000 users who wrote at least 1000 words and completed the personality measure while the training set contained the 27749 remaining users.,,
https://ieeexplore.ieee.org/abstract/document/6784326,Affective and Content Analysis of Online Depression Communities,Datasets Feature Extraction,0,a) CLINICAL Communities: Communities who are interested in ‘depression’ and with at least 200 posts are extracted from LiveJournal. This is identified through the ‘Search communities by interest’2 provided by LiveJournal and results in 24 communities with 38401 posts. The CLINICAL communities are grouped based on name and description of the individual community: depression bipolar self-harm attachment/separation and suicide (See Table 1 for statistics). The earliest community creation date was in 2001 thus our data set spans over 10 years. b) CONTROL communities: We constructed a CONTROL data set using five popular categories of communities in the LiveJournal Directory.3 We select communities who have at least 200 posts resulting in 23 communities with 229563 posts. This set is called CONTROL and the statistics of these 23 communities and their description are shown in Table 2.,To characterize the difference between CLINICAL and CONTROL communities a variety of features are extracted: Affective features: We use the lexicon—Affective Norms for English Words (ANEW) [5]—to extract the sentiment conveyed in the content. This lexicon consists of 1034 words rated in terms of valence and arousal and is thus suitable for a quantitative estimation. The valence of ANEW words is on a scale of 1 (very unpleasant) to 9 (very pleasant). The arousal is measured on the same scale—1 (least active) to 9 (most active). A cloud visualization of ANEW words used in the blog posts made by CLINICAL and CONTROL groups is illustrated in Fig. 1. Mood tags: LiveJournal provides a mechanism for users to tag their posts from a list of 132 pre-defined mood labels.4 Thus in addition to the emotion expressed in the text of posts the mood tag produced allows us direct access to the user sentiment. A cloud visualization of moods tagged on blog posts made by CLINICAL and CONTROL communities is illustrated in Fig. 2. LIWC features: We examine the proportions of words in psycholinguistic categories as defined in the LIWC package [27]: linguistic social affective cognitive perceptual biological relativity personal concerns and spoken.5 Table 3 presents the mean of these LIWC psycholinguistic processes for the CLINICAL and CONTROL communities. Whilst similar in the use words with positive emotion people in the CLINICAL communities tend to use words with more negative emotion—as examples anxiety anger and sadness. Further they discuss more issues about health and death in comparison with the CONTROL group. On the other hand the users in the CONTROL group discuss more neutral life related topics—ingestion home and leisure words. Topics: For extracting topics latent Dirichlet allocation (LDA) [4] is used as a Bayesian probabilistic modelling framework. LDA extracts the probabilities pðvocabulary jtopicÞ—that is words in a topic and then assigns a topic to each word in a document. For the inference part we implemented Gibbs inference detailed in [10]. We set the number of topics to 50 run the Gibbs for 5000 samples and use the last Gibbs sample to interpret the results.,
https://www.jmir.org/2017/7/e243/,Assessing Suicide Risk and Emotional Distress in Chinese Social Media: A Text Mining and Machine Learning Study,Data Collection,2,A Web-based survey of Weibo users was conducted to assess the respondents’ suicide risk and emotional distress (ie depression anxiety and stress). The invitation letter to participate in this survey was widely sent out to general Weibo users by various promotion activities. For a Weibo user to be eligible for the study she or he had to be 18 years or older (by self-report). A 30 Renminbi incentive for each complete survey was provided to boost the respond rate. With the respondents’ consent their Weibo posts that were posted in the public domain during the 12 months before the survey were downloaded by calling Weibo API. The survey fulfilled the Checklist for Reporting Results of Internet E-Surveys (CHERRIES) checklist and details of the procedure have been reported in previous publications [2232]. In addition when multiple survey feedback were submitted from the same Internet protocol addresses only the first submission was used to avoid duplicate participation. In contrast to a previous study [32] this study excluded those who posted nothing throughout the 12 months but not those who posted fewer than 100 posts. Eventually data provided by 974 respondents remained for further analyses. The study has obtained ethical approvals from the Human Research Ethical Review Committee at the University of Hong Kong and the Institute Review Board of the Institute of Psychology at the Chinese Academy of Sciences. The survey measured respondents’ suicide probability score depression anxiety stress and Weibo suicide communication (WSC) as the outcome variables. In addition the respondents’ Weibo posts language features were extracted as independent variables or features for machine learning. The details of how those data were obtained are elaborated in the following subsections.,,
https://dl.acm.org/doi/abs/10.1145/3025453.3025909,A Social Media Based Index of Mental Well-Being in College Campuses,Demographic Representativeness Mental Health and Control Data,2,We note it is possible that the type of students who frequent the university subreddits could be consistently different from the student body at the same university. To examine the representativeness of our university subreddit data we employed a random sample of 500 posts distributed across the subreddits and the years for manual examination of demographics. Two researchers then independently coded these posts for self-reported gender race or academic stage (undergraduate/graduate). For instance from the post “I’m a junior transfer and this will be my second semester” the researchers identified the post author to be an undergraduate whereas from “Hi all! I’m a new grad student (male 22) here the gender of the author can be inferred to be male. We found the interrater agreement to be high: Cohen’s  = .84. The relative ratios between the gender race and academic stage distributions of the coded posts and the university student body (obtained based on our methodology in the subsection “University Data”) showed significant positive correlation: The mean undergrad/grad ratio in our labeled data was 2.9 while it was 2.6 in the universities. A two-sample test of equivalence gave p-values of .016 and .011 respectively with respect to the difference interval [.4 .4]. The sex (male/female) ratio for our labeled data was 1.6 also observed to be statistically equivalent to that of the student body 1.1 (p = .02 p = .03 w.r.t. the difference interval [.5 .5]). This establishes the validity of our acquired Reddit data as a representative data source for studying mental health disclosures in university campuses.,We gained access to a sample of 63485 public posts from 35038 unique users shared between 2014 and 2016 in a variety of mental health subreddits—this repository of posts has been used in prior work to study mental health selfdisclosure and support seeking manifested in social media [18 50 39 20]. This dataset includes posts and associated metadata spanning 14 mental health related subreddits such as r/depression r/mentalhealth and r/traumatoolbox r/bipolarreddit. From this corpus we excluded posts that contained only a title without a post body. This gave us 21734 posts. We refer to these posts as MH posts. Our control data also relied on a dataset compiled and utilized in prior work [50]; it contains posts from subreddits such as r/WorldNews r/food and r/AskReddit. We randomly sampled an equal number of posts (21734) as the MH posts above for our control dataset. We refer to these posts as CL posts.,
https://ieeexplore.ieee.org/abstract/document/7752434,MIDAS: Mental illness detection and analysis via social media,Data Collection Prediction Model,2,To train our models we require information from two different types of users: patients and non-patients. Therefore we employed a combined - manual effort and keyword matching - data collection approach to efficiently collect data for these users. For the collection of patients we manually collect the community portals relevant to both mental disorders. 2 From these portals' followers list we select the self-reported users who explicitly state in their profile description that they suffer from a mental illness; i.e. for a given user we are checking if his/her profile contains any keyword related to a target disorder (e.g. “borderline” “bpd” “bipolar”). Non-patients are referred to as random active Twitter users who are not explicitly stating that they are suffering from Bipolar disorder (hereinafter referred to as “BD”) or Borderline Personality Disorder (hereinafter referred to as “BPD”). To obtain these users we randomly sampled Twitter IDs. Thereafter we proceeded to download the tweets from the selected IDs. After the users have been identified we manually label them into one of two categories: 1) Patient: a person who is suffering from a mental disorder 2) Not-related: any user who we don't consider to be a patient. Lastly after having obtained the final list of patients we retrieve their tweets. These steps are applied for the collection of both BPD and BD patient datasets.,In this study we collected 17 BPD and 12 BD community portals. 5000 followers for each portal was collected for a total of 145000 accounts. From these accounts we manually picked a subset and annotated each user into positive examples (patient) or not-related examples. After filtering we gathered a total of 278 BD accounts and 203 BPD accounts. A total of 548 random samples (negative examples) were obtained directly from the Twitter REST API. We experimentally chose Random Forest Classifier to be our main learning model. Consequently we trained separate classifiers one for each mental disorder and equally distributed the random samples to each. We used a 10-fold cross validation to evaluate our models. Applying only the TD-IDF features we achieved a precision of 96% for both the BP and BPD models. On the other hand by applying the pattern of life features we achieved a precision of 91% and 92% for the BD and BPD models respectively. All models were further pre-trained and prepared as a REST-API service which produce statistical outputs further converted into insightful visuals.,
https://www.nature.com/articles/s41598-020-68764-y,A deep learning model for detecting mental illness from user content on social media,Classification Models,0,To train our models we require information from two different types of users: patients and non-patients. Therefore we employed a combined - manual effort and keyword matching - data collection approach to efficiently collect data for these users. For the collection of patients we manually collect the community portals relevant to both mental disorders. 2 From these portals' followers list we select the self-reported users who explicitly state in their profile description that they suffer from a mental illness; i.e. for a given user we are checking if his/her profile contains any keyword related to a target disorder (e.g. “borderline” “bpd” “bipolar”). Non-patients are referred to as random active Twitter users who are not explicitly stating that they are suffering from Bipolar disorder (hereinafter referred to as “BD”) or Borderline Personality Disorder (hereinafter referred to as “BPD”). To obtain these users we randomly sampled Twitter IDs. Thereafter we proceeded to download the tweets from the selected IDs. After the users have been identified we manually label them into one of two categories: 1) Patient: a person who is suffering from a mental disorder 2) Not-related: any user who we don't consider to be a patient. Lastly after having obtained the final list of patients we retrieve their tweets. These steps are applied for the collection of both BPD and BD patient datasets.,,
https://aclanthology.org/W19-3013.pdf,Mental Health Surveillance over Social Media with Digital Cohorts,Building Digital Cohorts Mental Health Classifiers,2,Our cohort construction process entails two key steps: first randomly selecting a large sample of Twitter users; and second annotating those users with key demographic attributes. While such attributes are not provided by the API automated methods can be used to infer such traits from data (Cesare et al. 2017). Following this approach we develop a demographic inference pipeline to automatically infer age gender race/ethnicity and location for each cohort candidate. Age Identifying age based on the content of a user can be challenging and exact age often cannot be determined based on language use alone. Therefore we use discrete categories that provide a more accurate estimate of age: Teenager (below 19) 20s 30s 40s 50s (50 years or older). Gender The gender was inferred using Demographer a supervised model that predicts the (binary) gender of Twitter users with features based on the name field on the user profile (Knowles et al. 2016). Race/Ethnicity The standard formulation of race and ethnicity is not well understood by the general public so categorizing social media users along these two axes may not be reasonable. Therefore we use a single measure of multicultural expression that includes five categories: White (W) Asian (A) Black (B) Hispanic (H) and Other. Location The location was inferred using Carmen an open-source library for geolocating tweets that uses a series of rules to lookup location strings in a location knowledge-base (Dredze et al. 2013). We use the inferred location to select users that live in the United States. The age and race/ethnicity attributes were inferred with custom supervised classifiers based on Amir et al. (2017)’s user-level model. The classifiers were trained and evaluated on a dataset of 5K annotated users attaining performances of 0.28 and 0.41 Average F1 respectively. See the supplemental notes for additional details on these experiments1 .,We build on prior work on supervised models for mental health inference over social media data. We focus on two mental health conditions — depression and PTSD — and develop classifiers with the self-reported datasets created for CLPysch 2015 (Mitchell et al. 2015; Coppersmith et al. 2015b). These labeled datasets derive from users that have publicly disclosed on Twitter a diagnosis of depression (327 users) or PTSD (246 users) with an equal number of randomly selected demographically-matched (with respect to age and gender) users as controls. For each user the associated metadata and posting history was also collected — up to the 3000 most recent tweets per limitations of the Twitter API. The participants of the task proposed a host of methods ranging from rule-based systems to various supervised models (Pedersen 2015; PreotiucPietro et al. 2015; Coppersmith et al. 2015b). More recently the neural user-level classifier proposed by Amir et al. (2017) showed not only good performance on this task but also the ability to capture implicit similarities between users affected by the same diseases thus opening the door to more interpretable analyses2 . Hence we adopt their model for this analysis.,We constructed a cohort for our analysis by randomly selecting a sample of Twitter users and processing it with the aforementioned demographic inference pipeline. After discarding accounts from users located outside the United States we obtained a cohort of 48K Twitter users with the demographic composition shown in Figure 1. Some demographic groups are over-represented (e.g. young adults) while others are grossly underrepresented (e.g. teenagers) which illustrates the need for methodologies that can take these disparities into account. We then processed the cohort through the mental-health classifiers to estimate the prevalence of depression and PTSD and examine how these illnesses manifest across the population. The analysis revealed that 30.2% of the cohort members are likely to suffer from depression 30.8% from PTSD and 20% from both. We observe a significant overlap between people affected by depression and PTSD which is not surprising given that the comorbidity of these disorders is wellknown with approximately half of people with PTSD also having a diagnosis of major depressive disorder (Flory and Yehuda 2015). How do these conditions affect different parts of the population? To answer this question we looked at the affected users and measured how the demographics of individual sub-populations differ from those of the cohort as a whole. Figures 2 and 3 show the estimates for depression PTSD and both controlled for the cohort demographics. We observe large generational differences — PTSD seems to be more prevalent among older people whereas depression affects predominantly younger people. We also observe that in all cases Women are more susceptible than Men and Blacks and Hispanics are more likely to be affected than Whites. This may represent a bias in the underlying data used to construct the classifiers or a difference in how social media is used by different demographic groups. For example models that were trained with a majority of data from White users maybe oversensitive to specific dialects used by other communities.
https://link.springer.com/chapter/10.1007/978-3-319-67186-4_6,Social Media Mining to Understand Public Mental Health,Validation,2,To evaluate the effectiveness of our topic modelling methodology we selected random subsets of 60 public journals for each topic and 100 public journals with no assigned topic. We manually labeled the sampled journals looking for one of the 14 available topics no topic or “other” topic. Including “other” allowed us to validate whether our list of manually labeled topic names were accurate and complete. We then compared our labels with those assigned by the model. For journals which were assigned two topics by the model we considered the model correct if either one of the assigned topics was equal to the topic we chose manually. Table 2 shows the topic accuracies of our model. Overall our model works well with an average accuracy well above 80%. Journals without topics were much shorter in length. The average journal length of a journal with no topic was 114 characters while one topic was 142 and two topics was 185. Manual inspection confirmed that these journals indeed did not contain any topic more than 70% of the time. Instead they mostly contained sentiment that was already available from mood labels. We conclude this section by remarking that we analyzed activity around significant events such as the 2016 American Election. We did not find statistically significant anomalies in topics mentioned since the topics we derived are mostly related to day-to-day activities.,,
https://dl.acm.org/doi/abs/10.1145/2556288.2557214,Seeking and sharing health information online: comparing search engines and social media,Intent of Search Engine Use Log Data Collection,2,The 197 survey respondents who reported seeking health information using a search engine were asked to recall the most recent instance and to answer questions related to that specific incident. Respondents also described their search objective. These responses were classified using an iterative open coding process. A second rater used this scheme to rate a random sample of 30 responses; inter-rater reliability (Cohen’s ) indicated substantial agreement ( = .72). The same verification strategy is used in the rest of this section. We coded the intent for 183 respondents (the others had unclear intent). Some searches had multiple intents and received multiple categorizations so the percentages sum to more than 100%. The most common motivation for using a search engine was to identify treatment options (53.0%) e.g. “stretches to cure or ease [tight hamstring].” Alternative and holistic treatment was a popular sub-category comprising 13.5% of treatment searches e.g. “alternative treatment [hypothyroidism].” The next most common motivation was diagnosis of a health condition (26.8%) (e.g. “whether or not the symptoms matched my behavior [depression]”) or interpreting the symptoms that they experienced (e.g. “what may be a cause of this and if it may mean something more may be wrong [very heavy menstrual cycle]”). A third motivation was general understanding of a health condition or procedure (20.8%) including understanding what a medical procedure might entail (e.g. “more about the surgery process healing time [umbilical hernia]”) understanding the causes of an illness (e.g. “the caused [sic] of it… [infertility]”) or other general learning about a condition (e.g. “prognosis [congestive heart failure]”). 7.1% of recalled searches were motivated by understanding medications such as understanding side effects (e.g. “to be able to learn the side effects [cholesterol medications]”) comparing and contrasting medications (e.g. “effectiveness [of cancer treatments]”) or seeking information on available medications such as whether non-prescription options are available or learning more about how a medication worked. 6.0% sought lifestyle information for chronic concerns particularly nutrition information for managing diabetes cholesterol or weight loss (“special diet [cholesterol]”). Beyond these broad categories participants also described other intents behind their search activity. 2.2% sought recent medical research findings on conditions or their treatments and 1.1% sought social support such as online support groups for people with their diagnosis. Intent of Twitter Use for Health Information Seeking In a similar way to engine use the 40 respondents who indicated they had sought health information on Twitter were asked to recall the most recent event. They explained their objectives in free-text. Open coding was used to categorize responses; the same categories were used for why people used search engines with additional categories added as needed. Once again substantial agreement was observed with the second coder (30 ratings =.77). Three responses were unclassifiable; percentages are of the remaining 37. As with search engines the most common objective was locating treatment information (56.8%) e.g. “how to help relieve the pain [numbness in the legs]”. 8.1% specifically sought natural or alternative treatments e.g. “natural remedies to headaches.” 16.2% of respondents sought information about healthy lifestyles such as nutrition dieting or fitness (e.g. “different ways to lose weight”) and 13.5% sought to gain a general understanding of a health condition e.g. causes (e.g. “why it happens…” [enlarged prostate]) consequences or general knowledge. 5.4% sought new research about conditions or treatments and 5.4% looked to find others with a similar situation to offer support or advice (e.g. “if anyone else hsd [sic] allergy problems”). Only 2.7% reported seeking diagnostic information on Twitter. Intent of Sharing Health Information on Twitter The 48 respondents who recalled sharing information related to their health on Twitter were asked to consider the most recent incident and to answer a series of questions with that specific occurrence in mind. Participants explained in free-text what their intent was behind sharing health information on Twitter (substantial agreement with second coder =.65). 10 were not coded due to vagueness or missing responses. Of the remaining 38 responses 63.2% reported that they intended to share information about their immediate health status or symptoms (e.g. “I was having a few teeth removed and I may not be online for a few days”). 34.2% wanted to share information or news about a condition (e.g. “treatments that work for me [fibromyalgia and neuropathy]”).,We focus on the social media platform Twitter a popular microblogging service used by 18% of U.S. Internet users and whose popularity continues to increase [28]. Twitter is particularly interesting to study since nearly all posts are public; the public nature of tweets provides an interesting counterpoint to the private nature of search engine activity. We gathered a 15-month sample of Twitter’s Firehose stream (which includes all public tweets) between November 1 2011 and March 31 2013 made available to us under contract focusing on English-language tweets. Twitter post count and unique user count were computed for each condition and aggregated over the full time period. Specifically we considered a post to belong to a certain health condition if there was a regular expression match of the condition to the text of the post (this would not permit substring matches within terms). To reduce noise we excluded posts that were retweets or contained hyperlinks since they were likely related to general news and not a user’s personal health. Using this method we obtained 125166549 tweets on the 165 health conditions from 62269225 users in the time period of interest. The median number of posts was 51687 per condition from a median of 40152 users per condition.,
https://www.aaai.org/ocs/index.php/ICWSM/ICWSM14/paper/viewPaper/8075,Mental Health Discourse on reddit: Self-Disclosure Social Support and Anonymity,Data and Methods,0,reddit is a social news website where registered users submit content in the form of links or text posts. Users also known as “redditors” can then vote each submission “up” or “down” to rank the post and determine its position or prominence on the site’s pages. These two attributes associated with a post are referred to as “upvotes” and “downvotes”. Redditors can also comment on posts and respond back in a conversation tree of comments. Content entries that is the posts are organized by areas of interest or sub-communities called “subreddits” such as politics programming or science. As of 2013 reddit’s official statistics included 56 billion page views 731 million unique visitors 40855032 posts and 404603286 comments (http://blog.reddit.com/2013/ 12/top-posts-of-2013-stats-and-snoo-years.html). We used of reddit’s official API (http://www.reddit.com/ dev /api) to collect posts comments and associated metadata from several mental health subreddits: specifically using a Python wrapper PRAW (https://praw.readthedocs.org/en/ latest/index.html). The subreddits we crawled were: alcoholism anxiety bipolarreddit depression mentalhealth MMFB (Make Me Feel Better) socialanxiety SuicideWatch. All of these subreddits host public content. In order to arrive at a comprehensive list of subreddits to focus on we utilized reddit’s native subreddit search feature (http://www.reddit.com/reddits) and searched for subreddits on “mental health”. Two researchers familiar with reddit employed an initial filtering step on the search results returned so that we focus on high precision subreddits discussing mental health concerns and issues. Thereafter we focused on a snowball approach in which starting with a few seed subreddits (mentalhealth depression) we compiled a second list of “related” or “similar” subreddits that are listed in the profile pages of the seed subreddits. Following a second filtering step we arrived at the list of subreddits listed above. For each of these subreddits we obtained daily crawls of their posts in the New category. Corresponding to each post we collected information on the title of the post the body or textual content id timestamp when the post was made author id and the number of upvotes and downvotes it obtained. Since posts gather comments over a period of time following the time of sharing we crawled all of the comments per post that were shared over a three day period after the post was made. Qualitative examinations of the subreddits of interest revealed that 90% or more of the comments to any post were typically made in a three day window following the time the post is made—hence the choice. The crawl of the subreddits used in this paper We present some descriptive statistics of our crawled data. Our dataset contained 20411 posts with at least one comment and 97661 comments in all with 27102 unique users who made posts comments or both. A set of 7823 users (28.79%) were found to write both at least one post and comment. CDF of the user distribution over posts and comments is given in Figure 1. The figure shows the expected heavy tail trend observed in several social phenomena. Also see Figure 2 for the distribution of comments over time following post share. It illustrates the quick responsivity culture in the communities we study (peak at 3 hours). Some of the additional statistics of our dataset are given in Table 1. Further example titles of a few,,
https://dl.acm.org/doi/abs/10.1145/3025453.3025932,Modeling and Understanding Visual Attributes of Mental Health Disclosures in Social Media,Data Collection,0,We utilized Instagram’s official API1 to obtain the dataset used in this paper. Each post in this dataset is public and contains post-related information such as the image caption likes comments hashtags filter and geolocation if tagged. Referring to prior literature [10] we adopted an iterative approach to first identify a set of appropriate distinguishing hashtags around different prominent mental illnesses prevalent in social media. With the seed tags we performed an initial data collection of 1.5 million posts shared on Instagram between Dec 2010 and Nov 2015. Then by leveraging an association rule mining approach we compiled the top k (k = 39 frequency ≥ 5000) co-occurring tags in the 1.5M posts and then appended them to the original seed tag list for further data collection. Table 1 lists a sample set of tags used to crawl the dataset. This final list of 45 tags was thereafter passed on to a psychiatry researcher to be categorized into different disorder types. For tags that described experiences or symptoms crosscutting across different conditions (e.g. “anxiety”) they were counted toward each disorder type. Table 2 gives a list of the ten different disorders identified in our data. We additionally consulted the Diagnostic and Statistical Manual of Mental Health Disorders (DSM-V [5]) that indicates these disorders to be prominent mental health challenges in populations. This categorization of the mental health challenges was conducted to ensure that our data used in the ensuing analysis focused on well-validated and clinically recognized conditions. At the same time it allowed us to focus on a diverse range of disorders expressed on social media rather than specific ones studied in prior work [12 13 27]; thus enabling us to discover generalized patterns in visual disclosures of mental health challenges in social media. Our final crawl included 2757044 posts from 151638 users spanning these disorders.,,
https://www.sciencedirect.com/science/article/pii/S0747563215300996,A content analysis of depression-related tweets,Tweets Related to Depression Themes,2,Tweets about depression were collected by Simply Measured a company that specializes in social media measurement and analytics (Simply Measured 2014). Simply Measured has access to the Twitter “firehose” (or full volume of tweets) via Gnip a licensed company that can retrieve the full Twitter data stream. All tweets in the English language that contained at least either “depressed” “#depressed” “depression” or “#depression” were collected between April 11 and May 4 2014. We scanned a random sample of the tweets to identify common phrases that included our keywords of interest but were not about mental health. In SAS version 9.3 (SAS Institute Inc. Cary NC) we used the index function which searches a character expression (in this case the text of the tweet) for a specific string of characters to locate and remove such tweets from our sample. We removed tweets that included the following terms regardless of capitalization: “Great Depression” “economic depression” “during the depression” “depression era” “tropical depression” and “depressed real estate”. The popularity and influence of the Tweeters was described using the distribution of followers and Klout Scores. While number of followers is a measure of popularity Klout Score is a measure of influence. Klout Scores range from 0 to 100 with a higher score indicating higher influence. Klout Score is calculated based on an algorithm that considers over 400 signals from eight different online networks. Examples of signals include the amount of retweets a person generates in relation to the amount of tweets shared and the amount of engagement a user drives from unique individuals (e.g. lots of retweets from different individuals as opposed to lots of retweets from one person) (Klout Inc. 2014).,Using the SAS surveyselect procedure we selected a simple random sample of 2000 tweets (that were not direct @replies) from the total volume of depression-related tweets. Two members of the research team each with graduate level degrees (i.e. Ph.D. and M.P.H) and more than 10 years of experience in mental health research scanned approximately 300 random tweets in order to determine their most common themes and generate a codebook. We defined themes as topics that occur and reoccur (Ryan & Bernard 2003). Tweets were coded for presence of the following themes: 1) Tweeter discloses feelings of depression; 2) Tweet is a supportive or helpful message about depression; 3) Tweeter discloses feeling school or work-related pressures related to depression; 4) Tweeter engages in substance use to deal with depression; and 5) Tweeter discloses self-harm or suicidal thoughts. Tweets could be assigned as many themes as were pertinent. For tweets where the user indicated that he or she was feeling depressed those that appeared trivial (i.e. not concerning) were identified. These were tweets where the depression terms were used casually or in a humorous manner or referenced depression caused by trivial things such as being depressed after finishing a good book seeing a concert or watching a sad movie. In addition we ascertained the source of the tweet by viewing the Tweeter's profile picture and studying their Twitter handle name. The source was then coded into one of the three following categories: clinician/therapist health-focused handle (e.g. health/government organization handle focused on healthy lifestyle etc.) or regular person or other (handle did not fall into the above categories). Using this codebook the 2000 randomly sampled tweets were coded in teams of two trained student research interns who coded the tweets together discussing each tweet and coming to an agreement on the final assigned codes. A sample of 150 tweets was also coded by a senior team member (Ph.D. clinician) with extensive mental health research experience. Inter-coder reliability for each theme was as follows: 1) Tweeter discloses feelings of depression: percent agreement 85% kappa 0.67; 2) Tweet is a supportive or helpful message about depression: percent agreement 85% kappa 0.70; 3) Tweeter discloses feeling school or work-related pressures related to depression: percent agreement 98% kappa 0.72; 4) Tweeter engages in substance use to deal with depression: percent agreement 100% kappa 1.0; 5) Tweeter discloses self-harm or suicidal thoughts: percent agreement 98% kappa 0.56; 6) trivial disclosures of depression: percent agreement 90% kappa 0.70; 7) source of Tweet: percent agreement 91% kappa 0.51. Because both prevalence of and kappa for diminished ability to think/concentrate were low we chose not to report on this code.,
https://aclanthology.org/W15-1202.pdf,Quantifying the Language of Schizophrenia in Social Media,Data Perplexity,2,We follow the data acquisition and curation process of Coppersmith et al. (2014a) summarizing the major points here: Social media such as Twitter contains frequent public statements by users reporting diagnoses for various medical conditions. Many talk about physical health conditions (e.g. cancer flu) but some also discuss mental illness including schizophrenia. There are a variety of motivations for users to share this information on social media: to offer or seek support to fight the stigma of mental illness or perhaps to offer an explanation for certain behaviors.4 We obtain messages with these self-reported diagnoses using the Twitter API and filtered via (caseinsensitive) regular expression to require “schizo” or a close phonetic approximation to be present; our expression matched “schizophrenia” its subtypes and various approximations: “schizo” “skitzo” “skitso” “schizotypal” “schizoid” etc. All data we collect are public posts made between 2008 and 2015 and exclude any message marked as ‘private’ by the author. All use of the data reported in this paper has been approved by the appropriate Institutional Review Board (IRB). Each self-stated diagnosis included in this study was examined by a human annotator (one of the authors) to verify that it appeared to be a genuine statement of a schizophrenia diagnosis excluding jokes quotes or disingenuous statements. We obtained 174 users with an apparently genuine selfstated diagnosis of a schizophrenia-related condition. Note that we cannot be certain that the Twitter user was actually diagnosed with schizophrenia only that their statement of being diagnosed appears to be genuine. Previous work indicates that interannotator agreement for this task is good: κ = 0.77 (Coppersmith et al. 2014a). For each user we obtained a set of their public Twitter posts via the Twitter API collecting up to 3200 tweets.5 As we wish to focus on user-authored content we exclude from analysis all retweets and any tweets that contain a URL (which often contain text that the user did not author). We lowercase all words and convert any non-standard characters (including emoji) to a systematic ASCII representation via Unidecode.6 For our community controls we used randomlyselected Twitter users who primarily tweet in English. Specifically during a two week period in early 2014 each Twitter user who was included in Twitter’s 1% “spritzer” sample had an equal chance for inclusion in our pool of community controls. We then collected some of their historic tweets and assessed the language(s) they tweeted in according to the Chromium Compact Language Detector.7 Users were excluded from our community controls if their tweets were less than 75% English.8,The breadth of language used (to include vocabulary topic areas and syntactic construction) can be measured via perplexity – a measurement based on entropy and roughly interpreted as a measurement of how predictable the language is. We train a trigram language model on one million randomly selected tweets from the 2014 1% feed and then use this model to score the perplexity on all the tweets for each user. If a user’s language wanders broadly (and potentially has the word salad effect sometimes a symptom of schizophrenia) we would expect a high perplexity score for the user. This gives us a single feature value for the perplexity feature for each user.,
https://dl.acm.org/doi/abs/10.1145/2700171.2791026,Detecting Changes in Suicide Content Manifested in Social Media Following Celebrity Suicides,Methods Measuring Post Volume,0,Our goal is to measure the change in both the quantity and quality of posts to SW following a celebrity suicide. First we will measure the volume of posts on SW preceding and succeeding a celebrity suicide to obtain a measure of increased interest in the topic of suicide. Next we will use a series of content analysis techniques to examine the nature of these posts: how the topic of posts changed in the wake of the suicide.,We begin by constructing a baseline as to the expected variability in posts by measuring pairs of subsequent two week periods. Deviations from these expected trends following a celebrity suicide would provide evidence for the Werther effect. Our goal is therefore to identify per celebrity a set of k consecutive two-week time period pairs in the entire timeframe of our data. We refer to the first item of the pair (i.e. the first two-week window) as the “preceding” window and the immediately following two-week window as the “succeeding” window. Collectively these baseline pairs yield an empirical distribution of the expected variation when there is not a celebrity suicide. Specifically each of the k two-week window pairs (1) have no reported celebrity suicide and (2) take periods that start on the same as the day of week. This accounts for day-ofweek related variations on SW. For the purposes of this paper we choose k as 20. 4.1.2 Developing a Control Next we develop a control to establish that changes in volume of posts in SW succeeding a celebrity suicide compared to that preceding it is attributed to the topic of suicide in particular and such changes are not part of a broader shift in interest in mental health topics. For this purpose we identified a set of “control group” subreddits which are on topics related to mental health but are unlikely to be specifically about suicide or suicidal ideation. These mental health subreddits (henceforth referred to as MH subreddits) were compiled based on our prior work in [11]; refer to the paper for details on how these subreddits are identified and crawled. Table 3 lists the control subreddits crawled in the same timeframe as SW. We obtained 32509 posts from 23807 unique users. Like SW all of these subreddits are public.,
https://aclanthology.org/W17-3110.pdf,Small but Mighty: Affective Micropatterns for Quantifying Mental Health from Social Media Language,Data,2,We briefly explain the data collection method here but we refer the interested reader with further questions on the methodology to Coppersmith et al. (2016) for the suicide attempt data and Coppersmith et al. (2014a) for all other conditions. The data for these analyses are Twitter posts collected via two methods. Most of the data come from users who have publicly discussed their mental health conditions. These users are frequently referred to as “self-stated diagnosis” users as they state publicly something like “I was diagnosed with schizophrenia” or “I’m so thankful to have survived my suicide attempt last year”. The data for users with a suicide attempt was supplemented by data from OurDataHelps.org a data donation site where people provide access to their public posts and fill out a short questionnaire about their mental health history. Data are then deidentified and made available to researchers addressing questions of interest to the mental health community. Donors provide consent for their data to be used in mental health research upon signup. Of the users who attempted suicide 146 came from OurDataHelps.org. Specifically we examine generalized anxiety disorder eating disorders panic attacks schizophrenia and attempted suicides. These conditions were selected based on the theory that there are important timing aspects to their symptoms – ebbing and flowing of symptoms as treatment is effective (especially schizophrenia) onset and exacerbation of symptoms by external events and stress and punctuated events in time of psychological symptoms (suicide attempts panic attacks and binging/purging behavior with eating disorders). We use the Twitter streaming API to collect a sample of users who used a series of mental health words or phrases in their tweet text (e.g. ‘schizophrenia‘ or ‘suicide attempt‘). Each tweet that uses one of these phrases is examined via regular expression to indicate that the user is talking about themselves. Finally those tweets that pass the regular expression are examined by a human to confirm (to the best of our ability) that their selfstatement of diagnosis appears to be genuine. This results in a dataset with users that have a self-stated diagnosis of generalized anxiety disorder (n = 2408) an eating disorder (749) panic attacks (263) schizophrenia (350) or someone who would go on to attempt suicide (423). Some of these users do not exhibit the sort of posting behavior required to create micropatterns (i.e. they rarely post multiple times within a 3 hour time window). We exclude these users from our analysis which is 5-9% of users for most conditions with the exception of those with a suicide attempt where a little over half the users do not exhibit this posting behavior. The resultant dataset used for analyses is: generalized anxiety disorder (n = 2271) eating disorders (687) panic attacks (247) schizophrenia (318) suicide attempts (157). In order to allow comparisons of each condition to control users we gather a random sample of 10000 Twitter users for whom at least 75% of their posts are identified by Twitter as English. All the users with a self-stated diagnoses and all members of this control population have their age and gender estimated according to Sap et al. (2014). For each user with a self-stated diagnosis we find a matched control through the following procedure: create a pool of users where the estimated gender matches and the estimated age is within the same 10-year bracket (the suggested accuracy of the age estimator). From that pool of age- and gender- matched users we select the user whose tweets start and end over the most similar timeframe. We will refer to these age- gender- and time-matched controls simply as “matched controls” throughout the rest of this paper. All tweets were publicly posted by their author (i.e. no users marked at “protected” or “private” were included). On average users had 2949 tweets. The distribution of estimated age and genders for users with each self-stated condition can be seen in Figure 1. For most conditions the population skews female though for schizophrenia the genders are roughly balanced. The average age tends to be in the early-to-mid 20s.,,
https://dl.acm.org/doi/abs/10.1145/2998181.2998220,Gender and Cross-Cultural Differences in Social Media Disclosures of Mental Illness,Twitter Data Handling Population and Internet Penetration Bias Expert Verification of Mental Health Disclosures,2,Our study uses publicly shared mental illness self-disclosure data collected from the social media Twitter. We started by obtaining a large sample of English language candidate selfdisclosure posts from the full archive of public Twitter data around a variety of mental health concerns. Specifically we filtered the Twitter posts shared in March 2015 containing any of the keyphrases included in Table 1. These keyphrases were collated by a combination of reference to prior work [11 12] and consultation with a trained psychiatrist practitioner. Through these keyphrases we sought to identify who publicly state that they have been diagnosed with or suffering from some form of mental illness. As noted by Coppersmith et al. users may make such a statement to seek support from others in their Twitter social network to fight the stigma of mental illness or perhaps as an explanation of some of their behavior [11]. We obtained 1319064 posts from 534829 unique users at the end of this initial data collection phase. Parallelly we obtained a candidate control data sample from Twitter’s Firehose stream so as to allow robust statistical comparisons between Twitter users who choose to self-disclose their mental illness and those who do not. This dataset included a random sample of 1513279 posts from 673898 unique users made on Twitter during March 2015 ensuring that none of these posts matched any of the keyphrases given in Table 1. Thereafter for both of the candidate mental health disclosure sample and the candidate control sample of posts we utilized Twitter’s official API3 to obtain the last 3200 posts for each of the unique users in both datasets. For the control dataset if any of the users had any post in their crawled posts matching one of the keyphrases above we disregard them from our analysis. Further employing the Google Compact Language Detector4  we disregarded any users if at least 75% of their posts were not written in English. Our final candidate disclosure sample contained 51038914 posts from 470337 users (µ = 108.5) while the control sample contained 66214850 posts from 480685 users (µ = 137.7).,We note that the populations of the four countries are widely different along with their overall reported internet penetration rates6 . Hence we devise a subsampling strategy to filter users belonging to one of the four countries from the set of users in the candidate disclosure and the control datasets with inferrable country information. First for population based subsampling we use the inverse of the population ranks of the countries4 as the respective rates of sampling. Then we use the internet penetration percentages of the countries5 to randomly sample that fraction of users from the population subsampled sets. In this manner across both the datasets we obtained 211132 Twitter users from the US 61816 users from the UK 10808 from IN and 5769 from ZA.,Next we qualitatively verified whether posts from these users indeed were self-disclosures of mental health challenges. For the purpose we consulted a licensed psychologist and also included two researchers who were familiar with mental health content shared on social media. Over a random sample of 100 posts complied from the timeline of 100 randomly selected MID users we obtained independent binary annotations on whether a post was likely to be related to mental health. The Fleiss’ κ for interrater agreement was found to be high .87 along with an accuracy of 96% in distinguishing users who engage in genuine disclosures from those who do not. This establishes adequacy of our approach. Table 2 gives some paraphrased mental health disclosure posts of Twitter users who were identified to be genuine mental health disclosers by our approach. In Figure 2 we show the pipeline of steps involved in our approach of arriving at this final dataset.
https://www.jmir.org/2019/6/e14199/,Detecting Signs of Depression in Tweets in Spanish: Behavioral and Linguistic Analysis,Data Collection and User Selection Methods Study Steps,2,The selection of the tweets and their users was based on the filtered real-time streaming support provided by the Twitter API. In the first step we selected the users who showed potential signs of depression on Twitter on the basis of the 20 most frequent words in Spanish expressed by patients suffering from depression in clinical settings. These words were jointly identified and selected by a psychologist and a family physician with clinical experience and were based on the definition and general features of depression according to the Diagnostic and Statistical Manual of Mental Disorders [42]. The list of words used and their English translations are shown in Textbox 1. During June 2018 1470000 tweets including 1 or more occurrences of the words listed in Textbox 1 were collected. From this collection of tweets and to select the users who publicly stated in the textual description associated to their profile that they suffered from depression all the profile descriptions including 1 or more occurrences of the word “depr” and all the possible derivations related to the word depression in Spanish such as “depre” “depresión” “depresivo” “depresiva” “deprimido” and “deprimida” were considered. From the 720 users who included 1 or more of these words in their description profile 90 users who stated they suffered from depression or were receiving treatment for depression were selected for the analysis. This selection was performed by a psychologist verifying that the statements were related to real expressions of depression excluding quotes jokes or fake ones. For each of these depressed Twitter users we collected all the most recent tweets from their timeline up to a maximum of about 3200 tweets. Thus a total of 189669 tweets were collected a figure that was reduced to 140946 after discarding the retweets. These 140946 tweets constituted the depressive users dataset. Examples of sentences appearing in the user profiles that were used for selecting the depressive users are: “Paciente psiquiátrico con depresión crónica” (Psychiatric patient with chronic depression; example of a profile sentence that indicates depression). “Colecciono errores traducidos a tweets depresivos y a uno que otro impulso de amor” (I gather errors translated into depressing tweets and into one or another love impulse; example of a profile sentence that does not indicate depression). Once the users with profile sentences indicating depression had been retrieved their Twitter timelines were collected. Only those users having in their timeline at least 10 tweets that suggested signs of depression were retained for further analyses. For each user the selection of these tweets was performed by manually inspecting the tweets of the user’s complete timeline in reverse temporal order starting from the most recent one to the oldest tweet of the timeline retrieved by means of the Twitter API . Finally a total number of 1000 tweets issued by the 90 depressive users suggesting signs of depression were detected and used for the analysis. This set of tweets provided us with the depressive tweets dataset which was used to analyze linguistic features of tweets showing signs of depression. It has to be mentioned that these 1000 tweets were not to be included in the depressive users dataset (see Figure 1). At the same time more than 97500000 tweets were also collected in June 2018: such tweets were gathered by listening to the public Twitter stream during this time span by only considering tweets with Spanish textual contents (as detected by Twitter language identification support). Given that Twitter requires more restrictive filters than just the language of the tweets we used a list of the most frequently used Spanish words (stopwords) to retrieve all tweets that included 1 or more of these words. The vast majority of Spanish tweets should match this criterion. A sample of 450 users who did not mention in their profile the word depression and its derivations were selected randomly from the 97500000 tweets. The complete timelines of these users were compiled (1141021 tweets) which were reduced to 712589 once retweets were removed. These 712589 tweets constituted the control dataset. To identify the language of a tweet we relied on the language automatically identified by Twitter for each tweet selecting tweets in Spanish. It has to be noted that these data can contain some tweets from unidentified depressive users.,This study was developed in 2 steps. In the first step the selection of users and the compilation of tweets were performed. A total of 3 datasets of tweets were created a depressive users dataset (made up of the timeline of 90 users who explicitly mentioned that they suffer from depression) a depressive tweets dataset (a manual selection of tweets from the previous users which included expressions indicative of depression) and a control dataset (made up of the timeline of 450 randomly selected users). In the second step the comparison and analysis of the 3 datasets of tweets were carried out.,This study was designed and developed in 2 steps with the aim of analyzing the linguistic patterns and behavioral features of Twitter users suffering from depression in comparison with the general population of Twitter users. The study was focused on tweets written in Spanish. In the first step the selection of users and the compilation of tweets were performed. Given the design and purpose of the study we decided to use the Twitter Application Programming Interface (API) [41]. Using this API 3 datasets of tweets were created: The depressive users dataset was made up of the timeline of 90 users who publicly mentioned on their Twitter profile that they suffer from depression. The control dataset was made up of the timeline of 450 randomly selected Twitter users. The depressive tweets dataset was constituted by a manual selection of tweets from the depressive users dataset which specifically included expressions indicative of depression. In the second step comparison and analysis of the 3 datasets of tweets (control depressive users and depressive tweets datasets) were carried out to spot their distinguishing features. In the rest of this section we will describe the methodology in detail. The flow diagram of the study is depicted in Figure 1.
https://aclanthology.org/W18-0608.pdf,Cross-cultural differences in language markers of depression online,Data Collection  ,1,Data was collected from 7 Cups of Tea an anonymous online chat-based peer support community for emotional distress1 . Users agree at signup that their data may be used for the purposes of research. All the data used for the current study was anonymous and securely stored. This research was performed in line with the ethical and privacy protocols outlined in detail in (Benton et al. 2017). Data from 7 Cups takes the form of written dialogue between users of the service and volunteers who are trained as “active listeners”. A fragment of an exchange between the user of the service (U) and the volunteer (V) might go as follows: For the analyses reported in this paper we used only text generated by users of the service not the volunteers providing peer support. Users who reported depression as their primary concern at sign up were eligible for inclusion in analyses. Our original sample was comprised of 23048 conversations involving 1937 unique users. Users were excluded from the sample if they did not indicate their culture or if they selected ‘Other’. This resulted in the exclusion of 199 and 130 users respectively. The original sample also included users identifying as Native American or American Indian. This group was excluded from analyses since the majority of the data among these users was not English. This resulted in the removal of 15 users leaving a total sample size of 1593.,,
https://dl.acm.org/doi/pdf/10.1145/3359169,Cross-Cultural Differences in the Use of Online Mental Health Support Forums,Method of Analysis,2,"Selection Criteria and Data Scope. To understand the impact of cultural differences on how individuals use online mental health platforms we begin our analysis by creating a dataset of users from different national communities on Talklife a support platform with over half a million users [91]. For this analysis due to the fact that most research in CSCW on mental health online has been done either agnostic of cultural context [12 34] or in a Western context [60 67 88] we choose to focus on users from non-Western countries following Zhang et al. [103]. As researchers located in the Global South and with lived experience interacting with the health system and diverse explanatory models [52] of mental illness we believe that moving the focus of CSCW and CSCWadjacent mental health research away from the West is crucial to better meet the needs of people often underserved by the medical system [70]. To create these subgroups of users we choose the three non-Western countries with the highest user populations on Talklife or India Malaysia and the Philippines. Guided by the rich amount of literature on the unique nuances to mental health expression for each country [35 62 77 80] we examine the national identity linguistic and behavior-based differences of use between each user subgroup. In particular this research notes that as a result of cultural norms around the sharing of distress and alternative conceptualizations of mental illness in India Malaysia and the Philippines symptoms are often expressed in somatic and religious terms as opposed to traditionally clinical or psychiatric terms. We choose to analyze each subgroup at the national level for both theoretical and practical reasons. On a theoretical level in past work in the medical anthropology of mental health national identity has commonly been used for a approximate level of analysis for cultural identity [31 33 52]. Additionally on a more practical level each user’s country was determined using their IP address by Talklife and shared with us in an user-anonymized dataset. Inferring a more precise location could potentially compromise user anonymity as discussed in past work [47] and did not seem to have any more significant value for our analysis of cultural differences than analysis at the national level. We analyze data from 10532 Indian users 3370 Malaysian users and 3370 Filipino users as shown in Table 2. Collectively we refer to these countries as the minority sample. As a comparison set we construct a random sample of all threads on Talklife and refer to it as the majority sample. Due to the relative prevalence of users from Western English-speaking countries in Talklife most of the threads in the majority sample include posts from countries such as the USA UK and Canada. Indians are the largest non-Western minority subgroup on Talklife. Data was sampled from May 2012 to June 2018. Following this cross-national analysis to see if our broader results on Talklife generalize to a differently structured online mental health community we picked the largest Western country (the United States) and the largest non-Western country (India) represented on 7Cups a similar support platform with more than 15000 users actively using the platform each week [7]. Using 7Cups data we repeat our analysis testing for the same cultural differences we found in our Talklife sample. For this analysis we were provided a sample of data on activity from 6055 Indian users and 18581 American users as shown in Table 2. Unlike our sample of Talklife users this dataset is not a random sample. There is an upsampling of Indian users to ensure that we have data from a sufficient number of Indians in the dataset. Like on Talklife Indians are the largest non-Western minority subgroup on 7Cups. We focus on Indian users due to a lack of sufficient data on users from Malaysia or the Philippines. Data was sampled from March 2014 - August 2018. 3.1.2 Defining Cultural Identity and Use of Clinical Language. In this work we examine the relationship between cultural identity and use of online mental health support forums. To do so we leverage Tomlinson’s definition of cultural identity as “self and communal definitions based around specific usually politically inflected differentiations: gender sexuality class religion race and ethnicity nationality"" [94] particularly looking at the aspect that of modern cultural identity that runs along national lines as delineated by Hall et al. [41]. As a diverse and amorphous form of identity cultural identity can often intersect and interact with other forms of identity including religious or ethnic identity. However in the absence of direct information about religious or ethnic identity based on the data available we use national identity as a proxy for cultural identity. Additionally following Schlesinger et al’s [83] call for more intersectional analyses and methods within HCI we also include analyses of adjacent and intersecting identities when relevant including religious identity. To analyze clinical language we use a broader definition of clinical language than just specific medical diagnoses. Following methods used in past work to analyze antidepressant related language [30] we create a dataset of clinical mental health language including unigrams bigrams and trigrams from a list of mental disorders as defined by the International Classification of Diseases (ICD-10) and Diagnostic and Statistical Manual of Mental Disorders (DSM-5) [100]. We also included all unigrams from the MacMillan Dictionary list of words used to describe illnesses and diseases both specifically for mental illness and general illness [1–3]. As a result we include unigrams like “night"" (from night terrors) or sleep (from “sleep disorder"") as these are often correlated with specific symptoms of mental illness or distress such as sleep issues or being awake at night [30]. This included any clinically common abbreviations for mental disorders such as OCD for “obsessive compulsive disorder"" or BPD for “borderline personality disorder."" Shorthand for disorders commonly used by online communities such as “pro-ana” (as used in pro-eating disorder communities) [22] were not included due to the difficulty in finding an exhaustive list of these terms across disorders. We choose to use terms from and associated with DSM and ICD categorized disorders as a result of the common usage of these frameworks globally [99]. Throughout our analysis of these varied factors we use µ to represent means and σ to represent standard deviations. 3.1.3 Constraints Limitations and Tradeoffs. Cultural identity can exist at many different and intersecting levels including subcultures and subcommunities within the larger umbrella of a cultural identity. As a result for the purpose of this analysis we had to adopt some constraints in order to do a meaningful and specific analysis. One large limiting constraint that we chose for this study is to use national identity at the state level as a proxy for cultural identity. Though a major and formative part of modern cultural identity as argued by both Hall [41] and Tomlinson [94] each country we analyze is incredibly diverse with many individual cultural identities that both intersect and diverge from a greater national identity [54 64 89]. A more rich analysis of these other forms of cultural identity is beyond the scope of this work but could lead to richer conclusions about the nature of cultural identity in online mental health support communities particularly with regard to cultural differences between users with the same national identity. Additionally to stay consistent between analyses as a result of a lack of data on users from Malaysia and the Philippines we only analyze users in India on 7Cups and extend these findings to the experience of being part of a minority group on an online mental health forum. We draw validity for these exploratory findings from similar consistent patterns we observe between Indian Malaysian and Filipino users but a deeper analysis with a larger dataset is likely necessary to determine when and for which minority communities these conclusions do not hold true. Additionally while we construct clinical language through use of the commonly used DSM and ICD both frameworks of illness categorization have significant limitations particularly in the countries we have selected. For example there are both mental health disorders that are culturebound [74] as well as mental health language that is used in different ways within the specific countries we analyze such as depression often being an umbrella term for all mental illnesses [53]. Additionally it is clear that online support communities often develop their own cultural norms and language around mental health [21 72] and a deeper understanding of how this plays out on Talklife and 7Cups is neither the focus nor within the scope of this work. In this work we intentionally use standard clinical and medical terms for mental health disorders in our analysis of clinical language. As detailed in past anthropological research [52] it is theorized that the use of medical and clinical language is representative of a medicalized explanatory model of illness and we frame use of this language across cultures as a approximate signifier of a greater awareness of the presence of a mental disorder as opposed to conceptualizing distress as “stress"" “tension"" or “depression"" [25 53 98]. For our analysis we strictly analyzed posts that were in the Latin alphabet with almost all posts on both Talklife and 7Cups being in English. However as both Malay [8] and Tagalog [82] are most commonly written in the Latin script and since it is common for users from India speakers to use romanized versions of Indian languages online [79] it is possible that a small minority of posts in our analysis were text in a different language. However as confirmed by only seeing English words used in our analysis of the top n-grams among each user subgroup it is clear that English is the predominant language on both platforms. Though beyond the immediate scope of this work a greater analysis of non-English code-switching on these platforms could lead to a deeper understanding of the impact of interactions on expression between users with the same national identity but different language preferences.",,
https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-018-2197-z,Automatic extraction of informal topics from online suicidal ideation,Suicide Watch,2,In this section we first present the data gathered and used in our analysis. Researchers interested in the code and the data are invited to contact the authors. Reddit is a website which enables users to aggregate rate and discuss news entertainment politics and many other topics. According to Alexa it is the 8th most popular website in the world. It was estimated by the Pew research center that 6% of online adults use Reddit [26]. The site is organized into a collection of “subreddits” each focused on a particular topic and administered by a collection of moderators. The subreddit r/SuicideWatch is a forum in which online users are encouraged to post their thoughts regarding suicide. At the time of our data collection it had over 58000 subscribers. Sometimes users express a preoccupation with the thought of suicide. Other times users discuss immediate plans to take their own life. These posts often contain a description of their mental state including depression reaction to stress their feelings of being alone and having a low self-esteem. While most online sources of data are notoriously noisy this particular subreddit is remarkably clean. Given the serious nature of the subreddit individuals are less likely to post harassing comments or off-topic remarks. When users post such comments the moderators of the subreddit quickly remove them. We collected all posts from its inception in 2008 to 2016. Each post is often commented on by other individuals. In this work we focused on the original post as it most often represents the suicidal ideation of a user and comments often represent emotional support from other users. We cleaned this data. First we removed empty posts in which the content had been deleted. Second we removed links and replaced them with the word “link”. Third we concatenated the text of the post to the title as many users begin their post in the title and continue in the body of the post. Finally we removed punctuation and other special characters. After cleaning this data we had 131728 posts with 27978246 words of which 84607 words were unique posted by 63252 unique users.,,
https://ieeexplore.ieee.org/abstract/document/8609647,Online Social Networks in Health Care: A Study of Mental Disorders on Reddit,Relationship Modelif Network (RMN),2,RMN is a recursive neural network designed to model relationships between pairs of entities from text [13]. Each relationship is represented at a given point in time as a vector of weights over K descriptors. Entities that form a relationship do not need to be of the same class: for example in this work we model the relationships between a user and the community in which she interacts. Each post or comment corresponds to a different instant. Words are represented as embeddings of dimension P that is each word w of a vocabulary V is a vector in RP. Users and communities are represented by embeddings of dimension U and C respectively. As in previous works we generated embeddings using GloVe [15]. The descriptors obtained from RMN are vectors in RP allowing us to find the closest words to each descriptor. The post's (or comment's) representation is denoted by vpost∈RP. This vector is the average of the word embeddings contained in the post. The representations of users and communities are denoted by vuser and vcomm. For each post or comment RMN takes as input a vector v∈RP+U+C obtained by concatenating vpostvuser and vcomm. These vectors are combined through the weights of the neural network to obtain a representation dt∈RK of the relationship between the user and the community at that particular time. RMN uses a smoothing parameter α∈(01) to avoid abrupt changes in the representation of the same relation in consecutive instants dt and dt−1. The descriptor array R∈RK×P is used for attempting to reconstruct the post vpost by making rt=R⊤dt. RMN parameters (weights and matrix of descriptors) are trained in order to maximize an objective function that aims to approximate rt and vpost while retaining some distance between rt and other randomly sampled posts. See [13] for more details on RMN. The input data for RMN was preprocessed as follows. First we removed all posts and comments marked as [deleted} or [removed] standard stop-words (using the NLTK library) punctuation and accents. Second for each subreddit we removed posts and comments from users who performed less than 50 activities (posts or comments) following the methodology presented in [16]. Finally we selected the words that appear at least once in each of the four subreddits analyzed seeking to find similarities in the way people express themselves when discussing mental health disorders. The final subset of data analyzed by RMN is composed of 18020 unique words 25101 posts and 401428 comments.,,