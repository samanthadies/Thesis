Link to paper,Title,Section,Score,Text,
https://link.springer.com/content/pdf/10.1140/epjds/s13688-017-0110-z.pdf,Instagram photos reveal predictive markers of depression,Feature Extraction,2,Several different types of information were extracted from the collected Instagram data. We used total posts per user per day as a measure of user activity. We gauged community reaction by counting the number of comments and ‘likes’ each posted photograph received. Face detection software was used to determine whether or not a photograph contained a human face as well as count the total number of faces in each photo as a proxy measure for participants’ social activity levels. Pixel-level averages were computed for Hue Saturation and Value (HSV) three color properties commonly used in image analysis. Hue describes an image’s coloring on the light spectrum (ranging from red to blue/purple). Lower hue values indicate more red and higher hue values indicate more blue. Saturation refers to the vividness of an image. Low saturation makes an image appear grey and faded. Value refers to image brightness. Lower brightness scores indicate a darker image. See Figure  for a comparison of high and low HSV values. We also checked metadata to assess whether an Instagram-provided filter was applied to alter the appearance of a photograph. Collectively these measures served as the feature set in our primary model. For the separate model fit on ratings data we used only the four ratings categories (happy sad likable interesting) as predictors.,
https://dl.acm.org/doi/abs/10.1145/2858036.2858207 ,Discovering Shifts to Suicidal Ideation from Mental Health Content in Social Media,Linguistic Interpersonal and Interaction Methods,1,Our first set of methods include developing three sets of measures spanning: linguistic structure interpersonal awareness and interaction. The choice of these measures is motivated by literature that examines associations between the behavioral expression of individuals and their responses to crises including vulnerability due to mental illness [13 23]. Each of these measure categories consists of the following variables: Linguistic Structure. For this measure we compute the fraction of nouns verbs2  and adverbs in posts and comments; automated readability index a measure to gauge the understandability of text [62]; and linguistic accommodation a process by which individuals in a conversation adjust their language style according to that of others [20]3 . Together these variables characterize the text shared by the user classes beyond their informational content. Per literature in psycholinguistics such structure is known to relate to an individual’s underlying psychological and cognitive state and can reveal cues about their social coordination [54]. Interpersonal Awareness. This measure category includes: proportion of first person singular (indicating pre-occupation with self) first person plural (indicating collective attention) second person and third person pronouns (indicating social interactivity and reference to people or objects in the environment). Literature has indicated that pronoun use can quantify an individual’s self and social awareness and can reveal mental well-being including that manifested in social media [21]. Interaction. Variables corresponding to this measure category include: volume of posts and comments authored post length length of comments authored volume of comments received on shared posts length of comments received mean vote difference (difference between upvotes and downvotes on posts authored) and response velocity (in minutes) given by the time elapsed between the first comment and the time the corresponding post was shared.,
https://dl.acm.org/doi/abs/10.1145/2702613.2732733,Detecting and Characterizing Mental Health Related Self-Disclosure in Social Media,Self-Disclosure Inference,1,Based on the training data thus created we pursued the use of supervised learning to develop a classifier which would indicate whether a post is of high low or no self-disclosure. We tested a variety of different classification techniques (decision trees k Nearest Neighbor naive Bayes). The best performing classifier was found to be a perceptron classifier with adaptive boosting used to amplify performance [17] whose results will be used in the remainder of this paper. We used the following feature generation rules: First we eliminated stopwords from each post based on standard list provided by Python’s NLTK library. Next we performed stemming using Porter Stemmer. We extracted uni- bi- and tri-grams from each post and considered those with five or more occurrences. We also computed two additional features – length of each post and whether the author of the post is an exclusive poster on mental health forums or is observed in our dataset to post on other forums as well. Thus each post was characterized by 1070 features. We used standard 10-fold cross validation (CV) to evaluate the classifier and ran our model over 100 random 10-fold CV assignments for generalizability of the results. We report the average accuracy precision recall F1 specificity as metrics of performance. We find that our classifier based on the perception model yields an average accuracy of 78.4% in detecting high or low self-disclosure with .74 precision and .86 recall (see Table 4 for details). Other methods like k-NN (k=5) give higher precision but at the expense of very low recall. Figure 1 gives the ROC (receiver operating characteristic) curves for all the models. Per the ROC curve corresponding to the perceptron model we find it to yield the maximum area under curve (.81) hence best performance. We further identify in Table 5 the n-grams (or features) with the highest weights given by the perceptron – it implies these features were the most significant in the classification task. We provide some brief qualitative examinations of these n-grams in the light of prior psychology literature on selfdisclosure and mental health [10 11]. We find that the ngrams primarily are associated with vulnerable and selfloathing thoughts (e.g. thoughts of suicide) bear a negative tone or depict confessional experiences. Based on prior research [10 11 12] and our own work on mental health discourse on Reddit [4] we find that these are the topical dimensions along which high self-disclosure and low/no selfdisclosure posts vary. In essence high self-disclosure posts share extensively their personal beliefs and fear for instance their vital constructs and private sensitive informational attributes. The post excerpts below have been classified to be of high self-disclosure and through them we demonstrate the use of some of the n-grams in Table 7: “I don’t want to kill myself I haven’t felt suicidal in a long time but I just want to stop life for a while you know?” “My dad would beat the living shit out of me […]. I’ve been to the hospital so many times I’ve lost track” “I hate this. I hate myself. I don't want to f****** be this person anymore. I'm unmotivated unfocused immature.”,
https://dl.acm.org/doi/abs/10.1145/2702123.2702280,Recognizing Depression from Twitter Activity,Features for Reconstructing Depression,1,It is possible to extract various features from the activity histories of Twitter users. This section explains what kinds of features are used to estimate degree of depression and the way in which these quantities are extracted. Table 2 shows the features used in this study. A detailed explanation of each feature follows. The frequencies of words in a tweet (i.e. its bag of words) are used as a basic feature relating to the content of the tweet. Tsugawa et al. showed that the word frequencies are useful for identifying depression [37]. MeCab [20] was used to for morphological stemming and categorization of the Japanese tweet text to obtain accurate word frequencies. Particles auxiliary verbs adnominal adjectives and visual symbols were excluded for extracting content words. Words used by only one participant were also excluded resulting in a total of 84255 distinct words. However most of these words were rarely used and the distribution of word frequencies is extremely biased (see Fig. 4). Because words with a low rate of use were regarded as unlikely to be associated with depression for most users the frequencies of only the 20000 words with the highest rate of use (corresponding to 25 or more uses across all participants) were used as a feature in this study. Furthermore because the number and length of tweets differed by participant the word frequencies were normalized by the total number of words in the tweets. The topics of the tweets of each user as estimated by using a representative topic model LDA [5] were used as a second feature relating to the content of the tweets. With LDA the distribution of topics in each document is estimated from the word frequencies in each text through unsupervised learning on the assumption that the text and the words in it are generated according to a particular topic [5]. In LDA the number of topics to identify and a set of documents (as bags of words) are used as input and a topic distribution is output for each document. As mentioned in Related Work Section the topics of essays written by university students were estimated by using LDA and found to be useful in evaluating degree of depression [31]. From that study topics are expected to be a useful feature. A set of all tweets of each user was used as the user document for input in LDA and the 20000 words selected as described above were used as the words. We used LDA with collapsed Gibbs sampling [16]. As the parameters of LDA we used α = 50/K and β = 0.1 where K is the number of topics [16]. All extracted topics were used as the features. The ratio of positive words and the ratio of negative words used in the tweet text are used as the final features relating to tweet content. Users with depression are intuitively expected to use negative words more frequently than users without depression do. To categorize words a dictionary of affective words [19] which is compiled by manual evaluation of a dictionary of positive and negative words extracted according to a technique proposed in the literature [35] is used. The dictionary contains 760 positive words and 862 negative words. The user’s timing of tweets frequency of tweets average number of words retweet rate (rate of republishing other users’ tweets) mention rate (rate of directly referencing at least one other user) ratio of tweets containing a uniform resource locator (URL) number of users being followed and number of users following are used as features independent of the content of the tweet. The relative ratios of tweets posted during each hour of the day were used to characterize the timing of tweets; the number of posts per day was used as the posting frequency; and the ratio of qualifying tweets to all tweets were used for the retweet ratio mention ratio and ratio of tweets containing a URL. These features are used in prior research [14].,
https://www.nature.com/articles/s41598-017-12961-9,Forecasting the onset and course of mental illness with Twitter data,Improving Data Quality,0,In an effort to minimize noisy and unreliable data we applied several quality assurance measures in our data collection process. MTurk workers who have completed at least 100 tasks with a minimum 95% approval rating have been found to provide reliable valid survey responses33. We restricted survey visibility only to workers with these qualifications. Survey access was also restricted to U.S. IP addresses as MTurk data collected from outside the United States are generally of poorer quality34. All participants were only permitted to take the survey once. We excluded participants with a total of fewer than five Twitter posts. We also excluded participants with CES-D scores of 21 or lower (depression) or TSQ scores of 5 or lower (PTSD). Studies have indicated that a CES-D score of 22 represents an optimal cutoff for identifying clinically relevant depression3536; an equivalent TSQ cutoff of 6 has been found to be optimal in the case of PTSD32. We note here that in the study that inspired the present work De Choudhury et al.8 used two depression scales (CES-D and BDI) and filtered individuals whose depression score did not correlate across the both scales. This additional criteria is a methodological strength of De Choudhury et al.8 with respect to the present work.,
https://aclanthology.org/W14-3214.pdf,Towards Assessing Changes in Degree of Depression through Facebook,Degree of Depression,2,We estimated user-level degree of depression (DDep) as the average response to seven depression facet items which are nested within the larger Neuroticism item pool. For each item users indicated how accurately short phrases described themselves (e.g. “often feel blue” “dislike myself”; responses ranged from 1 = very inaccurate to 5 = very accurate). Figure 1a shows the distribution of surveyassessed DDep (standardized). The items can be seen in Table 1. Figure 2 shows the daily averages of surveyassessed DDep collapsed across years. A LOESS smoother over the daily averages illustrates a seasonal trend with depression rising over the winter months and dropping during the summer.,
https://ieeexplore.ieee.org/abstract/document/6784326,Affective and Content Analysis of Online Depression Communities,Feature Extraction,0,To characterize the difference between CLINICAL and CONTROL communities a variety of features are extracted: Affective features: We use the lexicon—Affective Norms for English Words (ANEW) [5]—to extract the sentiment conveyed in the content. This lexicon consists of 1034 words rated in terms of valence and arousal and is thus suitable for a quantitative estimation. The valence of ANEW words is on a scale of 1 (very unpleasant) to 9 (very pleasant). The arousal is measured on the same scale—1 (least active) to 9 (most active). A cloud visualization of ANEW words used in the blog posts made by CLINICAL and CONTROL groups is illustrated in Fig. 1. Mood tags: LiveJournal provides a mechanism for users to tag their posts from a list of 132 pre-defined mood labels.4 Thus in addition to the emotion expressed in the text of posts the mood tag produced allows us direct access to the user sentiment. A cloud visualization of moods tagged on blog posts made by CLINICAL and CONTROL communities is llustrated in Fig. 2. LIWC features: We examine the proportions of words in psycholinguistic categories as defined in the LIWC package [27]: linguistic social affective cognitive perceptual biological relativity personal concerns and spoken.5 Table 3 presents the mean of these LIWC psycholinguistic processes for the CLINICAL and CONTROL communities. Whilst similar in the use words with positive emotion people in the CLINICAL communities tend to use words with more negative emotion—as examples anxiety anger and sadness. Further they discuss more issues about health and death in comparison with the CONTROL group. On the other hand the users in the CONTROL group discuss more neutral life related topics—ingestion home and leisure words. Topics: For extracting topics latent Dirichlet allocation (LDA) [4] is used as a Bayesian probabilistic modelling framework. LDA extracts the probabilities —that is words in a topic and then assigns a topic to each word in a document. For the inference part we implemented Gibbs inference detailed in [10]. We set the number of topics to 50 run the Gibbs for 5000 samples and use the last Gibbs sample to interpret the results.,
https://www.jmir.org/2017/7/e243/,Assessing Suicide Risk and Emotional Distress in Chinese Social Media: A Text Mining and Machine Learning Study,Language Features,0,Weibo posts were segmented using the Stanford word segmenter [39] that resulted in 349374 words and phrases. Thereafter the SC-LIWC [33] dictionary was applied to count the appearance of each category of words in every respondents’ Weibo posts. The SC-LIWC dictionary includes 7450 words that are grouped into 71 categories including 7 main linguistic or psychological categories and 64 subcategories. In addition the total number of words or phrases that each respondent published in the 12 months was counted as the 72nd category. Scores of the SC-LIWC categories were counted as percentages of the total number of words.,
https://dl.acm.org/doi/abs/10.1145/3025453.3025909,A Social Media Based Index of Mental Well-Being in College Campuses,Methods,0,We now present a methodology of identifying posts shared in university subreddits that that are likely to be mental health expressions. Note that our Reddit data does not contain any gold standard information around whether a post shared in a university subreddit is about one’s mental health experience or condition. Our proposed method overcomes this challenge by employing an inductive transfer learning approach [16]. First we include (as ground truth data) Reddit posts made on various mental health support communities. Prior work has established that in these communities individuals selfdisclose a variety of mental health challenges explicitly [50]. Parallelly we utilize another set of Reddit posts made on generic subreddits unrelated to mental health to be a control. Next we build a machine learning classifier to distinguish between these two types of posts. Then we learn features that could detect whether an post shared in a university subreddit could be an expression of some mental health concern. We discuss these steps in detail in the following subsections.,
https://ieeexplore.ieee.org/abstract/document/7752434,MIDAS: Mental illness detection and analysis via social media,Feature Extraction,2,In this work we are focused on two main type of features (linguistic and behavioral). TF-IDF is adopted to model the linguist features of patients and Pattern of Life Features (PLF) adopted from the work of Coppersmith et al. [1] is used to model the behavioral style of patients. TF-IDF Features To capture the frequent and representative words used by the patients TF-IDF is applied on the unigram and bigrams collected from all the patients' tweets. Pattern of Life Features (PLF) These features reveal the emotional patterns and behavioral tendency of users by measuring polarity emotion and social interactions. In order to fully compose the PLF we combined the following list of features: Age and Gender: Twitter does not publicly provide information about the age and gender of its users mainly due to privacy concerns so we adopted the work of Sap et al. [5] to fill in this information. Polarity Features: The Sentiment140 API 3 was used to label each tweet as either positive negative or neutral. The polarity is furthermore transformed into five different values to capture the affective traits of each user: 1) Positive Ratio: the percentage of positive tweets 2) Negative Ratio: the percentage of negative tweets 3) Positive Combo: captures the mania and hypomania traits of patients which is determined by the number of continuous positive posts appearing more than x amount of times within a period of time in minutes T. 4) Negative Combos: captures the depression traits of patients and is determined by the number of continuous negative posts appearing more than x amount of times within a period of time in minutes T. 5) Flips Ratio: quantifies the emotional unstableness and is determined by counting how frequently two continuous tweets with different polarity (either positive to negative or negative to positive) appear together within a period of time in minutes T. In our work x is set to 2 and T is set to 30 minutes. Social Features: These features can demonstrate how users are behaving with respect to their environment. The following are the social features designed for each user: 1) Tweeting Frequency; the frequency of daily posts; 2) Mention Ratio: the percentage of posts which contain at least one mention of another user; 3) Frequent Mentions: the number of Twitter users mentioned more than three times which is a measurement of how many close friends a particular user may have; 4) Unique Mentions: the number of unique users mentioned which is a measure of the width of a user's social network.,
https://www.nature.com/articles/s41598-020-68764-y,A deep learning model for detecting mental illness from user content on social media,The Data Preprocessing Procedure,0,The data pre-processing procedure for the collected post data is presented in Fig. 1. After collecting the data each title was combined with its corresponding post. We removed unnecessary punctuation marks and white spaces for each post. Then we used the natural language toolkit (NLTK) implemented in Python to tokenize users’ posts and filter frequently employed words (stop words). Porter Stemmer a tool used to define a series of guidelines for exploring word meaning and source was employed on the tokenized words to convert a word to its root meaning and to decrease the number of word corpus. After this procedure data from 228060 users with 488472 posts in total were employed for the analysis.,
https://aclanthology.org/W19-3013.pdf,Mental Health Surveillance over Social Media with Digital Cohorts,Building Difital Cohorts,1,Our cohort construction process entails two key steps: first randomly selecting a large sample of Twitter users; and second annotating those users with key demographic attributes. While such attributes are not provided by the API automated methods can be used to infer such traits from data (Cesare et al. 2017). Following this approach we develop a demographic inference pipeline to automatically infer age gender race/ethnicity and location for each cohort candidate. Age Identifying age based on the content of a user can be challenging and exact age often cannot be determined based on language use alone. Therefore we use discrete categories that provide a more accurate estimate of age: Teenager (below 19) 20s 30s 40s 50s (50 years or older). Gender The gender was inferred using Demographer a supervised model that predicts the (binary) gender of Twitter users with features based on the name field on the user profile (Knowles et al. 2016). Race/Ethnicity The standard formulation of race and ethnicity is not well understood by the general public so categorizing social media users along these two axes may not be reasonable. Therefore we use a single measure of multicultural expression that includes five categories: White (W) Asian (A) Black (B) Hispanic (H) and Other. Location The location was inferred using Carmen an open-source library for geolocating tweets that uses a series of rules to lookup location strings in a location knowledge-base (Dredze et al. 2013). We use the inferred location to select users that live in the United States. The age and race/ethnicity attributes were inferred with custom supervised classifiers based on Amir et al. (2017)’s user-level model. The classifiers were trained and evaluated on a dataset of 5K annotated users attaining performances of 0.28 and 0.41 Average F1 respectively. See the supplemental notes for additional details on these experiments1 .,
https://link.springer.com/chapter/10.1007/978-3-319-67186-4_6,Social Media Mining to Understand Public Mental Health,Topic Modeling,2,First we removed journals with no text and those with fewer than 20 characters1 leaving 1.1 million journals for topic modelling. Next we pre-processed the text using the Stanford Tweet Tokenizer which is a “Twitter-aware” tokenizer designed to handle short informal text [1]. We used the option that truncates characters repeating 3 or more times converting phrases such as “I’m sooooo happyy” to “I’m soo happyy”. On average the number of tokens per journal was 27.7. Since we are interested in topics we removed stopwords and tokens with fewer than two letters and we only retained nouns which appear in the WordNet corpus [10]. After this filtering the average number of nouns per journal was 7. Examples of frequently appearing nouns in alphabetical order include “anxiety” “class” “dinner” “family” “god” “job” “lunch” “miss” “school” “sick” “sleep” and “work”. We then iteratively clustered the journals into topics (details below) and removed nouns that do not refer to topics such as numbers timings (e.g. “today” “yesterday”) general feelings (e.g. “feel” “like”) proper nouns and nouns that have ambiguous meanings (e.g. “overall” “true”). Lastly we only retained nouns that appeared more than ten times in the dataset. This process resulted in a vocabulary of 8386 words for topic modelling. Each journal is represented as a 8386-dimensional term frequency vector with each component denoting the term-frequency/ inverse-document-frequency (TF-IDF) of the corresponding term. Algorithm 1 summarizes our topic modelling methodology. Given a TF-IDF term frequency vector for each journal we run non-negative matrix factorization (NMF) [8] implemented in Python’s scikit-learn package [12]. The objective of NMF is to find two matrices whose product approximates the original matrix. In our case one matrix is the weighted set of topics in each journal and the other is the weighted set of words that belong to each topic. Hence each journal is represented as a combination of topics which are themselves composed of a weighted combination of words. We chose NMF because its non-negativity constraint aids with interpretability. In the context of analyzing word frequencies negative presence of a word would not be interpretable. This is because we only track word occurrences and not semantics or syntax. Unlike other matrix factorization methods NMF reconstructs each document from a sum of positive parts which enables us to easily manually label the discovered topics. Iterating from 4 to 40 topics we derived 37 different topic matrices (steps 1 and 2 of Algorithm 1). Each matrix consists of one topic per row. Each topic has a positive weight for each word in the vocabulary. Stronger weights indicate higher relevance to the topic. The final topic matrix we used has 14 topics and is shown in Table 1. We show the first six words in this table for simplicity where we sorted the words associated with each topic from highest relevance to lowest. When judging the topic matrices we considered the top twenty most important words per topic. Using this information we manually labeled each row in the matrix with a corresponding topic. Furthermore we manually evaluated each matrix based on the distinctness between topics consistency within topics and interpretability. During this process we compiled a custom list of removed words that we mentioned earlier in this section. The groups of words we removed appeared as stand-alone topics that did not offer information about what the journal was about. For example proper nouns appeared as a stand-alone topic. Other words which we deemed too general or ambiguous appeared across several topics and hence did not provide discriminative information. We tested different levels of regularization to enforce sparseness in our models (see [8] for a discussion) but did not find significant differences. However one important modification we made to regularize each topic was to make their first words only as strong as their second ones (by default first words are stronger than second words which are stronger than third words and so on). This is since the most relevant word for each topic tended to be too strong of a signal regardless of how we changed the number of topics pre-processing procedure or regularization in the objective function. For example the word “love” in a journal about sports would be so strong that the journal would be labeled as relating to romantic love. Lowering the importance of first words was sufficient to eliminate the false positives we identified. Given the final topic matrix (summarized in Table 1) the next step is to use it to assign labels to journals (steps 3 and 4 of Algorithm 1). We plotted the distribution of how important each topic was to all journals in the dataset with importance ranging from zero to one. Each distribution had a similar shape with a clear inflection point between 0.05 to 0.15 importance. Figure 4 shows an example importance distribution for the topic “Work” where the inflection point occurs at 0.1 importance.,
https://dl.acm.org/doi/abs/10.1145/2556288.2557214,Seeking and sharing health information online: comparing search engines and social media,Definitions,1,We first introduce several measures used in our analysis. Relative use – For a given health condition its relative use on Twitter or the search engine is given by the ratio of its volume of use in that medium to the volume of the health term that is most used in that medium. For example the most searched condition on the search engine was “cancer” appearing in 31443735 queries and the second-most queried was “pregnancy” found in 25721056 queries; the relative use for “cancer” on the engine=1 it was .82 for “pregnancy.” Rank – A relative ranking based on normalized relative mention of a health condition (on Twitter or the search engine)—lower ranks mean greater use. For example on Twitter the most-used term from our list is “headache” that appeared in 24607507 posts; it would receive a rank=1. Rank difference – We compute rank difference between the search engine and Twitter for each health term – large negative values indicate that a term is searched relatively more than tweeted whereas large positive values mean the reverse. For example “cough” has rank 4 on Twitter and rank 26 on the search engine – its rank difference is 22 reflecting its relative prominence on Twitter as compared to search. Based on these definitions in Table 1 we report the top 20 most searched and most shared health conditions in terms of their rank and relative use on each platform. The table also shows the top 20 most positive rank difference (relatively more tweeted than searched) and most negative rank difference (relatively more searched than tweeted) conditions along with severity/type and stigma level of each conditions.,
https://www.aaai.org/ocs/index.php/ICWSM/ICWSM14/paper/viewPaper/8075,Mental Health Discourse on reddit: Self-Disclosure Social Support and Anonymity,General Linguistic Attributes,0,To understand the nature of self-disclosure in reddit posts we first examine the general linguistic attributes manifested in their content. In Table 3 we first present a list of the most popular (stopword eliminated) unigrams that appear in reddit postings. We intended to look at these highly shared unigrams more deeply and systematically hence we organized these unigrams (stopword inclusive) in various semantic categories provided by the psycholinguistic lexicon LIWC (http://www.liwc.net/). We find that among the unigrams in Table 3 there are words that extensively span emotional or affective expressions (happy love bad anxiety good hate) e.g.: “I've been recently wondering if my love to numb the world around me has turned me into an alcoholic…” “Has anyone else battled numbness loss of feeling during recovery? Does it ever get better? i've been sober for about 2 years and still have pretty severe anxiety at times”. We observe presence of relationships and social life words too (family friends people person parents) e.g.: “i get really anxious out when i go home for big events.” “i do love my family they're just really loud and argumentative sometimes”. Temporal indicators in reddit discourse is also visible— e.g. time day years months: “hi all i'm ten weeks sober today and while i wish i could say i'm physically and mentally in great shape the truth is i judge my days by how i feel less bad as oppose to good”. Work and daily grind oriented words are common as well because lifestyle irregularities are often associated with the psychopathology of mental illness (Prigerson et al. 1995)—e.g. life school work job: “I am completely broke can't afford rehab and can't take time off work”. We also find a fair number of cognitive words in these highly used unigrams (felt hard feeling lot) e.g.: “I'm new here but having anxiety like I haven't felt in a long time” “I find a lot of strength in going to a concert. I have been understanding of my anxiety and depression since i was about 8 and i hated it”. These observations are supported by psychology literature where cognitive biases as manifested through dysfunctional attitudes depressive attributional biases and negative automatic thoughts were found to be characteristic of mental illness (Eaves & Rush 1984). Further inhibition words like avoid deny safe demonstrate that redditors are perhaps using the platform to broadcast their thoughts to an audience of strangers or weak ties on issues and topics they might consider to be socially stigmatic to be discussed elsewhere: “i can't escape the feeling of fright i have at all times. i don't feel safe in my own home” “ive been denying my (assumed) depression symptoms for close to two years now writing them off …” Comparing across different LIWC semantic categories over all posts we observe noticeable differences— KruskalWallis one-way analysis of variance indicated the differences across categories to be significant (χ2 (39; N=20411)=9.24; p<10-4). Table 4 reports the top 8 most common LIWC categories the mean proportion of words from each category in the posts and the corresponding standard deviation. Note that the percentages over all categories sum to greater than 100% since a word could belong to multiple categories. Observing closely many of the categories whose corresponding unigrams appeared in Table 3 are also highly prominent categories globally over all posts as given in Table 4. Not shown in Table 4 perhaps intuitively negative emotion anger and sadness words were considerably more prominent than positive emotion words (a Wilcoxon signed rank test reveals that the differences are statistically significant (z=-6.08 p<.001)). Likely these redditors experience several negative emotions: hence mental instability helplessness loneliness restlessness manifest in their postings (Rude et al. 2004). the health and social issues they are facing. In fact high selfattentional focus is a known psychological attribute of mental illness sufferers (Chung & Pennebaker 2007).,
https://dl.acm.org/doi/abs/10.1145/3025453.3025932,Modeling and Understanding Visual Attributes of Mental Health Disclosures in Social Media,Visual Features,2,Towards our first research goal RQ 1 to examine the visual features of images relating to mental health disorders we employ the extraction of color profiles i.e. grayscale histograms [50]. Grayscale histograms provide us intuition about the brightness saturation and contrast distribution of images. In these histograms images with high contrast pixels are binned in bins with lower numbers (near 0) whereas images with brighter pixels are binned in higher number bins (near 255). We utilize the OpenCV library2 to extract these color histograms of images in our dataset. We also assess the visual saliency of images (using OpenCV) – a distinct subjective perceptual quality that makes some images stand out from their neighbors [24]. A typical image in our dataset is of size 612px × 612px so by using a saliency metric we obtain a 612 × 612 grid matrix. For each image in these three visual feature categories we obtain an empirical threshold that ensures 1/3 rd of the pixels will be greater than this value when sorted based on their saliency.,
https://www.sciencedirect.com/science/article/pii/S0747563215300996,A content analysis of depression-related tweets,Tweets Related to Depression,0,Tweets about depression were collected by Simply Measured a company that specializes in social media measurement and analytics (Simply Measured 2014). Simply Measured has access to the Twitter “firehose” (or full volume of tweets) via Gnip a licensed company that can retrieve the full Twitter data stream. All tweets in the English language that contained at least either “depressed” “#depressed” “depression” or “#depression” were collected between April 11 and May 4 2014. We scanned a random sample of the tweets to identify common phrases that included our keywords of interest but were not about mental health. In SAS version 9.3 (SAS Institute Inc. Cary NC) we used the index function which searches a character expression (in this case the text of the tweet) for a specific string of characters to locate and remove such tweets from our sample. We removed tweets that included the following terms regardless of capitalization: “Great Depression” “economic depression” “during the depression” “depression era” “tropical depression” and “depressed real estate”. The popularity and influence of the Tweeters was described using the distribution of followers and Klout Scores. While number of followers is a measure of popularity Klout Score is a measure of influence. Klout Scores range from 0 to 100 with a higher score indicating higher influence. Klout Score is calculated based on an algorithm that considers over 400 signals from eight different online networks. Examples of signals include the amount of retweets a person generates in relation to the amount of tweets shared and the amount of engagement a user drives from unique individuals (e.g. lots of retweets from different individuals as opposed to lots of retweets from one person) (Klout Inc. 2014).,
https://aclanthology.org/W15-1202.pdf,Quantifying the Language of Schizophrenia in Social Media,Character n-grams,0,Character n-gram language models are models built on sequences (n-grams) of characters. Here we use 5-grams: for all the tweets a user authored we count the number of times each sequence of 5 characters is observed. For example for this sentence we would observe the sequences: “for e” “or ex” “r exa” “ exam” and so on. The general approach is to examine how likely a sequence of characters is to be generated by a given type of user (schizophrenic or non-schizophrenic). To featurize character n-grams for each character 5-gram in the training data we calculate its probability in schizophrenic users and its probability in control users. At test time we search for sets of 50 sequential tweets that look “most schizophrenic” by comparing the schizophrenic and control probabilities estimated from the training data for all the 5-grams in those tweets. We experimented with different window sizes for the number of tweets and different n for n-grams; for brevity we report only the highest performing parameter settings at low false alarm rates: 5-grams and a window size of 50 tweets. An example of this can be found in Figure 1 where one schizophrenic and one control user’s score over time is plotted (top). To show the overall trend we plot the same for all users in this study (bottom) where separation between the schizophrenics (in red) and control users (in blue) is apparent. The highest score from this windowed analysis becomes the feature value. Note that this feature corresponds to only a subset of a user’s timeline. For schizophrenia sufferers this is perhaps when their symptoms were most severe a subtle but critical distinction when one considers that many of these people are receiving treatment of some sort and thus may have their symptoms change or subside over the course of our data.,
https://dl.acm.org/doi/abs/10.1145/2700171.2791026,Detecting Changes in Suicide Content Manifested in Social Media Following Celebrity Suicides,Wikipedia Data Linguistic Measures,2,Next we compiled a list of reported celebrity suicides which fell within the time range of our Reddit data. Defining who is a “celebrity” is nontrivial so we refer to the Wikipedia page listing celebrity suicides7 as a way to measure who has sufficient celebrity status for inclusion. We obtained 10 reported celebrity suicides in the same period as our Reddit data; their names and reported suicides are shown in Table 2. We measure the prominence of a celebrity’s death by measuring the change in Wikipedia page views for the celebrity’s Wikipedia page. Wikipedia provides daily page view statistics for each page.8 We compare the number of page-views in the two weeks prior to their death with the two weeks following their death in terms of z-score (Figure 1). Here z-scores are computed by converting the page views to standard normal variable with 0-mean and standard deviation of 1. For 9/10 of the cases we see a notable spike in number of views showing that the suicides of these individuals were well-known enough to be viewable on such a macro scale and for examining the presence of Werther Effect in social media. We note two aspects related to the above analysis and which will be used through the rest of this paper. First since we are focusing on different types of data sources—Wikipedia and Reddit we use z-score conversion as a normalization technique for the Wikipedia page views and Reddit’s SW posting activity volume. Further the above observation in Wikipedia data and the analyses that ensue focus on observing changes over a two week window preceding and succeeding a celebrity suicide; this choice is motivated by our initial analyses and from the literature on Werther Effect [17].,We propose four categories of linguistic and non-linguistic attributes to examine preceding/succeeding celebrity suicides. These are: (1) affective attributes (2) cognitive attributes (3) linguistic style attributes and (4) social attributes. Measures belonging to all of these attribute categories are largely based on the psycholinguistic lexicon LIWC [40] and were motivated from prior literature that examine associations between the behavioral expression of individuals and their responses to traumatic context and crises including vulnerability due to mental illness [7 11]. Note that LIWC has been extensively validated to perform well on Internet language [7 18]. (1) We consider two measures of affect derived from LIWC: positive affect (PA) and negative affect (NA) and four other measures of emotional expression: anger anxiety sadness and swear. (2) We use LIWC to define the cognitive measures as well: (a) cognition comprising cognitive mech discrepancies inhibition negation death causation certainty and tentativeness; and (b) perception comprising set of words in LIWC around see hear feel percept insight and relative. (3) Next we consider four measures of linguistic style: (a) Lexical Density: consisting of words that are verbs auxiliary verbs nouns adjectives (identified using NLTK’s [2] POS tagger) and adverbs. (b) Temporal References: consisting of past present and future tenses. (c) Social/Personal Concerns: words belonging to family friends social work health humans religion bio body money achievement home and sexual. (d) Interpersonal Awareness and Focus: words that are 1st person singular 1st person plural 2nd person and 3rd person pronouns. (4) For social attributes we utilized a variety of content sharing social interaction and social support indicators. These are: post length number of comments vote difference (difference between upvotes and downvotes divided by total upvotes and downvotes) comment arrival rate (average time difference between any two subsequent comments in a post’s comment thread) time to first comment (time elapsed between the first comment and the timestamp of the corresponding post) and median comment length9 . We compute each of the above linguistic measures of behavior at the post level – the value of a measure is given by the ratio of the number of words in a post that match words belonging to the measure to the total number of words in the post. For each measure we take the average across all celebrities to ensure each suicide event is equally weighted i.e. to avoid skew due to a single suicide. For statistical comparison we used the Welch t-test; a negative tstatistic value means the measure increased after suicide.
https://aclanthology.org/W17-3110.pdf,Small but Mighty: Affective Micropatterns for Quantifying Mental Health from Social Media Language,Methods,1,This study aimed to examine the prevalence of affective micropatterns in social media posts and highlight differences in micropattern occurrence that might be relevant to quantifying mental health. Primarily we do this through comparison of users with anxiety disorders eating disorders schizophrenia suicide attempt history and their matched controls. We use a straightforward and well-understood method for sentiment analysis VADER (Hutto and Gilbert 2014) to produce a trinary label for each message: positive neutral or negative. VADER outputs a [0 1] score for each sentiment label; we use the label with the maximum score. Specifically we examined trajectories of posted emotional content in three subsequent tweets no more than three hours from earliest to latest. The same tweet will be counted in more than one over lapping micropattern if more than three tweets occur in the three-hour time window – so if 5 tweets occur in 3 hours 3 micropatterns will be recorded from those 5 tweets likewise for 4 tweets 2 micropatterns will be recorded. The potential overlap exists for both patients and neurotypical users and subsequent analyses (e.g. classifying users based on proportion of micropatterns) were designed to be robust to this property of overlapping micropattern generation. The number of sequential tweets to examine was chosen to minimize the complexity of the analysis while allowing significant variability to be observed. Critically we aimed for the resulting dimensions (i.e. number of distinct micropatterns) to be small enough for meaningful interpretation by clinical psychologists.,
https://dl.acm.org/doi/abs/10.1145/2998181.2998220,Gender and Cross-Cultural Differences in Social Media Disclosures of Mental Illness,Quantifying Differences in Disclosure Obtaining Genuine Mental Health Disclosures,2,In this subsection we present methods to quantify the differences between the disclosure characteristics of females and males and individuals reported to be from one of the four countries of interest: US GB IN and ZA in the MID dataset. Linguistic Measures Language is a powerful source of expression [26]. A rich body of work such as Boroditsky et al. [6] showed how the perception of objects in different languages can relate to as well as impact one’s social and pscyhological status. It is recognized that language specifically one’s native language shapes and drives one’s thoughts actions and social relationships [10]. Further it is established that cross-cultural and sex differences exist in one’s underlying thought processes [21 55]. For instance according to Kovecses [ ¨ 31] cultural models are known to define one’s emotional concepts. To quantify gender and cross-cultural dimensions in the language of individuals who engage in mental health disclosure on social media we propose three categories of measures: (1) affective attributes (2) cognitive attributes and (3) linguistic style attributes. Measures belonging to all of these attribute categories are largely based on the psycholinguistic lexicon LIWC [42] and were motivated from prior literature that examines associations between the behavioral expression of individuals and their psychological distress including vulnerability to mental illness [9 14]. Specifically with LIWC we are able to study the psychological value of language in gender and culture subgroups—such as parts of speech that include pronouns articles prepositions conjunctives and auxiliary verbs [9]. (1) We consider two measures of affect derived from LIWC: positive affect (PA) and negative affect (NA) and four other measures of emotional expression: anger anxiety sadness and swear. Literature in mental health [49] identifies emotional expression to be key to characterizing one’s psychological vulnerability (2) We use LIWC to define the cognitive measures as well: (a) cognition comprising cognitive mech discrepancies inhibition negation death causation certainty and tentativeness; and (b) perception comprising set of words in LIWC around see hear feel percept insight and relative. Quantifying one’s cognition and perception as manifested linguistically can provide insights into emotional stability and cognitive complexity—these attributes are important with regard to understanding one’s mental well-being [19]. (3) Next we consider four measures of linguistic style: (a) Lexical Density: consisting of words that are verbs auxiliary verbs nouns adjectives (identified using NLTK’s [4] POS tagger) and adverbs. (b) Temporal References: consisting of past present and future tenses. (c) Social/Personal Concerns: words belonging to family friends social work health humans religion bio body money achievement home and sexual. (d) Interpersonal Awareness and Focus: words that are 1st person singular 1st person plural 2nd person and 3rd person pronouns. Together linguistic styles are known to indicate one’s underlying psychological processes (lexical density) personality (temporal references) social support and connectivity (social/personal concerns) and awareness of one’s surroundings and environment (interpersonal focus). Prior work identifies all of these cues to be valuable in understanding mental health in both offline and online contexts including social media [43]. Topic Modeling Our second method for comparing mental illness disclosures uses a topic model which have been commonly employed to analyze health data [41]. We obtain topics by running Latent Dirichlet Allocation (LDA) [5] over all posts. We pre-processed the data by removing a standard list of Twitter-specific stop words words with very high frequency (> 0.25× datasize) and words that occur fewer than five times. Thereafter we used Gensim’s implementation of online LDA from [23]. We used the default hyper-parameter settings and 100 topics which we determined based on the value of average corpus likelihood over ten runs. To measure topic differences in one cohort (e.g. IN MID users) over the other (e.g. UK MID users) we first compute the posterior probability of each topic separately for all posts in both cohorts. We then compute three comparison metrics: (1) the rate of change for each topic given as the difference between the posterior topic probabilities of the cohorts divided by the probability of the first cohort; (2) the pointwise mutual information between the posterior topic probabilities of the same cohorts; and (3) the Spearman’s rank correlation between the topic distributions for the two cohorts. Additionally we compare all gender and culture cohorts based on significance tests (e.g. Mann Whitney U test for gender and the Kruskal Wallis test for cultural differences). We also present a method to qualitatively examine the differences between the topics used by different MID user cohorts. For the purpose two researchers familiar with mental health content on social media independently inspected the words associated with each of the topics given by the above topic model. They used a semi-open coding approach to develop a codebook and extracted descriptive topical themes for the topics (Cohen’s κ=.74). During the codebook development the two annotators referred to prior literature on gender and cultural differences in mental health [46 20 52]. In the results section we will present an examination of these qualitative differences.,We note that the candidate mental health disclosure data sample is prone to significant noise. It is possible that although a user uses one of the mental health keyphrases in their Twitter post it may not indicate a genuine disclosure (e.g. “when I have to wake up at 6am I feel like killing myself” does not indicate a person’s real intention of taking their life). To eliminate such users we adopt a semi-supervised machine learning method [58] in which we compare the language of each user in our Twitter dataset with the language of a selfidentified set of social media users suffering a from mental illness. For the purpose we obtain a large sample of 79833 posts from 44262 users made on the Reddit subcommunities r/depression r/mentalhealth and r/SuicideWatch between February and November 2014. We use this dataset a weak signal of the language used by individuals identifying with a form of mental illness. Prior work has also indicated that the disclosures made on these forums are genuine disclosures of mental illnesses and have also been validated through consultation with a psychiatrist [16]. Our approach proceeds as follows: Step 1. We create Twitter user-centric vector-representations by collating all of their posts – this would give us as many vectors as the number of users in the candidate disclosure sample. We also similarly create a single vector representation by collating all posts in our Reddit dataset. Step 2. Next we establish comparative validity across the Twitter vectors and the Reddit vector. This is an important step because the language of Twitter and Reddit cannot be directly compared due to the unique affordances of each site and the seeming differences in the demographics of the two7 . For each of these vectors we perform linguistic normalization of tokenized items in them8 . Then we compute the average automated readability index (ARI [44]) on all of the normalized Twitter vectors and the Reddit vector. We observe the ARI differential between the two to be ∼6.7%9  indicating that the languages are close to standardized internet speak [25] and hence comparable following normalization. Step 3. Next we build n-gram language models (n = 3) for both the normalized Twitter vectors and the single Reddit vector. We then determine cosine similarity scores between the language models of each normalized Twitter vector and the Reddit vector. Step 4. Finally we obtain the distribution of cosine similarities over all normalized Twitter vectors (see Figure 1(a)). We construct the “genuine disclosure dataset” of users to be those vectors (of users) for whom the cosine similarity of their language models with that of Reddit’s is greater than or equal to the median similarity across all vectors (median distance=.71 σ = .157). Our final dataset of genuine mental illness disclosures consisted of 231611 users; we will refer to this set as the MID users.
https://www.jmir.org/2019/6/e14199/,Detecting Signs of Depression in Tweets in Spanish: Behavioral and Linguistic Analysis,Data Collection and User Selection,0,The selection of the tweets and their users was based on the filtered real-time streaming support provided by the Twitter API. In the first step we selected the users who showed potential signs of depression on Twitter on the basis of the 20 most frequent words in Spanish expressed by patients suffering from depression in clinical settings. These words were jointly identified and selected by a psychologist and a family physician with clinical experience and were based on the definition and general features of depression according to the Diagnostic and Statistical Manual of Mental Disorders [42]. The list of words used and their English translations are shown in Textbox 1. During June 2018 1470000 tweets including 1 or more occurrences of the words listed in Textbox 1 were collected. From this collection of tweets and to select the users who publicly stated in the textual description associated to their profile that they suffered from depression all the profile descriptions including 1 or more occurrences of the word “depr” and all the possible derivations related to the word depression in Spanish such as “depre” “depresión” “depresivo” “depresiva” “deprimido” and “deprimida” were considered. From the 720 users who included 1 or more of these words in their description profile 90 users who stated they suffered from depression or were receiving treatment for depression were selected for the analysis. This selection was performed by a psychologist verifying that the statements were related to real expressions of depression excluding quotes jokes or fake ones. For each of these depressed Twitter users we collected all the most recent tweets from their timeline up to a maximum of about 3200 tweets. Thus a total of 189669 tweets were collected a figure that was reduced to 140946 after discarding the retweets. These 140946 tweets constituted the depressive users dataset. Examples of sentences appearing in the user profiles that were used for selecting the depressive users are: “Paciente psiquiátrico con depresión crónica” (Psychiatric patient with chronic depression; example of a profile sentence that indicates depression). “Colecciono errores traducidos a tweets depresivos y a uno que otro impulso de amor” (I gather errors translated into depressing tweets and into one or another love impulse; example of a profile sentence that does not indicate depression). Once the users with profile sentences indicating depression had been retrieved their Twitter timelines were collected. Only those users having in their timeline at least 10 tweets that suggested signs of depression were retained for further analyses. For each user the selection of these tweets was performed by manually inspecting the tweets of the user’s complete timeline in reverse temporal order starting from the most recent one to the oldest tweet of the timeline retrieved by means of the Twitter API . Finally a total number of 1000 tweets issued by the 90 depressive users suggesting signs of depression were detected and used for the analysis. This set of tweets provided us with the depressive tweets dataset which was used to analyze linguistic features of tweets showing signs of depression. It has to be mentioned that these 1000 tweets were not to be included in the depressive users dataset (see Figure 1). At the same time more than 97500000 tweets were also collected in June 2018: such tweets were gathered by listening to the public Twitter stream during this time span by only considering tweets with Spanish textual contents (as detected by Twitter language identification support). Given that Twitter requires more restrictive filters than just the language of the tweets we used a list of the most frequently used Spanish words (stopwords) to retrieve all tweets that included 1 or more of these words. The vast majority of Spanish tweets should match this criterion. A sample of 450 users who did not mention in their profile the word depression and its derivations were selected randomly from the 97500000 tweets. The complete timelines of these users were compiled (1141021 tweets) which were reduced to 712589 once retweets were removed. These 712589 tweets constituted the control dataset. To identify the language of a tweet we relied on the language automatically identified by Twitter for each tweet selecting tweets in Spanish. It has to be noted that these data can contain some tweets from unidentified depressive users.,
https://aclanthology.org/W18-0608.pdf,Cross-cultural differences in language markers of depression online,Data Collection,0,Data was collected from 7 Cups of Tea an anonymous online chat-based peer support community for emotional distress1 . Users agree at signup that their data may be used for the purposes of research. All the data used for the current study was anonymous and securely stored. This research was performed in line with the ethical and privacy protocols outlined in detail in (Benton et al. 2017). Data from 7 Cups takes the form of written dialogue between users of the service and volunteers who are trained as “active listeners”. A fragment of an exchange between the user of the service (U) and the volunteer (V) might go as follows: For the analyses reported in this paper we used only text generated by users of the service not the volunteers providing peer support. Users who reported depression as their primary concern at sign up were eligible for inclusion in analyses. Our original sample was comprised of 23048 conversations involving 1937 unique users. Users were excluded from the sample if they did not indicate their culture or if they selected ‘Other’. This resulted in the exclusion of 199 and 130 users respectively. The original sample also included users identifying as Native American or American Indian. This group was excluded from analyses since the majority of the data among these users was not English. This resulted in the removal of 15 users leaving a total sample size of 1593.,
https://dl.acm.org/doi/pdf/10.1145/3359169,Cross-Cultural Differences in the Use of Online Mental Health Support Forums,Method of Analysis,0,"Selection Criteria and Data Scope. To understand the impact of cultural differences on how individuals use online mental health platforms we begin our analysis by creating a dataset of users from different national communities on Talklife a support platform with over half a million users [91]. For this analysis due to the fact that most research in CSCW on mental health online has been done either agnostic of cultural context [12 34] or in a Western context [60 67 88] we choose to focus on users from non-Western countries following Zhang et al. [103]. As researchers located in the Global South and with lived experience interacting with the health system and diverse explanatory models [52] of mental illness we believe that moving the focus of CSCW and CSCWadjacent mental health research away from the West is crucial to better meet the needs of people often underserved by the medical system [70]. To create these subgroups of users we choose the three non-Western countries with the highest user populations on Talklife or India Malaysia and the Philippines. Guided by the rich amount of literature on the unique nuances to mental health expression for each country [35 62 77 80] we examine the national identity linguistic and behavior-based differences of use between each user subgroup. In particular this research notes that as a result of cultural norms around the sharing of distress and alternative conceptualizations of mental illness in India Malaysia and the Philippines symptoms are often expressed in somatic and religious terms as opposed to traditionally clinical or psychiatric terms. We choose to analyze each subgroup at the national level for both theoretical and practical reasons. On a theoretical level in past work in the medical anthropology of mental health national identity has commonly been used for a approximate level of analysis for cultural identity [31 33 52]. Additionally on a more practical level each user’s country was determined using their IP address by Talklife and shared with us in an user-anonymized dataset. Inferring a more precise location could potentially compromise user anonymity as discussed in past work [47] and did not seem to have any more significant value for our analysis of cultural differences than analysis at the national level. We analyze data from 10532 Indian users 3370 Malaysian users and 3370 Filipino users as shown in Table 2. Collectively we refer to these countries as the minority sample. As a comparison set we construct a random sample of all threads on Talklife and refer to it as the majority sample. Due to the relative prevalence of users from Western English-speaking countries in Talklife most of the threads in the majority sample include posts from countries such as the USA UK and Canada. Indians are the largest non-Western minority subgroup on Talklife. Data was sampled from May 2012 to June 2018. Following this cross-national analysis to see if our broader results on Talklife generalize to a differently structured online mental health community we picked the largest Western country (the United States) and the largest non-Western country (India) represented on 7Cups a similar support platform with more than 15000 users actively using the platform each week [7]. Using 7Cups data we repeat our analysis testing for the same cultural differences we found in our Talklife sample. For this analysis we were provided a sample of data on activity from 6055 Indian users and 18581 American users as shown in Table 2. Unlike our sample of Talklife users this dataset is not a random sample. There is an upsampling of Indian users to ensure that we have data from a sufficient number of Indians in the dataset. Like on Talklife Indians are the largest non-Western minority subgroup on 7Cups. We focus on Indian users due to a lack of sufficient data on users from Malaysia or the Philippines. Data was sampled from March 2014 - August 2018. 3.1.2 Defining Cultural Identity and Use of Clinical Language. In this work we examine the relationship between cultural identity and use of online mental health support forums. To do so we leverage Tomlinson’s definition of cultural identity as “self and communal definitions based around specific usually politically inflected differentiations: gender sexuality class religion race and ethnicity nationality"" [94] particularly looking at the aspect that of modern cultural identity that runs along national lines as delineated by Hall et al. [41]. As a diverse and amorphous form of identity cultural identity can often intersect and interact with other forms of identity including religious or ethnic identity. However in the absence of direct information about religious or ethnic identity based on the data available we use national identity as a proxy for cultural identity. Additionally following Schlesinger et al’s [83] call for more intersectional analyses and methods within HCI we also include analyses of adjacent and intersecting identities when relevant including religious identity. To analyze clinical language we use a broader definition of clinical language than just specific medical diagnoses. Following methods used in past work to analyze antidepressant related language [30] we create a dataset of clinical mental health language including unigrams bigrams and trigrams from a list of mental disorders as defined by the International Classification of Diseases (ICD-10) and Diagnostic and Statistical Manual of Mental Disorders (DSM-5) [100]. We also included all unigrams from the MacMillan Dictionary list of words used to describe illnesses and diseases both specifically for mental illness and general illness [1–3]. As a result we include unigrams like “night"" (from night terrors) or sleep (from “sleep disorder"") as these are often correlated with specific symptoms of mental illness or distress such as sleep issues or being awake at night [30]. This included any clinically common abbreviations for mental disorders such as OCD for “obsessive compulsive disorder"" or BPD for “borderline personality disorder."" Shorthand for disorders commonly used by online communities such as “pro-ana” (as used in pro-eating disorder communities) [22] were not included due to the difficulty in finding an exhaustive list of these terms across disorders. We choose to use terms from and associated with DSM and ICD categorized disorders as a result of the common usage of these frameworks globally [99]. Throughout our analysis of these varied factors we use µ to represent means and σ to represent standard deviations. 3.1.3 Constraints Limitations and Tradeoffs. Cultural identity can exist at many different and intersecting levels including subcultures and subcommunities within the larger umbrella of a cultural identity. As a result for the purpose of this analysis we had to adopt some constraints in order to do a meaningful and specific analysis. One large limiting constraint that we chose for this study is to use national identity at the state level as a proxy for cultural identity. Though a major and formative part of modern cultural identity as argued by both Hall [41] and Tomlinson [94] each country we analyze is incredibly diverse with many individual cultural identities that both intersect and diverge from a greater national identity [54 64 89]. A more rich analysis of these other forms of cultural identity is beyond the scope of this work but could lead to richer conclusions about the nature of cultural identity in online mental health support communities particularly with regard to cultural differences between users with the same national identity. Additionally to stay consistent between analyses as a result of a lack of data on users from Malaysia and the Philippines we only analyze users in India on 7Cups and extend these findings to the experience of being part of a minority group on an online mental health forum. We draw validity for these exploratory findings from similar consistent patterns we observe between Indian Malaysian and Filipino users but a deeper analysis with a larger dataset is likely necessary to determine when and for which minority communities these conclusions do not hold true. Additionally while we construct clinical language through use of the commonly used DSM and ICD both frameworks of illness categorization have significant limitations particularly in the countries we have selected. For example there are both mental health disorders that are culturebound [74] as well as mental health language that is used in different ways within the specific countries we analyze such as depression often being an umbrella term for all mental illnesses [53]. Additionally it is clear that online support communities often develop their own cultural norms and language around mental health [21 72] and a deeper understanding of how this plays out on Talklife and 7Cups is neither the focus nor within the scope of this work. In this work we intentionally use standard clinical and medical terms for mental health disorders in our analysis of clinical language. As detailed in past anthropological research [52] it is theorized that the use of medical and clinical language is representative of a medicalized explanatory model of illness and we frame use of this language across cultures as a approximate signifier of a greater awareness of the presence of a mental disorder as opposed to conceptualizing distress as “stress"" “tension"" or “depression"" [25 53 98]. For our analysis we strictly analyzed posts that were in the Latin alphabet with almost all posts on both Talklife and 7Cups being in English. However as both Malay [8] and Tagalog [82] are most commonly written in the Latin script and since it is common for users from India speakers to use romanized versions of Indian languages online [79] it is possible that a small minority of posts in our analysis were text in a different language. However as confirmed by only seeing English words used in our analysis of the top n-grams among each user subgroup it is clear that English is the predominant language on both platforms. Though beyond the immediate scope of this work a greater analysis of non-English code-switching on these platforms could lead to a deeper understanding of the impact of interactions on expression between users with the same national identity but different language preferences.",
https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-018-2197-z,Automatic extraction of informal topics from online suicidal ideation,Word2Vec,1,Many topic modeling algorithms exist including latent semantic indexing latent Dirichlet allocation and non-negative matrix factorization. In this work we turn our attention to Word2Vec which has been argued to have many advantages over these earlier algorithms [19 20]. Word2Vec describes two implementations of a shallow neural network the continuous bag of words (CBOW) model and the skip-gram model. We focus on the skip-gram model in this work which learns vector representations of words by predicting neighboring words in a text. See Fig. 2.,
https://ieeexplore.ieee.org/abstract/document/8609647,Online Social Networks in Health Care: A Study of Mental Disorders on Reddit,Relationship Modeling Network (RMN),0,RMN is a recursive neural network designed to model relationships between pairs of entities from text [13]. Each relationship is represented at a given point in time as a vector of weights over K descriptors. Entities that form a relationship do not need to be of the same class: for example in this work we model the relationships between a user and the community in which she interacts. Each post or comment corresponds to a different instant. Words are represented as embeddings of dimension P that is each word w of a vocabulary V is a vector in RP . Users and communities are represented by embeddings of dimension U and C respectively. As in previous works we generated embeddings using GloVe [15]. The descriptors obtained from RMN are vectors in RP  allowing us to find the closest words to each descriptor. The post’s (or comment’s) representation is denoted by vpost ∈ RP . This vector is the average of the word embeddings contained in the post. The representations of users and communities are denoted by vuser and vcomm. For each post or comment RMN takes as input a vector v ∈ RP +U+C obtained by concatenating vpost vuser and vcomm. These vectors are combined through the weights of the neural network to obtain a representation dt ∈ RK of the relationship between the user and the community at that particular time. RMN uses a smoothing parameter α ∈ (0 1) to avoid abrupt changes in the representation of the same relation in consecutive instants dt and dt−1. The descriptor array R ∈ RK×P is used for attempting to reconstruct the post vpost by making rt = Rdt. RMN parameters (weights and matrix of descriptors) are trained in order to maximize an objective function that aims to approximate rt and vpost while retaining some distance between rt and other randomly sampled posts. See [13] for more details on RMN. The input data for RMN was preprocessed as follows. First we removed all posts and comments marked as [deleted] or [removed] standard stop-words (using the NLTK library) punctuation and accents. Second for each subreddit we removed posts and comments from users who performed less than 50 activities (posts or comments) following the methodology presented in [16]. Finally we selected the words that appear at least once in each of the four subreddits analyzed seeking to find similarities in the way people express themselves when discussing mental health disorders. The final subset of data analyzed by RMN is composed of 18020 unique words 25101 posts and 401428 comments.,