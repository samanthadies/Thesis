Link to paper,Score,Text,Cleaned,unigrams,bigrams,trigrams
https://link.springer.com/content/pdf/10.1140/epjds/s13688-017-0110-z.pdf,1,Several different types of information were extracted from the collected Instagram data. We used total posts per user per day as a measure of user activity. We gauged community reaction by counting the number of comments and ‘likes’ each posted photograph received. Face detection software was used to determine whether or not a photograph contained a human face as well as count the total number of faces in each photo as a proxy measure for participants’ social activity levels. Pixel-level averages were computed for Hue Saturation and Value (HSV) three color properties commonly used in image analysis. Hue describes an image’s coloring on the light spectrum (ranging from red to blue/purple). Lower hue values indicate more red and higher hue values indicate more blue. Saturation refers to the vividness of an image. Low saturation makes an image appear grey and faded. Value refers to image brightness. Lower brightness scores indicate a darker image. See Figure  for a comparison of high and low HSV values. We also checked metadata to assess whether an Instagram-provided filter was applied to alter the appearance of a photograph. Collectively these measures served as the feature set in our primary model. For the separate model fit on ratings data we used only the four ratings categories (happy sad likable interesting) as predictors.,several different type of information were extracted from the collected instagram data we used total post per user per day a a measure of user activity we gauged community reaction by counting the number of comment and like each posted photograph received face detection software wa used to determine whether or not a photograph contained a human face a well a count the total number of face in each photo a a proxy measure for participant social activity level pixellevel average were computed for hue saturation and value hsv three color property commonly used in image analysis hue describes an image coloring on the light spectrum ranging from red to bluepurple lower hue value indicate more red and higher hue value indicate more blue saturation refers to the vividness of an image low saturation make an image appear grey and faded value refers to image brightness lower brightness score indicate a darker image see figure for a comparison of high and low hsv value we also checked metadata to ass whether an instagramprovided filter wa applied to alter the appearance of a photograph collectively these measure served a the feature set in our primary model for the separate model fit on rating data we used only the four rating category happy sad likable interesting a predictor,"['several', 'different', 'type', 'information', 'extracted', 'collected', 'instagram', 'data', 'used', 'total', 'post', 'per', 'user', 'per', 'day', 'measure', 'user', 'activity', 'gauged', 'community', 'reaction', 'counting', 'number', 'comment', 'like', 'posted', 'photograph', 'received', 'face', 'detection', 'software', 'wa', 'used', 'determine', 'whether', 'photograph', 'contained', 'human', 'face', 'well', 'count', 'total', 'number', 'face', 'photo', 'proxy', 'measure', 'participant', 'social', 'activity', 'level', 'pixellevel', 'average', 'computed', 'hue', 'saturation', 'value', 'hsv', 'three', 'color', 'property', 'commonly', 'used', 'image', 'analysis', 'hue', 'describes', 'image', 'coloring', 'light', 'spectrum', 'ranging', 'red', 'bluepurple', 'lower', 'hue', 'value', 'indicate', 'red', 'higher', 'hue', 'value', 'indicate', 'blue', 'saturation', 'refers', 'vividness', 'image', 'low', 'saturation', 'make', 'image', 'appear', 'grey', 'faded', 'value', 'refers', 'image', 'brightness', 'lower', 'brightness', 'score', 'indicate', 'darker', 'image', 'see', 'figure', 'comparison', 'high', 'low', 'hsv', 'value', 'also', 'checked', 'metadata', 'ass', 'whether', 'instagramprovided', 'filter', 'wa', 'applied', 'alter', 'appearance', 'photograph', 'collectively', 'measure', 'served', 'feature', 'set', 'primary', 'model', 'separate', 'model', 'fit', 'rating', 'data', 'used', 'four', 'rating', 'category', 'happy', 'sad', 'likable', 'interesting', 'predictor']","['several different', 'different type', 'type information', 'information extracted', 'extracted collected', 'collected instagram', 'instagram data', 'data used', 'used total', 'total post', 'post per', 'per user', 'user per', 'per day', 'day measure', 'measure user', 'user activity', 'activity gauged', 'gauged community', 'community reaction', 'reaction counting', 'counting number', 'number comment', 'comment like', 'like posted', 'posted photograph', 'photograph received', 'received face', 'face detection', 'detection software', 'software wa', 'wa used', 'used determine', 'determine whether', 'whether photograph', 'photograph contained', 'contained human', 'human face', 'face well', 'well count', 'count total', 'total number', 'number face', 'face photo', 'photo proxy', 'proxy measure', 'measure participant', 'participant social', 'social activity', 'activity level', 'level pixellevel', 'pixellevel average', 'average computed', 'computed hue', 'hue saturation', 'saturation value', 'value hsv', 'hsv three', 'three color', 'color property', 'property commonly', 'commonly used', 'used image', 'image analysis', 'analysis hue', 'hue describes', 'describes image', 'image coloring', 'coloring light', 'light spectrum', 'spectrum ranging', 'ranging red', 'red bluepurple', 'bluepurple lower', 'lower hue', 'hue value', 'value indicate', 'indicate red', 'red higher', 'higher hue', 'hue value', 'value indicate', 'indicate blue', 'blue saturation', 'saturation refers', 'refers vividness', 'vividness image', 'image low', 'low saturation', 'saturation make', 'make image', 'image appear', 'appear grey', 'grey faded', 'faded value', 'value refers', 'refers image', 'image brightness', 'brightness lower', 'lower brightness', 'brightness score', 'score indicate', 'indicate darker', 'darker image', 'image see', 'see figure', 'figure comparison', 'comparison high', 'high low', 'low hsv', 'hsv value', 'value also', 'also checked', 'checked metadata', 'metadata ass', 'ass whether', 'whether instagramprovided', 'instagramprovided filter', 'filter wa', 'wa applied', 'applied alter', 'alter appearance', 'appearance photograph', 'photograph collectively', 'collectively measure', 'measure served', 'served feature', 'feature set', 'set primary', 'primary model', 'model separate', 'separate model', 'model fit', 'fit rating', 'rating data', 'data used', 'used four', 'four rating', 'rating category', 'category happy', 'happy sad', 'sad likable', 'likable interesting', 'interesting predictor']","['several different type', 'different type information', 'type information extracted', 'information extracted collected', 'extracted collected instagram', 'collected instagram data', 'instagram data used', 'data used total', 'used total post', 'total post per', 'post per user', 'per user per', 'user per day', 'per day measure', 'day measure user', 'measure user activity', 'user activity gauged', 'activity gauged community', 'gauged community reaction', 'community reaction counting', 'reaction counting number', 'counting number comment', 'number comment like', 'comment like posted', 'like posted photograph', 'posted photograph received', 'photograph received face', 'received face detection', 'face detection software', 'detection software wa', 'software wa used', 'wa used determine', 'used determine whether', 'determine whether photograph', 'whether photograph contained', 'photograph contained human', 'contained human face', 'human face well', 'face well count', 'well count total', 'count total number', 'total number face', 'number face photo', 'face photo proxy', 'photo proxy measure', 'proxy measure participant', 'measure participant social', 'participant social activity', 'social activity level', 'activity level pixellevel', 'level pixellevel average', 'pixellevel average computed', 'average computed hue', 'computed hue saturation', 'hue saturation value', 'saturation value hsv', 'value hsv three', 'hsv three color', 'three color property', 'color property commonly', 'property commonly used', 'commonly used image', 'used image analysis', 'image analysis hue', 'analysis hue describes', 'hue describes image', 'describes image coloring', 'image coloring light', 'coloring light spectrum', 'light spectrum ranging', 'spectrum ranging red', 'ranging red bluepurple', 'red bluepurple lower', 'bluepurple lower hue', 'lower hue value', 'hue value indicate', 'value indicate red', 'indicate red higher', 'red higher hue', 'higher hue value', 'hue value indicate', 'value indicate blue', 'indicate blue saturation', 'blue saturation refers', 'saturation refers vividness', 'refers vividness image', 'vividness image low', 'image low saturation', 'low saturation make', 'saturation make image', 'make image appear', 'image appear grey', 'appear grey faded', 'grey faded value', 'faded value refers', 'value refers image', 'refers image brightness', 'image brightness lower', 'brightness lower brightness', 'lower brightness score', 'brightness score indicate', 'score indicate darker', 'indicate darker image', 'darker image see', 'image see figure', 'see figure comparison', 'figure comparison high', 'comparison high low', 'high low hsv', 'low hsv value', 'hsv value also', 'value also checked', 'also checked metadata', 'checked metadata ass', 'metadata ass whether', 'ass whether instagramprovided', 'whether instagramprovided filter', 'instagramprovided filter wa', 'filter wa applied', 'wa applied alter', 'applied alter appearance', 'alter appearance photograph', 'appearance photograph collectively', 'photograph collectively measure', 'collectively measure served', 'measure served feature', 'served feature set', 'feature set primary', 'set primary model', 'primary model separate', 'model separate model', 'separate model fit', 'model fit rating', 'fit rating data', 'rating data used', 'data used four', 'used four rating', 'four rating category', 'rating category happy', 'category happy sad', 'happy sad likable', 'sad likable interesting', 'likable interesting predictor']"
https://dl.acm.org/doi/abs/10.1145/2858036.2858207 ,1,Our first set of methods include developing three sets of measures spanning: linguistic structure interpersonal awareness and interaction. The choice of these measures is motivated by literature that examines associations between the behavioral expression of individuals and their responses to crises including vulnerability due to mental illness [13 23]. Each of these measure categories consists of the following variables: Linguistic Structure. For this measure we compute the fraction of nouns verbs2  and adverbs in posts and comments; automated readability index a measure to gauge the understandability of text [62]; and linguistic accommodation a process by which individuals in a conversation adjust their language style according to that of others [20]3 . Together these variables characterize the text shared by the user classes beyond their informational content. Per literature in psycholinguistics such structure is known to relate to an individual’s underlying psychological and cognitive state and can reveal cues about their social coordination [54]. Interpersonal Awareness. This measure category includes: proportion of first person singular (indicating pre-occupation with self) first person plural (indicating collective attention) second person and third person pronouns (indicating social interactivity and reference to people or objects in the environment). Literature has indicated that pronoun use can quantify an individual’s self and social awareness and can reveal mental well-being including that manifested in social media [21]. Interaction. Variables corresponding to this measure category include: volume of posts and comments authored post length length of comments authored volume of comments received on shared posts length of comments received mean vote difference (difference between upvotes and downvotes on posts authored) and response velocity (in minutes) given by the time elapsed between the first comment and the time the corresponding post was shared.,our first set of method include developing three set of measure spanning linguistic structure interpersonal awareness and interaction the choice of these measure is motivated by literature that examines association between the behavioral expression of individual and their response to crisis including vulnerability due to mental illness each of these measure category consists of the following variable linguistic structure for this measure we compute the fraction of noun verb and adverb in post and comment automated readability index a measure to gauge the understandability of text and linguistic accommodation a process by which individual in a conversation adjust their language style according to that of others together these variable characterize the text shared by the user class beyond their informational content per literature in psycholinguistics such structure is known to relate to an individual underlying psychological and cognitive state and can reveal cue about their social coordination interpersonal awareness this measure category includes proportion of first person singular indicating preoccupation with self first person plural indicating collective attention second person and third person pronoun indicating social interactivity and reference to people or object in the environment literature ha indicated that pronoun use can quantify an individual self and social awareness and can reveal mental wellbeing including that manifested in social medium interaction variable corresponding to this measure category include volume of post and comment authored post length length of comment authored volume of comment received on shared post length of comment received mean vote difference difference between upvotes and downvotes on post authored and response velocity in minute given by the time elapsed between the first comment and the time the corresponding post wa shared,"['first', 'set', 'method', 'include', 'developing', 'three', 'set', 'measure', 'spanning', 'linguistic', 'structure', 'interpersonal', 'awareness', 'interaction', 'choice', 'measure', 'motivated', 'literature', 'examines', 'association', 'behavioral', 'expression', 'individual', 'response', 'crisis', 'including', 'vulnerability', 'due', 'mental', 'illness', 'measure', 'category', 'consists', 'following', 'variable', 'linguistic', 'structure', 'measure', 'compute', 'fraction', 'noun', 'verb', 'adverb', 'post', 'comment', 'automated', 'readability', 'index', 'measure', 'gauge', 'understandability', 'text', 'linguistic', 'accommodation', 'process', 'individual', 'conversation', 'adjust', 'language', 'style', 'according', 'others', 'together', 'variable', 'characterize', 'text', 'shared', 'user', 'class', 'beyond', 'informational', 'content', 'per', 'literature', 'psycholinguistics', 'structure', 'known', 'relate', 'individual', 'underlying', 'psychological', 'cognitive', 'state', 'reveal', 'cue', 'social', 'coordination', 'interpersonal', 'awareness', 'measure', 'category', 'includes', 'proportion', 'first', 'person', 'singular', 'indicating', 'preoccupation', 'self', 'first', 'person', 'plural', 'indicating', 'collective', 'attention', 'second', 'person', 'third', 'person', 'pronoun', 'indicating', 'social', 'interactivity', 'reference', 'people', 'object', 'environment', 'literature', 'ha', 'indicated', 'pronoun', 'use', 'quantify', 'individual', 'self', 'social', 'awareness', 'reveal', 'mental', 'wellbeing', 'including', 'manifested', 'social', 'medium', 'interaction', 'variable', 'corresponding', 'measure', 'category', 'include', 'volume', 'post', 'comment', 'authored', 'post', 'length', 'length', 'comment', 'authored', 'volume', 'comment', 'received', 'shared', 'post', 'length', 'comment', 'received', 'mean', 'vote', 'difference', 'difference', 'upvotes', 'downvotes', 'post', 'authored', 'response', 'velocity', 'minute', 'given', 'time', 'elapsed', 'first', 'comment', 'time', 'corresponding', 'post', 'wa', 'shared']","['first set', 'set method', 'method include', 'include developing', 'developing three', 'three set', 'set measure', 'measure spanning', 'spanning linguistic', 'linguistic structure', 'structure interpersonal', 'interpersonal awareness', 'awareness interaction', 'interaction choice', 'choice measure', 'measure motivated', 'motivated literature', 'literature examines', 'examines association', 'association behavioral', 'behavioral expression', 'expression individual', 'individual response', 'response crisis', 'crisis including', 'including vulnerability', 'vulnerability due', 'due mental', 'mental illness', 'illness measure', 'measure category', 'category consists', 'consists following', 'following variable', 'variable linguistic', 'linguistic structure', 'structure measure', 'measure compute', 'compute fraction', 'fraction noun', 'noun verb', 'verb adverb', 'adverb post', 'post comment', 'comment automated', 'automated readability', 'readability index', 'index measure', 'measure gauge', 'gauge understandability', 'understandability text', 'text linguistic', 'linguistic accommodation', 'accommodation process', 'process individual', 'individual conversation', 'conversation adjust', 'adjust language', 'language style', 'style according', 'according others', 'others together', 'together variable', 'variable characterize', 'characterize text', 'text shared', 'shared user', 'user class', 'class beyond', 'beyond informational', 'informational content', 'content per', 'per literature', 'literature psycholinguistics', 'psycholinguistics structure', 'structure known', 'known relate', 'relate individual', 'individual underlying', 'underlying psychological', 'psychological cognitive', 'cognitive state', 'state reveal', 'reveal cue', 'cue social', 'social coordination', 'coordination interpersonal', 'interpersonal awareness', 'awareness measure', 'measure category', 'category includes', 'includes proportion', 'proportion first', 'first person', 'person singular', 'singular indicating', 'indicating preoccupation', 'preoccupation self', 'self first', 'first person', 'person plural', 'plural indicating', 'indicating collective', 'collective attention', 'attention second', 'second person', 'person third', 'third person', 'person pronoun', 'pronoun indicating', 'indicating social', 'social interactivity', 'interactivity reference', 'reference people', 'people object', 'object environment', 'environment literature', 'literature ha', 'ha indicated', 'indicated pronoun', 'pronoun use', 'use quantify', 'quantify individual', 'individual self', 'self social', 'social awareness', 'awareness reveal', 'reveal mental', 'mental wellbeing', 'wellbeing including', 'including manifested', 'manifested social', 'social medium', 'medium interaction', 'interaction variable', 'variable corresponding', 'corresponding measure', 'measure category', 'category include', 'include volume', 'volume post', 'post comment', 'comment authored', 'authored post', 'post length', 'length length', 'length comment', 'comment authored', 'authored volume', 'volume comment', 'comment received', 'received shared', 'shared post', 'post length', 'length comment', 'comment received', 'received mean', 'mean vote', 'vote difference', 'difference difference', 'difference upvotes', 'upvotes downvotes', 'downvotes post', 'post authored', 'authored response', 'response velocity', 'velocity minute', 'minute given', 'given time', 'time elapsed', 'elapsed first', 'first comment', 'comment time', 'time corresponding', 'corresponding post', 'post wa', 'wa shared']","['first set method', 'set method include', 'method include developing', 'include developing three', 'developing three set', 'three set measure', 'set measure spanning', 'measure spanning linguistic', 'spanning linguistic structure', 'linguistic structure interpersonal', 'structure interpersonal awareness', 'interpersonal awareness interaction', 'awareness interaction choice', 'interaction choice measure', 'choice measure motivated', 'measure motivated literature', 'motivated literature examines', 'literature examines association', 'examines association behavioral', 'association behavioral expression', 'behavioral expression individual', 'expression individual response', 'individual response crisis', 'response crisis including', 'crisis including vulnerability', 'including vulnerability due', 'vulnerability due mental', 'due mental illness', 'mental illness measure', 'illness measure category', 'measure category consists', 'category consists following', 'consists following variable', 'following variable linguistic', 'variable linguistic structure', 'linguistic structure measure', 'structure measure compute', 'measure compute fraction', 'compute fraction noun', 'fraction noun verb', 'noun verb adverb', 'verb adverb post', 'adverb post comment', 'post comment automated', 'comment automated readability', 'automated readability index', 'readability index measure', 'index measure gauge', 'measure gauge understandability', 'gauge understandability text', 'understandability text linguistic', 'text linguistic accommodation', 'linguistic accommodation process', 'accommodation process individual', 'process individual conversation', 'individual conversation adjust', 'conversation adjust language', 'adjust language style', 'language style according', 'style according others', 'according others together', 'others together variable', 'together variable characterize', 'variable characterize text', 'characterize text shared', 'text shared user', 'shared user class', 'user class beyond', 'class beyond informational', 'beyond informational content', 'informational content per', 'content per literature', 'per literature psycholinguistics', 'literature psycholinguistics structure', 'psycholinguistics structure known', 'structure known relate', 'known relate individual', 'relate individual underlying', 'individual underlying psychological', 'underlying psychological cognitive', 'psychological cognitive state', 'cognitive state reveal', 'state reveal cue', 'reveal cue social', 'cue social coordination', 'social coordination interpersonal', 'coordination interpersonal awareness', 'interpersonal awareness measure', 'awareness measure category', 'measure category includes', 'category includes proportion', 'includes proportion first', 'proportion first person', 'first person singular', 'person singular indicating', 'singular indicating preoccupation', 'indicating preoccupation self', 'preoccupation self first', 'self first person', 'first person plural', 'person plural indicating', 'plural indicating collective', 'indicating collective attention', 'collective attention second', 'attention second person', 'second person third', 'person third person', 'third person pronoun', 'person pronoun indicating', 'pronoun indicating social', 'indicating social interactivity', 'social interactivity reference', 'interactivity reference people', 'reference people object', 'people object environment', 'object environment literature', 'environment literature ha', 'literature ha indicated', 'ha indicated pronoun', 'indicated pronoun use', 'pronoun use quantify', 'use quantify individual', 'quantify individual self', 'individual self social', 'self social awareness', 'social awareness reveal', 'awareness reveal mental', 'reveal mental wellbeing', 'mental wellbeing including', 'wellbeing including manifested', 'including manifested social', 'manifested social medium', 'social medium interaction', 'medium interaction variable', 'interaction variable corresponding', 'variable corresponding measure', 'corresponding measure category', 'measure category include', 'category include volume', 'include volume post', 'volume post comment', 'post comment authored', 'comment authored post', 'authored post length', 'post length length', 'length length comment', 'length comment authored', 'comment authored volume', 'authored volume comment', 'volume comment received', 'comment received shared', 'received shared post', 'shared post length', 'post length comment', 'length comment received', 'comment received mean', 'received mean vote', 'mean vote difference', 'vote difference difference', 'difference difference upvotes', 'difference upvotes downvotes', 'upvotes downvotes post', 'downvotes post authored', 'post authored response', 'authored response velocity', 'response velocity minute', 'velocity minute given', 'minute given time', 'given time elapsed', 'time elapsed first', 'elapsed first comment', 'first comment time', 'comment time corresponding', 'time corresponding post', 'corresponding post wa', 'post wa shared']"
https://dl.acm.org/doi/abs/10.1145/2702613.2732733,1,Based on the training data thus created we pursued the use of supervised learning to develop a classifier which would indicate whether a post is of high low or no self-disclosure. We tested a variety of different classification techniques (decision trees k Nearest Neighbor naive Bayes). The best performing classifier was found to be a perceptron classifier with adaptive boosting used to amplify performance [17] whose results will be used in the remainder of this paper. We used the following feature generation rules: First we eliminated stopwords from each post based on standard list provided by Python’s NLTK library. Next we performed stemming using Porter Stemmer. We extracted uni- bi- and tri-grams from each post and considered those with five or more occurrences. We also computed two additional features – length of each post and whether the author of the post is an exclusive poster on mental health forums or is observed in our dataset to post on other forums as well. Thus each post was characterized by 1070 features. We used standard 10-fold cross validation (CV) to evaluate the classifier and ran our model over 100 random 10-fold CV assignments for generalizability of the results. We report the average accuracy precision recall F1 specificity as metrics of performance. We find that our classifier based on the perception model yields an average accuracy of 78.4% in detecting high or low self-disclosure with .74 precision and .86 recall (see Table 4 for details). Other methods like k-NN (k=5) give higher precision but at the expense of very low recall. Figure 1 gives the ROC (receiver operating characteristic) curves for all the models. Per the ROC curve corresponding to the perceptron model we find it to yield the maximum area under curve (.81) hence best performance. We further identify in Table 5 the n-grams (or features) with the highest weights given by the perceptron – it implies these features were the most significant in the classification task. We provide some brief qualitative examinations of these n-grams in the light of prior psychology literature on selfdisclosure and mental health [10 11]. We find that the ngrams primarily are associated with vulnerable and selfloathing thoughts (e.g. thoughts of suicide) bear a negative tone or depict confessional experiences. Based on prior research [10 11 12] and our own work on mental health discourse on Reddit [4] we find that these are the topical dimensions along which high self-disclosure and low/no selfdisclosure posts vary. In essence high self-disclosure posts share extensively their personal beliefs and fear for instance their vital constructs and private sensitive informational attributes. The post excerpts below have been classified to be of high self-disclosure and through them we demonstrate the use of some of the n-grams in Table 7: “I don’t want to kill myself I haven’t felt suicidal in a long time but I just want to stop life for a while you know?” “My dad would beat the living shit out of me […]. I’ve been to the hospital so many times I’ve lost track” “I hate this. I hate myself. I don't want to f****** be this person anymore. I'm unmotivated unfocused immature.”,based on the training data thus created we pursued the use of supervised learning to develop a classifier which would indicate whether a post is of high low or no selfdisclosure we tested a variety of different classification technique decision tree k nearest neighbor naive bayes the best performing classifier wa found to be a perceptron classifier with adaptive boosting used to amplify performance whose result will be used in the remainder of this paper we used the following feature generation rule first we eliminated stopwords from each post based on standard list provided by python nltk library next we performed stemming using porter stemmer we extracted uni bi and trigram from each post and considered those with five or more occurrence we also computed two additional feature length of each post and whether the author of the post is an exclusive poster on mental health forum or is observed in our dataset to post on other forum a well thus each post wa characterized by feature we used standard fold cross validation cv to evaluate the classifier and ran our model over random fold cv assignment for generalizability of the result we report the average accuracy precision recall f specificity a metric of performance we find that our classifier based on the perception model yield an average accuracy of in detecting high or low selfdisclosure with precision and recall see table for detail other method like knn k give higher precision but at the expense of very low recall figure give the roc receiver operating characteristic curve for all the model per the roc curve corresponding to the perceptron model we find it to yield the maximum area under curve hence best performance we further identify in table the ngrams or feature with the highest weight given by the perceptron it implies these feature were the most significant in the classification task we provide some brief qualitative examination of these ngrams in the light of prior psychology literature on selfdisclosure and mental health we find that the ngrams primarily are associated with vulnerable and selfloathing thought eg thought of suicide bear a negative tone or depict confessional experience based on prior research and our own work on mental health discourse on reddit we find that these are the topical dimension along which high selfdisclosure and lowno selfdisclosure post vary in essence high selfdisclosure post share extensively their personal belief and fear for instance their vital construct and private sensitive informational attribute the post excerpt below have been classified to be of high selfdisclosure and through them we demonstrate the use of some of the ngrams in table i dont want to kill myself i havent felt suicidal in a long time but i just want to stop life for a while you know my dad would beat the living shit out of me ive been to the hospital so many time ive lost track i hate this i hate myself i dont want to f be this person anymore im unmotivated unfocused immature,"['based', 'training', 'data', 'thus', 'created', 'pursued', 'use', 'supervised', 'learning', 'develop', 'classifier', 'would', 'indicate', 'whether', 'post', 'high', 'low', 'selfdisclosure', 'tested', 'variety', 'different', 'classification', 'technique', 'decision', 'tree', 'k', 'nearest', 'neighbor', 'naive', 'bayes', 'best', 'performing', 'classifier', 'wa', 'found', 'perceptron', 'classifier', 'adaptive', 'boosting', 'used', 'amplify', 'performance', 'whose', 'result', 'used', 'remainder', 'paper', 'used', 'following', 'feature', 'generation', 'rule', 'first', 'eliminated', 'stopwords', 'post', 'based', 'standard', 'list', 'provided', 'python', 'nltk', 'library', 'next', 'performed', 'stemming', 'using', 'porter', 'stemmer', 'extracted', 'uni', 'bi', 'trigram', 'post', 'considered', 'five', 'occurrence', 'also', 'computed', 'two', 'additional', 'feature', 'length', 'post', 'whether', 'author', 'post', 'exclusive', 'poster', 'mental', 'health', 'forum', 'observed', 'dataset', 'post', 'forum', 'well', 'thus', 'post', 'wa', 'characterized', 'feature', 'used', 'standard', 'fold', 'cross', 'validation', 'cv', 'evaluate', 'classifier', 'ran', 'model', 'random', 'fold', 'cv', 'assignment', 'generalizability', 'result', 'report', 'average', 'accuracy', 'precision', 'recall', 'f', 'specificity', 'metric', 'performance', 'find', 'classifier', 'based', 'perception', 'model', 'yield', 'average', 'accuracy', 'detecting', 'high', 'low', 'selfdisclosure', 'precision', 'recall', 'see', 'table', 'detail', 'method', 'like', 'knn', 'k', 'give', 'higher', 'precision', 'expense', 'low', 'recall', 'figure', 'give', 'roc', 'receiver', 'operating', 'characteristic', 'curve', 'model', 'per', 'roc', 'curve', 'corresponding', 'perceptron', 'model', 'find', 'yield', 'maximum', 'area', 'curve', 'hence', 'best', 'performance', 'identify', 'table', 'ngrams', 'feature', 'highest', 'weight', 'given', 'perceptron', 'implies', 'feature', 'significant', 'classification', 'task', 'provide', 'brief', 'qualitative', 'examination', 'ngrams', 'light', 'prior', 'psychology', 'literature', 'selfdisclosure', 'mental', 'health', 'find', 'ngrams', 'primarily', 'associated', 'vulnerable', 'selfloathing', 'thought', 'eg', 'thought', 'suicide', 'bear', 'negative', 'tone', 'depict', 'confessional', 'experience', 'based', 'prior', 'research', 'work', 'mental', 'health', 'discourse', 'reddit', 'find', 'topical', 'dimension', 'along', 'high', 'selfdisclosure', 'lowno', 'selfdisclosure', 'post', 'vary', 'essence', 'high', 'selfdisclosure', 'post', 'share', 'extensively', 'personal', 'belief', 'fear', 'instance', 'vital', 'construct', 'private', 'sensitive', 'informational', 'attribute', 'post', 'excerpt', 'classified', 'high', 'selfdisclosure', 'demonstrate', 'use', 'ngrams', 'table', 'dont', 'want', 'kill', 'havent', 'felt', 'suicidal', 'long', 'time', 'want', 'stop', 'life', 'know', 'dad', 'would', 'beat', 'living', 'shit', 'ive', 'hospital', 'many', 'time', 'ive', 'lost', 'track', 'hate', 'hate', 'dont', 'want', 'f', 'person', 'anymore', 'im', 'unmotivated', 'unfocused', 'immature']","['based training', 'training data', 'data thus', 'thus created', 'created pursued', 'pursued use', 'use supervised', 'supervised learning', 'learning develop', 'develop classifier', 'classifier would', 'would indicate', 'indicate whether', 'whether post', 'post high', 'high low', 'low selfdisclosure', 'selfdisclosure tested', 'tested variety', 'variety different', 'different classification', 'classification technique', 'technique decision', 'decision tree', 'tree k', 'k nearest', 'nearest neighbor', 'neighbor naive', 'naive bayes', 'bayes best', 'best performing', 'performing classifier', 'classifier wa', 'wa found', 'found perceptron', 'perceptron classifier', 'classifier adaptive', 'adaptive boosting', 'boosting used', 'used amplify', 'amplify performance', 'performance whose', 'whose result', 'result used', 'used remainder', 'remainder paper', 'paper used', 'used following', 'following feature', 'feature generation', 'generation rule', 'rule first', 'first eliminated', 'eliminated stopwords', 'stopwords post', 'post based', 'based standard', 'standard list', 'list provided', 'provided python', 'python nltk', 'nltk library', 'library next', 'next performed', 'performed stemming', 'stemming using', 'using porter', 'porter stemmer', 'stemmer extracted', 'extracted uni', 'uni bi', 'bi trigram', 'trigram post', 'post considered', 'considered five', 'five occurrence', 'occurrence also', 'also computed', 'computed two', 'two additional', 'additional feature', 'feature length', 'length post', 'post whether', 'whether author', 'author post', 'post exclusive', 'exclusive poster', 'poster mental', 'mental health', 'health forum', 'forum observed', 'observed dataset', 'dataset post', 'post forum', 'forum well', 'well thus', 'thus post', 'post wa', 'wa characterized', 'characterized feature', 'feature used', 'used standard', 'standard fold', 'fold cross', 'cross validation', 'validation cv', 'cv evaluate', 'evaluate classifier', 'classifier ran', 'ran model', 'model random', 'random fold', 'fold cv', 'cv assignment', 'assignment generalizability', 'generalizability result', 'result report', 'report average', 'average accuracy', 'accuracy precision', 'precision recall', 'recall f', 'f specificity', 'specificity metric', 'metric performance', 'performance find', 'find classifier', 'classifier based', 'based perception', 'perception model', 'model yield', 'yield average', 'average accuracy', 'accuracy detecting', 'detecting high', 'high low', 'low selfdisclosure', 'selfdisclosure precision', 'precision recall', 'recall see', 'see table', 'table detail', 'detail method', 'method like', 'like knn', 'knn k', 'k give', 'give higher', 'higher precision', 'precision expense', 'expense low', 'low recall', 'recall figure', 'figure give', 'give roc', 'roc receiver', 'receiver operating', 'operating characteristic', 'characteristic curve', 'curve model', 'model per', 'per roc', 'roc curve', 'curve corresponding', 'corresponding perceptron', 'perceptron model', 'model find', 'find yield', 'yield maximum', 'maximum area', 'area curve', 'curve hence', 'hence best', 'best performance', 'performance identify', 'identify table', 'table ngrams', 'ngrams feature', 'feature highest', 'highest weight', 'weight given', 'given perceptron', 'perceptron implies', 'implies feature', 'feature significant', 'significant classification', 'classification task', 'task provide', 'provide brief', 'brief qualitative', 'qualitative examination', 'examination ngrams', 'ngrams light', 'light prior', 'prior psychology', 'psychology literature', 'literature selfdisclosure', 'selfdisclosure mental', 'mental health', 'health find', 'find ngrams', 'ngrams primarily', 'primarily associated', 'associated vulnerable', 'vulnerable selfloathing', 'selfloathing thought', 'thought eg', 'eg thought', 'thought suicide', 'suicide bear', 'bear negative', 'negative tone', 'tone depict', 'depict confessional', 'confessional experience', 'experience based', 'based prior', 'prior research', 'research work', 'work mental', 'mental health', 'health discourse', 'discourse reddit', 'reddit find', 'find topical', 'topical dimension', 'dimension along', 'along high', 'high selfdisclosure', 'selfdisclosure lowno', 'lowno selfdisclosure', 'selfdisclosure post', 'post vary', 'vary essence', 'essence high', 'high selfdisclosure', 'selfdisclosure post', 'post share', 'share extensively', 'extensively personal', 'personal belief', 'belief fear', 'fear instance', 'instance vital', 'vital construct', 'construct private', 'private sensitive', 'sensitive informational', 'informational attribute', 'attribute post', 'post excerpt', 'excerpt classified', 'classified high', 'high selfdisclosure', 'selfdisclosure demonstrate', 'demonstrate use', 'use ngrams', 'ngrams table', 'table dont', 'dont want', 'want kill', 'kill havent', 'havent felt', 'felt suicidal', 'suicidal long', 'long time', 'time want', 'want stop', 'stop life', 'life know', 'know dad', 'dad would', 'would beat', 'beat living', 'living shit', 'shit ive', 'ive hospital', 'hospital many', 'many time', 'time ive', 'ive lost', 'lost track', 'track hate', 'hate hate', 'hate dont', 'dont want', 'want f', 'f person', 'person anymore', 'anymore im', 'im unmotivated', 'unmotivated unfocused', 'unfocused immature']","['based training data', 'training data thus', 'data thus created', 'thus created pursued', 'created pursued use', 'pursued use supervised', 'use supervised learning', 'supervised learning develop', 'learning develop classifier', 'develop classifier would', 'classifier would indicate', 'would indicate whether', 'indicate whether post', 'whether post high', 'post high low', 'high low selfdisclosure', 'low selfdisclosure tested', 'selfdisclosure tested variety', 'tested variety different', 'variety different classification', 'different classification technique', 'classification technique decision', 'technique decision tree', 'decision tree k', 'tree k nearest', 'k nearest neighbor', 'nearest neighbor naive', 'neighbor naive bayes', 'naive bayes best', 'bayes best performing', 'best performing classifier', 'performing classifier wa', 'classifier wa found', 'wa found perceptron', 'found perceptron classifier', 'perceptron classifier adaptive', 'classifier adaptive boosting', 'adaptive boosting used', 'boosting used amplify', 'used amplify performance', 'amplify performance whose', 'performance whose result', 'whose result used', 'result used remainder', 'used remainder paper', 'remainder paper used', 'paper used following', 'used following feature', 'following feature generation', 'feature generation rule', 'generation rule first', 'rule first eliminated', 'first eliminated stopwords', 'eliminated stopwords post', 'stopwords post based', 'post based standard', 'based standard list', 'standard list provided', 'list provided python', 'provided python nltk', 'python nltk library', 'nltk library next', 'library next performed', 'next performed stemming', 'performed stemming using', 'stemming using porter', 'using porter stemmer', 'porter stemmer extracted', 'stemmer extracted uni', 'extracted uni bi', 'uni bi trigram', 'bi trigram post', 'trigram post considered', 'post considered five', 'considered five occurrence', 'five occurrence also', 'occurrence also computed', 'also computed two', 'computed two additional', 'two additional feature', 'additional feature length', 'feature length post', 'length post whether', 'post whether author', 'whether author post', 'author post exclusive', 'post exclusive poster', 'exclusive poster mental', 'poster mental health', 'mental health forum', 'health forum observed', 'forum observed dataset', 'observed dataset post', 'dataset post forum', 'post forum well', 'forum well thus', 'well thus post', 'thus post wa', 'post wa characterized', 'wa characterized feature', 'characterized feature used', 'feature used standard', 'used standard fold', 'standard fold cross', 'fold cross validation', 'cross validation cv', 'validation cv evaluate', 'cv evaluate classifier', 'evaluate classifier ran', 'classifier ran model', 'ran model random', 'model random fold', 'random fold cv', 'fold cv assignment', 'cv assignment generalizability', 'assignment generalizability result', 'generalizability result report', 'result report average', 'report average accuracy', 'average accuracy precision', 'accuracy precision recall', 'precision recall f', 'recall f specificity', 'f specificity metric', 'specificity metric performance', 'metric performance find', 'performance find classifier', 'find classifier based', 'classifier based perception', 'based perception model', 'perception model yield', 'model yield average', 'yield average accuracy', 'average accuracy detecting', 'accuracy detecting high', 'detecting high low', 'high low selfdisclosure', 'low selfdisclosure precision', 'selfdisclosure precision recall', 'precision recall see', 'recall see table', 'see table detail', 'table detail method', 'detail method like', 'method like knn', 'like knn k', 'knn k give', 'k give higher', 'give higher precision', 'higher precision expense', 'precision expense low', 'expense low recall', 'low recall figure', 'recall figure give', 'figure give roc', 'give roc receiver', 'roc receiver operating', 'receiver operating characteristic', 'operating characteristic curve', 'characteristic curve model', 'curve model per', 'model per roc', 'per roc curve', 'roc curve corresponding', 'curve corresponding perceptron', 'corresponding perceptron model', 'perceptron model find', 'model find yield', 'find yield maximum', 'yield maximum area', 'maximum area curve', 'area curve hence', 'curve hence best', 'hence best performance', 'best performance identify', 'performance identify table', 'identify table ngrams', 'table ngrams feature', 'ngrams feature highest', 'feature highest weight', 'highest weight given', 'weight given perceptron', 'given perceptron implies', 'perceptron implies feature', 'implies feature significant', 'feature significant classification', 'significant classification task', 'classification task provide', 'task provide brief', 'provide brief qualitative', 'brief qualitative examination', 'qualitative examination ngrams', 'examination ngrams light', 'ngrams light prior', 'light prior psychology', 'prior psychology literature', 'psychology literature selfdisclosure', 'literature selfdisclosure mental', 'selfdisclosure mental health', 'mental health find', 'health find ngrams', 'find ngrams primarily', 'ngrams primarily associated', 'primarily associated vulnerable', 'associated vulnerable selfloathing', 'vulnerable selfloathing thought', 'selfloathing thought eg', 'thought eg thought', 'eg thought suicide', 'thought suicide bear', 'suicide bear negative', 'bear negative tone', 'negative tone depict', 'tone depict confessional', 'depict confessional experience', 'confessional experience based', 'experience based prior', 'based prior research', 'prior research work', 'research work mental', 'work mental health', 'mental health discourse', 'health discourse reddit', 'discourse reddit find', 'reddit find topical', 'find topical dimension', 'topical dimension along', 'dimension along high', 'along high selfdisclosure', 'high selfdisclosure lowno', 'selfdisclosure lowno selfdisclosure', 'lowno selfdisclosure post', 'selfdisclosure post vary', 'post vary essence', 'vary essence high', 'essence high selfdisclosure', 'high selfdisclosure post', 'selfdisclosure post share', 'post share extensively', 'share extensively personal', 'extensively personal belief', 'personal belief fear', 'belief fear instance', 'fear instance vital', 'instance vital construct', 'vital construct private', 'construct private sensitive', 'private sensitive informational', 'sensitive informational attribute', 'informational attribute post', 'attribute post excerpt', 'post excerpt classified', 'excerpt classified high', 'classified high selfdisclosure', 'high selfdisclosure demonstrate', 'selfdisclosure demonstrate use', 'demonstrate use ngrams', 'use ngrams table', 'ngrams table dont', 'table dont want', 'dont want kill', 'want kill havent', 'kill havent felt', 'havent felt suicidal', 'felt suicidal long', 'suicidal long time', 'long time want', 'time want stop', 'want stop life', 'stop life know', 'life know dad', 'know dad would', 'dad would beat', 'would beat living', 'beat living shit', 'living shit ive', 'shit ive hospital', 'ive hospital many', 'hospital many time', 'many time ive', 'time ive lost', 'ive lost track', 'lost track hate', 'track hate hate', 'hate hate dont', 'hate dont want', 'dont want f', 'want f person', 'f person anymore', 'person anymore im', 'anymore im unmotivated', 'im unmotivated unfocused', 'unmotivated unfocused immature']"
https://dl.acm.org/doi/abs/10.1145/2702123.2702280,1,It is possible to extract various features from the activity histories of Twitter users. This section explains what kinds of features are used to estimate degree of depression and the way in which these quantities are extracted. Table 2 shows the features used in this study. A detailed explanation of each feature follows. The frequencies of words in a tweet (i.e. its bag of words) are used as a basic feature relating to the content of the tweet. Tsugawa et al. showed that the word frequencies are useful for identifying depression [37]. MeCab [20] was used to for morphological stemming and categorization of the Japanese tweet text to obtain accurate word frequencies. Particles auxiliary verbs adnominal adjectives and visual symbols were excluded for extracting content words. Words used by only one participant were also excluded resulting in a total of 84255 distinct words. However most of these words were rarely used and the distribution of word frequencies is extremely biased (see Fig. 4). Because words with a low rate of use were regarded as unlikely to be associated with depression for most users the frequencies of only the 20000 words with the highest rate of use (corresponding to 25 or more uses across all participants) were used as a feature in this study. Furthermore because the number and length of tweets differed by participant the word frequencies were normalized by the total number of words in the tweets. The topics of the tweets of each user as estimated by using a representative topic model LDA [5] were used as a second feature relating to the content of the tweets. With LDA the distribution of topics in each document is estimated from the word frequencies in each text through unsupervised learning on the assumption that the text and the words in it are generated according to a particular topic [5]. In LDA the number of topics to identify and a set of documents (as bags of words) are used as input and a topic distribution is output for each document. As mentioned in Related Work Section the topics of essays written by university students were estimated by using LDA and found to be useful in evaluating degree of depression [31]. From that study topics are expected to be a useful feature. A set of all tweets of each user was used as the user document for input in LDA and the 20000 words selected as described above were used as the words. We used LDA with collapsed Gibbs sampling [16]. As the parameters of LDA we used α = 50/K and β = 0.1 where K is the number of topics [16]. All extracted topics were used as the features. The ratio of positive words and the ratio of negative words used in the tweet text are used as the final features relating to tweet content. Users with depression are intuitively expected to use negative words more frequently than users without depression do. To categorize words a dictionary of affective words [19] which is compiled by manual evaluation of a dictionary of positive and negative words extracted according to a technique proposed in the literature [35] is used. The dictionary contains 760 positive words and 862 negative words. The user’s timing of tweets frequency of tweets average number of words retweet rate (rate of republishing other users’ tweets) mention rate (rate of directly referencing at least one other user) ratio of tweets containing a uniform resource locator (URL) number of users being followed and number of users following are used as features independent of the content of the tweet. The relative ratios of tweets posted during each hour of the day were used to characterize the timing of tweets; the number of posts per day was used as the posting frequency; and the ratio of qualifying tweets to all tweets were used for the retweet ratio mention ratio and ratio of tweets containing a URL. These features are used in prior research [14].,it is possible to extract various feature from the activity history of twitter user this section explains what kind of feature are used to estimate degree of depression and the way in which these quantity are extracted table show the feature used in this study a detailed explanation of each feature follows the frequency of word in a tweet ie it bag of word are used a a basic feature relating to the content of the tweet tsugawa et al showed that the word frequency are useful for identifying depression mecab wa used to for morphological stemming and categorization of the japanese tweet text to obtain accurate word frequency particle auxiliary verb adnominal adjective and visual symbol were excluded for extracting content word word used by only one participant were also excluded resulting in a total of distinct word however most of these word were rarely used and the distribution of word frequency is extremely biased see fig because word with a low rate of use were regarded a unlikely to be associated with depression for most user the frequency of only the word with the highest rate of use corresponding to or more us across all participant were used a a feature in this study furthermore because the number and length of tweet differed by participant the word frequency were normalized by the total number of word in the tweet the topic of the tweet of each user a estimated by using a representative topic model lda were used a a second feature relating to the content of the tweet with lda the distribution of topic in each document is estimated from the word frequency in each text through unsupervised learning on the assumption that the text and the word in it are generated according to a particular topic in lda the number of topic to identify and a set of document a bag of word are used a input and a topic distribution is output for each document a mentioned in related work section the topic of essay written by university student were estimated by using lda and found to be useful in evaluating degree of depression from that study topic are expected to be a useful feature a set of all tweet of each user wa used a the user document for input in lda and the word selected a described above were used a the word we used lda with collapsed gibbs sampling a the parameter of lda we used k and where k is the number of topic all extracted topic were used a the feature the ratio of positive word and the ratio of negative word used in the tweet text are used a the final feature relating to tweet content user with depression are intuitively expected to use negative word more frequently than user without depression do to categorize word a dictionary of affective word which is compiled by manual evaluation of a dictionary of positive and negative word extracted according to a technique proposed in the literature is used the dictionary contains positive word and negative word the user timing of tweet frequency of tweet average number of word retweet rate rate of republishing other user tweet mention rate rate of directly referencing at least one other user ratio of tweet containing a uniform resource locator url number of user being followed and number of user following are used a feature independent of the content of the tweet the relative ratio of tweet posted during each hour of the day were used to characterize the timing of tweet the number of post per day wa used a the posting frequency and the ratio of qualifying tweet to all tweet were used for the retweet ratio mention ratio and ratio of tweet containing a url these feature are used in prior research,"['possible', 'extract', 'various', 'feature', 'activity', 'history', 'twitter', 'user', 'section', 'explains', 'kind', 'feature', 'used', 'estimate', 'degree', 'depression', 'way', 'quantity', 'extracted', 'table', 'show', 'feature', 'used', 'study', 'detailed', 'explanation', 'feature', 'follows', 'frequency', 'word', 'tweet', 'ie', 'bag', 'word', 'used', 'basic', 'feature', 'relating', 'content', 'tweet', 'tsugawa', 'et', 'al', 'showed', 'word', 'frequency', 'useful', 'identifying', 'depression', 'mecab', 'wa', 'used', 'morphological', 'stemming', 'categorization', 'japanese', 'tweet', 'text', 'obtain', 'accurate', 'word', 'frequency', 'particle', 'auxiliary', 'verb', 'adnominal', 'adjective', 'visual', 'symbol', 'excluded', 'extracting', 'content', 'word', 'word', 'used', 'one', 'participant', 'also', 'excluded', 'resulting', 'total', 'distinct', 'word', 'however', 'word', 'rarely', 'used', 'distribution', 'word', 'frequency', 'extremely', 'biased', 'see', 'fig', 'word', 'low', 'rate', 'use', 'regarded', 'unlikely', 'associated', 'depression', 'user', 'frequency', 'word', 'highest', 'rate', 'use', 'corresponding', 'us', 'across', 'participant', 'used', 'feature', 'study', 'furthermore', 'number', 'length', 'tweet', 'differed', 'participant', 'word', 'frequency', 'normalized', 'total', 'number', 'word', 'tweet', 'topic', 'tweet', 'user', 'estimated', 'using', 'representative', 'topic', 'model', 'lda', 'used', 'second', 'feature', 'relating', 'content', 'tweet', 'lda', 'distribution', 'topic', 'document', 'estimated', 'word', 'frequency', 'text', 'unsupervised', 'learning', 'assumption', 'text', 'word', 'generated', 'according', 'particular', 'topic', 'lda', 'number', 'topic', 'identify', 'set', 'document', 'bag', 'word', 'used', 'input', 'topic', 'distribution', 'output', 'document', 'mentioned', 'related', 'work', 'section', 'topic', 'essay', 'written', 'university', 'student', 'estimated', 'using', 'lda', 'found', 'useful', 'evaluating', 'degree', 'depression', 'study', 'topic', 'expected', 'useful', 'feature', 'set', 'tweet', 'user', 'wa', 'used', 'user', 'document', 'input', 'lda', 'word', 'selected', 'described', 'used', 'word', 'used', 'lda', 'collapsed', 'gibbs', 'sampling', 'parameter', 'lda', 'used', 'k', 'k', 'number', 'topic', 'extracted', 'topic', 'used', 'feature', 'ratio', 'positive', 'word', 'ratio', 'negative', 'word', 'used', 'tweet', 'text', 'used', 'final', 'feature', 'relating', 'tweet', 'content', 'user', 'depression', 'intuitively', 'expected', 'use', 'negative', 'word', 'frequently', 'user', 'without', 'depression', 'categorize', 'word', 'dictionary', 'affective', 'word', 'compiled', 'manual', 'evaluation', 'dictionary', 'positive', 'negative', 'word', 'extracted', 'according', 'technique', 'proposed', 'literature', 'used', 'dictionary', 'contains', 'positive', 'word', 'negative', 'word', 'user', 'timing', 'tweet', 'frequency', 'tweet', 'average', 'number', 'word', 'retweet', 'rate', 'rate', 'republishing', 'user', 'tweet', 'mention', 'rate', 'rate', 'directly', 'referencing', 'least', 'one', 'user', 'ratio', 'tweet', 'containing', 'uniform', 'resource', 'locator', 'url', 'number', 'user', 'followed', 'number', 'user', 'following', 'used', 'feature', 'independent', 'content', 'tweet', 'relative', 'ratio', 'tweet', 'posted', 'hour', 'day', 'used', 'characterize', 'timing', 'tweet', 'number', 'post', 'per', 'day', 'wa', 'used', 'posting', 'frequency', 'ratio', 'qualifying', 'tweet', 'tweet', 'used', 'retweet', 'ratio', 'mention', 'ratio', 'ratio', 'tweet', 'containing', 'url', 'feature', 'used', 'prior', 'research']","['possible extract', 'extract various', 'various feature', 'feature activity', 'activity history', 'history twitter', 'twitter user', 'user section', 'section explains', 'explains kind', 'kind feature', 'feature used', 'used estimate', 'estimate degree', 'degree depression', 'depression way', 'way quantity', 'quantity extracted', 'extracted table', 'table show', 'show feature', 'feature used', 'used study', 'study detailed', 'detailed explanation', 'explanation feature', 'feature follows', 'follows frequency', 'frequency word', 'word tweet', 'tweet ie', 'ie bag', 'bag word', 'word used', 'used basic', 'basic feature', 'feature relating', 'relating content', 'content tweet', 'tweet tsugawa', 'tsugawa et', 'et al', 'al showed', 'showed word', 'word frequency', 'frequency useful', 'useful identifying', 'identifying depression', 'depression mecab', 'mecab wa', 'wa used', 'used morphological', 'morphological stemming', 'stemming categorization', 'categorization japanese', 'japanese tweet', 'tweet text', 'text obtain', 'obtain accurate', 'accurate word', 'word frequency', 'frequency particle', 'particle auxiliary', 'auxiliary verb', 'verb adnominal', 'adnominal adjective', 'adjective visual', 'visual symbol', 'symbol excluded', 'excluded extracting', 'extracting content', 'content word', 'word word', 'word used', 'used one', 'one participant', 'participant also', 'also excluded', 'excluded resulting', 'resulting total', 'total distinct', 'distinct word', 'word however', 'however word', 'word rarely', 'rarely used', 'used distribution', 'distribution word', 'word frequency', 'frequency extremely', 'extremely biased', 'biased see', 'see fig', 'fig word', 'word low', 'low rate', 'rate use', 'use regarded', 'regarded unlikely', 'unlikely associated', 'associated depression', 'depression user', 'user frequency', 'frequency word', 'word highest', 'highest rate', 'rate use', 'use corresponding', 'corresponding us', 'us across', 'across participant', 'participant used', 'used feature', 'feature study', 'study furthermore', 'furthermore number', 'number length', 'length tweet', 'tweet differed', 'differed participant', 'participant word', 'word frequency', 'frequency normalized', 'normalized total', 'total number', 'number word', 'word tweet', 'tweet topic', 'topic tweet', 'tweet user', 'user estimated', 'estimated using', 'using representative', 'representative topic', 'topic model', 'model lda', 'lda used', 'used second', 'second feature', 'feature relating', 'relating content', 'content tweet', 'tweet lda', 'lda distribution', 'distribution topic', 'topic document', 'document estimated', 'estimated word', 'word frequency', 'frequency text', 'text unsupervised', 'unsupervised learning', 'learning assumption', 'assumption text', 'text word', 'word generated', 'generated according', 'according particular', 'particular topic', 'topic lda', 'lda number', 'number topic', 'topic identify', 'identify set', 'set document', 'document bag', 'bag word', 'word used', 'used input', 'input topic', 'topic distribution', 'distribution output', 'output document', 'document mentioned', 'mentioned related', 'related work', 'work section', 'section topic', 'topic essay', 'essay written', 'written university', 'university student', 'student estimated', 'estimated using', 'using lda', 'lda found', 'found useful', 'useful evaluating', 'evaluating degree', 'degree depression', 'depression study', 'study topic', 'topic expected', 'expected useful', 'useful feature', 'feature set', 'set tweet', 'tweet user', 'user wa', 'wa used', 'used user', 'user document', 'document input', 'input lda', 'lda word', 'word selected', 'selected described', 'described used', 'used word', 'word used', 'used lda', 'lda collapsed', 'collapsed gibbs', 'gibbs sampling', 'sampling parameter', 'parameter lda', 'lda used', 'used k', 'k k', 'k number', 'number topic', 'topic extracted', 'extracted topic', 'topic used', 'used feature', 'feature ratio', 'ratio positive', 'positive word', 'word ratio', 'ratio negative', 'negative word', 'word used', 'used tweet', 'tweet text', 'text used', 'used final', 'final feature', 'feature relating', 'relating tweet', 'tweet content', 'content user', 'user depression', 'depression intuitively', 'intuitively expected', 'expected use', 'use negative', 'negative word', 'word frequently', 'frequently user', 'user without', 'without depression', 'depression categorize', 'categorize word', 'word dictionary', 'dictionary affective', 'affective word', 'word compiled', 'compiled manual', 'manual evaluation', 'evaluation dictionary', 'dictionary positive', 'positive negative', 'negative word', 'word extracted', 'extracted according', 'according technique', 'technique proposed', 'proposed literature', 'literature used', 'used dictionary', 'dictionary contains', 'contains positive', 'positive word', 'word negative', 'negative word', 'word user', 'user timing', 'timing tweet', 'tweet frequency', 'frequency tweet', 'tweet average', 'average number', 'number word', 'word retweet', 'retweet rate', 'rate rate', 'rate republishing', 'republishing user', 'user tweet', 'tweet mention', 'mention rate', 'rate rate', 'rate directly', 'directly referencing', 'referencing least', 'least one', 'one user', 'user ratio', 'ratio tweet', 'tweet containing', 'containing uniform', 'uniform resource', 'resource locator', 'locator url', 'url number', 'number user', 'user followed', 'followed number', 'number user', 'user following', 'following used', 'used feature', 'feature independent', 'independent content', 'content tweet', 'tweet relative', 'relative ratio', 'ratio tweet', 'tweet posted', 'posted hour', 'hour day', 'day used', 'used characterize', 'characterize timing', 'timing tweet', 'tweet number', 'number post', 'post per', 'per day', 'day wa', 'wa used', 'used posting', 'posting frequency', 'frequency ratio', 'ratio qualifying', 'qualifying tweet', 'tweet tweet', 'tweet used', 'used retweet', 'retweet ratio', 'ratio mention', 'mention ratio', 'ratio ratio', 'ratio tweet', 'tweet containing', 'containing url', 'url feature', 'feature used', 'used prior', 'prior research']","['possible extract various', 'extract various feature', 'various feature activity', 'feature activity history', 'activity history twitter', 'history twitter user', 'twitter user section', 'user section explains', 'section explains kind', 'explains kind feature', 'kind feature used', 'feature used estimate', 'used estimate degree', 'estimate degree depression', 'degree depression way', 'depression way quantity', 'way quantity extracted', 'quantity extracted table', 'extracted table show', 'table show feature', 'show feature used', 'feature used study', 'used study detailed', 'study detailed explanation', 'detailed explanation feature', 'explanation feature follows', 'feature follows frequency', 'follows frequency word', 'frequency word tweet', 'word tweet ie', 'tweet ie bag', 'ie bag word', 'bag word used', 'word used basic', 'used basic feature', 'basic feature relating', 'feature relating content', 'relating content tweet', 'content tweet tsugawa', 'tweet tsugawa et', 'tsugawa et al', 'et al showed', 'al showed word', 'showed word frequency', 'word frequency useful', 'frequency useful identifying', 'useful identifying depression', 'identifying depression mecab', 'depression mecab wa', 'mecab wa used', 'wa used morphological', 'used morphological stemming', 'morphological stemming categorization', 'stemming categorization japanese', 'categorization japanese tweet', 'japanese tweet text', 'tweet text obtain', 'text obtain accurate', 'obtain accurate word', 'accurate word frequency', 'word frequency particle', 'frequency particle auxiliary', 'particle auxiliary verb', 'auxiliary verb adnominal', 'verb adnominal adjective', 'adnominal adjective visual', 'adjective visual symbol', 'visual symbol excluded', 'symbol excluded extracting', 'excluded extracting content', 'extracting content word', 'content word word', 'word word used', 'word used one', 'used one participant', 'one participant also', 'participant also excluded', 'also excluded resulting', 'excluded resulting total', 'resulting total distinct', 'total distinct word', 'distinct word however', 'word however word', 'however word rarely', 'word rarely used', 'rarely used distribution', 'used distribution word', 'distribution word frequency', 'word frequency extremely', 'frequency extremely biased', 'extremely biased see', 'biased see fig', 'see fig word', 'fig word low', 'word low rate', 'low rate use', 'rate use regarded', 'use regarded unlikely', 'regarded unlikely associated', 'unlikely associated depression', 'associated depression user', 'depression user frequency', 'user frequency word', 'frequency word highest', 'word highest rate', 'highest rate use', 'rate use corresponding', 'use corresponding us', 'corresponding us across', 'us across participant', 'across participant used', 'participant used feature', 'used feature study', 'feature study furthermore', 'study furthermore number', 'furthermore number length', 'number length tweet', 'length tweet differed', 'tweet differed participant', 'differed participant word', 'participant word frequency', 'word frequency normalized', 'frequency normalized total', 'normalized total number', 'total number word', 'number word tweet', 'word tweet topic', 'tweet topic tweet', 'topic tweet user', 'tweet user estimated', 'user estimated using', 'estimated using representative', 'using representative topic', 'representative topic model', 'topic model lda', 'model lda used', 'lda used second', 'used second feature', 'second feature relating', 'feature relating content', 'relating content tweet', 'content tweet lda', 'tweet lda distribution', 'lda distribution topic', 'distribution topic document', 'topic document estimated', 'document estimated word', 'estimated word frequency', 'word frequency text', 'frequency text unsupervised', 'text unsupervised learning', 'unsupervised learning assumption', 'learning assumption text', 'assumption text word', 'text word generated', 'word generated according', 'generated according particular', 'according particular topic', 'particular topic lda', 'topic lda number', 'lda number topic', 'number topic identify', 'topic identify set', 'identify set document', 'set document bag', 'document bag word', 'bag word used', 'word used input', 'used input topic', 'input topic distribution', 'topic distribution output', 'distribution output document', 'output document mentioned', 'document mentioned related', 'mentioned related work', 'related work section', 'work section topic', 'section topic essay', 'topic essay written', 'essay written university', 'written university student', 'university student estimated', 'student estimated using', 'estimated using lda', 'using lda found', 'lda found useful', 'found useful evaluating', 'useful evaluating degree', 'evaluating degree depression', 'degree depression study', 'depression study topic', 'study topic expected', 'topic expected useful', 'expected useful feature', 'useful feature set', 'feature set tweet', 'set tweet user', 'tweet user wa', 'user wa used', 'wa used user', 'used user document', 'user document input', 'document input lda', 'input lda word', 'lda word selected', 'word selected described', 'selected described used', 'described used word', 'used word used', 'word used lda', 'used lda collapsed', 'lda collapsed gibbs', 'collapsed gibbs sampling', 'gibbs sampling parameter', 'sampling parameter lda', 'parameter lda used', 'lda used k', 'used k k', 'k k number', 'k number topic', 'number topic extracted', 'topic extracted topic', 'extracted topic used', 'topic used feature', 'used feature ratio', 'feature ratio positive', 'ratio positive word', 'positive word ratio', 'word ratio negative', 'ratio negative word', 'negative word used', 'word used tweet', 'used tweet text', 'tweet text used', 'text used final', 'used final feature', 'final feature relating', 'feature relating tweet', 'relating tweet content', 'tweet content user', 'content user depression', 'user depression intuitively', 'depression intuitively expected', 'intuitively expected use', 'expected use negative', 'use negative word', 'negative word frequently', 'word frequently user', 'frequently user without', 'user without depression', 'without depression categorize', 'depression categorize word', 'categorize word dictionary', 'word dictionary affective', 'dictionary affective word', 'affective word compiled', 'word compiled manual', 'compiled manual evaluation', 'manual evaluation dictionary', 'evaluation dictionary positive', 'dictionary positive negative', 'positive negative word', 'negative word extracted', 'word extracted according', 'extracted according technique', 'according technique proposed', 'technique proposed literature', 'proposed literature used', 'literature used dictionary', 'used dictionary contains', 'dictionary contains positive', 'contains positive word', 'positive word negative', 'word negative word', 'negative word user', 'word user timing', 'user timing tweet', 'timing tweet frequency', 'tweet frequency tweet', 'frequency tweet average', 'tweet average number', 'average number word', 'number word retweet', 'word retweet rate', 'retweet rate rate', 'rate rate republishing', 'rate republishing user', 'republishing user tweet', 'user tweet mention', 'tweet mention rate', 'mention rate rate', 'rate rate directly', 'rate directly referencing', 'directly referencing least', 'referencing least one', 'least one user', 'one user ratio', 'user ratio tweet', 'ratio tweet containing', 'tweet containing uniform', 'containing uniform resource', 'uniform resource locator', 'resource locator url', 'locator url number', 'url number user', 'number user followed', 'user followed number', 'followed number user', 'number user following', 'user following used', 'following used feature', 'used feature independent', 'feature independent content', 'independent content tweet', 'content tweet relative', 'tweet relative ratio', 'relative ratio tweet', 'ratio tweet posted', 'tweet posted hour', 'posted hour day', 'hour day used', 'day used characterize', 'used characterize timing', 'characterize timing tweet', 'timing tweet number', 'tweet number post', 'number post per', 'post per day', 'per day wa', 'day wa used', 'wa used posting', 'used posting frequency', 'posting frequency ratio', 'frequency ratio qualifying', 'ratio qualifying tweet', 'qualifying tweet tweet', 'tweet tweet used', 'tweet used retweet', 'used retweet ratio', 'retweet ratio mention', 'ratio mention ratio', 'mention ratio ratio', 'ratio ratio tweet', 'ratio tweet containing', 'tweet containing url', 'containing url feature', 'url feature used', 'feature used prior', 'used prior research']"
https://www.nature.com/articles/s41598-017-12961-9,0,In an effort to minimize noisy and unreliable data we applied several quality assurance measures in our data collection process. MTurk workers who have completed at least 100 tasks with a minimum 95% approval rating have been found to provide reliable valid survey responses33. We restricted survey visibility only to workers with these qualifications. Survey access was also restricted to U.S. IP addresses as MTurk data collected from outside the United States are generally of poorer quality34. All participants were only permitted to take the survey once. We excluded participants with a total of fewer than five Twitter posts. We also excluded participants with CES-D scores of 21 or lower (depression) or TSQ scores of 5 or lower (PTSD). Studies have indicated that a CES-D score of 22 represents an optimal cutoff for identifying clinically relevant depression3536; an equivalent TSQ cutoff of 6 has been found to be optimal in the case of PTSD32. We note here that in the study that inspired the present work De Choudhury et al.8 used two depression scales (CES-D and BDI) and filtered individuals whose depression score did not correlate across the both scales. This additional criteria is a methodological strength of De Choudhury et al.8 with respect to the present work.,in an effort to minimize noisy and unreliable data we applied several quality assurance measure in our data collection process mturk worker who have completed at least task with a minimum approval rating have been found to provide reliable valid survey response we restricted survey visibility only to worker with these qualification survey access wa also restricted to u ip address a mturk data collected from outside the united state are generally of poorer quality all participant were only permitted to take the survey once we excluded participant with a total of fewer than five twitter post we also excluded participant with cesd score of or lower depression or tsq score of or lower ptsd study have indicated that a cesd score of represents an optimal cutoff for identifying clinically relevant depression an equivalent tsq cutoff of ha been found to be optimal in the case of ptsd we note here that in the study that inspired the present work de choudhury et al used two depression scale cesd and bdi and filtered individual whose depression score did not correlate across the both scale this additional criterion is a methodological strength of de choudhury et al with respect to the present work,"['effort', 'minimize', 'noisy', 'unreliable', 'data', 'applied', 'several', 'quality', 'assurance', 'measure', 'data', 'collection', 'process', 'mturk', 'worker', 'completed', 'least', 'task', 'minimum', 'approval', 'rating', 'found', 'provide', 'reliable', 'valid', 'survey', 'response', 'restricted', 'survey', 'visibility', 'worker', 'qualification', 'survey', 'access', 'wa', 'also', 'restricted', 'u', 'ip', 'address', 'mturk', 'data', 'collected', 'outside', 'united', 'state', 'generally', 'poorer', 'quality', 'participant', 'permitted', 'take', 'survey', 'excluded', 'participant', 'total', 'fewer', 'five', 'twitter', 'post', 'also', 'excluded', 'participant', 'cesd', 'score', 'lower', 'depression', 'tsq', 'score', 'lower', 'ptsd', 'study', 'indicated', 'cesd', 'score', 'represents', 'optimal', 'cutoff', 'identifying', 'clinically', 'relevant', 'depression', 'equivalent', 'tsq', 'cutoff', 'ha', 'found', 'optimal', 'case', 'ptsd', 'note', 'study', 'inspired', 'present', 'work', 'de', 'choudhury', 'et', 'al', 'used', 'two', 'depression', 'scale', 'cesd', 'bdi', 'filtered', 'individual', 'whose', 'depression', 'score', 'correlate', 'across', 'scale', 'additional', 'criterion', 'methodological', 'strength', 'de', 'choudhury', 'et', 'al', 'respect', 'present', 'work']","['effort minimize', 'minimize noisy', 'noisy unreliable', 'unreliable data', 'data applied', 'applied several', 'several quality', 'quality assurance', 'assurance measure', 'measure data', 'data collection', 'collection process', 'process mturk', 'mturk worker', 'worker completed', 'completed least', 'least task', 'task minimum', 'minimum approval', 'approval rating', 'rating found', 'found provide', 'provide reliable', 'reliable valid', 'valid survey', 'survey response', 'response restricted', 'restricted survey', 'survey visibility', 'visibility worker', 'worker qualification', 'qualification survey', 'survey access', 'access wa', 'wa also', 'also restricted', 'restricted u', 'u ip', 'ip address', 'address mturk', 'mturk data', 'data collected', 'collected outside', 'outside united', 'united state', 'state generally', 'generally poorer', 'poorer quality', 'quality participant', 'participant permitted', 'permitted take', 'take survey', 'survey excluded', 'excluded participant', 'participant total', 'total fewer', 'fewer five', 'five twitter', 'twitter post', 'post also', 'also excluded', 'excluded participant', 'participant cesd', 'cesd score', 'score lower', 'lower depression', 'depression tsq', 'tsq score', 'score lower', 'lower ptsd', 'ptsd study', 'study indicated', 'indicated cesd', 'cesd score', 'score represents', 'represents optimal', 'optimal cutoff', 'cutoff identifying', 'identifying clinically', 'clinically relevant', 'relevant depression', 'depression equivalent', 'equivalent tsq', 'tsq cutoff', 'cutoff ha', 'ha found', 'found optimal', 'optimal case', 'case ptsd', 'ptsd note', 'note study', 'study inspired', 'inspired present', 'present work', 'work de', 'de choudhury', 'choudhury et', 'et al', 'al used', 'used two', 'two depression', 'depression scale', 'scale cesd', 'cesd bdi', 'bdi filtered', 'filtered individual', 'individual whose', 'whose depression', 'depression score', 'score correlate', 'correlate across', 'across scale', 'scale additional', 'additional criterion', 'criterion methodological', 'methodological strength', 'strength de', 'de choudhury', 'choudhury et', 'et al', 'al respect', 'respect present', 'present work']","['effort minimize noisy', 'minimize noisy unreliable', 'noisy unreliable data', 'unreliable data applied', 'data applied several', 'applied several quality', 'several quality assurance', 'quality assurance measure', 'assurance measure data', 'measure data collection', 'data collection process', 'collection process mturk', 'process mturk worker', 'mturk worker completed', 'worker completed least', 'completed least task', 'least task minimum', 'task minimum approval', 'minimum approval rating', 'approval rating found', 'rating found provide', 'found provide reliable', 'provide reliable valid', 'reliable valid survey', 'valid survey response', 'survey response restricted', 'response restricted survey', 'restricted survey visibility', 'survey visibility worker', 'visibility worker qualification', 'worker qualification survey', 'qualification survey access', 'survey access wa', 'access wa also', 'wa also restricted', 'also restricted u', 'restricted u ip', 'u ip address', 'ip address mturk', 'address mturk data', 'mturk data collected', 'data collected outside', 'collected outside united', 'outside united state', 'united state generally', 'state generally poorer', 'generally poorer quality', 'poorer quality participant', 'quality participant permitted', 'participant permitted take', 'permitted take survey', 'take survey excluded', 'survey excluded participant', 'excluded participant total', 'participant total fewer', 'total fewer five', 'fewer five twitter', 'five twitter post', 'twitter post also', 'post also excluded', 'also excluded participant', 'excluded participant cesd', 'participant cesd score', 'cesd score lower', 'score lower depression', 'lower depression tsq', 'depression tsq score', 'tsq score lower', 'score lower ptsd', 'lower ptsd study', 'ptsd study indicated', 'study indicated cesd', 'indicated cesd score', 'cesd score represents', 'score represents optimal', 'represents optimal cutoff', 'optimal cutoff identifying', 'cutoff identifying clinically', 'identifying clinically relevant', 'clinically relevant depression', 'relevant depression equivalent', 'depression equivalent tsq', 'equivalent tsq cutoff', 'tsq cutoff ha', 'cutoff ha found', 'ha found optimal', 'found optimal case', 'optimal case ptsd', 'case ptsd note', 'ptsd note study', 'note study inspired', 'study inspired present', 'inspired present work', 'present work de', 'work de choudhury', 'de choudhury et', 'choudhury et al', 'et al used', 'al used two', 'used two depression', 'two depression scale', 'depression scale cesd', 'scale cesd bdi', 'cesd bdi filtered', 'bdi filtered individual', 'filtered individual whose', 'individual whose depression', 'whose depression score', 'depression score correlate', 'score correlate across', 'correlate across scale', 'across scale additional', 'scale additional criterion', 'additional criterion methodological', 'criterion methodological strength', 'methodological strength de', 'strength de choudhury', 'de choudhury et', 'choudhury et al', 'et al respect', 'al respect present', 'respect present work']"
https://aclanthology.org/W14-3214.pdf,1,We estimated user-level degree of depression (DDep) as the average response to seven depression facet items which are nested within the larger Neuroticism item pool. For each item users indicated how accurately short phrases described themselves (e.g. “often feel blue” “dislike myself”; responses ranged from 1 = very inaccurate to 5 = very accurate). Figure 1a shows the distribution of surveyassessed DDep (standardized). The items can be seen in Table 1. Figure 2 shows the daily averages of surveyassessed DDep collapsed across years. A LOESS smoother over the daily averages illustrates a seasonal trend with depression rising over the winter months and dropping during the summer.,we estimated userlevel degree of depression ddep a the average response to seven depression facet item which are nested within the larger neuroticism item pool for each item user indicated how accurately short phrase described themselves eg often feel blue dislike myself response ranged from very inaccurate to very accurate figure a show the distribution of surveyassessed ddep standardized the item can be seen in table figure show the daily average of surveyassessed ddep collapsed across year a loess smoother over the daily average illustrates a seasonal trend with depression rising over the winter month and dropping during the summer,"['estimated', 'userlevel', 'degree', 'depression', 'ddep', 'average', 'response', 'seven', 'depression', 'facet', 'item', 'nested', 'within', 'larger', 'neuroticism', 'item', 'pool', 'item', 'user', 'indicated', 'accurately', 'short', 'phrase', 'described', 'eg', 'often', 'feel', 'blue', 'dislike', 'response', 'ranged', 'inaccurate', 'accurate', 'figure', 'show', 'distribution', 'surveyassessed', 'ddep', 'standardized', 'item', 'seen', 'table', 'figure', 'show', 'daily', 'average', 'surveyassessed', 'ddep', 'collapsed', 'across', 'year', 'loess', 'smoother', 'daily', 'average', 'illustrates', 'seasonal', 'trend', 'depression', 'rising', 'winter', 'month', 'dropping', 'summer']","['estimated userlevel', 'userlevel degree', 'degree depression', 'depression ddep', 'ddep average', 'average response', 'response seven', 'seven depression', 'depression facet', 'facet item', 'item nested', 'nested within', 'within larger', 'larger neuroticism', 'neuroticism item', 'item pool', 'pool item', 'item user', 'user indicated', 'indicated accurately', 'accurately short', 'short phrase', 'phrase described', 'described eg', 'eg often', 'often feel', 'feel blue', 'blue dislike', 'dislike response', 'response ranged', 'ranged inaccurate', 'inaccurate accurate', 'accurate figure', 'figure show', 'show distribution', 'distribution surveyassessed', 'surveyassessed ddep', 'ddep standardized', 'standardized item', 'item seen', 'seen table', 'table figure', 'figure show', 'show daily', 'daily average', 'average surveyassessed', 'surveyassessed ddep', 'ddep collapsed', 'collapsed across', 'across year', 'year loess', 'loess smoother', 'smoother daily', 'daily average', 'average illustrates', 'illustrates seasonal', 'seasonal trend', 'trend depression', 'depression rising', 'rising winter', 'winter month', 'month dropping', 'dropping summer']","['estimated userlevel degree', 'userlevel degree depression', 'degree depression ddep', 'depression ddep average', 'ddep average response', 'average response seven', 'response seven depression', 'seven depression facet', 'depression facet item', 'facet item nested', 'item nested within', 'nested within larger', 'within larger neuroticism', 'larger neuroticism item', 'neuroticism item pool', 'item pool item', 'pool item user', 'item user indicated', 'user indicated accurately', 'indicated accurately short', 'accurately short phrase', 'short phrase described', 'phrase described eg', 'described eg often', 'eg often feel', 'often feel blue', 'feel blue dislike', 'blue dislike response', 'dislike response ranged', 'response ranged inaccurate', 'ranged inaccurate accurate', 'inaccurate accurate figure', 'accurate figure show', 'figure show distribution', 'show distribution surveyassessed', 'distribution surveyassessed ddep', 'surveyassessed ddep standardized', 'ddep standardized item', 'standardized item seen', 'item seen table', 'seen table figure', 'table figure show', 'figure show daily', 'show daily average', 'daily average surveyassessed', 'average surveyassessed ddep', 'surveyassessed ddep collapsed', 'ddep collapsed across', 'collapsed across year', 'across year loess', 'year loess smoother', 'loess smoother daily', 'smoother daily average', 'daily average illustrates', 'average illustrates seasonal', 'illustrates seasonal trend', 'seasonal trend depression', 'trend depression rising', 'depression rising winter', 'rising winter month', 'winter month dropping', 'month dropping summer']"
https://ieeexplore.ieee.org/abstract/document/6784326,0,To characterize the difference between CLINICAL and CONTROL communities a variety of features are extracted: Affective features: We use the lexicon—Affective Norms for English Words (ANEW) [5]—to extract the sentiment conveyed in the content. This lexicon consists of 1034 words rated in terms of valence and arousal and is thus suitable for a quantitative estimation. The valence of ANEW words is on a scale of 1 (very unpleasant) to 9 (very pleasant). The arousal is measured on the same scale—1 (least active) to 9 (most active). A cloud visualization of ANEW words used in the blog posts made by CLINICAL and CONTROL groups is illustrated in Fig. 1. Mood tags: LiveJournal provides a mechanism for users to tag their posts from a list of 132 pre-defined mood labels.4 Thus in addition to the emotion expressed in the text of posts the mood tag produced allows us direct access to the user sentiment. A cloud visualization of moods tagged on blog posts made by CLINICAL and CONTROL communities is llustrated in Fig. 2. LIWC features: We examine the proportions of words in psycholinguistic categories as defined in the LIWC package [27]: linguistic social affective cognitive perceptual biological relativity personal concerns and spoken.5 Table 3 presents the mean of these LIWC psycholinguistic processes for the CLINICAL and CONTROL communities. Whilst similar in the use words with positive emotion people in the CLINICAL communities tend to use words with more negative emotion—as examples anxiety anger and sadness. Further they discuss more issues about health and death in comparison with the CONTROL group. On the other hand the users in the CONTROL group discuss more neutral life related topics—ingestion home and leisure words. Topics: For extracting topics latent Dirichlet allocation (LDA) [4] is used as a Bayesian probabilistic modelling framework. LDA extracts the probabilities —that is words in a topic and then assigns a topic to each word in a document. For the inference part we implemented Gibbs inference detailed in [10]. We set the number of topics to 50 run the Gibbs for 5000 samples and use the last Gibbs sample to interpret the results.,to characterize the difference between clinical and control community a variety of feature are extracted affective feature we use the lexiconaffective norm for english word anew to extract the sentiment conveyed in the content this lexicon consists of word rated in term of valence and arousal and is thus suitable for a quantitative estimation the valence of anew word is on a scale of very unpleasant to very pleasant the arousal is measured on the same scale least active to most active a cloud visualization of anew word used in the blog post made by clinical and control group is illustrated in fig mood tag livejournal provides a mechanism for user to tag their post from a list of predefined mood label thus in addition to the emotion expressed in the text of post the mood tag produced allows u direct access to the user sentiment a cloud visualization of mood tagged on blog post made by clinical and control community is llustrated in fig liwc feature we examine the proportion of word in psycholinguistic category a defined in the liwc package linguistic social affective cognitive perceptual biological relativity personal concern and spoken table present the mean of these liwc psycholinguistic process for the clinical and control community whilst similar in the use word with positive emotion people in the clinical community tend to use word with more negative emotionas example anxiety anger and sadness further they discus more issue about health and death in comparison with the control group on the other hand the user in the control group discus more neutral life related topicsingestion home and leisure word topic for extracting topic latent dirichlet allocation lda is used a a bayesian probabilistic modelling framework lda extract the probability that is word in a topic and then assigns a topic to each word in a document for the inference part we implemented gibbs inference detailed in we set the number of topic to run the gibbs for sample and use the last gibbs sample to interpret the result,"['characterize', 'difference', 'clinical', 'control', 'community', 'variety', 'feature', 'extracted', 'affective', 'feature', 'use', 'lexiconaffective', 'norm', 'english', 'word', 'anew', 'extract', 'sentiment', 'conveyed', 'content', 'lexicon', 'consists', 'word', 'rated', 'term', 'valence', 'arousal', 'thus', 'suitable', 'quantitative', 'estimation', 'valence', 'anew', 'word', 'scale', 'unpleasant', 'pleasant', 'arousal', 'measured', 'scale', 'least', 'active', 'active', 'cloud', 'visualization', 'anew', 'word', 'used', 'blog', 'post', 'made', 'clinical', 'control', 'group', 'illustrated', 'fig', 'mood', 'tag', 'livejournal', 'provides', 'mechanism', 'user', 'tag', 'post', 'list', 'predefined', 'mood', 'label', 'thus', 'addition', 'emotion', 'expressed', 'text', 'post', 'mood', 'tag', 'produced', 'allows', 'u', 'direct', 'access', 'user', 'sentiment', 'cloud', 'visualization', 'mood', 'tagged', 'blog', 'post', 'made', 'clinical', 'control', 'community', 'llustrated', 'fig', 'liwc', 'feature', 'examine', 'proportion', 'word', 'psycholinguistic', 'category', 'defined', 'liwc', 'package', 'linguistic', 'social', 'affective', 'cognitive', 'perceptual', 'biological', 'relativity', 'personal', 'concern', 'spoken', 'table', 'present', 'mean', 'liwc', 'psycholinguistic', 'process', 'clinical', 'control', 'community', 'whilst', 'similar', 'use', 'word', 'positive', 'emotion', 'people', 'clinical', 'community', 'tend', 'use', 'word', 'negative', 'emotionas', 'example', 'anxiety', 'anger', 'sadness', 'discus', 'issue', 'health', 'death', 'comparison', 'control', 'group', 'hand', 'user', 'control', 'group', 'discus', 'neutral', 'life', 'related', 'topicsingestion', 'home', 'leisure', 'word', 'topic', 'extracting', 'topic', 'latent', 'dirichlet', 'allocation', 'lda', 'used', 'bayesian', 'probabilistic', 'modelling', 'framework', 'lda', 'extract', 'probability', 'word', 'topic', 'assigns', 'topic', 'word', 'document', 'inference', 'part', 'implemented', 'gibbs', 'inference', 'detailed', 'set', 'number', 'topic', 'run', 'gibbs', 'sample', 'use', 'last', 'gibbs', 'sample', 'interpret', 'result']","['characterize difference', 'difference clinical', 'clinical control', 'control community', 'community variety', 'variety feature', 'feature extracted', 'extracted affective', 'affective feature', 'feature use', 'use lexiconaffective', 'lexiconaffective norm', 'norm english', 'english word', 'word anew', 'anew extract', 'extract sentiment', 'sentiment conveyed', 'conveyed content', 'content lexicon', 'lexicon consists', 'consists word', 'word rated', 'rated term', 'term valence', 'valence arousal', 'arousal thus', 'thus suitable', 'suitable quantitative', 'quantitative estimation', 'estimation valence', 'valence anew', 'anew word', 'word scale', 'scale unpleasant', 'unpleasant pleasant', 'pleasant arousal', 'arousal measured', 'measured scale', 'scale least', 'least active', 'active active', 'active cloud', 'cloud visualization', 'visualization anew', 'anew word', 'word used', 'used blog', 'blog post', 'post made', 'made clinical', 'clinical control', 'control group', 'group illustrated', 'illustrated fig', 'fig mood', 'mood tag', 'tag livejournal', 'livejournal provides', 'provides mechanism', 'mechanism user', 'user tag', 'tag post', 'post list', 'list predefined', 'predefined mood', 'mood label', 'label thus', 'thus addition', 'addition emotion', 'emotion expressed', 'expressed text', 'text post', 'post mood', 'mood tag', 'tag produced', 'produced allows', 'allows u', 'u direct', 'direct access', 'access user', 'user sentiment', 'sentiment cloud', 'cloud visualization', 'visualization mood', 'mood tagged', 'tagged blog', 'blog post', 'post made', 'made clinical', 'clinical control', 'control community', 'community llustrated', 'llustrated fig', 'fig liwc', 'liwc feature', 'feature examine', 'examine proportion', 'proportion word', 'word psycholinguistic', 'psycholinguistic category', 'category defined', 'defined liwc', 'liwc package', 'package linguistic', 'linguistic social', 'social affective', 'affective cognitive', 'cognitive perceptual', 'perceptual biological', 'biological relativity', 'relativity personal', 'personal concern', 'concern spoken', 'spoken table', 'table present', 'present mean', 'mean liwc', 'liwc psycholinguistic', 'psycholinguistic process', 'process clinical', 'clinical control', 'control community', 'community whilst', 'whilst similar', 'similar use', 'use word', 'word positive', 'positive emotion', 'emotion people', 'people clinical', 'clinical community', 'community tend', 'tend use', 'use word', 'word negative', 'negative emotionas', 'emotionas example', 'example anxiety', 'anxiety anger', 'anger sadness', 'sadness discus', 'discus issue', 'issue health', 'health death', 'death comparison', 'comparison control', 'control group', 'group hand', 'hand user', 'user control', 'control group', 'group discus', 'discus neutral', 'neutral life', 'life related', 'related topicsingestion', 'topicsingestion home', 'home leisure', 'leisure word', 'word topic', 'topic extracting', 'extracting topic', 'topic latent', 'latent dirichlet', 'dirichlet allocation', 'allocation lda', 'lda used', 'used bayesian', 'bayesian probabilistic', 'probabilistic modelling', 'modelling framework', 'framework lda', 'lda extract', 'extract probability', 'probability word', 'word topic', 'topic assigns', 'assigns topic', 'topic word', 'word document', 'document inference', 'inference part', 'part implemented', 'implemented gibbs', 'gibbs inference', 'inference detailed', 'detailed set', 'set number', 'number topic', 'topic run', 'run gibbs', 'gibbs sample', 'sample use', 'use last', 'last gibbs', 'gibbs sample', 'sample interpret', 'interpret result']","['characterize difference clinical', 'difference clinical control', 'clinical control community', 'control community variety', 'community variety feature', 'variety feature extracted', 'feature extracted affective', 'extracted affective feature', 'affective feature use', 'feature use lexiconaffective', 'use lexiconaffective norm', 'lexiconaffective norm english', 'norm english word', 'english word anew', 'word anew extract', 'anew extract sentiment', 'extract sentiment conveyed', 'sentiment conveyed content', 'conveyed content lexicon', 'content lexicon consists', 'lexicon consists word', 'consists word rated', 'word rated term', 'rated term valence', 'term valence arousal', 'valence arousal thus', 'arousal thus suitable', 'thus suitable quantitative', 'suitable quantitative estimation', 'quantitative estimation valence', 'estimation valence anew', 'valence anew word', 'anew word scale', 'word scale unpleasant', 'scale unpleasant pleasant', 'unpleasant pleasant arousal', 'pleasant arousal measured', 'arousal measured scale', 'measured scale least', 'scale least active', 'least active active', 'active active cloud', 'active cloud visualization', 'cloud visualization anew', 'visualization anew word', 'anew word used', 'word used blog', 'used blog post', 'blog post made', 'post made clinical', 'made clinical control', 'clinical control group', 'control group illustrated', 'group illustrated fig', 'illustrated fig mood', 'fig mood tag', 'mood tag livejournal', 'tag livejournal provides', 'livejournal provides mechanism', 'provides mechanism user', 'mechanism user tag', 'user tag post', 'tag post list', 'post list predefined', 'list predefined mood', 'predefined mood label', 'mood label thus', 'label thus addition', 'thus addition emotion', 'addition emotion expressed', 'emotion expressed text', 'expressed text post', 'text post mood', 'post mood tag', 'mood tag produced', 'tag produced allows', 'produced allows u', 'allows u direct', 'u direct access', 'direct access user', 'access user sentiment', 'user sentiment cloud', 'sentiment cloud visualization', 'cloud visualization mood', 'visualization mood tagged', 'mood tagged blog', 'tagged blog post', 'blog post made', 'post made clinical', 'made clinical control', 'clinical control community', 'control community llustrated', 'community llustrated fig', 'llustrated fig liwc', 'fig liwc feature', 'liwc feature examine', 'feature examine proportion', 'examine proportion word', 'proportion word psycholinguistic', 'word psycholinguistic category', 'psycholinguistic category defined', 'category defined liwc', 'defined liwc package', 'liwc package linguistic', 'package linguistic social', 'linguistic social affective', 'social affective cognitive', 'affective cognitive perceptual', 'cognitive perceptual biological', 'perceptual biological relativity', 'biological relativity personal', 'relativity personal concern', 'personal concern spoken', 'concern spoken table', 'spoken table present', 'table present mean', 'present mean liwc', 'mean liwc psycholinguistic', 'liwc psycholinguistic process', 'psycholinguistic process clinical', 'process clinical control', 'clinical control community', 'control community whilst', 'community whilst similar', 'whilst similar use', 'similar use word', 'use word positive', 'word positive emotion', 'positive emotion people', 'emotion people clinical', 'people clinical community', 'clinical community tend', 'community tend use', 'tend use word', 'use word negative', 'word negative emotionas', 'negative emotionas example', 'emotionas example anxiety', 'example anxiety anger', 'anxiety anger sadness', 'anger sadness discus', 'sadness discus issue', 'discus issue health', 'issue health death', 'health death comparison', 'death comparison control', 'comparison control group', 'control group hand', 'group hand user', 'hand user control', 'user control group', 'control group discus', 'group discus neutral', 'discus neutral life', 'neutral life related', 'life related topicsingestion', 'related topicsingestion home', 'topicsingestion home leisure', 'home leisure word', 'leisure word topic', 'word topic extracting', 'topic extracting topic', 'extracting topic latent', 'topic latent dirichlet', 'latent dirichlet allocation', 'dirichlet allocation lda', 'allocation lda used', 'lda used bayesian', 'used bayesian probabilistic', 'bayesian probabilistic modelling', 'probabilistic modelling framework', 'modelling framework lda', 'framework lda extract', 'lda extract probability', 'extract probability word', 'probability word topic', 'word topic assigns', 'topic assigns topic', 'assigns topic word', 'topic word document', 'word document inference', 'document inference part', 'inference part implemented', 'part implemented gibbs', 'implemented gibbs inference', 'gibbs inference detailed', 'inference detailed set', 'detailed set number', 'set number topic', 'number topic run', 'topic run gibbs', 'run gibbs sample', 'gibbs sample use', 'sample use last', 'use last gibbs', 'last gibbs sample', 'gibbs sample interpret', 'sample interpret result']"
https://www.jmir.org/2017/7/e243/,0,Weibo posts were segmented using the Stanford word segmenter [39] that resulted in 349374 words and phrases. Thereafter the SC-LIWC [33] dictionary was applied to count the appearance of each category of words in every respondents’ Weibo posts. The SC-LIWC dictionary includes 7450 words that are grouped into 71 categories including 7 main linguistic or psychological categories and 64 subcategories. In addition the total number of words or phrases that each respondent published in the 12 months was counted as the 72nd category. Scores of the SC-LIWC categories were counted as percentages of the total number of words.,weibo post were segmented using the stanford word segmenter that resulted in word and phrase thereafter the scliwc dictionary wa applied to count the appearance of each category of word in every respondent weibo post the scliwc dictionary includes word that are grouped into category including main linguistic or psychological category and subcategories in addition the total number of word or phrase that each respondent published in the month wa counted a the nd category score of the scliwc category were counted a percentage of the total number of word,"['weibo', 'post', 'segmented', 'using', 'stanford', 'word', 'segmenter', 'resulted', 'word', 'phrase', 'thereafter', 'scliwc', 'dictionary', 'wa', 'applied', 'count', 'appearance', 'category', 'word', 'every', 'respondent', 'weibo', 'post', 'scliwc', 'dictionary', 'includes', 'word', 'grouped', 'category', 'including', 'main', 'linguistic', 'psychological', 'category', 'subcategories', 'addition', 'total', 'number', 'word', 'phrase', 'respondent', 'published', 'month', 'wa', 'counted', 'nd', 'category', 'score', 'scliwc', 'category', 'counted', 'percentage', 'total', 'number', 'word']","['weibo post', 'post segmented', 'segmented using', 'using stanford', 'stanford word', 'word segmenter', 'segmenter resulted', 'resulted word', 'word phrase', 'phrase thereafter', 'thereafter scliwc', 'scliwc dictionary', 'dictionary wa', 'wa applied', 'applied count', 'count appearance', 'appearance category', 'category word', 'word every', 'every respondent', 'respondent weibo', 'weibo post', 'post scliwc', 'scliwc dictionary', 'dictionary includes', 'includes word', 'word grouped', 'grouped category', 'category including', 'including main', 'main linguistic', 'linguistic psychological', 'psychological category', 'category subcategories', 'subcategories addition', 'addition total', 'total number', 'number word', 'word phrase', 'phrase respondent', 'respondent published', 'published month', 'month wa', 'wa counted', 'counted nd', 'nd category', 'category score', 'score scliwc', 'scliwc category', 'category counted', 'counted percentage', 'percentage total', 'total number', 'number word']","['weibo post segmented', 'post segmented using', 'segmented using stanford', 'using stanford word', 'stanford word segmenter', 'word segmenter resulted', 'segmenter resulted word', 'resulted word phrase', 'word phrase thereafter', 'phrase thereafter scliwc', 'thereafter scliwc dictionary', 'scliwc dictionary wa', 'dictionary wa applied', 'wa applied count', 'applied count appearance', 'count appearance category', 'appearance category word', 'category word every', 'word every respondent', 'every respondent weibo', 'respondent weibo post', 'weibo post scliwc', 'post scliwc dictionary', 'scliwc dictionary includes', 'dictionary includes word', 'includes word grouped', 'word grouped category', 'grouped category including', 'category including main', 'including main linguistic', 'main linguistic psychological', 'linguistic psychological category', 'psychological category subcategories', 'category subcategories addition', 'subcategories addition total', 'addition total number', 'total number word', 'number word phrase', 'word phrase respondent', 'phrase respondent published', 'respondent published month', 'published month wa', 'month wa counted', 'wa counted nd', 'counted nd category', 'nd category score', 'category score scliwc', 'score scliwc category', 'scliwc category counted', 'category counted percentage', 'counted percentage total', 'percentage total number', 'total number word']"
https://dl.acm.org/doi/abs/10.1145/3025453.3025909,0,We now present a methodology of identifying posts shared in university subreddits that that are likely to be mental health expressions. Note that our Reddit data does not contain any gold standard information around whether a post shared in a university subreddit is about one’s mental health experience or condition. Our proposed method overcomes this challenge by employing an inductive transfer learning approach [16]. First we include (as ground truth data) Reddit posts made on various mental health support communities. Prior work has established that in these communities individuals selfdisclose a variety of mental health challenges explicitly [50]. Parallelly we utilize another set of Reddit posts made on generic subreddits unrelated to mental health to be a control. Next we build a machine learning classifier to distinguish between these two types of posts. Then we learn features that could detect whether an post shared in a university subreddit could be an expression of some mental health concern. We discuss these steps in detail in the following subsections.,we now present a methodology of identifying post shared in university subreddits that that are likely to be mental health expression note that our reddit data doe not contain any gold standard information around whether a post shared in a university subreddit is about one mental health experience or condition our proposed method overcomes this challenge by employing an inductive transfer learning approach first we include a ground truth data reddit post made on various mental health support community prior work ha established that in these community individual selfdisclose a variety of mental health challenge explicitly parallelly we utilize another set of reddit post made on generic subreddits unrelated to mental health to be a control next we build a machine learning classifier to distinguish between these two type of post then we learn feature that could detect whether an post shared in a university subreddit could be an expression of some mental health concern we discus these step in detail in the following subsection,"['present', 'methodology', 'identifying', 'post', 'shared', 'university', 'subreddits', 'likely', 'mental', 'health', 'expression', 'note', 'reddit', 'data', 'doe', 'contain', 'gold', 'standard', 'information', 'around', 'whether', 'post', 'shared', 'university', 'subreddit', 'one', 'mental', 'health', 'experience', 'condition', 'proposed', 'method', 'overcomes', 'challenge', 'employing', 'inductive', 'transfer', 'learning', 'approach', 'first', 'include', 'ground', 'truth', 'data', 'reddit', 'post', 'made', 'various', 'mental', 'health', 'support', 'community', 'prior', 'work', 'ha', 'established', 'community', 'individual', 'selfdisclose', 'variety', 'mental', 'health', 'challenge', 'explicitly', 'parallelly', 'utilize', 'another', 'set', 'reddit', 'post', 'made', 'generic', 'subreddits', 'unrelated', 'mental', 'health', 'control', 'next', 'build', 'machine', 'learning', 'classifier', 'distinguish', 'two', 'type', 'post', 'learn', 'feature', 'could', 'detect', 'whether', 'post', 'shared', 'university', 'subreddit', 'could', 'expression', 'mental', 'health', 'concern', 'discus', 'step', 'detail', 'following', 'subsection']","['present methodology', 'methodology identifying', 'identifying post', 'post shared', 'shared university', 'university subreddits', 'subreddits likely', 'likely mental', 'mental health', 'health expression', 'expression note', 'note reddit', 'reddit data', 'data doe', 'doe contain', 'contain gold', 'gold standard', 'standard information', 'information around', 'around whether', 'whether post', 'post shared', 'shared university', 'university subreddit', 'subreddit one', 'one mental', 'mental health', 'health experience', 'experience condition', 'condition proposed', 'proposed method', 'method overcomes', 'overcomes challenge', 'challenge employing', 'employing inductive', 'inductive transfer', 'transfer learning', 'learning approach', 'approach first', 'first include', 'include ground', 'ground truth', 'truth data', 'data reddit', 'reddit post', 'post made', 'made various', 'various mental', 'mental health', 'health support', 'support community', 'community prior', 'prior work', 'work ha', 'ha established', 'established community', 'community individual', 'individual selfdisclose', 'selfdisclose variety', 'variety mental', 'mental health', 'health challenge', 'challenge explicitly', 'explicitly parallelly', 'parallelly utilize', 'utilize another', 'another set', 'set reddit', 'reddit post', 'post made', 'made generic', 'generic subreddits', 'subreddits unrelated', 'unrelated mental', 'mental health', 'health control', 'control next', 'next build', 'build machine', 'machine learning', 'learning classifier', 'classifier distinguish', 'distinguish two', 'two type', 'type post', 'post learn', 'learn feature', 'feature could', 'could detect', 'detect whether', 'whether post', 'post shared', 'shared university', 'university subreddit', 'subreddit could', 'could expression', 'expression mental', 'mental health', 'health concern', 'concern discus', 'discus step', 'step detail', 'detail following', 'following subsection']","['present methodology identifying', 'methodology identifying post', 'identifying post shared', 'post shared university', 'shared university subreddits', 'university subreddits likely', 'subreddits likely mental', 'likely mental health', 'mental health expression', 'health expression note', 'expression note reddit', 'note reddit data', 'reddit data doe', 'data doe contain', 'doe contain gold', 'contain gold standard', 'gold standard information', 'standard information around', 'information around whether', 'around whether post', 'whether post shared', 'post shared university', 'shared university subreddit', 'university subreddit one', 'subreddit one mental', 'one mental health', 'mental health experience', 'health experience condition', 'experience condition proposed', 'condition proposed method', 'proposed method overcomes', 'method overcomes challenge', 'overcomes challenge employing', 'challenge employing inductive', 'employing inductive transfer', 'inductive transfer learning', 'transfer learning approach', 'learning approach first', 'approach first include', 'first include ground', 'include ground truth', 'ground truth data', 'truth data reddit', 'data reddit post', 'reddit post made', 'post made various', 'made various mental', 'various mental health', 'mental health support', 'health support community', 'support community prior', 'community prior work', 'prior work ha', 'work ha established', 'ha established community', 'established community individual', 'community individual selfdisclose', 'individual selfdisclose variety', 'selfdisclose variety mental', 'variety mental health', 'mental health challenge', 'health challenge explicitly', 'challenge explicitly parallelly', 'explicitly parallelly utilize', 'parallelly utilize another', 'utilize another set', 'another set reddit', 'set reddit post', 'reddit post made', 'post made generic', 'made generic subreddits', 'generic subreddits unrelated', 'subreddits unrelated mental', 'unrelated mental health', 'mental health control', 'health control next', 'control next build', 'next build machine', 'build machine learning', 'machine learning classifier', 'learning classifier distinguish', 'classifier distinguish two', 'distinguish two type', 'two type post', 'type post learn', 'post learn feature', 'learn feature could', 'feature could detect', 'could detect whether', 'detect whether post', 'whether post shared', 'post shared university', 'shared university subreddit', 'university subreddit could', 'subreddit could expression', 'could expression mental', 'expression mental health', 'mental health concern', 'health concern discus', 'concern discus step', 'discus step detail', 'step detail following', 'detail following subsection']"
https://ieeexplore.ieee.org/abstract/document/7752434,1,In this work we are focused on two main type of features (linguistic and behavioral). TF-IDF is adopted to model the linguist features of patients and Pattern of Life Features (PLF) adopted from the work of Coppersmith et al. [1] is used to model the behavioral style of patients. TF-IDF Features To capture the frequent and representative words used by the patients TF-IDF is applied on the unigram and bigrams collected from all the patients' tweets. Pattern of Life Features (PLF) These features reveal the emotional patterns and behavioral tendency of users by measuring polarity emotion and social interactions. In order to fully compose the PLF we combined the following list of features: Age and Gender: Twitter does not publicly provide information about the age and gender of its users mainly due to privacy concerns so we adopted the work of Sap et al. [5] to fill in this information. Polarity Features: The Sentiment140 API 3 was used to label each tweet as either positive negative or neutral. The polarity is furthermore transformed into five different values to capture the affective traits of each user: 1) Positive Ratio: the percentage of positive tweets 2) Negative Ratio: the percentage of negative tweets 3) Positive Combo: captures the mania and hypomania traits of patients which is determined by the number of continuous positive posts appearing more than x amount of times within a period of time in minutes T. 4) Negative Combos: captures the depression traits of patients and is determined by the number of continuous negative posts appearing more than x amount of times within a period of time in minutes T. 5) Flips Ratio: quantifies the emotional unstableness and is determined by counting how frequently two continuous tweets with different polarity (either positive to negative or negative to positive) appear together within a period of time in minutes T. In our work x is set to 2 and T is set to 30 minutes. Social Features: These features can demonstrate how users are behaving with respect to their environment. The following are the social features designed for each user: 1) Tweeting Frequency; the frequency of daily posts; 2) Mention Ratio: the percentage of posts which contain at least one mention of another user; 3) Frequent Mentions: the number of Twitter users mentioned more than three times which is a measurement of how many close friends a particular user may have; 4) Unique Mentions: the number of unique users mentioned which is a measure of the width of a user's social network.,in this work we are focused on two main type of feature linguistic and behavioral tfidf is adopted to model the linguist feature of patient and pattern of life feature plf adopted from the work of coppersmith et al is used to model the behavioral style of patient tfidf feature to capture the frequent and representative word used by the patient tfidf is applied on the unigram and bigram collected from all the patient tweet pattern of life feature plf these feature reveal the emotional pattern and behavioral tendency of user by measuring polarity emotion and social interaction in order to fully compose the plf we combined the following list of feature age and gender twitter doe not publicly provide information about the age and gender of it user mainly due to privacy concern so we adopted the work of sap et al to fill in this information polarity feature the sentiment api wa used to label each tweet a either positive negative or neutral the polarity is furthermore transformed into five different value to capture the affective trait of each user positive ratio the percentage of positive tweet negative ratio the percentage of negative tweet positive combo capture the mania and hypomania trait of patient which is determined by the number of continuous positive post appearing more than x amount of time within a period of time in minute t negative combo capture the depression trait of patient and is determined by the number of continuous negative post appearing more than x amount of time within a period of time in minute t flip ratio quantifies the emotional unstableness and is determined by counting how frequently two continuous tweet with different polarity either positive to negative or negative to positive appear together within a period of time in minute t in our work x is set to and t is set to minute social feature these feature can demonstrate how user are behaving with respect to their environment the following are the social feature designed for each user tweeting frequency the frequency of daily post mention ratio the percentage of post which contain at least one mention of another user frequent mention the number of twitter user mentioned more than three time which is a measurement of how many close friend a particular user may have unique mention the number of unique user mentioned which is a measure of the width of a user social network,"['work', 'focused', 'two', 'main', 'type', 'feature', 'linguistic', 'behavioral', 'tfidf', 'adopted', 'model', 'linguist', 'feature', 'patient', 'pattern', 'life', 'feature', 'plf', 'adopted', 'work', 'coppersmith', 'et', 'al', 'used', 'model', 'behavioral', 'style', 'patient', 'tfidf', 'feature', 'capture', 'frequent', 'representative', 'word', 'used', 'patient', 'tfidf', 'applied', 'unigram', 'bigram', 'collected', 'patient', 'tweet', 'pattern', 'life', 'feature', 'plf', 'feature', 'reveal', 'emotional', 'pattern', 'behavioral', 'tendency', 'user', 'measuring', 'polarity', 'emotion', 'social', 'interaction', 'order', 'fully', 'compose', 'plf', 'combined', 'following', 'list', 'feature', 'age', 'gender', 'twitter', 'doe', 'publicly', 'provide', 'information', 'age', 'gender', 'user', 'mainly', 'due', 'privacy', 'concern', 'adopted', 'work', 'sap', 'et', 'al', 'fill', 'information', 'polarity', 'feature', 'sentiment', 'api', 'wa', 'used', 'label', 'tweet', 'either', 'positive', 'negative', 'neutral', 'polarity', 'furthermore', 'transformed', 'five', 'different', 'value', 'capture', 'affective', 'trait', 'user', 'positive', 'ratio', 'percentage', 'positive', 'tweet', 'negative', 'ratio', 'percentage', 'negative', 'tweet', 'positive', 'combo', 'capture', 'mania', 'hypomania', 'trait', 'patient', 'determined', 'number', 'continuous', 'positive', 'post', 'appearing', 'x', 'amount', 'time', 'within', 'period', 'time', 'minute', 'negative', 'combo', 'capture', 'depression', 'trait', 'patient', 'determined', 'number', 'continuous', 'negative', 'post', 'appearing', 'x', 'amount', 'time', 'within', 'period', 'time', 'minute', 'flip', 'ratio', 'quantifies', 'emotional', 'unstableness', 'determined', 'counting', 'frequently', 'two', 'continuous', 'tweet', 'different', 'polarity', 'either', 'positive', 'negative', 'negative', 'positive', 'appear', 'together', 'within', 'period', 'time', 'minute', 'work', 'x', 'set', 'set', 'minute', 'social', 'feature', 'feature', 'demonstrate', 'user', 'behaving', 'respect', 'environment', 'following', 'social', 'feature', 'designed', 'user', 'tweeting', 'frequency', 'frequency', 'daily', 'post', 'mention', 'ratio', 'percentage', 'post', 'contain', 'least', 'one', 'mention', 'another', 'user', 'frequent', 'mention', 'number', 'twitter', 'user', 'mentioned', 'three', 'time', 'measurement', 'many', 'close', 'friend', 'particular', 'user', 'may', 'unique', 'mention', 'number', 'unique', 'user', 'mentioned', 'measure', 'width', 'user', 'social', 'network']","['work focused', 'focused two', 'two main', 'main type', 'type feature', 'feature linguistic', 'linguistic behavioral', 'behavioral tfidf', 'tfidf adopted', 'adopted model', 'model linguist', 'linguist feature', 'feature patient', 'patient pattern', 'pattern life', 'life feature', 'feature plf', 'plf adopted', 'adopted work', 'work coppersmith', 'coppersmith et', 'et al', 'al used', 'used model', 'model behavioral', 'behavioral style', 'style patient', 'patient tfidf', 'tfidf feature', 'feature capture', 'capture frequent', 'frequent representative', 'representative word', 'word used', 'used patient', 'patient tfidf', 'tfidf applied', 'applied unigram', 'unigram bigram', 'bigram collected', 'collected patient', 'patient tweet', 'tweet pattern', 'pattern life', 'life feature', 'feature plf', 'plf feature', 'feature reveal', 'reveal emotional', 'emotional pattern', 'pattern behavioral', 'behavioral tendency', 'tendency user', 'user measuring', 'measuring polarity', 'polarity emotion', 'emotion social', 'social interaction', 'interaction order', 'order fully', 'fully compose', 'compose plf', 'plf combined', 'combined following', 'following list', 'list feature', 'feature age', 'age gender', 'gender twitter', 'twitter doe', 'doe publicly', 'publicly provide', 'provide information', 'information age', 'age gender', 'gender user', 'user mainly', 'mainly due', 'due privacy', 'privacy concern', 'concern adopted', 'adopted work', 'work sap', 'sap et', 'et al', 'al fill', 'fill information', 'information polarity', 'polarity feature', 'feature sentiment', 'sentiment api', 'api wa', 'wa used', 'used label', 'label tweet', 'tweet either', 'either positive', 'positive negative', 'negative neutral', 'neutral polarity', 'polarity furthermore', 'furthermore transformed', 'transformed five', 'five different', 'different value', 'value capture', 'capture affective', 'affective trait', 'trait user', 'user positive', 'positive ratio', 'ratio percentage', 'percentage positive', 'positive tweet', 'tweet negative', 'negative ratio', 'ratio percentage', 'percentage negative', 'negative tweet', 'tweet positive', 'positive combo', 'combo capture', 'capture mania', 'mania hypomania', 'hypomania trait', 'trait patient', 'patient determined', 'determined number', 'number continuous', 'continuous positive', 'positive post', 'post appearing', 'appearing x', 'x amount', 'amount time', 'time within', 'within period', 'period time', 'time minute', 'minute negative', 'negative combo', 'combo capture', 'capture depression', 'depression trait', 'trait patient', 'patient determined', 'determined number', 'number continuous', 'continuous negative', 'negative post', 'post appearing', 'appearing x', 'x amount', 'amount time', 'time within', 'within period', 'period time', 'time minute', 'minute flip', 'flip ratio', 'ratio quantifies', 'quantifies emotional', 'emotional unstableness', 'unstableness determined', 'determined counting', 'counting frequently', 'frequently two', 'two continuous', 'continuous tweet', 'tweet different', 'different polarity', 'polarity either', 'either positive', 'positive negative', 'negative negative', 'negative positive', 'positive appear', 'appear together', 'together within', 'within period', 'period time', 'time minute', 'minute work', 'work x', 'x set', 'set set', 'set minute', 'minute social', 'social feature', 'feature feature', 'feature demonstrate', 'demonstrate user', 'user behaving', 'behaving respect', 'respect environment', 'environment following', 'following social', 'social feature', 'feature designed', 'designed user', 'user tweeting', 'tweeting frequency', 'frequency frequency', 'frequency daily', 'daily post', 'post mention', 'mention ratio', 'ratio percentage', 'percentage post', 'post contain', 'contain least', 'least one', 'one mention', 'mention another', 'another user', 'user frequent', 'frequent mention', 'mention number', 'number twitter', 'twitter user', 'user mentioned', 'mentioned three', 'three time', 'time measurement', 'measurement many', 'many close', 'close friend', 'friend particular', 'particular user', 'user may', 'may unique', 'unique mention', 'mention number', 'number unique', 'unique user', 'user mentioned', 'mentioned measure', 'measure width', 'width user', 'user social', 'social network']","['work focused two', 'focused two main', 'two main type', 'main type feature', 'type feature linguistic', 'feature linguistic behavioral', 'linguistic behavioral tfidf', 'behavioral tfidf adopted', 'tfidf adopted model', 'adopted model linguist', 'model linguist feature', 'linguist feature patient', 'feature patient pattern', 'patient pattern life', 'pattern life feature', 'life feature plf', 'feature plf adopted', 'plf adopted work', 'adopted work coppersmith', 'work coppersmith et', 'coppersmith et al', 'et al used', 'al used model', 'used model behavioral', 'model behavioral style', 'behavioral style patient', 'style patient tfidf', 'patient tfidf feature', 'tfidf feature capture', 'feature capture frequent', 'capture frequent representative', 'frequent representative word', 'representative word used', 'word used patient', 'used patient tfidf', 'patient tfidf applied', 'tfidf applied unigram', 'applied unigram bigram', 'unigram bigram collected', 'bigram collected patient', 'collected patient tweet', 'patient tweet pattern', 'tweet pattern life', 'pattern life feature', 'life feature plf', 'feature plf feature', 'plf feature reveal', 'feature reveal emotional', 'reveal emotional pattern', 'emotional pattern behavioral', 'pattern behavioral tendency', 'behavioral tendency user', 'tendency user measuring', 'user measuring polarity', 'measuring polarity emotion', 'polarity emotion social', 'emotion social interaction', 'social interaction order', 'interaction order fully', 'order fully compose', 'fully compose plf', 'compose plf combined', 'plf combined following', 'combined following list', 'following list feature', 'list feature age', 'feature age gender', 'age gender twitter', 'gender twitter doe', 'twitter doe publicly', 'doe publicly provide', 'publicly provide information', 'provide information age', 'information age gender', 'age gender user', 'gender user mainly', 'user mainly due', 'mainly due privacy', 'due privacy concern', 'privacy concern adopted', 'concern adopted work', 'adopted work sap', 'work sap et', 'sap et al', 'et al fill', 'al fill information', 'fill information polarity', 'information polarity feature', 'polarity feature sentiment', 'feature sentiment api', 'sentiment api wa', 'api wa used', 'wa used label', 'used label tweet', 'label tweet either', 'tweet either positive', 'either positive negative', 'positive negative neutral', 'negative neutral polarity', 'neutral polarity furthermore', 'polarity furthermore transformed', 'furthermore transformed five', 'transformed five different', 'five different value', 'different value capture', 'value capture affective', 'capture affective trait', 'affective trait user', 'trait user positive', 'user positive ratio', 'positive ratio percentage', 'ratio percentage positive', 'percentage positive tweet', 'positive tweet negative', 'tweet negative ratio', 'negative ratio percentage', 'ratio percentage negative', 'percentage negative tweet', 'negative tweet positive', 'tweet positive combo', 'positive combo capture', 'combo capture mania', 'capture mania hypomania', 'mania hypomania trait', 'hypomania trait patient', 'trait patient determined', 'patient determined number', 'determined number continuous', 'number continuous positive', 'continuous positive post', 'positive post appearing', 'post appearing x', 'appearing x amount', 'x amount time', 'amount time within', 'time within period', 'within period time', 'period time minute', 'time minute negative', 'minute negative combo', 'negative combo capture', 'combo capture depression', 'capture depression trait', 'depression trait patient', 'trait patient determined', 'patient determined number', 'determined number continuous', 'number continuous negative', 'continuous negative post', 'negative post appearing', 'post appearing x', 'appearing x amount', 'x amount time', 'amount time within', 'time within period', 'within period time', 'period time minute', 'time minute flip', 'minute flip ratio', 'flip ratio quantifies', 'ratio quantifies emotional', 'quantifies emotional unstableness', 'emotional unstableness determined', 'unstableness determined counting', 'determined counting frequently', 'counting frequently two', 'frequently two continuous', 'two continuous tweet', 'continuous tweet different', 'tweet different polarity', 'different polarity either', 'polarity either positive', 'either positive negative', 'positive negative negative', 'negative negative positive', 'negative positive appear', 'positive appear together', 'appear together within', 'together within period', 'within period time', 'period time minute', 'time minute work', 'minute work x', 'work x set', 'x set set', 'set set minute', 'set minute social', 'minute social feature', 'social feature feature', 'feature feature demonstrate', 'feature demonstrate user', 'demonstrate user behaving', 'user behaving respect', 'behaving respect environment', 'respect environment following', 'environment following social', 'following social feature', 'social feature designed', 'feature designed user', 'designed user tweeting', 'user tweeting frequency', 'tweeting frequency frequency', 'frequency frequency daily', 'frequency daily post', 'daily post mention', 'post mention ratio', 'mention ratio percentage', 'ratio percentage post', 'percentage post contain', 'post contain least', 'contain least one', 'least one mention', 'one mention another', 'mention another user', 'another user frequent', 'user frequent mention', 'frequent mention number', 'mention number twitter', 'number twitter user', 'twitter user mentioned', 'user mentioned three', 'mentioned three time', 'three time measurement', 'time measurement many', 'measurement many close', 'many close friend', 'close friend particular', 'friend particular user', 'particular user may', 'user may unique', 'may unique mention', 'unique mention number', 'mention number unique', 'number unique user', 'unique user mentioned', 'user mentioned measure', 'mentioned measure width', 'measure width user', 'width user social', 'user social network']"
https://www.nature.com/articles/s41598-020-68764-y,0,The data pre-processing procedure for the collected post data is presented in Fig. 1. After collecting the data each title was combined with its corresponding post. We removed unnecessary punctuation marks and white spaces for each post. Then we used the natural language toolkit (NLTK) implemented in Python to tokenize users’ posts and filter frequently employed words (stop words). Porter Stemmer a tool used to define a series of guidelines for exploring word meaning and source was employed on the tokenized words to convert a word to its root meaning and to decrease the number of word corpus. After this procedure data from 228060 users with 488472 posts in total were employed for the analysis.,the data preprocessing procedure for the collected post data is presented in fig after collecting the data each title wa combined with it corresponding post we removed unnecessary punctuation mark and white space for each post then we used the natural language toolkit nltk implemented in python to tokenize user post and filter frequently employed word stop word porter stemmer a tool used to define a series of guideline for exploring word meaning and source wa employed on the tokenized word to convert a word to it root meaning and to decrease the number of word corpus after this procedure data from user with post in total were employed for the analysis,"['data', 'preprocessing', 'procedure', 'collected', 'post', 'data', 'presented', 'fig', 'collecting', 'data', 'title', 'wa', 'combined', 'corresponding', 'post', 'removed', 'unnecessary', 'punctuation', 'mark', 'white', 'space', 'post', 'used', 'natural', 'language', 'toolkit', 'nltk', 'implemented', 'python', 'tokenize', 'user', 'post', 'filter', 'frequently', 'employed', 'word', 'stop', 'word', 'porter', 'stemmer', 'tool', 'used', 'define', 'series', 'guideline', 'exploring', 'word', 'meaning', 'source', 'wa', 'employed', 'tokenized', 'word', 'convert', 'word', 'root', 'meaning', 'decrease', 'number', 'word', 'corpus', 'procedure', 'data', 'user', 'post', 'total', 'employed', 'analysis']","['data preprocessing', 'preprocessing procedure', 'procedure collected', 'collected post', 'post data', 'data presented', 'presented fig', 'fig collecting', 'collecting data', 'data title', 'title wa', 'wa combined', 'combined corresponding', 'corresponding post', 'post removed', 'removed unnecessary', 'unnecessary punctuation', 'punctuation mark', 'mark white', 'white space', 'space post', 'post used', 'used natural', 'natural language', 'language toolkit', 'toolkit nltk', 'nltk implemented', 'implemented python', 'python tokenize', 'tokenize user', 'user post', 'post filter', 'filter frequently', 'frequently employed', 'employed word', 'word stop', 'stop word', 'word porter', 'porter stemmer', 'stemmer tool', 'tool used', 'used define', 'define series', 'series guideline', 'guideline exploring', 'exploring word', 'word meaning', 'meaning source', 'source wa', 'wa employed', 'employed tokenized', 'tokenized word', 'word convert', 'convert word', 'word root', 'root meaning', 'meaning decrease', 'decrease number', 'number word', 'word corpus', 'corpus procedure', 'procedure data', 'data user', 'user post', 'post total', 'total employed', 'employed analysis']","['data preprocessing procedure', 'preprocessing procedure collected', 'procedure collected post', 'collected post data', 'post data presented', 'data presented fig', 'presented fig collecting', 'fig collecting data', 'collecting data title', 'data title wa', 'title wa combined', 'wa combined corresponding', 'combined corresponding post', 'corresponding post removed', 'post removed unnecessary', 'removed unnecessary punctuation', 'unnecessary punctuation mark', 'punctuation mark white', 'mark white space', 'white space post', 'space post used', 'post used natural', 'used natural language', 'natural language toolkit', 'language toolkit nltk', 'toolkit nltk implemented', 'nltk implemented python', 'implemented python tokenize', 'python tokenize user', 'tokenize user post', 'user post filter', 'post filter frequently', 'filter frequently employed', 'frequently employed word', 'employed word stop', 'word stop word', 'stop word porter', 'word porter stemmer', 'porter stemmer tool', 'stemmer tool used', 'tool used define', 'used define series', 'define series guideline', 'series guideline exploring', 'guideline exploring word', 'exploring word meaning', 'word meaning source', 'meaning source wa', 'source wa employed', 'wa employed tokenized', 'employed tokenized word', 'tokenized word convert', 'word convert word', 'convert word root', 'word root meaning', 'root meaning decrease', 'meaning decrease number', 'decrease number word', 'number word corpus', 'word corpus procedure', 'corpus procedure data', 'procedure data user', 'data user post', 'user post total', 'post total employed', 'total employed analysis']"
https://aclanthology.org/W19-3013.pdf,1,Our cohort construction process entails two key steps: first randomly selecting a large sample of Twitter users; and second annotating those users with key demographic attributes. While such attributes are not provided by the API automated methods can be used to infer such traits from data (Cesare et al. 2017). Following this approach we develop a demographic inference pipeline to automatically infer age gender race/ethnicity and location for each cohort candidate. Age Identifying age based on the content of a user can be challenging and exact age often cannot be determined based on language use alone. Therefore we use discrete categories that provide a more accurate estimate of age: Teenager (below 19) 20s 30s 40s 50s (50 years or older). Gender The gender was inferred using Demographer a supervised model that predicts the (binary) gender of Twitter users with features based on the name field on the user profile (Knowles et al. 2016). Race/Ethnicity The standard formulation of race and ethnicity is not well understood by the general public so categorizing social media users along these two axes may not be reasonable. Therefore we use a single measure of multicultural expression that includes five categories: White (W) Asian (A) Black (B) Hispanic (H) and Other. Location The location was inferred using Carmen an open-source library for geolocating tweets that uses a series of rules to lookup location strings in a location knowledge-base (Dredze et al. 2013). We use the inferred location to select users that live in the United States. The age and race/ethnicity attributes were inferred with custom supervised classifiers based on Amir et al. (2017)’s user-level model. The classifiers were trained and evaluated on a dataset of 5K annotated users attaining performances of 0.28 and 0.41 Average F1 respectively. See the supplemental notes for additional details on these experiments1 .,our cohort construction process entail two key step first randomly selecting a large sample of twitter user and second annotating those user with key demographic attribute while such attribute are not provided by the api automated method can be used to infer such trait from data cesare et al following this approach we develop a demographic inference pipeline to automatically infer age gender raceethnicity and location for each cohort candidate age identifying age based on the content of a user can be challenging and exact age often cannot be determined based on language use alone therefore we use discrete category that provide a more accurate estimate of age teenager below s s s s year or older gender the gender wa inferred using demographer a supervised model that predicts the binary gender of twitter user with feature based on the name field on the user profile knowles et al raceethnicity the standard formulation of race and ethnicity is not well understood by the general public so categorizing social medium user along these two ax may not be reasonable therefore we use a single measure of multicultural expression that includes five category white w asian a black b hispanic h and other location the location wa inferred using carmen an opensource library for geolocating tweet that us a series of rule to lookup location string in a location knowledgebase dredze et al we use the inferred location to select user that live in the united state the age and raceethnicity attribute were inferred with custom supervised classifier based on amir et al s userlevel model the classifier were trained and evaluated on a dataset of k annotated user attaining performance of and average f respectively see the supplemental note for additional detail on these experiment,"['cohort', 'construction', 'process', 'entail', 'two', 'key', 'step', 'first', 'randomly', 'selecting', 'large', 'sample', 'twitter', 'user', 'second', 'annotating', 'user', 'key', 'demographic', 'attribute', 'attribute', 'provided', 'api', 'automated', 'method', 'used', 'infer', 'trait', 'data', 'cesare', 'et', 'al', 'following', 'approach', 'develop', 'demographic', 'inference', 'pipeline', 'automatically', 'infer', 'age', 'gender', 'raceethnicity', 'location', 'cohort', 'candidate', 'age', 'identifying', 'age', 'based', 'content', 'user', 'challenging', 'exact', 'age', 'often', 'cannot', 'determined', 'based', 'language', 'use', 'alone', 'therefore', 'use', 'discrete', 'category', 'provide', 'accurate', 'estimate', 'age', 'teenager', 'year', 'older', 'gender', 'gender', 'wa', 'inferred', 'using', 'demographer', 'supervised', 'model', 'predicts', 'binary', 'gender', 'twitter', 'user', 'feature', 'based', 'name', 'field', 'user', 'profile', 'knowles', 'et', 'al', 'raceethnicity', 'standard', 'formulation', 'race', 'ethnicity', 'well', 'understood', 'general', 'public', 'categorizing', 'social', 'medium', 'user', 'along', 'two', 'ax', 'may', 'reasonable', 'therefore', 'use', 'single', 'measure', 'multicultural', 'expression', 'includes', 'five', 'category', 'white', 'w', 'asian', 'black', 'b', 'hispanic', 'h', 'location', 'location', 'wa', 'inferred', 'using', 'carmen', 'opensource', 'library', 'geolocating', 'tweet', 'us', 'series', 'rule', 'lookup', 'location', 'string', 'location', 'knowledgebase', 'dredze', 'et', 'al', 'use', 'inferred', 'location', 'select', 'user', 'live', 'united', 'state', 'age', 'raceethnicity', 'attribute', 'inferred', 'custom', 'supervised', 'classifier', 'based', 'amir', 'et', 'al', 'userlevel', 'model', 'classifier', 'trained', 'evaluated', 'dataset', 'k', 'annotated', 'user', 'attaining', 'performance', 'average', 'f', 'respectively', 'see', 'supplemental', 'note', 'additional', 'detail', 'experiment']","['cohort construction', 'construction process', 'process entail', 'entail two', 'two key', 'key step', 'step first', 'first randomly', 'randomly selecting', 'selecting large', 'large sample', 'sample twitter', 'twitter user', 'user second', 'second annotating', 'annotating user', 'user key', 'key demographic', 'demographic attribute', 'attribute attribute', 'attribute provided', 'provided api', 'api automated', 'automated method', 'method used', 'used infer', 'infer trait', 'trait data', 'data cesare', 'cesare et', 'et al', 'al following', 'following approach', 'approach develop', 'develop demographic', 'demographic inference', 'inference pipeline', 'pipeline automatically', 'automatically infer', 'infer age', 'age gender', 'gender raceethnicity', 'raceethnicity location', 'location cohort', 'cohort candidate', 'candidate age', 'age identifying', 'identifying age', 'age based', 'based content', 'content user', 'user challenging', 'challenging exact', 'exact age', 'age often', 'often cannot', 'cannot determined', 'determined based', 'based language', 'language use', 'use alone', 'alone therefore', 'therefore use', 'use discrete', 'discrete category', 'category provide', 'provide accurate', 'accurate estimate', 'estimate age', 'age teenager', 'teenager year', 'year older', 'older gender', 'gender gender', 'gender wa', 'wa inferred', 'inferred using', 'using demographer', 'demographer supervised', 'supervised model', 'model predicts', 'predicts binary', 'binary gender', 'gender twitter', 'twitter user', 'user feature', 'feature based', 'based name', 'name field', 'field user', 'user profile', 'profile knowles', 'knowles et', 'et al', 'al raceethnicity', 'raceethnicity standard', 'standard formulation', 'formulation race', 'race ethnicity', 'ethnicity well', 'well understood', 'understood general', 'general public', 'public categorizing', 'categorizing social', 'social medium', 'medium user', 'user along', 'along two', 'two ax', 'ax may', 'may reasonable', 'reasonable therefore', 'therefore use', 'use single', 'single measure', 'measure multicultural', 'multicultural expression', 'expression includes', 'includes five', 'five category', 'category white', 'white w', 'w asian', 'asian black', 'black b', 'b hispanic', 'hispanic h', 'h location', 'location location', 'location wa', 'wa inferred', 'inferred using', 'using carmen', 'carmen opensource', 'opensource library', 'library geolocating', 'geolocating tweet', 'tweet us', 'us series', 'series rule', 'rule lookup', 'lookup location', 'location string', 'string location', 'location knowledgebase', 'knowledgebase dredze', 'dredze et', 'et al', 'al use', 'use inferred', 'inferred location', 'location select', 'select user', 'user live', 'live united', 'united state', 'state age', 'age raceethnicity', 'raceethnicity attribute', 'attribute inferred', 'inferred custom', 'custom supervised', 'supervised classifier', 'classifier based', 'based amir', 'amir et', 'et al', 'al userlevel', 'userlevel model', 'model classifier', 'classifier trained', 'trained evaluated', 'evaluated dataset', 'dataset k', 'k annotated', 'annotated user', 'user attaining', 'attaining performance', 'performance average', 'average f', 'f respectively', 'respectively see', 'see supplemental', 'supplemental note', 'note additional', 'additional detail', 'detail experiment']","['cohort construction process', 'construction process entail', 'process entail two', 'entail two key', 'two key step', 'key step first', 'step first randomly', 'first randomly selecting', 'randomly selecting large', 'selecting large sample', 'large sample twitter', 'sample twitter user', 'twitter user second', 'user second annotating', 'second annotating user', 'annotating user key', 'user key demographic', 'key demographic attribute', 'demographic attribute attribute', 'attribute attribute provided', 'attribute provided api', 'provided api automated', 'api automated method', 'automated method used', 'method used infer', 'used infer trait', 'infer trait data', 'trait data cesare', 'data cesare et', 'cesare et al', 'et al following', 'al following approach', 'following approach develop', 'approach develop demographic', 'develop demographic inference', 'demographic inference pipeline', 'inference pipeline automatically', 'pipeline automatically infer', 'automatically infer age', 'infer age gender', 'age gender raceethnicity', 'gender raceethnicity location', 'raceethnicity location cohort', 'location cohort candidate', 'cohort candidate age', 'candidate age identifying', 'age identifying age', 'identifying age based', 'age based content', 'based content user', 'content user challenging', 'user challenging exact', 'challenging exact age', 'exact age often', 'age often cannot', 'often cannot determined', 'cannot determined based', 'determined based language', 'based language use', 'language use alone', 'use alone therefore', 'alone therefore use', 'therefore use discrete', 'use discrete category', 'discrete category provide', 'category provide accurate', 'provide accurate estimate', 'accurate estimate age', 'estimate age teenager', 'age teenager year', 'teenager year older', 'year older gender', 'older gender gender', 'gender gender wa', 'gender wa inferred', 'wa inferred using', 'inferred using demographer', 'using demographer supervised', 'demographer supervised model', 'supervised model predicts', 'model predicts binary', 'predicts binary gender', 'binary gender twitter', 'gender twitter user', 'twitter user feature', 'user feature based', 'feature based name', 'based name field', 'name field user', 'field user profile', 'user profile knowles', 'profile knowles et', 'knowles et al', 'et al raceethnicity', 'al raceethnicity standard', 'raceethnicity standard formulation', 'standard formulation race', 'formulation race ethnicity', 'race ethnicity well', 'ethnicity well understood', 'well understood general', 'understood general public', 'general public categorizing', 'public categorizing social', 'categorizing social medium', 'social medium user', 'medium user along', 'user along two', 'along two ax', 'two ax may', 'ax may reasonable', 'may reasonable therefore', 'reasonable therefore use', 'therefore use single', 'use single measure', 'single measure multicultural', 'measure multicultural expression', 'multicultural expression includes', 'expression includes five', 'includes five category', 'five category white', 'category white w', 'white w asian', 'w asian black', 'asian black b', 'black b hispanic', 'b hispanic h', 'hispanic h location', 'h location location', 'location location wa', 'location wa inferred', 'wa inferred using', 'inferred using carmen', 'using carmen opensource', 'carmen opensource library', 'opensource library geolocating', 'library geolocating tweet', 'geolocating tweet us', 'tweet us series', 'us series rule', 'series rule lookup', 'rule lookup location', 'lookup location string', 'location string location', 'string location knowledgebase', 'location knowledgebase dredze', 'knowledgebase dredze et', 'dredze et al', 'et al use', 'al use inferred', 'use inferred location', 'inferred location select', 'location select user', 'select user live', 'user live united', 'live united state', 'united state age', 'state age raceethnicity', 'age raceethnicity attribute', 'raceethnicity attribute inferred', 'attribute inferred custom', 'inferred custom supervised', 'custom supervised classifier', 'supervised classifier based', 'classifier based amir', 'based amir et', 'amir et al', 'et al userlevel', 'al userlevel model', 'userlevel model classifier', 'model classifier trained', 'classifier trained evaluated', 'trained evaluated dataset', 'evaluated dataset k', 'dataset k annotated', 'k annotated user', 'annotated user attaining', 'user attaining performance', 'attaining performance average', 'performance average f', 'average f respectively', 'f respectively see', 'respectively see supplemental', 'see supplemental note', 'supplemental note additional', 'note additional detail', 'additional detail experiment']"
https://link.springer.com/chapter/10.1007/978-3-319-67186-4_6,1,First we removed journals with no text and those with fewer than 20 characters1 leaving 1.1 million journals for topic modelling. Next we pre-processed the text using the Stanford Tweet Tokenizer which is a “Twitter-aware” tokenizer designed to handle short informal text [1]. We used the option that truncates characters repeating 3 or more times converting phrases such as “I’m sooooo happyy” to “I’m soo happyy”. On average the number of tokens per journal was 27.7. Since we are interested in topics we removed stopwords and tokens with fewer than two letters and we only retained nouns which appear in the WordNet corpus [10]. After this filtering the average number of nouns per journal was 7. Examples of frequently appearing nouns in alphabetical order include “anxiety” “class” “dinner” “family” “god” “job” “lunch” “miss” “school” “sick” “sleep” and “work”. We then iteratively clustered the journals into topics (details below) and removed nouns that do not refer to topics such as numbers timings (e.g. “today” “yesterday”) general feelings (e.g. “feel” “like”) proper nouns and nouns that have ambiguous meanings (e.g. “overall” “true”). Lastly we only retained nouns that appeared more than ten times in the dataset. This process resulted in a vocabulary of 8386 words for topic modelling. Each journal is represented as a 8386-dimensional term frequency vector with each component denoting the term-frequency/ inverse-document-frequency (TF-IDF) of the corresponding term. Algorithm 1 summarizes our topic modelling methodology. Given a TF-IDF term frequency vector for each journal we run non-negative matrix factorization (NMF) [8] implemented in Python’s scikit-learn package [12]. The objective of NMF is to find two matrices whose product approximates the original matrix. In our case one matrix is the weighted set of topics in each journal and the other is the weighted set of words that belong to each topic. Hence each journal is represented as a combination of topics which are themselves composed of a weighted combination of words. We chose NMF because its non-negativity constraint aids with interpretability. In the context of analyzing word frequencies negative presence of a word would not be interpretable. This is because we only track word occurrences and not semantics or syntax. Unlike other matrix factorization methods NMF reconstructs each document from a sum of positive parts which enables us to easily manually label the discovered topics. Iterating from 4 to 40 topics we derived 37 different topic matrices (steps 1 and 2 of Algorithm 1). Each matrix consists of one topic per row. Each topic has a positive weight for each word in the vocabulary. Stronger weights indicate higher relevance to the topic. The final topic matrix we used has 14 topics and is shown in Table 1. We show the first six words in this table for simplicity where we sorted the words associated with each topic from highest relevance to lowest. When judging the topic matrices we considered the top twenty most important words per topic. Using this information we manually labeled each row in the matrix with a corresponding topic. Furthermore we manually evaluated each matrix based on the distinctness between topics consistency within topics and interpretability. During this process we compiled a custom list of removed words that we mentioned earlier in this section. The groups of words we removed appeared as stand-alone topics that did not offer information about what the journal was about. For example proper nouns appeared as a stand-alone topic. Other words which we deemed too general or ambiguous appeared across several topics and hence did not provide discriminative information. We tested different levels of regularization to enforce sparseness in our models (see [8] for a discussion) but did not find significant differences. However one important modification we made to regularize each topic was to make their first words only as strong as their second ones (by default first words are stronger than second words which are stronger than third words and so on). This is since the most relevant word for each topic tended to be too strong of a signal regardless of how we changed the number of topics pre-processing procedure or regularization in the objective function. For example the word “love” in a journal about sports would be so strong that the journal would be labeled as relating to romantic love. Lowering the importance of first words was sufficient to eliminate the false positives we identified. Given the final topic matrix (summarized in Table 1) the next step is to use it to assign labels to journals (steps 3 and 4 of Algorithm 1). We plotted the distribution of how important each topic was to all journals in the dataset with importance ranging from zero to one. Each distribution had a similar shape with a clear inflection point between 0.05 to 0.15 importance. Figure 4 shows an example importance distribution for the topic “Work” where the inflection point occurs at 0.1 importance.,first we removed journal with no text and those with fewer than character leaving million journal for topic modelling next we preprocessed the text using the stanford tweet tokenizer which is a twitteraware tokenizer designed to handle short informal text we used the option that truncates character repeating or more time converting phrase such a im sooooo happyy to im soo happyy on average the number of token per journal wa since we are interested in topic we removed stopwords and token with fewer than two letter and we only retained noun which appear in the wordnet corpus after this filtering the average number of noun per journal wa example of frequently appearing noun in alphabetical order include anxiety class dinner family god job lunch miss school sick sleep and work we then iteratively clustered the journal into topic detail below and removed noun that do not refer to topic such a number timing eg today yesterday general feeling eg feel like proper noun and noun that have ambiguous meaning eg overall true lastly we only retained noun that appeared more than ten time in the dataset this process resulted in a vocabulary of word for topic modelling each journal is represented a a dimensional term frequency vector with each component denoting the termfrequency inversedocumentfrequency tfidf of the corresponding term algorithm summarizes our topic modelling methodology given a tfidf term frequency vector for each journal we run nonnegative matrix factorization nmf implemented in python scikitlearn package the objective of nmf is to find two matrix whose product approximates the original matrix in our case one matrix is the weighted set of topic in each journal and the other is the weighted set of word that belong to each topic hence each journal is represented a a combination of topic which are themselves composed of a weighted combination of word we chose nmf because it nonnegativity constraint aid with interpretability in the context of analyzing word frequency negative presence of a word would not be interpretable this is because we only track word occurrence and not semantics or syntax unlike other matrix factorization method nmf reconstructs each document from a sum of positive part which enables u to easily manually label the discovered topic iterating from to topic we derived different topic matrix step and of algorithm each matrix consists of one topic per row each topic ha a positive weight for each word in the vocabulary stronger weight indicate higher relevance to the topic the final topic matrix we used ha topic and is shown in table we show the first six word in this table for simplicity where we sorted the word associated with each topic from highest relevance to lowest when judging the topic matrix we considered the top twenty most important word per topic using this information we manually labeled each row in the matrix with a corresponding topic furthermore we manually evaluated each matrix based on the distinctness between topic consistency within topic and interpretability during this process we compiled a custom list of removed word that we mentioned earlier in this section the group of word we removed appeared a standalone topic that did not offer information about what the journal wa about for example proper noun appeared a a standalone topic other word which we deemed too general or ambiguous appeared across several topic and hence did not provide discriminative information we tested different level of regularization to enforce sparseness in our model see for a discussion but did not find significant difference however one important modification we made to regularize each topic wa to make their first word only a strong a their second one by default first word are stronger than second word which are stronger than third word and so on this is since the most relevant word for each topic tended to be too strong of a signal regardless of how we changed the number of topic preprocessing procedure or regularization in the objective function for example the word love in a journal about sport would be so strong that the journal would be labeled a relating to romantic love lowering the importance of first word wa sufficient to eliminate the false positive we identified given the final topic matrix summarized in table the next step is to use it to assign label to journal step and of algorithm we plotted the distribution of how important each topic wa to all journal in the dataset with importance ranging from zero to one each distribution had a similar shape with a clear inflection point between to importance figure show an example importance distribution for the topic work where the inflection point occurs at importance,"['first', 'removed', 'journal', 'text', 'fewer', 'character', 'leaving', 'million', 'journal', 'topic', 'modelling', 'next', 'preprocessed', 'text', 'using', 'stanford', 'tweet', 'tokenizer', 'twitteraware', 'tokenizer', 'designed', 'handle', 'short', 'informal', 'text', 'used', 'option', 'truncates', 'character', 'repeating', 'time', 'converting', 'phrase', 'im', 'sooooo', 'happyy', 'im', 'soo', 'happyy', 'average', 'number', 'token', 'per', 'journal', 'wa', 'since', 'interested', 'topic', 'removed', 'stopwords', 'token', 'fewer', 'two', 'letter', 'retained', 'noun', 'appear', 'wordnet', 'corpus', 'filtering', 'average', 'number', 'noun', 'per', 'journal', 'wa', 'example', 'frequently', 'appearing', 'noun', 'alphabetical', 'order', 'include', 'anxiety', 'class', 'dinner', 'family', 'god', 'job', 'lunch', 'miss', 'school', 'sick', 'sleep', 'work', 'iteratively', 'clustered', 'journal', 'topic', 'detail', 'removed', 'noun', 'refer', 'topic', 'number', 'timing', 'eg', 'today', 'yesterday', 'general', 'feeling', 'eg', 'feel', 'like', 'proper', 'noun', 'noun', 'ambiguous', 'meaning', 'eg', 'overall', 'true', 'lastly', 'retained', 'noun', 'appeared', 'ten', 'time', 'dataset', 'process', 'resulted', 'vocabulary', 'word', 'topic', 'modelling', 'journal', 'represented', 'dimensional', 'term', 'frequency', 'vector', 'component', 'denoting', 'termfrequency', 'inversedocumentfrequency', 'tfidf', 'corresponding', 'term', 'algorithm', 'summarizes', 'topic', 'modelling', 'methodology', 'given', 'tfidf', 'term', 'frequency', 'vector', 'journal', 'run', 'nonnegative', 'matrix', 'factorization', 'nmf', 'implemented', 'python', 'scikitlearn', 'package', 'objective', 'nmf', 'find', 'two', 'matrix', 'whose', 'product', 'approximates', 'original', 'matrix', 'case', 'one', 'matrix', 'weighted', 'set', 'topic', 'journal', 'weighted', 'set', 'word', 'belong', 'topic', 'hence', 'journal', 'represented', 'combination', 'topic', 'composed', 'weighted', 'combination', 'word', 'chose', 'nmf', 'nonnegativity', 'constraint', 'aid', 'interpretability', 'context', 'analyzing', 'word', 'frequency', 'negative', 'presence', 'word', 'would', 'interpretable', 'track', 'word', 'occurrence', 'semantics', 'syntax', 'unlike', 'matrix', 'factorization', 'method', 'nmf', 'reconstructs', 'document', 'sum', 'positive', 'part', 'enables', 'u', 'easily', 'manually', 'label', 'discovered', 'topic', 'iterating', 'topic', 'derived', 'different', 'topic', 'matrix', 'step', 'algorithm', 'matrix', 'consists', 'one', 'topic', 'per', 'row', 'topic', 'ha', 'positive', 'weight', 'word', 'vocabulary', 'stronger', 'weight', 'indicate', 'higher', 'relevance', 'topic', 'final', 'topic', 'matrix', 'used', 'ha', 'topic', 'shown', 'table', 'show', 'first', 'six', 'word', 'table', 'simplicity', 'sorted', 'word', 'associated', 'topic', 'highest', 'relevance', 'lowest', 'judging', 'topic', 'matrix', 'considered', 'top', 'twenty', 'important', 'word', 'per', 'topic', 'using', 'information', 'manually', 'labeled', 'row', 'matrix', 'corresponding', 'topic', 'furthermore', 'manually', 'evaluated', 'matrix', 'based', 'distinctness', 'topic', 'consistency', 'within', 'topic', 'interpretability', 'process', 'compiled', 'custom', 'list', 'removed', 'word', 'mentioned', 'earlier', 'section', 'group', 'word', 'removed', 'appeared', 'standalone', 'topic', 'offer', 'information', 'journal', 'wa', 'example', 'proper', 'noun', 'appeared', 'standalone', 'topic', 'word', 'deemed', 'general', 'ambiguous', 'appeared', 'across', 'several', 'topic', 'hence', 'provide', 'discriminative', 'information', 'tested', 'different', 'level', 'regularization', 'enforce', 'sparseness', 'model', 'see', 'discussion', 'find', 'significant', 'difference', 'however', 'one', 'important', 'modification', 'made', 'regularize', 'topic', 'wa', 'make', 'first', 'word', 'strong', 'second', 'one', 'default', 'first', 'word', 'stronger', 'second', 'word', 'stronger', 'third', 'word', 'since', 'relevant', 'word', 'topic', 'tended', 'strong', 'signal', 'regardless', 'changed', 'number', 'topic', 'preprocessing', 'procedure', 'regularization', 'objective', 'function', 'example', 'word', 'love', 'journal', 'sport', 'would', 'strong', 'journal', 'would', 'labeled', 'relating', 'romantic', 'love', 'lowering', 'importance', 'first', 'word', 'wa', 'sufficient', 'eliminate', 'false', 'positive', 'identified', 'given', 'final', 'topic', 'matrix', 'summarized', 'table', 'next', 'step', 'use', 'assign', 'label', 'journal', 'step', 'algorithm', 'plotted', 'distribution', 'important', 'topic', 'wa', 'journal', 'dataset', 'importance', 'ranging', 'zero', 'one', 'distribution', 'similar', 'shape', 'clear', 'inflection', 'point', 'importance', 'figure', 'show', 'example', 'importance', 'distribution', 'topic', 'work', 'inflection', 'point', 'occurs', 'importance']","['first removed', 'removed journal', 'journal text', 'text fewer', 'fewer character', 'character leaving', 'leaving million', 'million journal', 'journal topic', 'topic modelling', 'modelling next', 'next preprocessed', 'preprocessed text', 'text using', 'using stanford', 'stanford tweet', 'tweet tokenizer', 'tokenizer twitteraware', 'twitteraware tokenizer', 'tokenizer designed', 'designed handle', 'handle short', 'short informal', 'informal text', 'text used', 'used option', 'option truncates', 'truncates character', 'character repeating', 'repeating time', 'time converting', 'converting phrase', 'phrase im', 'im sooooo', 'sooooo happyy', 'happyy im', 'im soo', 'soo happyy', 'happyy average', 'average number', 'number token', 'token per', 'per journal', 'journal wa', 'wa since', 'since interested', 'interested topic', 'topic removed', 'removed stopwords', 'stopwords token', 'token fewer', 'fewer two', 'two letter', 'letter retained', 'retained noun', 'noun appear', 'appear wordnet', 'wordnet corpus', 'corpus filtering', 'filtering average', 'average number', 'number noun', 'noun per', 'per journal', 'journal wa', 'wa example', 'example frequently', 'frequently appearing', 'appearing noun', 'noun alphabetical', 'alphabetical order', 'order include', 'include anxiety', 'anxiety class', 'class dinner', 'dinner family', 'family god', 'god job', 'job lunch', 'lunch miss', 'miss school', 'school sick', 'sick sleep', 'sleep work', 'work iteratively', 'iteratively clustered', 'clustered journal', 'journal topic', 'topic detail', 'detail removed', 'removed noun', 'noun refer', 'refer topic', 'topic number', 'number timing', 'timing eg', 'eg today', 'today yesterday', 'yesterday general', 'general feeling', 'feeling eg', 'eg feel', 'feel like', 'like proper', 'proper noun', 'noun noun', 'noun ambiguous', 'ambiguous meaning', 'meaning eg', 'eg overall', 'overall true', 'true lastly', 'lastly retained', 'retained noun', 'noun appeared', 'appeared ten', 'ten time', 'time dataset', 'dataset process', 'process resulted', 'resulted vocabulary', 'vocabulary word', 'word topic', 'topic modelling', 'modelling journal', 'journal represented', 'represented dimensional', 'dimensional term', 'term frequency', 'frequency vector', 'vector component', 'component denoting', 'denoting termfrequency', 'termfrequency inversedocumentfrequency', 'inversedocumentfrequency tfidf', 'tfidf corresponding', 'corresponding term', 'term algorithm', 'algorithm summarizes', 'summarizes topic', 'topic modelling', 'modelling methodology', 'methodology given', 'given tfidf', 'tfidf term', 'term frequency', 'frequency vector', 'vector journal', 'journal run', 'run nonnegative', 'nonnegative matrix', 'matrix factorization', 'factorization nmf', 'nmf implemented', 'implemented python', 'python scikitlearn', 'scikitlearn package', 'package objective', 'objective nmf', 'nmf find', 'find two', 'two matrix', 'matrix whose', 'whose product', 'product approximates', 'approximates original', 'original matrix', 'matrix case', 'case one', 'one matrix', 'matrix weighted', 'weighted set', 'set topic', 'topic journal', 'journal weighted', 'weighted set', 'set word', 'word belong', 'belong topic', 'topic hence', 'hence journal', 'journal represented', 'represented combination', 'combination topic', 'topic composed', 'composed weighted', 'weighted combination', 'combination word', 'word chose', 'chose nmf', 'nmf nonnegativity', 'nonnegativity constraint', 'constraint aid', 'aid interpretability', 'interpretability context', 'context analyzing', 'analyzing word', 'word frequency', 'frequency negative', 'negative presence', 'presence word', 'word would', 'would interpretable', 'interpretable track', 'track word', 'word occurrence', 'occurrence semantics', 'semantics syntax', 'syntax unlike', 'unlike matrix', 'matrix factorization', 'factorization method', 'method nmf', 'nmf reconstructs', 'reconstructs document', 'document sum', 'sum positive', 'positive part', 'part enables', 'enables u', 'u easily', 'easily manually', 'manually label', 'label discovered', 'discovered topic', 'topic iterating', 'iterating topic', 'topic derived', 'derived different', 'different topic', 'topic matrix', 'matrix step', 'step algorithm', 'algorithm matrix', 'matrix consists', 'consists one', 'one topic', 'topic per', 'per row', 'row topic', 'topic ha', 'ha positive', 'positive weight', 'weight word', 'word vocabulary', 'vocabulary stronger', 'stronger weight', 'weight indicate', 'indicate higher', 'higher relevance', 'relevance topic', 'topic final', 'final topic', 'topic matrix', 'matrix used', 'used ha', 'ha topic', 'topic shown', 'shown table', 'table show', 'show first', 'first six', 'six word', 'word table', 'table simplicity', 'simplicity sorted', 'sorted word', 'word associated', 'associated topic', 'topic highest', 'highest relevance', 'relevance lowest', 'lowest judging', 'judging topic', 'topic matrix', 'matrix considered', 'considered top', 'top twenty', 'twenty important', 'important word', 'word per', 'per topic', 'topic using', 'using information', 'information manually', 'manually labeled', 'labeled row', 'row matrix', 'matrix corresponding', 'corresponding topic', 'topic furthermore', 'furthermore manually', 'manually evaluated', 'evaluated matrix', 'matrix based', 'based distinctness', 'distinctness topic', 'topic consistency', 'consistency within', 'within topic', 'topic interpretability', 'interpretability process', 'process compiled', 'compiled custom', 'custom list', 'list removed', 'removed word', 'word mentioned', 'mentioned earlier', 'earlier section', 'section group', 'group word', 'word removed', 'removed appeared', 'appeared standalone', 'standalone topic', 'topic offer', 'offer information', 'information journal', 'journal wa', 'wa example', 'example proper', 'proper noun', 'noun appeared', 'appeared standalone', 'standalone topic', 'topic word', 'word deemed', 'deemed general', 'general ambiguous', 'ambiguous appeared', 'appeared across', 'across several', 'several topic', 'topic hence', 'hence provide', 'provide discriminative', 'discriminative information', 'information tested', 'tested different', 'different level', 'level regularization', 'regularization enforce', 'enforce sparseness', 'sparseness model', 'model see', 'see discussion', 'discussion find', 'find significant', 'significant difference', 'difference however', 'however one', 'one important', 'important modification', 'modification made', 'made regularize', 'regularize topic', 'topic wa', 'wa make', 'make first', 'first word', 'word strong', 'strong second', 'second one', 'one default', 'default first', 'first word', 'word stronger', 'stronger second', 'second word', 'word stronger', 'stronger third', 'third word', 'word since', 'since relevant', 'relevant word', 'word topic', 'topic tended', 'tended strong', 'strong signal', 'signal regardless', 'regardless changed', 'changed number', 'number topic', 'topic preprocessing', 'preprocessing procedure', 'procedure regularization', 'regularization objective', 'objective function', 'function example', 'example word', 'word love', 'love journal', 'journal sport', 'sport would', 'would strong', 'strong journal', 'journal would', 'would labeled', 'labeled relating', 'relating romantic', 'romantic love', 'love lowering', 'lowering importance', 'importance first', 'first word', 'word wa', 'wa sufficient', 'sufficient eliminate', 'eliminate false', 'false positive', 'positive identified', 'identified given', 'given final', 'final topic', 'topic matrix', 'matrix summarized', 'summarized table', 'table next', 'next step', 'step use', 'use assign', 'assign label', 'label journal', 'journal step', 'step algorithm', 'algorithm plotted', 'plotted distribution', 'distribution important', 'important topic', 'topic wa', 'wa journal', 'journal dataset', 'dataset importance', 'importance ranging', 'ranging zero', 'zero one', 'one distribution', 'distribution similar', 'similar shape', 'shape clear', 'clear inflection', 'inflection point', 'point importance', 'importance figure', 'figure show', 'show example', 'example importance', 'importance distribution', 'distribution topic', 'topic work', 'work inflection', 'inflection point', 'point occurs', 'occurs importance']","['first removed journal', 'removed journal text', 'journal text fewer', 'text fewer character', 'fewer character leaving', 'character leaving million', 'leaving million journal', 'million journal topic', 'journal topic modelling', 'topic modelling next', 'modelling next preprocessed', 'next preprocessed text', 'preprocessed text using', 'text using stanford', 'using stanford tweet', 'stanford tweet tokenizer', 'tweet tokenizer twitteraware', 'tokenizer twitteraware tokenizer', 'twitteraware tokenizer designed', 'tokenizer designed handle', 'designed handle short', 'handle short informal', 'short informal text', 'informal text used', 'text used option', 'used option truncates', 'option truncates character', 'truncates character repeating', 'character repeating time', 'repeating time converting', 'time converting phrase', 'converting phrase im', 'phrase im sooooo', 'im sooooo happyy', 'sooooo happyy im', 'happyy im soo', 'im soo happyy', 'soo happyy average', 'happyy average number', 'average number token', 'number token per', 'token per journal', 'per journal wa', 'journal wa since', 'wa since interested', 'since interested topic', 'interested topic removed', 'topic removed stopwords', 'removed stopwords token', 'stopwords token fewer', 'token fewer two', 'fewer two letter', 'two letter retained', 'letter retained noun', 'retained noun appear', 'noun appear wordnet', 'appear wordnet corpus', 'wordnet corpus filtering', 'corpus filtering average', 'filtering average number', 'average number noun', 'number noun per', 'noun per journal', 'per journal wa', 'journal wa example', 'wa example frequently', 'example frequently appearing', 'frequently appearing noun', 'appearing noun alphabetical', 'noun alphabetical order', 'alphabetical order include', 'order include anxiety', 'include anxiety class', 'anxiety class dinner', 'class dinner family', 'dinner family god', 'family god job', 'god job lunch', 'job lunch miss', 'lunch miss school', 'miss school sick', 'school sick sleep', 'sick sleep work', 'sleep work iteratively', 'work iteratively clustered', 'iteratively clustered journal', 'clustered journal topic', 'journal topic detail', 'topic detail removed', 'detail removed noun', 'removed noun refer', 'noun refer topic', 'refer topic number', 'topic number timing', 'number timing eg', 'timing eg today', 'eg today yesterday', 'today yesterday general', 'yesterday general feeling', 'general feeling eg', 'feeling eg feel', 'eg feel like', 'feel like proper', 'like proper noun', 'proper noun noun', 'noun noun ambiguous', 'noun ambiguous meaning', 'ambiguous meaning eg', 'meaning eg overall', 'eg overall true', 'overall true lastly', 'true lastly retained', 'lastly retained noun', 'retained noun appeared', 'noun appeared ten', 'appeared ten time', 'ten time dataset', 'time dataset process', 'dataset process resulted', 'process resulted vocabulary', 'resulted vocabulary word', 'vocabulary word topic', 'word topic modelling', 'topic modelling journal', 'modelling journal represented', 'journal represented dimensional', 'represented dimensional term', 'dimensional term frequency', 'term frequency vector', 'frequency vector component', 'vector component denoting', 'component denoting termfrequency', 'denoting termfrequency inversedocumentfrequency', 'termfrequency inversedocumentfrequency tfidf', 'inversedocumentfrequency tfidf corresponding', 'tfidf corresponding term', 'corresponding term algorithm', 'term algorithm summarizes', 'algorithm summarizes topic', 'summarizes topic modelling', 'topic modelling methodology', 'modelling methodology given', 'methodology given tfidf', 'given tfidf term', 'tfidf term frequency', 'term frequency vector', 'frequency vector journal', 'vector journal run', 'journal run nonnegative', 'run nonnegative matrix', 'nonnegative matrix factorization', 'matrix factorization nmf', 'factorization nmf implemented', 'nmf implemented python', 'implemented python scikitlearn', 'python scikitlearn package', 'scikitlearn package objective', 'package objective nmf', 'objective nmf find', 'nmf find two', 'find two matrix', 'two matrix whose', 'matrix whose product', 'whose product approximates', 'product approximates original', 'approximates original matrix', 'original matrix case', 'matrix case one', 'case one matrix', 'one matrix weighted', 'matrix weighted set', 'weighted set topic', 'set topic journal', 'topic journal weighted', 'journal weighted set', 'weighted set word', 'set word belong', 'word belong topic', 'belong topic hence', 'topic hence journal', 'hence journal represented', 'journal represented combination', 'represented combination topic', 'combination topic composed', 'topic composed weighted', 'composed weighted combination', 'weighted combination word', 'combination word chose', 'word chose nmf', 'chose nmf nonnegativity', 'nmf nonnegativity constraint', 'nonnegativity constraint aid', 'constraint aid interpretability', 'aid interpretability context', 'interpretability context analyzing', 'context analyzing word', 'analyzing word frequency', 'word frequency negative', 'frequency negative presence', 'negative presence word', 'presence word would', 'word would interpretable', 'would interpretable track', 'interpretable track word', 'track word occurrence', 'word occurrence semantics', 'occurrence semantics syntax', 'semantics syntax unlike', 'syntax unlike matrix', 'unlike matrix factorization', 'matrix factorization method', 'factorization method nmf', 'method nmf reconstructs', 'nmf reconstructs document', 'reconstructs document sum', 'document sum positive', 'sum positive part', 'positive part enables', 'part enables u', 'enables u easily', 'u easily manually', 'easily manually label', 'manually label discovered', 'label discovered topic', 'discovered topic iterating', 'topic iterating topic', 'iterating topic derived', 'topic derived different', 'derived different topic', 'different topic matrix', 'topic matrix step', 'matrix step algorithm', 'step algorithm matrix', 'algorithm matrix consists', 'matrix consists one', 'consists one topic', 'one topic per', 'topic per row', 'per row topic', 'row topic ha', 'topic ha positive', 'ha positive weight', 'positive weight word', 'weight word vocabulary', 'word vocabulary stronger', 'vocabulary stronger weight', 'stronger weight indicate', 'weight indicate higher', 'indicate higher relevance', 'higher relevance topic', 'relevance topic final', 'topic final topic', 'final topic matrix', 'topic matrix used', 'matrix used ha', 'used ha topic', 'ha topic shown', 'topic shown table', 'shown table show', 'table show first', 'show first six', 'first six word', 'six word table', 'word table simplicity', 'table simplicity sorted', 'simplicity sorted word', 'sorted word associated', 'word associated topic', 'associated topic highest', 'topic highest relevance', 'highest relevance lowest', 'relevance lowest judging', 'lowest judging topic', 'judging topic matrix', 'topic matrix considered', 'matrix considered top', 'considered top twenty', 'top twenty important', 'twenty important word', 'important word per', 'word per topic', 'per topic using', 'topic using information', 'using information manually', 'information manually labeled', 'manually labeled row', 'labeled row matrix', 'row matrix corresponding', 'matrix corresponding topic', 'corresponding topic furthermore', 'topic furthermore manually', 'furthermore manually evaluated', 'manually evaluated matrix', 'evaluated matrix based', 'matrix based distinctness', 'based distinctness topic', 'distinctness topic consistency', 'topic consistency within', 'consistency within topic', 'within topic interpretability', 'topic interpretability process', 'interpretability process compiled', 'process compiled custom', 'compiled custom list', 'custom list removed', 'list removed word', 'removed word mentioned', 'word mentioned earlier', 'mentioned earlier section', 'earlier section group', 'section group word', 'group word removed', 'word removed appeared', 'removed appeared standalone', 'appeared standalone topic', 'standalone topic offer', 'topic offer information', 'offer information journal', 'information journal wa', 'journal wa example', 'wa example proper', 'example proper noun', 'proper noun appeared', 'noun appeared standalone', 'appeared standalone topic', 'standalone topic word', 'topic word deemed', 'word deemed general', 'deemed general ambiguous', 'general ambiguous appeared', 'ambiguous appeared across', 'appeared across several', 'across several topic', 'several topic hence', 'topic hence provide', 'hence provide discriminative', 'provide discriminative information', 'discriminative information tested', 'information tested different', 'tested different level', 'different level regularization', 'level regularization enforce', 'regularization enforce sparseness', 'enforce sparseness model', 'sparseness model see', 'model see discussion', 'see discussion find', 'discussion find significant', 'find significant difference', 'significant difference however', 'difference however one', 'however one important', 'one important modification', 'important modification made', 'modification made regularize', 'made regularize topic', 'regularize topic wa', 'topic wa make', 'wa make first', 'make first word', 'first word strong', 'word strong second', 'strong second one', 'second one default', 'one default first', 'default first word', 'first word stronger', 'word stronger second', 'stronger second word', 'second word stronger', 'word stronger third', 'stronger third word', 'third word since', 'word since relevant', 'since relevant word', 'relevant word topic', 'word topic tended', 'topic tended strong', 'tended strong signal', 'strong signal regardless', 'signal regardless changed', 'regardless changed number', 'changed number topic', 'number topic preprocessing', 'topic preprocessing procedure', 'preprocessing procedure regularization', 'procedure regularization objective', 'regularization objective function', 'objective function example', 'function example word', 'example word love', 'word love journal', 'love journal sport', 'journal sport would', 'sport would strong', 'would strong journal', 'strong journal would', 'journal would labeled', 'would labeled relating', 'labeled relating romantic', 'relating romantic love', 'romantic love lowering', 'love lowering importance', 'lowering importance first', 'importance first word', 'first word wa', 'word wa sufficient', 'wa sufficient eliminate', 'sufficient eliminate false', 'eliminate false positive', 'false positive identified', 'positive identified given', 'identified given final', 'given final topic', 'final topic matrix', 'topic matrix summarized', 'matrix summarized table', 'summarized table next', 'table next step', 'next step use', 'step use assign', 'use assign label', 'assign label journal', 'label journal step', 'journal step algorithm', 'step algorithm plotted', 'algorithm plotted distribution', 'plotted distribution important', 'distribution important topic', 'important topic wa', 'topic wa journal', 'wa journal dataset', 'journal dataset importance', 'dataset importance ranging', 'importance ranging zero', 'ranging zero one', 'zero one distribution', 'one distribution similar', 'distribution similar shape', 'similar shape clear', 'shape clear inflection', 'clear inflection point', 'inflection point importance', 'point importance figure', 'importance figure show', 'figure show example', 'show example importance', 'example importance distribution', 'importance distribution topic', 'distribution topic work', 'topic work inflection', 'work inflection point', 'inflection point occurs', 'point occurs importance']"
https://dl.acm.org/doi/abs/10.1145/2556288.2557214,1,We first introduce several measures used in our analysis. Relative use – For a given health condition its relative use on Twitter or the search engine is given by the ratio of its volume of use in that medium to the volume of the health term that is most used in that medium. For example the most searched condition on the search engine was “cancer” appearing in 31443735 queries and the second-most queried was “pregnancy” found in 25721056 queries; the relative use for “cancer” on the engine=1 it was .82 for “pregnancy.” Rank – A relative ranking based on normalized relative mention of a health condition (on Twitter or the search engine)—lower ranks mean greater use. For example on Twitter the most-used term from our list is “headache” that appeared in 24607507 posts; it would receive a rank=1. Rank difference – We compute rank difference between the search engine and Twitter for each health term – large negative values indicate that a term is searched relatively more than tweeted whereas large positive values mean the reverse. For example “cough” has rank 4 on Twitter and rank 26 on the search engine – its rank difference is 22 reflecting its relative prominence on Twitter as compared to search. Based on these definitions in Table 1 we report the top 20 most searched and most shared health conditions in terms of their rank and relative use on each platform. The table also shows the top 20 most positive rank difference (relatively more tweeted than searched) and most negative rank difference (relatively more searched than tweeted) conditions along with severity/type and stigma level of each conditions.,we first introduce several measure used in our analysis relative use for a given health condition it relative use on twitter or the search engine is given by the ratio of it volume of use in that medium to the volume of the health term that is most used in that medium for example the most searched condition on the search engine wa cancer appearing in query and the secondmost queried wa pregnancy found in query the relative use for cancer on the engine it wa for pregnancy rank a relative ranking based on normalized relative mention of a health condition on twitter or the search enginelower rank mean greater use for example on twitter the mostused term from our list is headache that appeared in post it would receive a rank rank difference we compute rank difference between the search engine and twitter for each health term large negative value indicate that a term is searched relatively more than tweeted whereas large positive value mean the reverse for example cough ha rank on twitter and rank on the search engine it rank difference is reflecting it relative prominence on twitter a compared to search based on these definition in table we report the top most searched and most shared health condition in term of their rank and relative use on each platform the table also show the top most positive rank difference relatively more tweeted than searched and most negative rank difference relatively more searched than tweeted condition along with severitytype and stigma level of each condition,"['first', 'introduce', 'several', 'measure', 'used', 'analysis', 'relative', 'use', 'given', 'health', 'condition', 'relative', 'use', 'twitter', 'search', 'engine', 'given', 'ratio', 'volume', 'use', 'medium', 'volume', 'health', 'term', 'used', 'medium', 'example', 'searched', 'condition', 'search', 'engine', 'wa', 'cancer', 'appearing', 'query', 'secondmost', 'queried', 'wa', 'pregnancy', 'found', 'query', 'relative', 'use', 'cancer', 'engine', 'wa', 'pregnancy', 'rank', 'relative', 'ranking', 'based', 'normalized', 'relative', 'mention', 'health', 'condition', 'twitter', 'search', 'enginelower', 'rank', 'mean', 'greater', 'use', 'example', 'twitter', 'mostused', 'term', 'list', 'headache', 'appeared', 'post', 'would', 'receive', 'rank', 'rank', 'difference', 'compute', 'rank', 'difference', 'search', 'engine', 'twitter', 'health', 'term', 'large', 'negative', 'value', 'indicate', 'term', 'searched', 'relatively', 'tweeted', 'whereas', 'large', 'positive', 'value', 'mean', 'reverse', 'example', 'cough', 'ha', 'rank', 'twitter', 'rank', 'search', 'engine', 'rank', 'difference', 'reflecting', 'relative', 'prominence', 'twitter', 'compared', 'search', 'based', 'definition', 'table', 'report', 'top', 'searched', 'shared', 'health', 'condition', 'term', 'rank', 'relative', 'use', 'platform', 'table', 'also', 'show', 'top', 'positive', 'rank', 'difference', 'relatively', 'tweeted', 'searched', 'negative', 'rank', 'difference', 'relatively', 'searched', 'tweeted', 'condition', 'along', 'severitytype', 'stigma', 'level', 'condition']","['first introduce', 'introduce several', 'several measure', 'measure used', 'used analysis', 'analysis relative', 'relative use', 'use given', 'given health', 'health condition', 'condition relative', 'relative use', 'use twitter', 'twitter search', 'search engine', 'engine given', 'given ratio', 'ratio volume', 'volume use', 'use medium', 'medium volume', 'volume health', 'health term', 'term used', 'used medium', 'medium example', 'example searched', 'searched condition', 'condition search', 'search engine', 'engine wa', 'wa cancer', 'cancer appearing', 'appearing query', 'query secondmost', 'secondmost queried', 'queried wa', 'wa pregnancy', 'pregnancy found', 'found query', 'query relative', 'relative use', 'use cancer', 'cancer engine', 'engine wa', 'wa pregnancy', 'pregnancy rank', 'rank relative', 'relative ranking', 'ranking based', 'based normalized', 'normalized relative', 'relative mention', 'mention health', 'health condition', 'condition twitter', 'twitter search', 'search enginelower', 'enginelower rank', 'rank mean', 'mean greater', 'greater use', 'use example', 'example twitter', 'twitter mostused', 'mostused term', 'term list', 'list headache', 'headache appeared', 'appeared post', 'post would', 'would receive', 'receive rank', 'rank rank', 'rank difference', 'difference compute', 'compute rank', 'rank difference', 'difference search', 'search engine', 'engine twitter', 'twitter health', 'health term', 'term large', 'large negative', 'negative value', 'value indicate', 'indicate term', 'term searched', 'searched relatively', 'relatively tweeted', 'tweeted whereas', 'whereas large', 'large positive', 'positive value', 'value mean', 'mean reverse', 'reverse example', 'example cough', 'cough ha', 'ha rank', 'rank twitter', 'twitter rank', 'rank search', 'search engine', 'engine rank', 'rank difference', 'difference reflecting', 'reflecting relative', 'relative prominence', 'prominence twitter', 'twitter compared', 'compared search', 'search based', 'based definition', 'definition table', 'table report', 'report top', 'top searched', 'searched shared', 'shared health', 'health condition', 'condition term', 'term rank', 'rank relative', 'relative use', 'use platform', 'platform table', 'table also', 'also show', 'show top', 'top positive', 'positive rank', 'rank difference', 'difference relatively', 'relatively tweeted', 'tweeted searched', 'searched negative', 'negative rank', 'rank difference', 'difference relatively', 'relatively searched', 'searched tweeted', 'tweeted condition', 'condition along', 'along severitytype', 'severitytype stigma', 'stigma level', 'level condition']","['first introduce several', 'introduce several measure', 'several measure used', 'measure used analysis', 'used analysis relative', 'analysis relative use', 'relative use given', 'use given health', 'given health condition', 'health condition relative', 'condition relative use', 'relative use twitter', 'use twitter search', 'twitter search engine', 'search engine given', 'engine given ratio', 'given ratio volume', 'ratio volume use', 'volume use medium', 'use medium volume', 'medium volume health', 'volume health term', 'health term used', 'term used medium', 'used medium example', 'medium example searched', 'example searched condition', 'searched condition search', 'condition search engine', 'search engine wa', 'engine wa cancer', 'wa cancer appearing', 'cancer appearing query', 'appearing query secondmost', 'query secondmost queried', 'secondmost queried wa', 'queried wa pregnancy', 'wa pregnancy found', 'pregnancy found query', 'found query relative', 'query relative use', 'relative use cancer', 'use cancer engine', 'cancer engine wa', 'engine wa pregnancy', 'wa pregnancy rank', 'pregnancy rank relative', 'rank relative ranking', 'relative ranking based', 'ranking based normalized', 'based normalized relative', 'normalized relative mention', 'relative mention health', 'mention health condition', 'health condition twitter', 'condition twitter search', 'twitter search enginelower', 'search enginelower rank', 'enginelower rank mean', 'rank mean greater', 'mean greater use', 'greater use example', 'use example twitter', 'example twitter mostused', 'twitter mostused term', 'mostused term list', 'term list headache', 'list headache appeared', 'headache appeared post', 'appeared post would', 'post would receive', 'would receive rank', 'receive rank rank', 'rank rank difference', 'rank difference compute', 'difference compute rank', 'compute rank difference', 'rank difference search', 'difference search engine', 'search engine twitter', 'engine twitter health', 'twitter health term', 'health term large', 'term large negative', 'large negative value', 'negative value indicate', 'value indicate term', 'indicate term searched', 'term searched relatively', 'searched relatively tweeted', 'relatively tweeted whereas', 'tweeted whereas large', 'whereas large positive', 'large positive value', 'positive value mean', 'value mean reverse', 'mean reverse example', 'reverse example cough', 'example cough ha', 'cough ha rank', 'ha rank twitter', 'rank twitter rank', 'twitter rank search', 'rank search engine', 'search engine rank', 'engine rank difference', 'rank difference reflecting', 'difference reflecting relative', 'reflecting relative prominence', 'relative prominence twitter', 'prominence twitter compared', 'twitter compared search', 'compared search based', 'search based definition', 'based definition table', 'definition table report', 'table report top', 'report top searched', 'top searched shared', 'searched shared health', 'shared health condition', 'health condition term', 'condition term rank', 'term rank relative', 'rank relative use', 'relative use platform', 'use platform table', 'platform table also', 'table also show', 'also show top', 'show top positive', 'top positive rank', 'positive rank difference', 'rank difference relatively', 'difference relatively tweeted', 'relatively tweeted searched', 'tweeted searched negative', 'searched negative rank', 'negative rank difference', 'rank difference relatively', 'difference relatively searched', 'relatively searched tweeted', 'searched tweeted condition', 'tweeted condition along', 'condition along severitytype', 'along severitytype stigma', 'severitytype stigma level', 'stigma level condition']"
https://www.aaai.org/ocs/index.php/ICWSM/ICWSM14/paper/viewPaper/8075,0,To understand the nature of self-disclosure in reddit posts we first examine the general linguistic attributes manifested in their content. In Table 3 we first present a list of the most popular (stopword eliminated) unigrams that appear in reddit postings. We intended to look at these highly shared unigrams more deeply and systematically hence we organized these unigrams (stopword inclusive) in various semantic categories provided by the psycholinguistic lexicon LIWC (http://www.liwc.net/). We find that among the unigrams in Table 3 there are words that extensively span emotional or affective expressions (happy love bad anxiety good hate) e.g.: “I've been recently wondering if my love to numb the world around me has turned me into an alcoholic…” “Has anyone else battled numbness loss of feeling during recovery? Does it ever get better? i've been sober for about 2 years and still have pretty severe anxiety at times”. We observe presence of relationships and social life words too (family friends people person parents) e.g.: “i get really anxious out when i go home for big events.” “i do love my family they're just really loud and argumentative sometimes”. Temporal indicators in reddit discourse is also visible— e.g. time day years months: “hi all i'm ten weeks sober today and while i wish i could say i'm physically and mentally in great shape the truth is i judge my days by how i feel less bad as oppose to good”. Work and daily grind oriented words are common as well because lifestyle irregularities are often associated with the psychopathology of mental illness (Prigerson et al. 1995)—e.g. life school work job: “I am completely broke can't afford rehab and can't take time off work”. We also find a fair number of cognitive words in these highly used unigrams (felt hard feeling lot) e.g.: “I'm new here but having anxiety like I haven't felt in a long time” “I find a lot of strength in going to a concert. I have been understanding of my anxiety and depression since i was about 8 and i hated it”. These observations are supported by psychology literature where cognitive biases as manifested through dysfunctional attitudes depressive attributional biases and negative automatic thoughts were found to be characteristic of mental illness (Eaves & Rush 1984). Further inhibition words like avoid deny safe demonstrate that redditors are perhaps using the platform to broadcast their thoughts to an audience of strangers or weak ties on issues and topics they might consider to be socially stigmatic to be discussed elsewhere: “i can't escape the feeling of fright i have at all times. i don't feel safe in my own home” “ive been denying my (assumed) depression symptoms for close to two years now writing them off …” Comparing across different LIWC semantic categories over all posts we observe noticeable differences— KruskalWallis one-way analysis of variance indicated the differences across categories to be significant (χ2 (39; N=20411)=9.24; p<10-4). Table 4 reports the top 8 most common LIWC categories the mean proportion of words from each category in the posts and the corresponding standard deviation. Note that the percentages over all categories sum to greater than 100% since a word could belong to multiple categories. Observing closely many of the categories whose corresponding unigrams appeared in Table 3 are also highly prominent categories globally over all posts as given in Table 4. Not shown in Table 4 perhaps intuitively negative emotion anger and sadness words were considerably more prominent than positive emotion words (a Wilcoxon signed rank test reveals that the differences are statistically significant (z=-6.08 p<.001)). Likely these redditors experience several negative emotions: hence mental instability helplessness loneliness restlessness manifest in their postings (Rude et al. 2004). the health and social issues they are facing. In fact high selfattentional focus is a known psychological attribute of mental illness sufferers (Chung & Pennebaker 2007).,to understand the nature of selfdisclosure in reddit post we first examine the general linguistic attribute manifested in their content in table we first present a list of the most popular stopword eliminated unigrams that appear in reddit posting we intended to look at these highly shared unigrams more deeply and systematically hence we organized these unigrams stopword inclusive in various semantic category provided by the psycholinguistic lexicon liwc httpwwwliwcnet we find that among the unigrams in table there are word that extensively span emotional or affective expression happy love bad anxiety good hate eg ive been recently wondering if my love to numb the world around me ha turned me into an alcoholic ha anyone else battled numbness loss of feeling during recovery doe it ever get better ive been sober for about year and still have pretty severe anxiety at time we observe presence of relationship and social life word too family friend people person parent eg i get really anxious out when i go home for big event i do love my family theyre just really loud and argumentative sometimes temporal indicator in reddit discourse is also visible eg time day year month hi all im ten week sober today and while i wish i could say im physically and mentally in great shape the truth is i judge my day by how i feel le bad a oppose to good work and daily grind oriented word are common a well because lifestyle irregularity are often associated with the psychopathology of mental illness prigerson et al eg life school work job i am completely broke cant afford rehab and cant take time off work we also find a fair number of cognitive word in these highly used unigrams felt hard feeling lot eg im new here but having anxiety like i havent felt in a long time i find a lot of strength in going to a concert i have been understanding of my anxiety and depression since i wa about and i hated it these observation are supported by psychology literature where cognitive bias a manifested through dysfunctional attitude depressive attributional bias and negative automatic thought were found to be characteristic of mental illness eaves rush further inhibition word like avoid deny safe demonstrate that redditors are perhaps using the platform to broadcast their thought to an audience of stranger or weak tie on issue and topic they might consider to be socially stigmatic to be discussed elsewhere i cant escape the feeling of fright i have at all time i dont feel safe in my own home ive been denying my assumed depression symptom for close to two year now writing them off comparing across different liwc semantic category over all post we observe noticeable difference kruskalwallis oneway analysis of variance indicated the difference across category to be significant n p table report the top most common liwc category the mean proportion of word from each category in the post and the corresponding standard deviation note that the percentage over all category sum to greater than since a word could belong to multiple category observing closely many of the category whose corresponding unigrams appeared in table are also highly prominent category globally over all post a given in table not shown in table perhaps intuitively negative emotion anger and sadness word were considerably more prominent than positive emotion word a wilcoxon signed rank test reveals that the difference are statistically significant z p likely these redditors experience several negative emotion hence mental instability helplessness loneliness restlessness manifest in their posting rude et al the health and social issue they are facing in fact high selfattentional focus is a known psychological attribute of mental illness sufferer chung pennebaker,"['understand', 'nature', 'selfdisclosure', 'reddit', 'post', 'first', 'examine', 'general', 'linguistic', 'attribute', 'manifested', 'content', 'table', 'first', 'present', 'list', 'popular', 'stopword', 'eliminated', 'unigrams', 'appear', 'reddit', 'posting', 'intended', 'look', 'highly', 'shared', 'unigrams', 'deeply', 'systematically', 'hence', 'organized', 'unigrams', 'stopword', 'inclusive', 'various', 'semantic', 'category', 'provided', 'psycholinguistic', 'lexicon', 'liwc', 'httpwwwliwcnet', 'find', 'among', 'unigrams', 'table', 'word', 'extensively', 'span', 'emotional', 'affective', 'expression', 'happy', 'love', 'bad', 'anxiety', 'good', 'hate', 'eg', 'ive', 'recently', 'wondering', 'love', 'numb', 'world', 'around', 'ha', 'turned', 'alcoholic', 'ha', 'anyone', 'else', 'battled', 'numbness', 'loss', 'feeling', 'recovery', 'doe', 'ever', 'get', 'better', 'ive', 'sober', 'year', 'still', 'pretty', 'severe', 'anxiety', 'time', 'observe', 'presence', 'relationship', 'social', 'life', 'word', 'family', 'friend', 'people', 'person', 'parent', 'eg', 'get', 'really', 'anxious', 'go', 'home', 'big', 'event', 'love', 'family', 'theyre', 'really', 'loud', 'argumentative', 'sometimes', 'temporal', 'indicator', 'reddit', 'discourse', 'also', 'visible', 'eg', 'time', 'day', 'year', 'month', 'hi', 'im', 'ten', 'week', 'sober', 'today', 'wish', 'could', 'say', 'im', 'physically', 'mentally', 'great', 'shape', 'truth', 'judge', 'day', 'feel', 'le', 'bad', 'oppose', 'good', 'work', 'daily', 'grind', 'oriented', 'word', 'common', 'well', 'lifestyle', 'irregularity', 'often', 'associated', 'psychopathology', 'mental', 'illness', 'prigerson', 'et', 'al', 'eg', 'life', 'school', 'work', 'job', 'completely', 'broke', 'cant', 'afford', 'rehab', 'cant', 'take', 'time', 'work', 'also', 'find', 'fair', 'number', 'cognitive', 'word', 'highly', 'used', 'unigrams', 'felt', 'hard', 'feeling', 'lot', 'eg', 'im', 'new', 'anxiety', 'like', 'havent', 'felt', 'long', 'time', 'find', 'lot', 'strength', 'going', 'concert', 'understanding', 'anxiety', 'depression', 'since', 'wa', 'hated', 'observation', 'supported', 'psychology', 'literature', 'cognitive', 'bias', 'manifested', 'dysfunctional', 'attitude', 'depressive', 'attributional', 'bias', 'negative', 'automatic', 'thought', 'found', 'characteristic', 'mental', 'illness', 'eaves', 'rush', 'inhibition', 'word', 'like', 'avoid', 'deny', 'safe', 'demonstrate', 'redditors', 'perhaps', 'using', 'platform', 'broadcast', 'thought', 'audience', 'stranger', 'weak', 'tie', 'issue', 'topic', 'might', 'consider', 'socially', 'stigmatic', 'discussed', 'elsewhere', 'cant', 'escape', 'feeling', 'fright', 'time', 'dont', 'feel', 'safe', 'home', 'ive', 'denying', 'assumed', 'depression', 'symptom', 'close', 'two', 'year', 'writing', 'comparing', 'across', 'different', 'liwc', 'semantic', 'category', 'post', 'observe', 'noticeable', 'difference', 'kruskalwallis', 'oneway', 'analysis', 'variance', 'indicated', 'difference', 'across', 'category', 'significant', 'n', 'p', 'table', 'report', 'top', 'common', 'liwc', 'category', 'mean', 'proportion', 'word', 'category', 'post', 'corresponding', 'standard', 'deviation', 'note', 'percentage', 'category', 'sum', 'greater', 'since', 'word', 'could', 'belong', 'multiple', 'category', 'observing', 'closely', 'many', 'category', 'whose', 'corresponding', 'unigrams', 'appeared', 'table', 'also', 'highly', 'prominent', 'category', 'globally', 'post', 'given', 'table', 'shown', 'table', 'perhaps', 'intuitively', 'negative', 'emotion', 'anger', 'sadness', 'word', 'considerably', 'prominent', 'positive', 'emotion', 'word', 'wilcoxon', 'signed', 'rank', 'test', 'reveals', 'difference', 'statistically', 'significant', 'z', 'p', 'likely', 'redditors', 'experience', 'several', 'negative', 'emotion', 'hence', 'mental', 'instability', 'helplessness', 'loneliness', 'restlessness', 'manifest', 'posting', 'rude', 'et', 'al', 'health', 'social', 'issue', 'facing', 'fact', 'high', 'selfattentional', 'focus', 'known', 'psychological', 'attribute', 'mental', 'illness', 'sufferer', 'chung', 'pennebaker']","['understand nature', 'nature selfdisclosure', 'selfdisclosure reddit', 'reddit post', 'post first', 'first examine', 'examine general', 'general linguistic', 'linguistic attribute', 'attribute manifested', 'manifested content', 'content table', 'table first', 'first present', 'present list', 'list popular', 'popular stopword', 'stopword eliminated', 'eliminated unigrams', 'unigrams appear', 'appear reddit', 'reddit posting', 'posting intended', 'intended look', 'look highly', 'highly shared', 'shared unigrams', 'unigrams deeply', 'deeply systematically', 'systematically hence', 'hence organized', 'organized unigrams', 'unigrams stopword', 'stopword inclusive', 'inclusive various', 'various semantic', 'semantic category', 'category provided', 'provided psycholinguistic', 'psycholinguistic lexicon', 'lexicon liwc', 'liwc httpwwwliwcnet', 'httpwwwliwcnet find', 'find among', 'among unigrams', 'unigrams table', 'table word', 'word extensively', 'extensively span', 'span emotional', 'emotional affective', 'affective expression', 'expression happy', 'happy love', 'love bad', 'bad anxiety', 'anxiety good', 'good hate', 'hate eg', 'eg ive', 'ive recently', 'recently wondering', 'wondering love', 'love numb', 'numb world', 'world around', 'around ha', 'ha turned', 'turned alcoholic', 'alcoholic ha', 'ha anyone', 'anyone else', 'else battled', 'battled numbness', 'numbness loss', 'loss feeling', 'feeling recovery', 'recovery doe', 'doe ever', 'ever get', 'get better', 'better ive', 'ive sober', 'sober year', 'year still', 'still pretty', 'pretty severe', 'severe anxiety', 'anxiety time', 'time observe', 'observe presence', 'presence relationship', 'relationship social', 'social life', 'life word', 'word family', 'family friend', 'friend people', 'people person', 'person parent', 'parent eg', 'eg get', 'get really', 'really anxious', 'anxious go', 'go home', 'home big', 'big event', 'event love', 'love family', 'family theyre', 'theyre really', 'really loud', 'loud argumentative', 'argumentative sometimes', 'sometimes temporal', 'temporal indicator', 'indicator reddit', 'reddit discourse', 'discourse also', 'also visible', 'visible eg', 'eg time', 'time day', 'day year', 'year month', 'month hi', 'hi im', 'im ten', 'ten week', 'week sober', 'sober today', 'today wish', 'wish could', 'could say', 'say im', 'im physically', 'physically mentally', 'mentally great', 'great shape', 'shape truth', 'truth judge', 'judge day', 'day feel', 'feel le', 'le bad', 'bad oppose', 'oppose good', 'good work', 'work daily', 'daily grind', 'grind oriented', 'oriented word', 'word common', 'common well', 'well lifestyle', 'lifestyle irregularity', 'irregularity often', 'often associated', 'associated psychopathology', 'psychopathology mental', 'mental illness', 'illness prigerson', 'prigerson et', 'et al', 'al eg', 'eg life', 'life school', 'school work', 'work job', 'job completely', 'completely broke', 'broke cant', 'cant afford', 'afford rehab', 'rehab cant', 'cant take', 'take time', 'time work', 'work also', 'also find', 'find fair', 'fair number', 'number cognitive', 'cognitive word', 'word highly', 'highly used', 'used unigrams', 'unigrams felt', 'felt hard', 'hard feeling', 'feeling lot', 'lot eg', 'eg im', 'im new', 'new anxiety', 'anxiety like', 'like havent', 'havent felt', 'felt long', 'long time', 'time find', 'find lot', 'lot strength', 'strength going', 'going concert', 'concert understanding', 'understanding anxiety', 'anxiety depression', 'depression since', 'since wa', 'wa hated', 'hated observation', 'observation supported', 'supported psychology', 'psychology literature', 'literature cognitive', 'cognitive bias', 'bias manifested', 'manifested dysfunctional', 'dysfunctional attitude', 'attitude depressive', 'depressive attributional', 'attributional bias', 'bias negative', 'negative automatic', 'automatic thought', 'thought found', 'found characteristic', 'characteristic mental', 'mental illness', 'illness eaves', 'eaves rush', 'rush inhibition', 'inhibition word', 'word like', 'like avoid', 'avoid deny', 'deny safe', 'safe demonstrate', 'demonstrate redditors', 'redditors perhaps', 'perhaps using', 'using platform', 'platform broadcast', 'broadcast thought', 'thought audience', 'audience stranger', 'stranger weak', 'weak tie', 'tie issue', 'issue topic', 'topic might', 'might consider', 'consider socially', 'socially stigmatic', 'stigmatic discussed', 'discussed elsewhere', 'elsewhere cant', 'cant escape', 'escape feeling', 'feeling fright', 'fright time', 'time dont', 'dont feel', 'feel safe', 'safe home', 'home ive', 'ive denying', 'denying assumed', 'assumed depression', 'depression symptom', 'symptom close', 'close two', 'two year', 'year writing', 'writing comparing', 'comparing across', 'across different', 'different liwc', 'liwc semantic', 'semantic category', 'category post', 'post observe', 'observe noticeable', 'noticeable difference', 'difference kruskalwallis', 'kruskalwallis oneway', 'oneway analysis', 'analysis variance', 'variance indicated', 'indicated difference', 'difference across', 'across category', 'category significant', 'significant n', 'n p', 'p table', 'table report', 'report top', 'top common', 'common liwc', 'liwc category', 'category mean', 'mean proportion', 'proportion word', 'word category', 'category post', 'post corresponding', 'corresponding standard', 'standard deviation', 'deviation note', 'note percentage', 'percentage category', 'category sum', 'sum greater', 'greater since', 'since word', 'word could', 'could belong', 'belong multiple', 'multiple category', 'category observing', 'observing closely', 'closely many', 'many category', 'category whose', 'whose corresponding', 'corresponding unigrams', 'unigrams appeared', 'appeared table', 'table also', 'also highly', 'highly prominent', 'prominent category', 'category globally', 'globally post', 'post given', 'given table', 'table shown', 'shown table', 'table perhaps', 'perhaps intuitively', 'intuitively negative', 'negative emotion', 'emotion anger', 'anger sadness', 'sadness word', 'word considerably', 'considerably prominent', 'prominent positive', 'positive emotion', 'emotion word', 'word wilcoxon', 'wilcoxon signed', 'signed rank', 'rank test', 'test reveals', 'reveals difference', 'difference statistically', 'statistically significant', 'significant z', 'z p', 'p likely', 'likely redditors', 'redditors experience', 'experience several', 'several negative', 'negative emotion', 'emotion hence', 'hence mental', 'mental instability', 'instability helplessness', 'helplessness loneliness', 'loneliness restlessness', 'restlessness manifest', 'manifest posting', 'posting rude', 'rude et', 'et al', 'al health', 'health social', 'social issue', 'issue facing', 'facing fact', 'fact high', 'high selfattentional', 'selfattentional focus', 'focus known', 'known psychological', 'psychological attribute', 'attribute mental', 'mental illness', 'illness sufferer', 'sufferer chung', 'chung pennebaker']","['understand nature selfdisclosure', 'nature selfdisclosure reddit', 'selfdisclosure reddit post', 'reddit post first', 'post first examine', 'first examine general', 'examine general linguistic', 'general linguistic attribute', 'linguistic attribute manifested', 'attribute manifested content', 'manifested content table', 'content table first', 'table first present', 'first present list', 'present list popular', 'list popular stopword', 'popular stopword eliminated', 'stopword eliminated unigrams', 'eliminated unigrams appear', 'unigrams appear reddit', 'appear reddit posting', 'reddit posting intended', 'posting intended look', 'intended look highly', 'look highly shared', 'highly shared unigrams', 'shared unigrams deeply', 'unigrams deeply systematically', 'deeply systematically hence', 'systematically hence organized', 'hence organized unigrams', 'organized unigrams stopword', 'unigrams stopword inclusive', 'stopword inclusive various', 'inclusive various semantic', 'various semantic category', 'semantic category provided', 'category provided psycholinguistic', 'provided psycholinguistic lexicon', 'psycholinguistic lexicon liwc', 'lexicon liwc httpwwwliwcnet', 'liwc httpwwwliwcnet find', 'httpwwwliwcnet find among', 'find among unigrams', 'among unigrams table', 'unigrams table word', 'table word extensively', 'word extensively span', 'extensively span emotional', 'span emotional affective', 'emotional affective expression', 'affective expression happy', 'expression happy love', 'happy love bad', 'love bad anxiety', 'bad anxiety good', 'anxiety good hate', 'good hate eg', 'hate eg ive', 'eg ive recently', 'ive recently wondering', 'recently wondering love', 'wondering love numb', 'love numb world', 'numb world around', 'world around ha', 'around ha turned', 'ha turned alcoholic', 'turned alcoholic ha', 'alcoholic ha anyone', 'ha anyone else', 'anyone else battled', 'else battled numbness', 'battled numbness loss', 'numbness loss feeling', 'loss feeling recovery', 'feeling recovery doe', 'recovery doe ever', 'doe ever get', 'ever get better', 'get better ive', 'better ive sober', 'ive sober year', 'sober year still', 'year still pretty', 'still pretty severe', 'pretty severe anxiety', 'severe anxiety time', 'anxiety time observe', 'time observe presence', 'observe presence relationship', 'presence relationship social', 'relationship social life', 'social life word', 'life word family', 'word family friend', 'family friend people', 'friend people person', 'people person parent', 'person parent eg', 'parent eg get', 'eg get really', 'get really anxious', 'really anxious go', 'anxious go home', 'go home big', 'home big event', 'big event love', 'event love family', 'love family theyre', 'family theyre really', 'theyre really loud', 'really loud argumentative', 'loud argumentative sometimes', 'argumentative sometimes temporal', 'sometimes temporal indicator', 'temporal indicator reddit', 'indicator reddit discourse', 'reddit discourse also', 'discourse also visible', 'also visible eg', 'visible eg time', 'eg time day', 'time day year', 'day year month', 'year month hi', 'month hi im', 'hi im ten', 'im ten week', 'ten week sober', 'week sober today', 'sober today wish', 'today wish could', 'wish could say', 'could say im', 'say im physically', 'im physically mentally', 'physically mentally great', 'mentally great shape', 'great shape truth', 'shape truth judge', 'truth judge day', 'judge day feel', 'day feel le', 'feel le bad', 'le bad oppose', 'bad oppose good', 'oppose good work', 'good work daily', 'work daily grind', 'daily grind oriented', 'grind oriented word', 'oriented word common', 'word common well', 'common well lifestyle', 'well lifestyle irregularity', 'lifestyle irregularity often', 'irregularity often associated', 'often associated psychopathology', 'associated psychopathology mental', 'psychopathology mental illness', 'mental illness prigerson', 'illness prigerson et', 'prigerson et al', 'et al eg', 'al eg life', 'eg life school', 'life school work', 'school work job', 'work job completely', 'job completely broke', 'completely broke cant', 'broke cant afford', 'cant afford rehab', 'afford rehab cant', 'rehab cant take', 'cant take time', 'take time work', 'time work also', 'work also find', 'also find fair', 'find fair number', 'fair number cognitive', 'number cognitive word', 'cognitive word highly', 'word highly used', 'highly used unigrams', 'used unigrams felt', 'unigrams felt hard', 'felt hard feeling', 'hard feeling lot', 'feeling lot eg', 'lot eg im', 'eg im new', 'im new anxiety', 'new anxiety like', 'anxiety like havent', 'like havent felt', 'havent felt long', 'felt long time', 'long time find', 'time find lot', 'find lot strength', 'lot strength going', 'strength going concert', 'going concert understanding', 'concert understanding anxiety', 'understanding anxiety depression', 'anxiety depression since', 'depression since wa', 'since wa hated', 'wa hated observation', 'hated observation supported', 'observation supported psychology', 'supported psychology literature', 'psychology literature cognitive', 'literature cognitive bias', 'cognitive bias manifested', 'bias manifested dysfunctional', 'manifested dysfunctional attitude', 'dysfunctional attitude depressive', 'attitude depressive attributional', 'depressive attributional bias', 'attributional bias negative', 'bias negative automatic', 'negative automatic thought', 'automatic thought found', 'thought found characteristic', 'found characteristic mental', 'characteristic mental illness', 'mental illness eaves', 'illness eaves rush', 'eaves rush inhibition', 'rush inhibition word', 'inhibition word like', 'word like avoid', 'like avoid deny', 'avoid deny safe', 'deny safe demonstrate', 'safe demonstrate redditors', 'demonstrate redditors perhaps', 'redditors perhaps using', 'perhaps using platform', 'using platform broadcast', 'platform broadcast thought', 'broadcast thought audience', 'thought audience stranger', 'audience stranger weak', 'stranger weak tie', 'weak tie issue', 'tie issue topic', 'issue topic might', 'topic might consider', 'might consider socially', 'consider socially stigmatic', 'socially stigmatic discussed', 'stigmatic discussed elsewhere', 'discussed elsewhere cant', 'elsewhere cant escape', 'cant escape feeling', 'escape feeling fright', 'feeling fright time', 'fright time dont', 'time dont feel', 'dont feel safe', 'feel safe home', 'safe home ive', 'home ive denying', 'ive denying assumed', 'denying assumed depression', 'assumed depression symptom', 'depression symptom close', 'symptom close two', 'close two year', 'two year writing', 'year writing comparing', 'writing comparing across', 'comparing across different', 'across different liwc', 'different liwc semantic', 'liwc semantic category', 'semantic category post', 'category post observe', 'post observe noticeable', 'observe noticeable difference', 'noticeable difference kruskalwallis', 'difference kruskalwallis oneway', 'kruskalwallis oneway analysis', 'oneway analysis variance', 'analysis variance indicated', 'variance indicated difference', 'indicated difference across', 'difference across category', 'across category significant', 'category significant n', 'significant n p', 'n p table', 'p table report', 'table report top', 'report top common', 'top common liwc', 'common liwc category', 'liwc category mean', 'category mean proportion', 'mean proportion word', 'proportion word category', 'word category post', 'category post corresponding', 'post corresponding standard', 'corresponding standard deviation', 'standard deviation note', 'deviation note percentage', 'note percentage category', 'percentage category sum', 'category sum greater', 'sum greater since', 'greater since word', 'since word could', 'word could belong', 'could belong multiple', 'belong multiple category', 'multiple category observing', 'category observing closely', 'observing closely many', 'closely many category', 'many category whose', 'category whose corresponding', 'whose corresponding unigrams', 'corresponding unigrams appeared', 'unigrams appeared table', 'appeared table also', 'table also highly', 'also highly prominent', 'highly prominent category', 'prominent category globally', 'category globally post', 'globally post given', 'post given table', 'given table shown', 'table shown table', 'shown table perhaps', 'table perhaps intuitively', 'perhaps intuitively negative', 'intuitively negative emotion', 'negative emotion anger', 'emotion anger sadness', 'anger sadness word', 'sadness word considerably', 'word considerably prominent', 'considerably prominent positive', 'prominent positive emotion', 'positive emotion word', 'emotion word wilcoxon', 'word wilcoxon signed', 'wilcoxon signed rank', 'signed rank test', 'rank test reveals', 'test reveals difference', 'reveals difference statistically', 'difference statistically significant', 'statistically significant z', 'significant z p', 'z p likely', 'p likely redditors', 'likely redditors experience', 'redditors experience several', 'experience several negative', 'several negative emotion', 'negative emotion hence', 'emotion hence mental', 'hence mental instability', 'mental instability helplessness', 'instability helplessness loneliness', 'helplessness loneliness restlessness', 'loneliness restlessness manifest', 'restlessness manifest posting', 'manifest posting rude', 'posting rude et', 'rude et al', 'et al health', 'al health social', 'health social issue', 'social issue facing', 'issue facing fact', 'facing fact high', 'fact high selfattentional', 'high selfattentional focus', 'selfattentional focus known', 'focus known psychological', 'known psychological attribute', 'psychological attribute mental', 'attribute mental illness', 'mental illness sufferer', 'illness sufferer chung', 'sufferer chung pennebaker']"
https://dl.acm.org/doi/abs/10.1145/3025453.3025932,1,Towards our first research goal RQ 1 to examine the visual features of images relating to mental health disorders we employ the extraction of color profiles i.e. grayscale histograms [50]. Grayscale histograms provide us intuition about the brightness saturation and contrast distribution of images. In these histograms images with high contrast pixels are binned in bins with lower numbers (near 0) whereas images with brighter pixels are binned in higher number bins (near 255). We utilize the OpenCV library2 to extract these color histograms of images in our dataset. We also assess the visual saliency of images (using OpenCV) – a distinct subjective perceptual quality that makes some images stand out from their neighbors [24]. A typical image in our dataset is of size 612px × 612px so by using a saliency metric we obtain a 612 × 612 grid matrix. For each image in these three visual feature categories we obtain an empirical threshold that ensures 1/3 rd of the pixels will be greater than this value when sorted based on their saliency.,towards our first research goal rq to examine the visual feature of image relating to mental health disorder we employ the extraction of color profile ie grayscale histogram grayscale histogram provide u intuition about the brightness saturation and contrast distribution of image in these histogram image with high contrast pixel are binned in bin with lower number near whereas image with brighter pixel are binned in higher number bin near we utilize the opencv library to extract these color histogram of image in our dataset we also ass the visual saliency of image using opencv a distinct subjective perceptual quality that make some image stand out from their neighbor a typical image in our dataset is of size px px so by using a saliency metric we obtain a grid matrix for each image in these three visual feature category we obtain an empirical threshold that ensures rd of the pixel will be greater than this value when sorted based on their saliency,"['towards', 'first', 'research', 'goal', 'rq', 'examine', 'visual', 'feature', 'image', 'relating', 'mental', 'health', 'disorder', 'employ', 'extraction', 'color', 'profile', 'ie', 'grayscale', 'histogram', 'grayscale', 'histogram', 'provide', 'u', 'intuition', 'brightness', 'saturation', 'contrast', 'distribution', 'image', 'histogram', 'image', 'high', 'contrast', 'pixel', 'binned', 'bin', 'lower', 'number', 'near', 'whereas', 'image', 'brighter', 'pixel', 'binned', 'higher', 'number', 'bin', 'near', 'utilize', 'opencv', 'library', 'extract', 'color', 'histogram', 'image', 'dataset', 'also', 'ass', 'visual', 'saliency', 'image', 'using', 'opencv', 'distinct', 'subjective', 'perceptual', 'quality', 'make', 'image', 'stand', 'neighbor', 'typical', 'image', 'dataset', 'size', 'px', 'px', 'using', 'saliency', 'metric', 'obtain', 'grid', 'matrix', 'image', 'three', 'visual', 'feature', 'category', 'obtain', 'empirical', 'threshold', 'ensures', 'rd', 'pixel', 'greater', 'value', 'sorted', 'based', 'saliency']","['towards first', 'first research', 'research goal', 'goal rq', 'rq examine', 'examine visual', 'visual feature', 'feature image', 'image relating', 'relating mental', 'mental health', 'health disorder', 'disorder employ', 'employ extraction', 'extraction color', 'color profile', 'profile ie', 'ie grayscale', 'grayscale histogram', 'histogram grayscale', 'grayscale histogram', 'histogram provide', 'provide u', 'u intuition', 'intuition brightness', 'brightness saturation', 'saturation contrast', 'contrast distribution', 'distribution image', 'image histogram', 'histogram image', 'image high', 'high contrast', 'contrast pixel', 'pixel binned', 'binned bin', 'bin lower', 'lower number', 'number near', 'near whereas', 'whereas image', 'image brighter', 'brighter pixel', 'pixel binned', 'binned higher', 'higher number', 'number bin', 'bin near', 'near utilize', 'utilize opencv', 'opencv library', 'library extract', 'extract color', 'color histogram', 'histogram image', 'image dataset', 'dataset also', 'also ass', 'ass visual', 'visual saliency', 'saliency image', 'image using', 'using opencv', 'opencv distinct', 'distinct subjective', 'subjective perceptual', 'perceptual quality', 'quality make', 'make image', 'image stand', 'stand neighbor', 'neighbor typical', 'typical image', 'image dataset', 'dataset size', 'size px', 'px px', 'px using', 'using saliency', 'saliency metric', 'metric obtain', 'obtain grid', 'grid matrix', 'matrix image', 'image three', 'three visual', 'visual feature', 'feature category', 'category obtain', 'obtain empirical', 'empirical threshold', 'threshold ensures', 'ensures rd', 'rd pixel', 'pixel greater', 'greater value', 'value sorted', 'sorted based', 'based saliency']","['towards first research', 'first research goal', 'research goal rq', 'goal rq examine', 'rq examine visual', 'examine visual feature', 'visual feature image', 'feature image relating', 'image relating mental', 'relating mental health', 'mental health disorder', 'health disorder employ', 'disorder employ extraction', 'employ extraction color', 'extraction color profile', 'color profile ie', 'profile ie grayscale', 'ie grayscale histogram', 'grayscale histogram grayscale', 'histogram grayscale histogram', 'grayscale histogram provide', 'histogram provide u', 'provide u intuition', 'u intuition brightness', 'intuition brightness saturation', 'brightness saturation contrast', 'saturation contrast distribution', 'contrast distribution image', 'distribution image histogram', 'image histogram image', 'histogram image high', 'image high contrast', 'high contrast pixel', 'contrast pixel binned', 'pixel binned bin', 'binned bin lower', 'bin lower number', 'lower number near', 'number near whereas', 'near whereas image', 'whereas image brighter', 'image brighter pixel', 'brighter pixel binned', 'pixel binned higher', 'binned higher number', 'higher number bin', 'number bin near', 'bin near utilize', 'near utilize opencv', 'utilize opencv library', 'opencv library extract', 'library extract color', 'extract color histogram', 'color histogram image', 'histogram image dataset', 'image dataset also', 'dataset also ass', 'also ass visual', 'ass visual saliency', 'visual saliency image', 'saliency image using', 'image using opencv', 'using opencv distinct', 'opencv distinct subjective', 'distinct subjective perceptual', 'subjective perceptual quality', 'perceptual quality make', 'quality make image', 'make image stand', 'image stand neighbor', 'stand neighbor typical', 'neighbor typical image', 'typical image dataset', 'image dataset size', 'dataset size px', 'size px px', 'px px using', 'px using saliency', 'using saliency metric', 'saliency metric obtain', 'metric obtain grid', 'obtain grid matrix', 'grid matrix image', 'matrix image three', 'image three visual', 'three visual feature', 'visual feature category', 'feature category obtain', 'category obtain empirical', 'obtain empirical threshold', 'empirical threshold ensures', 'threshold ensures rd', 'ensures rd pixel', 'rd pixel greater', 'pixel greater value', 'greater value sorted', 'value sorted based', 'sorted based saliency']"
https://www.sciencedirect.com/science/article/pii/S0747563215300996,0,Tweets about depression were collected by Simply Measured a company that specializes in social media measurement and analytics (Simply Measured 2014). Simply Measured has access to the Twitter “firehose” (or full volume of tweets) via Gnip a licensed company that can retrieve the full Twitter data stream. All tweets in the English language that contained at least either “depressed” “#depressed” “depression” or “#depression” were collected between April 11 and May 4 2014. We scanned a random sample of the tweets to identify common phrases that included our keywords of interest but were not about mental health. In SAS version 9.3 (SAS Institute Inc. Cary NC) we used the index function which searches a character expression (in this case the text of the tweet) for a specific string of characters to locate and remove such tweets from our sample. We removed tweets that included the following terms regardless of capitalization: “Great Depression” “economic depression” “during the depression” “depression era” “tropical depression” and “depressed real estate”. The popularity and influence of the Tweeters was described using the distribution of followers and Klout Scores. While number of followers is a measure of popularity Klout Score is a measure of influence. Klout Scores range from 0 to 100 with a higher score indicating higher influence. Klout Score is calculated based on an algorithm that considers over 400 signals from eight different online networks. Examples of signals include the amount of retweets a person generates in relation to the amount of tweets shared and the amount of engagement a user drives from unique individuals (e.g. lots of retweets from different individuals as opposed to lots of retweets from one person) (Klout Inc. 2014).,tweet about depression were collected by simply measured a company that specializes in social medium measurement and analytics simply measured simply measured ha access to the twitter firehose or full volume of tweet via gnip a licensed company that can retrieve the full twitter data stream all tweet in the english language that contained at least either depressed depressed depression or depression were collected between april and may we scanned a random sample of the tweet to identify common phrase that included our keywords of interest but were not about mental health in sa version sa institute inc cary nc we used the index function which search a character expression in this case the text of the tweet for a specific string of character to locate and remove such tweet from our sample we removed tweet that included the following term regardless of capitalization great depression economic depression during the depression depression era tropical depression and depressed real estate the popularity and influence of the tweeter wa described using the distribution of follower and klout score while number of follower is a measure of popularity klout score is a measure of influence klout score range from to with a higher score indicating higher influence klout score is calculated based on an algorithm that considers over signal from eight different online network example of signal include the amount of retweets a person generates in relation to the amount of tweet shared and the amount of engagement a user drive from unique individual eg lot of retweets from different individual a opposed to lot of retweets from one person klout inc,"['tweet', 'depression', 'collected', 'simply', 'measured', 'company', 'specializes', 'social', 'medium', 'measurement', 'analytics', 'simply', 'measured', 'simply', 'measured', 'ha', 'access', 'twitter', 'firehose', 'full', 'volume', 'tweet', 'via', 'gnip', 'licensed', 'company', 'retrieve', 'full', 'twitter', 'data', 'stream', 'tweet', 'english', 'language', 'contained', 'least', 'either', 'depressed', 'depressed', 'depression', 'depression', 'collected', 'april', 'may', 'scanned', 'random', 'sample', 'tweet', 'identify', 'common', 'phrase', 'included', 'keywords', 'interest', 'mental', 'health', 'sa', 'version', 'sa', 'institute', 'inc', 'cary', 'nc', 'used', 'index', 'function', 'search', 'character', 'expression', 'case', 'text', 'tweet', 'specific', 'string', 'character', 'locate', 'remove', 'tweet', 'sample', 'removed', 'tweet', 'included', 'following', 'term', 'regardless', 'capitalization', 'great', 'depression', 'economic', 'depression', 'depression', 'depression', 'era', 'tropical', 'depression', 'depressed', 'real', 'estate', 'popularity', 'influence', 'tweeter', 'wa', 'described', 'using', 'distribution', 'follower', 'klout', 'score', 'number', 'follower', 'measure', 'popularity', 'klout', 'score', 'measure', 'influence', 'klout', 'score', 'range', 'higher', 'score', 'indicating', 'higher', 'influence', 'klout', 'score', 'calculated', 'based', 'algorithm', 'considers', 'signal', 'eight', 'different', 'online', 'network', 'example', 'signal', 'include', 'amount', 'retweets', 'person', 'generates', 'relation', 'amount', 'tweet', 'shared', 'amount', 'engagement', 'user', 'drive', 'unique', 'individual', 'eg', 'lot', 'retweets', 'different', 'individual', 'opposed', 'lot', 'retweets', 'one', 'person', 'klout', 'inc']","['tweet depression', 'depression collected', 'collected simply', 'simply measured', 'measured company', 'company specializes', 'specializes social', 'social medium', 'medium measurement', 'measurement analytics', 'analytics simply', 'simply measured', 'measured simply', 'simply measured', 'measured ha', 'ha access', 'access twitter', 'twitter firehose', 'firehose full', 'full volume', 'volume tweet', 'tweet via', 'via gnip', 'gnip licensed', 'licensed company', 'company retrieve', 'retrieve full', 'full twitter', 'twitter data', 'data stream', 'stream tweet', 'tweet english', 'english language', 'language contained', 'contained least', 'least either', 'either depressed', 'depressed depressed', 'depressed depression', 'depression depression', 'depression collected', 'collected april', 'april may', 'may scanned', 'scanned random', 'random sample', 'sample tweet', 'tweet identify', 'identify common', 'common phrase', 'phrase included', 'included keywords', 'keywords interest', 'interest mental', 'mental health', 'health sa', 'sa version', 'version sa', 'sa institute', 'institute inc', 'inc cary', 'cary nc', 'nc used', 'used index', 'index function', 'function search', 'search character', 'character expression', 'expression case', 'case text', 'text tweet', 'tweet specific', 'specific string', 'string character', 'character locate', 'locate remove', 'remove tweet', 'tweet sample', 'sample removed', 'removed tweet', 'tweet included', 'included following', 'following term', 'term regardless', 'regardless capitalization', 'capitalization great', 'great depression', 'depression economic', 'economic depression', 'depression depression', 'depression depression', 'depression era', 'era tropical', 'tropical depression', 'depression depressed', 'depressed real', 'real estate', 'estate popularity', 'popularity influence', 'influence tweeter', 'tweeter wa', 'wa described', 'described using', 'using distribution', 'distribution follower', 'follower klout', 'klout score', 'score number', 'number follower', 'follower measure', 'measure popularity', 'popularity klout', 'klout score', 'score measure', 'measure influence', 'influence klout', 'klout score', 'score range', 'range higher', 'higher score', 'score indicating', 'indicating higher', 'higher influence', 'influence klout', 'klout score', 'score calculated', 'calculated based', 'based algorithm', 'algorithm considers', 'considers signal', 'signal eight', 'eight different', 'different online', 'online network', 'network example', 'example signal', 'signal include', 'include amount', 'amount retweets', 'retweets person', 'person generates', 'generates relation', 'relation amount', 'amount tweet', 'tweet shared', 'shared amount', 'amount engagement', 'engagement user', 'user drive', 'drive unique', 'unique individual', 'individual eg', 'eg lot', 'lot retweets', 'retweets different', 'different individual', 'individual opposed', 'opposed lot', 'lot retweets', 'retweets one', 'one person', 'person klout', 'klout inc']","['tweet depression collected', 'depression collected simply', 'collected simply measured', 'simply measured company', 'measured company specializes', 'company specializes social', 'specializes social medium', 'social medium measurement', 'medium measurement analytics', 'measurement analytics simply', 'analytics simply measured', 'simply measured simply', 'measured simply measured', 'simply measured ha', 'measured ha access', 'ha access twitter', 'access twitter firehose', 'twitter firehose full', 'firehose full volume', 'full volume tweet', 'volume tweet via', 'tweet via gnip', 'via gnip licensed', 'gnip licensed company', 'licensed company retrieve', 'company retrieve full', 'retrieve full twitter', 'full twitter data', 'twitter data stream', 'data stream tweet', 'stream tweet english', 'tweet english language', 'english language contained', 'language contained least', 'contained least either', 'least either depressed', 'either depressed depressed', 'depressed depressed depression', 'depressed depression depression', 'depression depression collected', 'depression collected april', 'collected april may', 'april may scanned', 'may scanned random', 'scanned random sample', 'random sample tweet', 'sample tweet identify', 'tweet identify common', 'identify common phrase', 'common phrase included', 'phrase included keywords', 'included keywords interest', 'keywords interest mental', 'interest mental health', 'mental health sa', 'health sa version', 'sa version sa', 'version sa institute', 'sa institute inc', 'institute inc cary', 'inc cary nc', 'cary nc used', 'nc used index', 'used index function', 'index function search', 'function search character', 'search character expression', 'character expression case', 'expression case text', 'case text tweet', 'text tweet specific', 'tweet specific string', 'specific string character', 'string character locate', 'character locate remove', 'locate remove tweet', 'remove tweet sample', 'tweet sample removed', 'sample removed tweet', 'removed tweet included', 'tweet included following', 'included following term', 'following term regardless', 'term regardless capitalization', 'regardless capitalization great', 'capitalization great depression', 'great depression economic', 'depression economic depression', 'economic depression depression', 'depression depression depression', 'depression depression era', 'depression era tropical', 'era tropical depression', 'tropical depression depressed', 'depression depressed real', 'depressed real estate', 'real estate popularity', 'estate popularity influence', 'popularity influence tweeter', 'influence tweeter wa', 'tweeter wa described', 'wa described using', 'described using distribution', 'using distribution follower', 'distribution follower klout', 'follower klout score', 'klout score number', 'score number follower', 'number follower measure', 'follower measure popularity', 'measure popularity klout', 'popularity klout score', 'klout score measure', 'score measure influence', 'measure influence klout', 'influence klout score', 'klout score range', 'score range higher', 'range higher score', 'higher score indicating', 'score indicating higher', 'indicating higher influence', 'higher influence klout', 'influence klout score', 'klout score calculated', 'score calculated based', 'calculated based algorithm', 'based algorithm considers', 'algorithm considers signal', 'considers signal eight', 'signal eight different', 'eight different online', 'different online network', 'online network example', 'network example signal', 'example signal include', 'signal include amount', 'include amount retweets', 'amount retweets person', 'retweets person generates', 'person generates relation', 'generates relation amount', 'relation amount tweet', 'amount tweet shared', 'tweet shared amount', 'shared amount engagement', 'amount engagement user', 'engagement user drive', 'user drive unique', 'drive unique individual', 'unique individual eg', 'individual eg lot', 'eg lot retweets', 'lot retweets different', 'retweets different individual', 'different individual opposed', 'individual opposed lot', 'opposed lot retweets', 'lot retweets one', 'retweets one person', 'one person klout', 'person klout inc']"
https://aclanthology.org/W15-1202.pdf,0,Character n-gram language models are models built on sequences (n-grams) of characters. Here we use 5-grams: for all the tweets a user authored we count the number of times each sequence of 5 characters is observed. For example for this sentence we would observe the sequences: “for e” “or ex” “r exa” “ exam” and so on. The general approach is to examine how likely a sequence of characters is to be generated by a given type of user (schizophrenic or non-schizophrenic). To featurize character n-grams for each character 5-gram in the training data we calculate its probability in schizophrenic users and its probability in control users. At test time we search for sets of 50 sequential tweets that look “most schizophrenic” by comparing the schizophrenic and control probabilities estimated from the training data for all the 5-grams in those tweets. We experimented with different window sizes for the number of tweets and different n for n-grams; for brevity we report only the highest performing parameter settings at low false alarm rates: 5-grams and a window size of 50 tweets. An example of this can be found in Figure 1 where one schizophrenic and one control user’s score over time is plotted (top). To show the overall trend we plot the same for all users in this study (bottom) where separation between the schizophrenics (in red) and control users (in blue) is apparent. The highest score from this windowed analysis becomes the feature value. Note that this feature corresponds to only a subset of a user’s timeline. For schizophrenia sufferers this is perhaps when their symptoms were most severe a subtle but critical distinction when one considers that many of these people are receiving treatment of some sort and thus may have their symptoms change or subside over the course of our data.,character ngram language model are model built on sequence ngrams of character here we use gram for all the tweet a user authored we count the number of time each sequence of character is observed for example for this sentence we would observe the sequence for e or ex r exa exam and so on the general approach is to examine how likely a sequence of character is to be generated by a given type of user schizophrenic or nonschizophrenic to featurize character ngrams for each character gram in the training data we calculate it probability in schizophrenic user and it probability in control user at test time we search for set of sequential tweet that look most schizophrenic by comparing the schizophrenic and control probability estimated from the training data for all the gram in those tweet we experimented with different window size for the number of tweet and different n for ngrams for brevity we report only the highest performing parameter setting at low false alarm rate gram and a window size of tweet an example of this can be found in figure where one schizophrenic and one control user score over time is plotted top to show the overall trend we plot the same for all user in this study bottom where separation between the schizophrenic in red and control user in blue is apparent the highest score from this windowed analysis becomes the feature value note that this feature corresponds to only a subset of a user timeline for schizophrenia sufferer this is perhaps when their symptom were most severe a subtle but critical distinction when one considers that many of these people are receiving treatment of some sort and thus may have their symptom change or subside over the course of our data,"['character', 'ngram', 'language', 'model', 'model', 'built', 'sequence', 'ngrams', 'character', 'use', 'gram', 'tweet', 'user', 'authored', 'count', 'number', 'time', 'sequence', 'character', 'observed', 'example', 'sentence', 'would', 'observe', 'sequence', 'e', 'ex', 'r', 'exa', 'exam', 'general', 'approach', 'examine', 'likely', 'sequence', 'character', 'generated', 'given', 'type', 'user', 'schizophrenic', 'nonschizophrenic', 'featurize', 'character', 'ngrams', 'character', 'gram', 'training', 'data', 'calculate', 'probability', 'schizophrenic', 'user', 'probability', 'control', 'user', 'test', 'time', 'search', 'set', 'sequential', 'tweet', 'look', 'schizophrenic', 'comparing', 'schizophrenic', 'control', 'probability', 'estimated', 'training', 'data', 'gram', 'tweet', 'experimented', 'different', 'window', 'size', 'number', 'tweet', 'different', 'n', 'ngrams', 'brevity', 'report', 'highest', 'performing', 'parameter', 'setting', 'low', 'false', 'alarm', 'rate', 'gram', 'window', 'size', 'tweet', 'example', 'found', 'figure', 'one', 'schizophrenic', 'one', 'control', 'user', 'score', 'time', 'plotted', 'top', 'show', 'overall', 'trend', 'plot', 'user', 'study', 'bottom', 'separation', 'schizophrenic', 'red', 'control', 'user', 'blue', 'apparent', 'highest', 'score', 'windowed', 'analysis', 'becomes', 'feature', 'value', 'note', 'feature', 'corresponds', 'subset', 'user', 'timeline', 'schizophrenia', 'sufferer', 'perhaps', 'symptom', 'severe', 'subtle', 'critical', 'distinction', 'one', 'considers', 'many', 'people', 'receiving', 'treatment', 'sort', 'thus', 'may', 'symptom', 'change', 'subside', 'course', 'data']","['character ngram', 'ngram language', 'language model', 'model model', 'model built', 'built sequence', 'sequence ngrams', 'ngrams character', 'character use', 'use gram', 'gram tweet', 'tweet user', 'user authored', 'authored count', 'count number', 'number time', 'time sequence', 'sequence character', 'character observed', 'observed example', 'example sentence', 'sentence would', 'would observe', 'observe sequence', 'sequence e', 'e ex', 'ex r', 'r exa', 'exa exam', 'exam general', 'general approach', 'approach examine', 'examine likely', 'likely sequence', 'sequence character', 'character generated', 'generated given', 'given type', 'type user', 'user schizophrenic', 'schizophrenic nonschizophrenic', 'nonschizophrenic featurize', 'featurize character', 'character ngrams', 'ngrams character', 'character gram', 'gram training', 'training data', 'data calculate', 'calculate probability', 'probability schizophrenic', 'schizophrenic user', 'user probability', 'probability control', 'control user', 'user test', 'test time', 'time search', 'search set', 'set sequential', 'sequential tweet', 'tweet look', 'look schizophrenic', 'schizophrenic comparing', 'comparing schizophrenic', 'schizophrenic control', 'control probability', 'probability estimated', 'estimated training', 'training data', 'data gram', 'gram tweet', 'tweet experimented', 'experimented different', 'different window', 'window size', 'size number', 'number tweet', 'tweet different', 'different n', 'n ngrams', 'ngrams brevity', 'brevity report', 'report highest', 'highest performing', 'performing parameter', 'parameter setting', 'setting low', 'low false', 'false alarm', 'alarm rate', 'rate gram', 'gram window', 'window size', 'size tweet', 'tweet example', 'example found', 'found figure', 'figure one', 'one schizophrenic', 'schizophrenic one', 'one control', 'control user', 'user score', 'score time', 'time plotted', 'plotted top', 'top show', 'show overall', 'overall trend', 'trend plot', 'plot user', 'user study', 'study bottom', 'bottom separation', 'separation schizophrenic', 'schizophrenic red', 'red control', 'control user', 'user blue', 'blue apparent', 'apparent highest', 'highest score', 'score windowed', 'windowed analysis', 'analysis becomes', 'becomes feature', 'feature value', 'value note', 'note feature', 'feature corresponds', 'corresponds subset', 'subset user', 'user timeline', 'timeline schizophrenia', 'schizophrenia sufferer', 'sufferer perhaps', 'perhaps symptom', 'symptom severe', 'severe subtle', 'subtle critical', 'critical distinction', 'distinction one', 'one considers', 'considers many', 'many people', 'people receiving', 'receiving treatment', 'treatment sort', 'sort thus', 'thus may', 'may symptom', 'symptom change', 'change subside', 'subside course', 'course data']","['character ngram language', 'ngram language model', 'language model model', 'model model built', 'model built sequence', 'built sequence ngrams', 'sequence ngrams character', 'ngrams character use', 'character use gram', 'use gram tweet', 'gram tweet user', 'tweet user authored', 'user authored count', 'authored count number', 'count number time', 'number time sequence', 'time sequence character', 'sequence character observed', 'character observed example', 'observed example sentence', 'example sentence would', 'sentence would observe', 'would observe sequence', 'observe sequence e', 'sequence e ex', 'e ex r', 'ex r exa', 'r exa exam', 'exa exam general', 'exam general approach', 'general approach examine', 'approach examine likely', 'examine likely sequence', 'likely sequence character', 'sequence character generated', 'character generated given', 'generated given type', 'given type user', 'type user schizophrenic', 'user schizophrenic nonschizophrenic', 'schizophrenic nonschizophrenic featurize', 'nonschizophrenic featurize character', 'featurize character ngrams', 'character ngrams character', 'ngrams character gram', 'character gram training', 'gram training data', 'training data calculate', 'data calculate probability', 'calculate probability schizophrenic', 'probability schizophrenic user', 'schizophrenic user probability', 'user probability control', 'probability control user', 'control user test', 'user test time', 'test time search', 'time search set', 'search set sequential', 'set sequential tweet', 'sequential tweet look', 'tweet look schizophrenic', 'look schizophrenic comparing', 'schizophrenic comparing schizophrenic', 'comparing schizophrenic control', 'schizophrenic control probability', 'control probability estimated', 'probability estimated training', 'estimated training data', 'training data gram', 'data gram tweet', 'gram tweet experimented', 'tweet experimented different', 'experimented different window', 'different window size', 'window size number', 'size number tweet', 'number tweet different', 'tweet different n', 'different n ngrams', 'n ngrams brevity', 'ngrams brevity report', 'brevity report highest', 'report highest performing', 'highest performing parameter', 'performing parameter setting', 'parameter setting low', 'setting low false', 'low false alarm', 'false alarm rate', 'alarm rate gram', 'rate gram window', 'gram window size', 'window size tweet', 'size tweet example', 'tweet example found', 'example found figure', 'found figure one', 'figure one schizophrenic', 'one schizophrenic one', 'schizophrenic one control', 'one control user', 'control user score', 'user score time', 'score time plotted', 'time plotted top', 'plotted top show', 'top show overall', 'show overall trend', 'overall trend plot', 'trend plot user', 'plot user study', 'user study bottom', 'study bottom separation', 'bottom separation schizophrenic', 'separation schizophrenic red', 'schizophrenic red control', 'red control user', 'control user blue', 'user blue apparent', 'blue apparent highest', 'apparent highest score', 'highest score windowed', 'score windowed analysis', 'windowed analysis becomes', 'analysis becomes feature', 'becomes feature value', 'feature value note', 'value note feature', 'note feature corresponds', 'feature corresponds subset', 'corresponds subset user', 'subset user timeline', 'user timeline schizophrenia', 'timeline schizophrenia sufferer', 'schizophrenia sufferer perhaps', 'sufferer perhaps symptom', 'perhaps symptom severe', 'symptom severe subtle', 'severe subtle critical', 'subtle critical distinction', 'critical distinction one', 'distinction one considers', 'one considers many', 'considers many people', 'many people receiving', 'people receiving treatment', 'receiving treatment sort', 'treatment sort thus', 'sort thus may', 'thus may symptom', 'may symptom change', 'symptom change subside', 'change subside course', 'subside course data']"
https://dl.acm.org/doi/abs/10.1145/2700171.2791026,1,Next we compiled a list of reported celebrity suicides which fell within the time range of our Reddit data. Defining who is a “celebrity” is nontrivial so we refer to the Wikipedia page listing celebrity suicides7 as a way to measure who has sufficient celebrity status for inclusion. We obtained 10 reported celebrity suicides in the same period as our Reddit data; their names and reported suicides are shown in Table 2. We measure the prominence of a celebrity’s death by measuring the change in Wikipedia page views for the celebrity’s Wikipedia page. Wikipedia provides daily page view statistics for each page.8 We compare the number of page-views in the two weeks prior to their death with the two weeks following their death in terms of z-score (Figure 1). Here z-scores are computed by converting the page views to standard normal variable with 0-mean and standard deviation of 1. For 9/10 of the cases we see a notable spike in number of views showing that the suicides of these individuals were well-known enough to be viewable on such a macro scale and for examining the presence of Werther Effect in social media. We note two aspects related to the above analysis and which will be used through the rest of this paper. First since we are focusing on different types of data sources—Wikipedia and Reddit we use z-score conversion as a normalization technique for the Wikipedia page views and Reddit’s SW posting activity volume. Further the above observation in Wikipedia data and the analyses that ensue focus on observing changes over a two week window preceding and succeeding a celebrity suicide; this choice is motivated by our initial analyses and from the literature on Werther Effect [17].,next we compiled a list of reported celebrity suicide which fell within the time range of our reddit data defining who is a celebrity is nontrivial so we refer to the wikipedia page listing celebrity suicide a a way to measure who ha sufficient celebrity status for inclusion we obtained reported celebrity suicide in the same period a our reddit data their name and reported suicide are shown in table we measure the prominence of a celebrity death by measuring the change in wikipedia page view for the celebrity wikipedia page wikipedia provides daily page view statistic for each page we compare the number of pageviews in the two week prior to their death with the two week following their death in term of zscore figure here zscores are computed by converting the page view to standard normal variable with mean and standard deviation of for of the case we see a notable spike in number of view showing that the suicide of these individual were wellknown enough to be viewable on such a macro scale and for examining the presence of werther effect in social medium we note two aspect related to the above analysis and which will be used through the rest of this paper first since we are focusing on different type of data sourceswikipedia and reddit we use zscore conversion a a normalization technique for the wikipedia page view and reddits sw posting activity volume further the above observation in wikipedia data and the analysis that ensue focus on observing change over a two week window preceding and succeeding a celebrity suicide this choice is motivated by our initial analysis and from the literature on werther effect,"['next', 'compiled', 'list', 'reported', 'celebrity', 'suicide', 'fell', 'within', 'time', 'range', 'reddit', 'data', 'defining', 'celebrity', 'nontrivial', 'refer', 'wikipedia', 'page', 'listing', 'celebrity', 'suicide', 'way', 'measure', 'ha', 'sufficient', 'celebrity', 'status', 'inclusion', 'obtained', 'reported', 'celebrity', 'suicide', 'period', 'reddit', 'data', 'name', 'reported', 'suicide', 'shown', 'table', 'measure', 'prominence', 'celebrity', 'death', 'measuring', 'change', 'wikipedia', 'page', 'view', 'celebrity', 'wikipedia', 'page', 'wikipedia', 'provides', 'daily', 'page', 'view', 'statistic', 'page', 'compare', 'number', 'pageviews', 'two', 'week', 'prior', 'death', 'two', 'week', 'following', 'death', 'term', 'zscore', 'figure', 'zscores', 'computed', 'converting', 'page', 'view', 'standard', 'normal', 'variable', 'mean', 'standard', 'deviation', 'case', 'see', 'notable', 'spike', 'number', 'view', 'showing', 'suicide', 'individual', 'wellknown', 'enough', 'viewable', 'macro', 'scale', 'examining', 'presence', 'werther', 'effect', 'social', 'medium', 'note', 'two', 'aspect', 'related', 'analysis', 'used', 'rest', 'paper', 'first', 'since', 'focusing', 'different', 'type', 'data', 'sourceswikipedia', 'reddit', 'use', 'zscore', 'conversion', 'normalization', 'technique', 'wikipedia', 'page', 'view', 'reddits', 'sw', 'posting', 'activity', 'volume', 'observation', 'wikipedia', 'data', 'analysis', 'ensue', 'focus', 'observing', 'change', 'two', 'week', 'window', 'preceding', 'succeeding', 'celebrity', 'suicide', 'choice', 'motivated', 'initial', 'analysis', 'literature', 'werther', 'effect']","['next compiled', 'compiled list', 'list reported', 'reported celebrity', 'celebrity suicide', 'suicide fell', 'fell within', 'within time', 'time range', 'range reddit', 'reddit data', 'data defining', 'defining celebrity', 'celebrity nontrivial', 'nontrivial refer', 'refer wikipedia', 'wikipedia page', 'page listing', 'listing celebrity', 'celebrity suicide', 'suicide way', 'way measure', 'measure ha', 'ha sufficient', 'sufficient celebrity', 'celebrity status', 'status inclusion', 'inclusion obtained', 'obtained reported', 'reported celebrity', 'celebrity suicide', 'suicide period', 'period reddit', 'reddit data', 'data name', 'name reported', 'reported suicide', 'suicide shown', 'shown table', 'table measure', 'measure prominence', 'prominence celebrity', 'celebrity death', 'death measuring', 'measuring change', 'change wikipedia', 'wikipedia page', 'page view', 'view celebrity', 'celebrity wikipedia', 'wikipedia page', 'page wikipedia', 'wikipedia provides', 'provides daily', 'daily page', 'page view', 'view statistic', 'statistic page', 'page compare', 'compare number', 'number pageviews', 'pageviews two', 'two week', 'week prior', 'prior death', 'death two', 'two week', 'week following', 'following death', 'death term', 'term zscore', 'zscore figure', 'figure zscores', 'zscores computed', 'computed converting', 'converting page', 'page view', 'view standard', 'standard normal', 'normal variable', 'variable mean', 'mean standard', 'standard deviation', 'deviation case', 'case see', 'see notable', 'notable spike', 'spike number', 'number view', 'view showing', 'showing suicide', 'suicide individual', 'individual wellknown', 'wellknown enough', 'enough viewable', 'viewable macro', 'macro scale', 'scale examining', 'examining presence', 'presence werther', 'werther effect', 'effect social', 'social medium', 'medium note', 'note two', 'two aspect', 'aspect related', 'related analysis', 'analysis used', 'used rest', 'rest paper', 'paper first', 'first since', 'since focusing', 'focusing different', 'different type', 'type data', 'data sourceswikipedia', 'sourceswikipedia reddit', 'reddit use', 'use zscore', 'zscore conversion', 'conversion normalization', 'normalization technique', 'technique wikipedia', 'wikipedia page', 'page view', 'view reddits', 'reddits sw', 'sw posting', 'posting activity', 'activity volume', 'volume observation', 'observation wikipedia', 'wikipedia data', 'data analysis', 'analysis ensue', 'ensue focus', 'focus observing', 'observing change', 'change two', 'two week', 'week window', 'window preceding', 'preceding succeeding', 'succeeding celebrity', 'celebrity suicide', 'suicide choice', 'choice motivated', 'motivated initial', 'initial analysis', 'analysis literature', 'literature werther', 'werther effect']","['next compiled list', 'compiled list reported', 'list reported celebrity', 'reported celebrity suicide', 'celebrity suicide fell', 'suicide fell within', 'fell within time', 'within time range', 'time range reddit', 'range reddit data', 'reddit data defining', 'data defining celebrity', 'defining celebrity nontrivial', 'celebrity nontrivial refer', 'nontrivial refer wikipedia', 'refer wikipedia page', 'wikipedia page listing', 'page listing celebrity', 'listing celebrity suicide', 'celebrity suicide way', 'suicide way measure', 'way measure ha', 'measure ha sufficient', 'ha sufficient celebrity', 'sufficient celebrity status', 'celebrity status inclusion', 'status inclusion obtained', 'inclusion obtained reported', 'obtained reported celebrity', 'reported celebrity suicide', 'celebrity suicide period', 'suicide period reddit', 'period reddit data', 'reddit data name', 'data name reported', 'name reported suicide', 'reported suicide shown', 'suicide shown table', 'shown table measure', 'table measure prominence', 'measure prominence celebrity', 'prominence celebrity death', 'celebrity death measuring', 'death measuring change', 'measuring change wikipedia', 'change wikipedia page', 'wikipedia page view', 'page view celebrity', 'view celebrity wikipedia', 'celebrity wikipedia page', 'wikipedia page wikipedia', 'page wikipedia provides', 'wikipedia provides daily', 'provides daily page', 'daily page view', 'page view statistic', 'view statistic page', 'statistic page compare', 'page compare number', 'compare number pageviews', 'number pageviews two', 'pageviews two week', 'two week prior', 'week prior death', 'prior death two', 'death two week', 'two week following', 'week following death', 'following death term', 'death term zscore', 'term zscore figure', 'zscore figure zscores', 'figure zscores computed', 'zscores computed converting', 'computed converting page', 'converting page view', 'page view standard', 'view standard normal', 'standard normal variable', 'normal variable mean', 'variable mean standard', 'mean standard deviation', 'standard deviation case', 'deviation case see', 'case see notable', 'see notable spike', 'notable spike number', 'spike number view', 'number view showing', 'view showing suicide', 'showing suicide individual', 'suicide individual wellknown', 'individual wellknown enough', 'wellknown enough viewable', 'enough viewable macro', 'viewable macro scale', 'macro scale examining', 'scale examining presence', 'examining presence werther', 'presence werther effect', 'werther effect social', 'effect social medium', 'social medium note', 'medium note two', 'note two aspect', 'two aspect related', 'aspect related analysis', 'related analysis used', 'analysis used rest', 'used rest paper', 'rest paper first', 'paper first since', 'first since focusing', 'since focusing different', 'focusing different type', 'different type data', 'type data sourceswikipedia', 'data sourceswikipedia reddit', 'sourceswikipedia reddit use', 'reddit use zscore', 'use zscore conversion', 'zscore conversion normalization', 'conversion normalization technique', 'normalization technique wikipedia', 'technique wikipedia page', 'wikipedia page view', 'page view reddits', 'view reddits sw', 'reddits sw posting', 'sw posting activity', 'posting activity volume', 'activity volume observation', 'volume observation wikipedia', 'observation wikipedia data', 'wikipedia data analysis', 'data analysis ensue', 'analysis ensue focus', 'ensue focus observing', 'focus observing change', 'observing change two', 'change two week', 'two week window', 'week window preceding', 'window preceding succeeding', 'preceding succeeding celebrity', 'succeeding celebrity suicide', 'celebrity suicide choice', 'suicide choice motivated', 'choice motivated initial', 'motivated initial analysis', 'initial analysis literature', 'analysis literature werther', 'literature werther effect']"
https://aclanthology.org/W17-3110.pdf,1,This study aimed to examine the prevalence of affective micropatterns in social media posts and highlight differences in micropattern occurrence that might be relevant to quantifying mental health. Primarily we do this through comparison of users with anxiety disorders eating disorders schizophrenia suicide attempt history and their matched controls. We use a straightforward and well-understood method for sentiment analysis VADER (Hutto and Gilbert 2014) to produce a trinary label for each message: positive neutral or negative. VADER outputs a [0 1] score for each sentiment label; we use the label with the maximum score. Specifically we examined trajectories of posted emotional content in three subsequent tweets no more than three hours from earliest to latest. The same tweet will be counted in more than one over lapping micropattern if more than three tweets occur in the three-hour time window – so if 5 tweets occur in 3 hours 3 micropatterns will be recorded from those 5 tweets likewise for 4 tweets 2 micropatterns will be recorded. The potential overlap exists for both patients and neurotypical users and subsequent analyses (e.g. classifying users based on proportion of micropatterns) were designed to be robust to this property of overlapping micropattern generation. The number of sequential tweets to examine was chosen to minimize the complexity of the analysis while allowing significant variability to be observed. Critically we aimed for the resulting dimensions (i.e. number of distinct micropatterns) to be small enough for meaningful interpretation by clinical psychologists.,this study aimed to examine the prevalence of affective micropatterns in social medium post and highlight difference in micropattern occurrence that might be relevant to quantifying mental health primarily we do this through comparison of user with anxiety disorder eating disorder schizophrenia suicide attempt history and their matched control we use a straightforward and wellunderstood method for sentiment analysis vader hutto and gilbert to produce a trinary label for each message positive neutral or negative vader output a score for each sentiment label we use the label with the maximum score specifically we examined trajectory of posted emotional content in three subsequent tweet no more than three hour from earliest to latest the same tweet will be counted in more than one over lapping micropattern if more than three tweet occur in the threehour time window so if tweet occur in hour micropatterns will be recorded from those tweet likewise for tweet micropatterns will be recorded the potential overlap exists for both patient and neurotypical user and subsequent analysis eg classifying user based on proportion of micropatterns were designed to be robust to this property of overlapping micropattern generation the number of sequential tweet to examine wa chosen to minimize the complexity of the analysis while allowing significant variability to be observed critically we aimed for the resulting dimension ie number of distinct micropatterns to be small enough for meaningful interpretation by clinical psychologist,"['study', 'aimed', 'examine', 'prevalence', 'affective', 'micropatterns', 'social', 'medium', 'post', 'highlight', 'difference', 'micropattern', 'occurrence', 'might', 'relevant', 'quantifying', 'mental', 'health', 'primarily', 'comparison', 'user', 'anxiety', 'disorder', 'eating', 'disorder', 'schizophrenia', 'suicide', 'attempt', 'history', 'matched', 'control', 'use', 'straightforward', 'wellunderstood', 'method', 'sentiment', 'analysis', 'vader', 'hutto', 'gilbert', 'produce', 'trinary', 'label', 'message', 'positive', 'neutral', 'negative', 'vader', 'output', 'score', 'sentiment', 'label', 'use', 'label', 'maximum', 'score', 'specifically', 'examined', 'trajectory', 'posted', 'emotional', 'content', 'three', 'subsequent', 'tweet', 'three', 'hour', 'earliest', 'latest', 'tweet', 'counted', 'one', 'lapping', 'micropattern', 'three', 'tweet', 'occur', 'threehour', 'time', 'window', 'tweet', 'occur', 'hour', 'micropatterns', 'recorded', 'tweet', 'likewise', 'tweet', 'micropatterns', 'recorded', 'potential', 'overlap', 'exists', 'patient', 'neurotypical', 'user', 'subsequent', 'analysis', 'eg', 'classifying', 'user', 'based', 'proportion', 'micropatterns', 'designed', 'robust', 'property', 'overlapping', 'micropattern', 'generation', 'number', 'sequential', 'tweet', 'examine', 'wa', 'chosen', 'minimize', 'complexity', 'analysis', 'allowing', 'significant', 'variability', 'observed', 'critically', 'aimed', 'resulting', 'dimension', 'ie', 'number', 'distinct', 'micropatterns', 'small', 'enough', 'meaningful', 'interpretation', 'clinical', 'psychologist']","['study aimed', 'aimed examine', 'examine prevalence', 'prevalence affective', 'affective micropatterns', 'micropatterns social', 'social medium', 'medium post', 'post highlight', 'highlight difference', 'difference micropattern', 'micropattern occurrence', 'occurrence might', 'might relevant', 'relevant quantifying', 'quantifying mental', 'mental health', 'health primarily', 'primarily comparison', 'comparison user', 'user anxiety', 'anxiety disorder', 'disorder eating', 'eating disorder', 'disorder schizophrenia', 'schizophrenia suicide', 'suicide attempt', 'attempt history', 'history matched', 'matched control', 'control use', 'use straightforward', 'straightforward wellunderstood', 'wellunderstood method', 'method sentiment', 'sentiment analysis', 'analysis vader', 'vader hutto', 'hutto gilbert', 'gilbert produce', 'produce trinary', 'trinary label', 'label message', 'message positive', 'positive neutral', 'neutral negative', 'negative vader', 'vader output', 'output score', 'score sentiment', 'sentiment label', 'label use', 'use label', 'label maximum', 'maximum score', 'score specifically', 'specifically examined', 'examined trajectory', 'trajectory posted', 'posted emotional', 'emotional content', 'content three', 'three subsequent', 'subsequent tweet', 'tweet three', 'three hour', 'hour earliest', 'earliest latest', 'latest tweet', 'tweet counted', 'counted one', 'one lapping', 'lapping micropattern', 'micropattern three', 'three tweet', 'tweet occur', 'occur threehour', 'threehour time', 'time window', 'window tweet', 'tweet occur', 'occur hour', 'hour micropatterns', 'micropatterns recorded', 'recorded tweet', 'tweet likewise', 'likewise tweet', 'tweet micropatterns', 'micropatterns recorded', 'recorded potential', 'potential overlap', 'overlap exists', 'exists patient', 'patient neurotypical', 'neurotypical user', 'user subsequent', 'subsequent analysis', 'analysis eg', 'eg classifying', 'classifying user', 'user based', 'based proportion', 'proportion micropatterns', 'micropatterns designed', 'designed robust', 'robust property', 'property overlapping', 'overlapping micropattern', 'micropattern generation', 'generation number', 'number sequential', 'sequential tweet', 'tweet examine', 'examine wa', 'wa chosen', 'chosen minimize', 'minimize complexity', 'complexity analysis', 'analysis allowing', 'allowing significant', 'significant variability', 'variability observed', 'observed critically', 'critically aimed', 'aimed resulting', 'resulting dimension', 'dimension ie', 'ie number', 'number distinct', 'distinct micropatterns', 'micropatterns small', 'small enough', 'enough meaningful', 'meaningful interpretation', 'interpretation clinical', 'clinical psychologist']","['study aimed examine', 'aimed examine prevalence', 'examine prevalence affective', 'prevalence affective micropatterns', 'affective micropatterns social', 'micropatterns social medium', 'social medium post', 'medium post highlight', 'post highlight difference', 'highlight difference micropattern', 'difference micropattern occurrence', 'micropattern occurrence might', 'occurrence might relevant', 'might relevant quantifying', 'relevant quantifying mental', 'quantifying mental health', 'mental health primarily', 'health primarily comparison', 'primarily comparison user', 'comparison user anxiety', 'user anxiety disorder', 'anxiety disorder eating', 'disorder eating disorder', 'eating disorder schizophrenia', 'disorder schizophrenia suicide', 'schizophrenia suicide attempt', 'suicide attempt history', 'attempt history matched', 'history matched control', 'matched control use', 'control use straightforward', 'use straightforward wellunderstood', 'straightforward wellunderstood method', 'wellunderstood method sentiment', 'method sentiment analysis', 'sentiment analysis vader', 'analysis vader hutto', 'vader hutto gilbert', 'hutto gilbert produce', 'gilbert produce trinary', 'produce trinary label', 'trinary label message', 'label message positive', 'message positive neutral', 'positive neutral negative', 'neutral negative vader', 'negative vader output', 'vader output score', 'output score sentiment', 'score sentiment label', 'sentiment label use', 'label use label', 'use label maximum', 'label maximum score', 'maximum score specifically', 'score specifically examined', 'specifically examined trajectory', 'examined trajectory posted', 'trajectory posted emotional', 'posted emotional content', 'emotional content three', 'content three subsequent', 'three subsequent tweet', 'subsequent tweet three', 'tweet three hour', 'three hour earliest', 'hour earliest latest', 'earliest latest tweet', 'latest tweet counted', 'tweet counted one', 'counted one lapping', 'one lapping micropattern', 'lapping micropattern three', 'micropattern three tweet', 'three tweet occur', 'tweet occur threehour', 'occur threehour time', 'threehour time window', 'time window tweet', 'window tweet occur', 'tweet occur hour', 'occur hour micropatterns', 'hour micropatterns recorded', 'micropatterns recorded tweet', 'recorded tweet likewise', 'tweet likewise tweet', 'likewise tweet micropatterns', 'tweet micropatterns recorded', 'micropatterns recorded potential', 'recorded potential overlap', 'potential overlap exists', 'overlap exists patient', 'exists patient neurotypical', 'patient neurotypical user', 'neurotypical user subsequent', 'user subsequent analysis', 'subsequent analysis eg', 'analysis eg classifying', 'eg classifying user', 'classifying user based', 'user based proportion', 'based proportion micropatterns', 'proportion micropatterns designed', 'micropatterns designed robust', 'designed robust property', 'robust property overlapping', 'property overlapping micropattern', 'overlapping micropattern generation', 'micropattern generation number', 'generation number sequential', 'number sequential tweet', 'sequential tweet examine', 'tweet examine wa', 'examine wa chosen', 'wa chosen minimize', 'chosen minimize complexity', 'minimize complexity analysis', 'complexity analysis allowing', 'analysis allowing significant', 'allowing significant variability', 'significant variability observed', 'variability observed critically', 'observed critically aimed', 'critically aimed resulting', 'aimed resulting dimension', 'resulting dimension ie', 'dimension ie number', 'ie number distinct', 'number distinct micropatterns', 'distinct micropatterns small', 'micropatterns small enough', 'small enough meaningful', 'enough meaningful interpretation', 'meaningful interpretation clinical', 'interpretation clinical psychologist']"
https://dl.acm.org/doi/abs/10.1145/2998181.2998220,1,In this subsection we present methods to quantify the differences between the disclosure characteristics of females and males and individuals reported to be from one of the four countries of interest: US GB IN and ZA in the MID dataset. Linguistic Measures Language is a powerful source of expression [26]. A rich body of work such as Boroditsky et al. [6] showed how the perception of objects in different languages can relate to as well as impact one’s social and pscyhological status. It is recognized that language specifically one’s native language shapes and drives one’s thoughts actions and social relationships [10]. Further it is established that cross-cultural and sex differences exist in one’s underlying thought processes [21 55]. For instance according to Kovecses [ ¨ 31] cultural models are known to define one’s emotional concepts. To quantify gender and cross-cultural dimensions in the language of individuals who engage in mental health disclosure on social media we propose three categories of measures: (1) affective attributes (2) cognitive attributes and (3) linguistic style attributes. Measures belonging to all of these attribute categories are largely based on the psycholinguistic lexicon LIWC [42] and were motivated from prior literature that examines associations between the behavioral expression of individuals and their psychological distress including vulnerability to mental illness [9 14]. Specifically with LIWC we are able to study the psychological value of language in gender and culture subgroups—such as parts of speech that include pronouns articles prepositions conjunctives and auxiliary verbs [9]. (1) We consider two measures of affect derived from LIWC: positive affect (PA) and negative affect (NA) and four other measures of emotional expression: anger anxiety sadness and swear. Literature in mental health [49] identifies emotional expression to be key to characterizing one’s psychological vulnerability (2) We use LIWC to define the cognitive measures as well: (a) cognition comprising cognitive mech discrepancies inhibition negation death causation certainty and tentativeness; and (b) perception comprising set of words in LIWC around see hear feel percept insight and relative. Quantifying one’s cognition and perception as manifested linguistically can provide insights into emotional stability and cognitive complexity—these attributes are important with regard to understanding one’s mental well-being [19]. (3) Next we consider four measures of linguistic style: (a) Lexical Density: consisting of words that are verbs auxiliary verbs nouns adjectives (identified using NLTK’s [4] POS tagger) and adverbs. (b) Temporal References: consisting of past present and future tenses. (c) Social/Personal Concerns: words belonging to family friends social work health humans religion bio body money achievement home and sexual. (d) Interpersonal Awareness and Focus: words that are 1st person singular 1st person plural 2nd person and 3rd person pronouns. Together linguistic styles are known to indicate one’s underlying psychological processes (lexical density) personality (temporal references) social support and connectivity (social/personal concerns) and awareness of one’s surroundings and environment (interpersonal focus). Prior work identifies all of these cues to be valuable in understanding mental health in both offline and online contexts including social media [43]. Topic Modeling Our second method for comparing mental illness disclosures uses a topic model which have been commonly employed to analyze health data [41]. We obtain topics by running Latent Dirichlet Allocation (LDA) [5] over all posts. We pre-processed the data by removing a standard list of Twitter-specific stop words words with very high frequency (> 0.25× datasize) and words that occur fewer than five times. Thereafter we used Gensim’s implementation of online LDA from [23]. We used the default hyper-parameter settings and 100 topics which we determined based on the value of average corpus likelihood over ten runs. To measure topic differences in one cohort (e.g. IN MID users) over the other (e.g. UK MID users) we first compute the posterior probability of each topic separately for all posts in both cohorts. We then compute three comparison metrics: (1) the rate of change for each topic given as the difference between the posterior topic probabilities of the cohorts divided by the probability of the first cohort; (2) the pointwise mutual information between the posterior topic probabilities of the same cohorts; and (3) the Spearman’s rank correlation between the topic distributions for the two cohorts. Additionally we compare all gender and culture cohorts based on significance tests (e.g. Mann Whitney U test for gender and the Kruskal Wallis test for cultural differences). We also present a method to qualitatively examine the differences between the topics used by different MID user cohorts. For the purpose two researchers familiar with mental health content on social media independently inspected the words associated with each of the topics given by the above topic model. They used a semi-open coding approach to develop a codebook and extracted descriptive topical themes for the topics (Cohen’s κ=.74). During the codebook development the two annotators referred to prior literature on gender and cultural differences in mental health [46 20 52]. In the results section we will present an examination of these qualitative differences.,in this subsection we present method to quantify the difference between the disclosure characteristic of female and male and individual reported to be from one of the four country of interest u gb in and za in the mid dataset linguistic measure language is a powerful source of expression a rich body of work such a boroditsky et al showed how the perception of object in different language can relate to a well a impact one social and pscyhological status it is recognized that language specifically one native language shape and drive one thought action and social relationship further it is established that crosscultural and sex difference exist in one underlying thought process for instance according to kovecses cultural model are known to define one emotional concept to quantify gender and crosscultural dimension in the language of individual who engage in mental health disclosure on social medium we propose three category of measure affective attribute cognitive attribute and linguistic style attribute measure belonging to all of these attribute category are largely based on the psycholinguistic lexicon liwc and were motivated from prior literature that examines association between the behavioral expression of individual and their psychological distress including vulnerability to mental illness specifically with liwc we are able to study the psychological value of language in gender and culture subgroupssuch a part of speech that include pronoun article preposition conjunctive and auxiliary verb we consider two measure of affect derived from liwc positive affect pa and negative affect na and four other measure of emotional expression anger anxiety sadness and swear literature in mental health identifies emotional expression to be key to characterizing one psychological vulnerability we use liwc to define the cognitive measure a well a cognition comprising cognitive mech discrepancy inhibition negation death causation certainty and tentativeness and b perception comprising set of word in liwc around see hear feel percept insight and relative quantifying one cognition and perception a manifested linguistically can provide insight into emotional stability and cognitive complexitythese attribute are important with regard to understanding one mental wellbeing next we consider four measure of linguistic style a lexical density consisting of word that are verb auxiliary verb noun adjective identified using nltks po tagger and adverb b temporal reference consisting of past present and future tense c socialpersonal concern word belonging to family friend social work health human religion bio body money achievement home and sexual d interpersonal awareness and focus word that are st person singular st person plural nd person and rd person pronoun together linguistic style are known to indicate one underlying psychological process lexical density personality temporal reference social support and connectivity socialpersonal concern and awareness of one surroundings and environment interpersonal focus prior work identifies all of these cue to be valuable in understanding mental health in both offline and online context including social medium topic modeling our second method for comparing mental illness disclosure us a topic model which have been commonly employed to analyze health data we obtain topic by running latent dirichlet allocation lda over all post we preprocessed the data by removing a standard list of twitterspecific stop word word with very high frequency datasize and word that occur fewer than five time thereafter we used gensims implementation of online lda from we used the default hyperparameter setting and topic which we determined based on the value of average corpus likelihood over ten run to measure topic difference in one cohort eg in mid user over the other eg uk mid user we first compute the posterior probability of each topic separately for all post in both cohort we then compute three comparison metric the rate of change for each topic given a the difference between the posterior topic probability of the cohort divided by the probability of the first cohort the pointwise mutual information between the posterior topic probability of the same cohort and the spearmans rank correlation between the topic distribution for the two cohort additionally we compare all gender and culture cohort based on significance test eg mann whitney u test for gender and the kruskal wallis test for cultural difference we also present a method to qualitatively examine the difference between the topic used by different mid user cohort for the purpose two researcher familiar with mental health content on social medium independently inspected the word associated with each of the topic given by the above topic model they used a semiopen coding approach to develop a codebook and extracted descriptive topical theme for the topic cohens during the codebook development the two annotator referred to prior literature on gender and cultural difference in mental health in the result section we will present an examination of these qualitative difference,"['subsection', 'present', 'method', 'quantify', 'difference', 'disclosure', 'characteristic', 'female', 'male', 'individual', 'reported', 'one', 'four', 'country', 'interest', 'u', 'gb', 'za', 'mid', 'dataset', 'linguistic', 'measure', 'language', 'powerful', 'source', 'expression', 'rich', 'body', 'work', 'boroditsky', 'et', 'al', 'showed', 'perception', 'object', 'different', 'language', 'relate', 'well', 'impact', 'one', 'social', 'pscyhological', 'status', 'recognized', 'language', 'specifically', 'one', 'native', 'language', 'shape', 'drive', 'one', 'thought', 'action', 'social', 'relationship', 'established', 'crosscultural', 'sex', 'difference', 'exist', 'one', 'underlying', 'thought', 'process', 'instance', 'according', 'kovecses', 'cultural', 'model', 'known', 'define', 'one', 'emotional', 'concept', 'quantify', 'gender', 'crosscultural', 'dimension', 'language', 'individual', 'engage', 'mental', 'health', 'disclosure', 'social', 'medium', 'propose', 'three', 'category', 'measure', 'affective', 'attribute', 'cognitive', 'attribute', 'linguistic', 'style', 'attribute', 'measure', 'belonging', 'attribute', 'category', 'largely', 'based', 'psycholinguistic', 'lexicon', 'liwc', 'motivated', 'prior', 'literature', 'examines', 'association', 'behavioral', 'expression', 'individual', 'psychological', 'distress', 'including', 'vulnerability', 'mental', 'illness', 'specifically', 'liwc', 'able', 'study', 'psychological', 'value', 'language', 'gender', 'culture', 'subgroupssuch', 'part', 'speech', 'include', 'pronoun', 'article', 'preposition', 'conjunctive', 'auxiliary', 'verb', 'consider', 'two', 'measure', 'affect', 'derived', 'liwc', 'positive', 'affect', 'pa', 'negative', 'affect', 'na', 'four', 'measure', 'emotional', 'expression', 'anger', 'anxiety', 'sadness', 'swear', 'literature', 'mental', 'health', 'identifies', 'emotional', 'expression', 'key', 'characterizing', 'one', 'psychological', 'vulnerability', 'use', 'liwc', 'define', 'cognitive', 'measure', 'well', 'cognition', 'comprising', 'cognitive', 'mech', 'discrepancy', 'inhibition', 'negation', 'death', 'causation', 'certainty', 'tentativeness', 'b', 'perception', 'comprising', 'set', 'word', 'liwc', 'around', 'see', 'hear', 'feel', 'percept', 'insight', 'relative', 'quantifying', 'one', 'cognition', 'perception', 'manifested', 'linguistically', 'provide', 'insight', 'emotional', 'stability', 'cognitive', 'complexitythese', 'attribute', 'important', 'regard', 'understanding', 'one', 'mental', 'wellbeing', 'next', 'consider', 'four', 'measure', 'linguistic', 'style', 'lexical', 'density', 'consisting', 'word', 'verb', 'auxiliary', 'verb', 'noun', 'adjective', 'identified', 'using', 'nltks', 'po', 'tagger', 'adverb', 'b', 'temporal', 'reference', 'consisting', 'past', 'present', 'future', 'tense', 'c', 'socialpersonal', 'concern', 'word', 'belonging', 'family', 'friend', 'social', 'work', 'health', 'human', 'religion', 'bio', 'body', 'money', 'achievement', 'home', 'sexual', 'interpersonal', 'awareness', 'focus', 'word', 'st', 'person', 'singular', 'st', 'person', 'plural', 'nd', 'person', 'rd', 'person', 'pronoun', 'together', 'linguistic', 'style', 'known', 'indicate', 'one', 'underlying', 'psychological', 'process', 'lexical', 'density', 'personality', 'temporal', 'reference', 'social', 'support', 'connectivity', 'socialpersonal', 'concern', 'awareness', 'one', 'surroundings', 'environment', 'interpersonal', 'focus', 'prior', 'work', 'identifies', 'cue', 'valuable', 'understanding', 'mental', 'health', 'offline', 'online', 'context', 'including', 'social', 'medium', 'topic', 'modeling', 'second', 'method', 'comparing', 'mental', 'illness', 'disclosure', 'us', 'topic', 'model', 'commonly', 'employed', 'analyze', 'health', 'data', 'obtain', 'topic', 'running', 'latent', 'dirichlet', 'allocation', 'lda', 'post', 'preprocessed', 'data', 'removing', 'standard', 'list', 'twitterspecific', 'stop', 'word', 'word', 'high', 'frequency', 'datasize', 'word', 'occur', 'fewer', 'five', 'time', 'thereafter', 'used', 'gensims', 'implementation', 'online', 'lda', 'used', 'default', 'hyperparameter', 'setting', 'topic', 'determined', 'based', 'value', 'average', 'corpus', 'likelihood', 'ten', 'run', 'measure', 'topic', 'difference', 'one', 'cohort', 'eg', 'mid', 'user', 'eg', 'uk', 'mid', 'user', 'first', 'compute', 'posterior', 'probability', 'topic', 'separately', 'post', 'cohort', 'compute', 'three', 'comparison', 'metric', 'rate', 'change', 'topic', 'given', 'difference', 'posterior', 'topic', 'probability', 'cohort', 'divided', 'probability', 'first', 'cohort', 'pointwise', 'mutual', 'information', 'posterior', 'topic', 'probability', 'cohort', 'spearmans', 'rank', 'correlation', 'topic', 'distribution', 'two', 'cohort', 'additionally', 'compare', 'gender', 'culture', 'cohort', 'based', 'significance', 'test', 'eg', 'mann', 'whitney', 'u', 'test', 'gender', 'kruskal', 'wallis', 'test', 'cultural', 'difference', 'also', 'present', 'method', 'qualitatively', 'examine', 'difference', 'topic', 'used', 'different', 'mid', 'user', 'cohort', 'purpose', 'two', 'researcher', 'familiar', 'mental', 'health', 'content', 'social', 'medium', 'independently', 'inspected', 'word', 'associated', 'topic', 'given', 'topic', 'model', 'used', 'semiopen', 'coding', 'approach', 'develop', 'codebook', 'extracted', 'descriptive', 'topical', 'theme', 'topic', 'cohens', 'codebook', 'development', 'two', 'annotator', 'referred', 'prior', 'literature', 'gender', 'cultural', 'difference', 'mental', 'health', 'result', 'section', 'present', 'examination', 'qualitative', 'difference']","['subsection present', 'present method', 'method quantify', 'quantify difference', 'difference disclosure', 'disclosure characteristic', 'characteristic female', 'female male', 'male individual', 'individual reported', 'reported one', 'one four', 'four country', 'country interest', 'interest u', 'u gb', 'gb za', 'za mid', 'mid dataset', 'dataset linguistic', 'linguistic measure', 'measure language', 'language powerful', 'powerful source', 'source expression', 'expression rich', 'rich body', 'body work', 'work boroditsky', 'boroditsky et', 'et al', 'al showed', 'showed perception', 'perception object', 'object different', 'different language', 'language relate', 'relate well', 'well impact', 'impact one', 'one social', 'social pscyhological', 'pscyhological status', 'status recognized', 'recognized language', 'language specifically', 'specifically one', 'one native', 'native language', 'language shape', 'shape drive', 'drive one', 'one thought', 'thought action', 'action social', 'social relationship', 'relationship established', 'established crosscultural', 'crosscultural sex', 'sex difference', 'difference exist', 'exist one', 'one underlying', 'underlying thought', 'thought process', 'process instance', 'instance according', 'according kovecses', 'kovecses cultural', 'cultural model', 'model known', 'known define', 'define one', 'one emotional', 'emotional concept', 'concept quantify', 'quantify gender', 'gender crosscultural', 'crosscultural dimension', 'dimension language', 'language individual', 'individual engage', 'engage mental', 'mental health', 'health disclosure', 'disclosure social', 'social medium', 'medium propose', 'propose three', 'three category', 'category measure', 'measure affective', 'affective attribute', 'attribute cognitive', 'cognitive attribute', 'attribute linguistic', 'linguistic style', 'style attribute', 'attribute measure', 'measure belonging', 'belonging attribute', 'attribute category', 'category largely', 'largely based', 'based psycholinguistic', 'psycholinguistic lexicon', 'lexicon liwc', 'liwc motivated', 'motivated prior', 'prior literature', 'literature examines', 'examines association', 'association behavioral', 'behavioral expression', 'expression individual', 'individual psychological', 'psychological distress', 'distress including', 'including vulnerability', 'vulnerability mental', 'mental illness', 'illness specifically', 'specifically liwc', 'liwc able', 'able study', 'study psychological', 'psychological value', 'value language', 'language gender', 'gender culture', 'culture subgroupssuch', 'subgroupssuch part', 'part speech', 'speech include', 'include pronoun', 'pronoun article', 'article preposition', 'preposition conjunctive', 'conjunctive auxiliary', 'auxiliary verb', 'verb consider', 'consider two', 'two measure', 'measure affect', 'affect derived', 'derived liwc', 'liwc positive', 'positive affect', 'affect pa', 'pa negative', 'negative affect', 'affect na', 'na four', 'four measure', 'measure emotional', 'emotional expression', 'expression anger', 'anger anxiety', 'anxiety sadness', 'sadness swear', 'swear literature', 'literature mental', 'mental health', 'health identifies', 'identifies emotional', 'emotional expression', 'expression key', 'key characterizing', 'characterizing one', 'one psychological', 'psychological vulnerability', 'vulnerability use', 'use liwc', 'liwc define', 'define cognitive', 'cognitive measure', 'measure well', 'well cognition', 'cognition comprising', 'comprising cognitive', 'cognitive mech', 'mech discrepancy', 'discrepancy inhibition', 'inhibition negation', 'negation death', 'death causation', 'causation certainty', 'certainty tentativeness', 'tentativeness b', 'b perception', 'perception comprising', 'comprising set', 'set word', 'word liwc', 'liwc around', 'around see', 'see hear', 'hear feel', 'feel percept', 'percept insight', 'insight relative', 'relative quantifying', 'quantifying one', 'one cognition', 'cognition perception', 'perception manifested', 'manifested linguistically', 'linguistically provide', 'provide insight', 'insight emotional', 'emotional stability', 'stability cognitive', 'cognitive complexitythese', 'complexitythese attribute', 'attribute important', 'important regard', 'regard understanding', 'understanding one', 'one mental', 'mental wellbeing', 'wellbeing next', 'next consider', 'consider four', 'four measure', 'measure linguistic', 'linguistic style', 'style lexical', 'lexical density', 'density consisting', 'consisting word', 'word verb', 'verb auxiliary', 'auxiliary verb', 'verb noun', 'noun adjective', 'adjective identified', 'identified using', 'using nltks', 'nltks po', 'po tagger', 'tagger adverb', 'adverb b', 'b temporal', 'temporal reference', 'reference consisting', 'consisting past', 'past present', 'present future', 'future tense', 'tense c', 'c socialpersonal', 'socialpersonal concern', 'concern word', 'word belonging', 'belonging family', 'family friend', 'friend social', 'social work', 'work health', 'health human', 'human religion', 'religion bio', 'bio body', 'body money', 'money achievement', 'achievement home', 'home sexual', 'sexual interpersonal', 'interpersonal awareness', 'awareness focus', 'focus word', 'word st', 'st person', 'person singular', 'singular st', 'st person', 'person plural', 'plural nd', 'nd person', 'person rd', 'rd person', 'person pronoun', 'pronoun together', 'together linguistic', 'linguistic style', 'style known', 'known indicate', 'indicate one', 'one underlying', 'underlying psychological', 'psychological process', 'process lexical', 'lexical density', 'density personality', 'personality temporal', 'temporal reference', 'reference social', 'social support', 'support connectivity', 'connectivity socialpersonal', 'socialpersonal concern', 'concern awareness', 'awareness one', 'one surroundings', 'surroundings environment', 'environment interpersonal', 'interpersonal focus', 'focus prior', 'prior work', 'work identifies', 'identifies cue', 'cue valuable', 'valuable understanding', 'understanding mental', 'mental health', 'health offline', 'offline online', 'online context', 'context including', 'including social', 'social medium', 'medium topic', 'topic modeling', 'modeling second', 'second method', 'method comparing', 'comparing mental', 'mental illness', 'illness disclosure', 'disclosure us', 'us topic', 'topic model', 'model commonly', 'commonly employed', 'employed analyze', 'analyze health', 'health data', 'data obtain', 'obtain topic', 'topic running', 'running latent', 'latent dirichlet', 'dirichlet allocation', 'allocation lda', 'lda post', 'post preprocessed', 'preprocessed data', 'data removing', 'removing standard', 'standard list', 'list twitterspecific', 'twitterspecific stop', 'stop word', 'word word', 'word high', 'high frequency', 'frequency datasize', 'datasize word', 'word occur', 'occur fewer', 'fewer five', 'five time', 'time thereafter', 'thereafter used', 'used gensims', 'gensims implementation', 'implementation online', 'online lda', 'lda used', 'used default', 'default hyperparameter', 'hyperparameter setting', 'setting topic', 'topic determined', 'determined based', 'based value', 'value average', 'average corpus', 'corpus likelihood', 'likelihood ten', 'ten run', 'run measure', 'measure topic', 'topic difference', 'difference one', 'one cohort', 'cohort eg', 'eg mid', 'mid user', 'user eg', 'eg uk', 'uk mid', 'mid user', 'user first', 'first compute', 'compute posterior', 'posterior probability', 'probability topic', 'topic separately', 'separately post', 'post cohort', 'cohort compute', 'compute three', 'three comparison', 'comparison metric', 'metric rate', 'rate change', 'change topic', 'topic given', 'given difference', 'difference posterior', 'posterior topic', 'topic probability', 'probability cohort', 'cohort divided', 'divided probability', 'probability first', 'first cohort', 'cohort pointwise', 'pointwise mutual', 'mutual information', 'information posterior', 'posterior topic', 'topic probability', 'probability cohort', 'cohort spearmans', 'spearmans rank', 'rank correlation', 'correlation topic', 'topic distribution', 'distribution two', 'two cohort', 'cohort additionally', 'additionally compare', 'compare gender', 'gender culture', 'culture cohort', 'cohort based', 'based significance', 'significance test', 'test eg', 'eg mann', 'mann whitney', 'whitney u', 'u test', 'test gender', 'gender kruskal', 'kruskal wallis', 'wallis test', 'test cultural', 'cultural difference', 'difference also', 'also present', 'present method', 'method qualitatively', 'qualitatively examine', 'examine difference', 'difference topic', 'topic used', 'used different', 'different mid', 'mid user', 'user cohort', 'cohort purpose', 'purpose two', 'two researcher', 'researcher familiar', 'familiar mental', 'mental health', 'health content', 'content social', 'social medium', 'medium independently', 'independently inspected', 'inspected word', 'word associated', 'associated topic', 'topic given', 'given topic', 'topic model', 'model used', 'used semiopen', 'semiopen coding', 'coding approach', 'approach develop', 'develop codebook', 'codebook extracted', 'extracted descriptive', 'descriptive topical', 'topical theme', 'theme topic', 'topic cohens', 'cohens codebook', 'codebook development', 'development two', 'two annotator', 'annotator referred', 'referred prior', 'prior literature', 'literature gender', 'gender cultural', 'cultural difference', 'difference mental', 'mental health', 'health result', 'result section', 'section present', 'present examination', 'examination qualitative', 'qualitative difference']","['subsection present method', 'present method quantify', 'method quantify difference', 'quantify difference disclosure', 'difference disclosure characteristic', 'disclosure characteristic female', 'characteristic female male', 'female male individual', 'male individual reported', 'individual reported one', 'reported one four', 'one four country', 'four country interest', 'country interest u', 'interest u gb', 'u gb za', 'gb za mid', 'za mid dataset', 'mid dataset linguistic', 'dataset linguistic measure', 'linguistic measure language', 'measure language powerful', 'language powerful source', 'powerful source expression', 'source expression rich', 'expression rich body', 'rich body work', 'body work boroditsky', 'work boroditsky et', 'boroditsky et al', 'et al showed', 'al showed perception', 'showed perception object', 'perception object different', 'object different language', 'different language relate', 'language relate well', 'relate well impact', 'well impact one', 'impact one social', 'one social pscyhological', 'social pscyhological status', 'pscyhological status recognized', 'status recognized language', 'recognized language specifically', 'language specifically one', 'specifically one native', 'one native language', 'native language shape', 'language shape drive', 'shape drive one', 'drive one thought', 'one thought action', 'thought action social', 'action social relationship', 'social relationship established', 'relationship established crosscultural', 'established crosscultural sex', 'crosscultural sex difference', 'sex difference exist', 'difference exist one', 'exist one underlying', 'one underlying thought', 'underlying thought process', 'thought process instance', 'process instance according', 'instance according kovecses', 'according kovecses cultural', 'kovecses cultural model', 'cultural model known', 'model known define', 'known define one', 'define one emotional', 'one emotional concept', 'emotional concept quantify', 'concept quantify gender', 'quantify gender crosscultural', 'gender crosscultural dimension', 'crosscultural dimension language', 'dimension language individual', 'language individual engage', 'individual engage mental', 'engage mental health', 'mental health disclosure', 'health disclosure social', 'disclosure social medium', 'social medium propose', 'medium propose three', 'propose three category', 'three category measure', 'category measure affective', 'measure affective attribute', 'affective attribute cognitive', 'attribute cognitive attribute', 'cognitive attribute linguistic', 'attribute linguistic style', 'linguistic style attribute', 'style attribute measure', 'attribute measure belonging', 'measure belonging attribute', 'belonging attribute category', 'attribute category largely', 'category largely based', 'largely based psycholinguistic', 'based psycholinguistic lexicon', 'psycholinguistic lexicon liwc', 'lexicon liwc motivated', 'liwc motivated prior', 'motivated prior literature', 'prior literature examines', 'literature examines association', 'examines association behavioral', 'association behavioral expression', 'behavioral expression individual', 'expression individual psychological', 'individual psychological distress', 'psychological distress including', 'distress including vulnerability', 'including vulnerability mental', 'vulnerability mental illness', 'mental illness specifically', 'illness specifically liwc', 'specifically liwc able', 'liwc able study', 'able study psychological', 'study psychological value', 'psychological value language', 'value language gender', 'language gender culture', 'gender culture subgroupssuch', 'culture subgroupssuch part', 'subgroupssuch part speech', 'part speech include', 'speech include pronoun', 'include pronoun article', 'pronoun article preposition', 'article preposition conjunctive', 'preposition conjunctive auxiliary', 'conjunctive auxiliary verb', 'auxiliary verb consider', 'verb consider two', 'consider two measure', 'two measure affect', 'measure affect derived', 'affect derived liwc', 'derived liwc positive', 'liwc positive affect', 'positive affect pa', 'affect pa negative', 'pa negative affect', 'negative affect na', 'affect na four', 'na four measure', 'four measure emotional', 'measure emotional expression', 'emotional expression anger', 'expression anger anxiety', 'anger anxiety sadness', 'anxiety sadness swear', 'sadness swear literature', 'swear literature mental', 'literature mental health', 'mental health identifies', 'health identifies emotional', 'identifies emotional expression', 'emotional expression key', 'expression key characterizing', 'key characterizing one', 'characterizing one psychological', 'one psychological vulnerability', 'psychological vulnerability use', 'vulnerability use liwc', 'use liwc define', 'liwc define cognitive', 'define cognitive measure', 'cognitive measure well', 'measure well cognition', 'well cognition comprising', 'cognition comprising cognitive', 'comprising cognitive mech', 'cognitive mech discrepancy', 'mech discrepancy inhibition', 'discrepancy inhibition negation', 'inhibition negation death', 'negation death causation', 'death causation certainty', 'causation certainty tentativeness', 'certainty tentativeness b', 'tentativeness b perception', 'b perception comprising', 'perception comprising set', 'comprising set word', 'set word liwc', 'word liwc around', 'liwc around see', 'around see hear', 'see hear feel', 'hear feel percept', 'feel percept insight', 'percept insight relative', 'insight relative quantifying', 'relative quantifying one', 'quantifying one cognition', 'one cognition perception', 'cognition perception manifested', 'perception manifested linguistically', 'manifested linguistically provide', 'linguistically provide insight', 'provide insight emotional', 'insight emotional stability', 'emotional stability cognitive', 'stability cognitive complexitythese', 'cognitive complexitythese attribute', 'complexitythese attribute important', 'attribute important regard', 'important regard understanding', 'regard understanding one', 'understanding one mental', 'one mental wellbeing', 'mental wellbeing next', 'wellbeing next consider', 'next consider four', 'consider four measure', 'four measure linguistic', 'measure linguistic style', 'linguistic style lexical', 'style lexical density', 'lexical density consisting', 'density consisting word', 'consisting word verb', 'word verb auxiliary', 'verb auxiliary verb', 'auxiliary verb noun', 'verb noun adjective', 'noun adjective identified', 'adjective identified using', 'identified using nltks', 'using nltks po', 'nltks po tagger', 'po tagger adverb', 'tagger adverb b', 'adverb b temporal', 'b temporal reference', 'temporal reference consisting', 'reference consisting past', 'consisting past present', 'past present future', 'present future tense', 'future tense c', 'tense c socialpersonal', 'c socialpersonal concern', 'socialpersonal concern word', 'concern word belonging', 'word belonging family', 'belonging family friend', 'family friend social', 'friend social work', 'social work health', 'work health human', 'health human religion', 'human religion bio', 'religion bio body', 'bio body money', 'body money achievement', 'money achievement home', 'achievement home sexual', 'home sexual interpersonal', 'sexual interpersonal awareness', 'interpersonal awareness focus', 'awareness focus word', 'focus word st', 'word st person', 'st person singular', 'person singular st', 'singular st person', 'st person plural', 'person plural nd', 'plural nd person', 'nd person rd', 'person rd person', 'rd person pronoun', 'person pronoun together', 'pronoun together linguistic', 'together linguistic style', 'linguistic style known', 'style known indicate', 'known indicate one', 'indicate one underlying', 'one underlying psychological', 'underlying psychological process', 'psychological process lexical', 'process lexical density', 'lexical density personality', 'density personality temporal', 'personality temporal reference', 'temporal reference social', 'reference social support', 'social support connectivity', 'support connectivity socialpersonal', 'connectivity socialpersonal concern', 'socialpersonal concern awareness', 'concern awareness one', 'awareness one surroundings', 'one surroundings environment', 'surroundings environment interpersonal', 'environment interpersonal focus', 'interpersonal focus prior', 'focus prior work', 'prior work identifies', 'work identifies cue', 'identifies cue valuable', 'cue valuable understanding', 'valuable understanding mental', 'understanding mental health', 'mental health offline', 'health offline online', 'offline online context', 'online context including', 'context including social', 'including social medium', 'social medium topic', 'medium topic modeling', 'topic modeling second', 'modeling second method', 'second method comparing', 'method comparing mental', 'comparing mental illness', 'mental illness disclosure', 'illness disclosure us', 'disclosure us topic', 'us topic model', 'topic model commonly', 'model commonly employed', 'commonly employed analyze', 'employed analyze health', 'analyze health data', 'health data obtain', 'data obtain topic', 'obtain topic running', 'topic running latent', 'running latent dirichlet', 'latent dirichlet allocation', 'dirichlet allocation lda', 'allocation lda post', 'lda post preprocessed', 'post preprocessed data', 'preprocessed data removing', 'data removing standard', 'removing standard list', 'standard list twitterspecific', 'list twitterspecific stop', 'twitterspecific stop word', 'stop word word', 'word word high', 'word high frequency', 'high frequency datasize', 'frequency datasize word', 'datasize word occur', 'word occur fewer', 'occur fewer five', 'fewer five time', 'five time thereafter', 'time thereafter used', 'thereafter used gensims', 'used gensims implementation', 'gensims implementation online', 'implementation online lda', 'online lda used', 'lda used default', 'used default hyperparameter', 'default hyperparameter setting', 'hyperparameter setting topic', 'setting topic determined', 'topic determined based', 'determined based value', 'based value average', 'value average corpus', 'average corpus likelihood', 'corpus likelihood ten', 'likelihood ten run', 'ten run measure', 'run measure topic', 'measure topic difference', 'topic difference one', 'difference one cohort', 'one cohort eg', 'cohort eg mid', 'eg mid user', 'mid user eg', 'user eg uk', 'eg uk mid', 'uk mid user', 'mid user first', 'user first compute', 'first compute posterior', 'compute posterior probability', 'posterior probability topic', 'probability topic separately', 'topic separately post', 'separately post cohort', 'post cohort compute', 'cohort compute three', 'compute three comparison', 'three comparison metric', 'comparison metric rate', 'metric rate change', 'rate change topic', 'change topic given', 'topic given difference', 'given difference posterior', 'difference posterior topic', 'posterior topic probability', 'topic probability cohort', 'probability cohort divided', 'cohort divided probability', 'divided probability first', 'probability first cohort', 'first cohort pointwise', 'cohort pointwise mutual', 'pointwise mutual information', 'mutual information posterior', 'information posterior topic', 'posterior topic probability', 'topic probability cohort', 'probability cohort spearmans', 'cohort spearmans rank', 'spearmans rank correlation', 'rank correlation topic', 'correlation topic distribution', 'topic distribution two', 'distribution two cohort', 'two cohort additionally', 'cohort additionally compare', 'additionally compare gender', 'compare gender culture', 'gender culture cohort', 'culture cohort based', 'cohort based significance', 'based significance test', 'significance test eg', 'test eg mann', 'eg mann whitney', 'mann whitney u', 'whitney u test', 'u test gender', 'test gender kruskal', 'gender kruskal wallis', 'kruskal wallis test', 'wallis test cultural', 'test cultural difference', 'cultural difference also', 'difference also present', 'also present method', 'present method qualitatively', 'method qualitatively examine', 'qualitatively examine difference', 'examine difference topic', 'difference topic used', 'topic used different', 'used different mid', 'different mid user', 'mid user cohort', 'user cohort purpose', 'cohort purpose two', 'purpose two researcher', 'two researcher familiar', 'researcher familiar mental', 'familiar mental health', 'mental health content', 'health content social', 'content social medium', 'social medium independently', 'medium independently inspected', 'independently inspected word', 'inspected word associated', 'word associated topic', 'associated topic given', 'topic given topic', 'given topic model', 'topic model used', 'model used semiopen', 'used semiopen coding', 'semiopen coding approach', 'coding approach develop', 'approach develop codebook', 'develop codebook extracted', 'codebook extracted descriptive', 'extracted descriptive topical', 'descriptive topical theme', 'topical theme topic', 'theme topic cohens', 'topic cohens codebook', 'cohens codebook development', 'codebook development two', 'development two annotator', 'two annotator referred', 'annotator referred prior', 'referred prior literature', 'prior literature gender', 'literature gender cultural', 'gender cultural difference', 'cultural difference mental', 'difference mental health', 'mental health result', 'health result section', 'result section present', 'section present examination', 'present examination qualitative', 'examination qualitative difference']"
https://www.jmir.org/2019/6/e14199/,0,The selection of the tweets and their users was based on the filtered real-time streaming support provided by the Twitter API. In the first step we selected the users who showed potential signs of depression on Twitter on the basis of the 20 most frequent words in Spanish expressed by patients suffering from depression in clinical settings. These words were jointly identified and selected by a psychologist and a family physician with clinical experience and were based on the definition and general features of depression according to the Diagnostic and Statistical Manual of Mental Disorders [42]. The list of words used and their English translations are shown in Textbox 1. During June 2018 1470000 tweets including 1 or more occurrences of the words listed in Textbox 1 were collected. From this collection of tweets and to select the users who publicly stated in the textual description associated to their profile that they suffered from depression all the profile descriptions including 1 or more occurrences of the word “depr” and all the possible derivations related to the word depression in Spanish such as “depre” “depresión” “depresivo” “depresiva” “deprimido” and “deprimida” were considered. From the 720 users who included 1 or more of these words in their description profile 90 users who stated they suffered from depression or were receiving treatment for depression were selected for the analysis. This selection was performed by a psychologist verifying that the statements were related to real expressions of depression excluding quotes jokes or fake ones. For each of these depressed Twitter users we collected all the most recent tweets from their timeline up to a maximum of about 3200 tweets. Thus a total of 189669 tweets were collected a figure that was reduced to 140946 after discarding the retweets. These 140946 tweets constituted the depressive users dataset. Examples of sentences appearing in the user profiles that were used for selecting the depressive users are: “Paciente psiquiátrico con depresión crónica” (Psychiatric patient with chronic depression; example of a profile sentence that indicates depression). “Colecciono errores traducidos a tweets depresivos y a uno que otro impulso de amor” (I gather errors translated into depressing tweets and into one or another love impulse; example of a profile sentence that does not indicate depression). Once the users with profile sentences indicating depression had been retrieved their Twitter timelines were collected. Only those users having in their timeline at least 10 tweets that suggested signs of depression were retained for further analyses. For each user the selection of these tweets was performed by manually inspecting the tweets of the user’s complete timeline in reverse temporal order starting from the most recent one to the oldest tweet of the timeline retrieved by means of the Twitter API . Finally a total number of 1000 tweets issued by the 90 depressive users suggesting signs of depression were detected and used for the analysis. This set of tweets provided us with the depressive tweets dataset which was used to analyze linguistic features of tweets showing signs of depression. It has to be mentioned that these 1000 tweets were not to be included in the depressive users dataset (see Figure 1). At the same time more than 97500000 tweets were also collected in June 2018: such tweets were gathered by listening to the public Twitter stream during this time span by only considering tweets with Spanish textual contents (as detected by Twitter language identification support). Given that Twitter requires more restrictive filters than just the language of the tweets we used a list of the most frequently used Spanish words (stopwords) to retrieve all tweets that included 1 or more of these words. The vast majority of Spanish tweets should match this criterion. A sample of 450 users who did not mention in their profile the word depression and its derivations were selected randomly from the 97500000 tweets. The complete timelines of these users were compiled (1141021 tweets) which were reduced to 712589 once retweets were removed. These 712589 tweets constituted the control dataset. To identify the language of a tweet we relied on the language automatically identified by Twitter for each tweet selecting tweets in Spanish. It has to be noted that these data can contain some tweets from unidentified depressive users.,the selection of the tweet and their user wa based on the filtered realtime streaming support provided by the twitter api in the first step we selected the user who showed potential sign of depression on twitter on the basis of the most frequent word in spanish expressed by patient suffering from depression in clinical setting these word were jointly identified and selected by a psychologist and a family physician with clinical experience and were based on the definition and general feature of depression according to the diagnostic and statistical manual of mental disorder the list of word used and their english translation are shown in textbox during june tweet including or more occurrence of the word listed in textbox were collected from this collection of tweet and to select the user who publicly stated in the textual description associated to their profile that they suffered from depression all the profile description including or more occurrence of the word depr and all the possible derivation related to the word depression in spanish such a depre depresin depresivo depresiva deprimido and deprimida were considered from the user who included or more of these word in their description profile user who stated they suffered from depression or were receiving treatment for depression were selected for the analysis this selection wa performed by a psychologist verifying that the statement were related to real expression of depression excluding quote joke or fake one for each of these depressed twitter user we collected all the most recent tweet from their timeline up to a maximum of about tweet thus a total of tweet were collected a figure that wa reduced to after discarding the retweets these tweet constituted the depressive user dataset example of sentence appearing in the user profile that were used for selecting the depressive user are paciente psiquitrico con depresin crnica psychiatric patient with chronic depression example of a profile sentence that indicates depression colecciono errores traducidos a tweet depresivos y a uno que otro impulso de amor i gather error translated into depressing tweet and into one or another love impulse example of a profile sentence that doe not indicate depression once the user with profile sentence indicating depression had been retrieved their twitter timeline were collected only those user having in their timeline at least tweet that suggested sign of depression were retained for further analysis for each user the selection of these tweet wa performed by manually inspecting the tweet of the user complete timeline in reverse temporal order starting from the most recent one to the oldest tweet of the timeline retrieved by mean of the twitter api finally a total number of tweet issued by the depressive user suggesting sign of depression were detected and used for the analysis this set of tweet provided u with the depressive tweet dataset which wa used to analyze linguistic feature of tweet showing sign of depression it ha to be mentioned that these tweet were not to be included in the depressive user dataset see figure at the same time more than tweet were also collected in june such tweet were gathered by listening to the public twitter stream during this time span by only considering tweet with spanish textual content a detected by twitter language identification support given that twitter requires more restrictive filter than just the language of the tweet we used a list of the most frequently used spanish word stopwords to retrieve all tweet that included or more of these word the vast majority of spanish tweet should match this criterion a sample of user who did not mention in their profile the word depression and it derivation were selected randomly from the tweet the complete timeline of these user were compiled tweet which were reduced to once retweets were removed these tweet constituted the control dataset to identify the language of a tweet we relied on the language automatically identified by twitter for each tweet selecting tweet in spanish it ha to be noted that these data can contain some tweet from unidentified depressive user,"['selection', 'tweet', 'user', 'wa', 'based', 'filtered', 'realtime', 'streaming', 'support', 'provided', 'twitter', 'api', 'first', 'step', 'selected', 'user', 'showed', 'potential', 'sign', 'depression', 'twitter', 'basis', 'frequent', 'word', 'spanish', 'expressed', 'patient', 'suffering', 'depression', 'clinical', 'setting', 'word', 'jointly', 'identified', 'selected', 'psychologist', 'family', 'physician', 'clinical', 'experience', 'based', 'definition', 'general', 'feature', 'depression', 'according', 'diagnostic', 'statistical', 'manual', 'mental', 'disorder', 'list', 'word', 'used', 'english', 'translation', 'shown', 'textbox', 'june', 'tweet', 'including', 'occurrence', 'word', 'listed', 'textbox', 'collected', 'collection', 'tweet', 'select', 'user', 'publicly', 'stated', 'textual', 'description', 'associated', 'profile', 'suffered', 'depression', 'profile', 'description', 'including', 'occurrence', 'word', 'depr', 'possible', 'derivation', 'related', 'word', 'depression', 'spanish', 'depre', 'depresin', 'depresivo', 'depresiva', 'deprimido', 'deprimida', 'considered', 'user', 'included', 'word', 'description', 'profile', 'user', 'stated', 'suffered', 'depression', 'receiving', 'treatment', 'depression', 'selected', 'analysis', 'selection', 'wa', 'performed', 'psychologist', 'verifying', 'statement', 'related', 'real', 'expression', 'depression', 'excluding', 'quote', 'joke', 'fake', 'one', 'depressed', 'twitter', 'user', 'collected', 'recent', 'tweet', 'timeline', 'maximum', 'tweet', 'thus', 'total', 'tweet', 'collected', 'figure', 'wa', 'reduced', 'discarding', 'retweets', 'tweet', 'constituted', 'depressive', 'user', 'dataset', 'example', 'sentence', 'appearing', 'user', 'profile', 'used', 'selecting', 'depressive', 'user', 'paciente', 'psiquitrico', 'con', 'depresin', 'crnica', 'psychiatric', 'patient', 'chronic', 'depression', 'example', 'profile', 'sentence', 'indicates', 'depression', 'colecciono', 'errores', 'traducidos', 'tweet', 'depresivos', 'uno', 'que', 'otro', 'impulso', 'de', 'amor', 'gather', 'error', 'translated', 'depressing', 'tweet', 'one', 'another', 'love', 'impulse', 'example', 'profile', 'sentence', 'doe', 'indicate', 'depression', 'user', 'profile', 'sentence', 'indicating', 'depression', 'retrieved', 'twitter', 'timeline', 'collected', 'user', 'timeline', 'least', 'tweet', 'suggested', 'sign', 'depression', 'retained', 'analysis', 'user', 'selection', 'tweet', 'wa', 'performed', 'manually', 'inspecting', 'tweet', 'user', 'complete', 'timeline', 'reverse', 'temporal', 'order', 'starting', 'recent', 'one', 'oldest', 'tweet', 'timeline', 'retrieved', 'mean', 'twitter', 'api', 'finally', 'total', 'number', 'tweet', 'issued', 'depressive', 'user', 'suggesting', 'sign', 'depression', 'detected', 'used', 'analysis', 'set', 'tweet', 'provided', 'u', 'depressive', 'tweet', 'dataset', 'wa', 'used', 'analyze', 'linguistic', 'feature', 'tweet', 'showing', 'sign', 'depression', 'ha', 'mentioned', 'tweet', 'included', 'depressive', 'user', 'dataset', 'see', 'figure', 'time', 'tweet', 'also', 'collected', 'june', 'tweet', 'gathered', 'listening', 'public', 'twitter', 'stream', 'time', 'span', 'considering', 'tweet', 'spanish', 'textual', 'content', 'detected', 'twitter', 'language', 'identification', 'support', 'given', 'twitter', 'requires', 'restrictive', 'filter', 'language', 'tweet', 'used', 'list', 'frequently', 'used', 'spanish', 'word', 'stopwords', 'retrieve', 'tweet', 'included', 'word', 'vast', 'majority', 'spanish', 'tweet', 'match', 'criterion', 'sample', 'user', 'mention', 'profile', 'word', 'depression', 'derivation', 'selected', 'randomly', 'tweet', 'complete', 'timeline', 'user', 'compiled', 'tweet', 'reduced', 'retweets', 'removed', 'tweet', 'constituted', 'control', 'dataset', 'identify', 'language', 'tweet', 'relied', 'language', 'automatically', 'identified', 'twitter', 'tweet', 'selecting', 'tweet', 'spanish', 'ha', 'noted', 'data', 'contain', 'tweet', 'unidentified', 'depressive', 'user']","['selection tweet', 'tweet user', 'user wa', 'wa based', 'based filtered', 'filtered realtime', 'realtime streaming', 'streaming support', 'support provided', 'provided twitter', 'twitter api', 'api first', 'first step', 'step selected', 'selected user', 'user showed', 'showed potential', 'potential sign', 'sign depression', 'depression twitter', 'twitter basis', 'basis frequent', 'frequent word', 'word spanish', 'spanish expressed', 'expressed patient', 'patient suffering', 'suffering depression', 'depression clinical', 'clinical setting', 'setting word', 'word jointly', 'jointly identified', 'identified selected', 'selected psychologist', 'psychologist family', 'family physician', 'physician clinical', 'clinical experience', 'experience based', 'based definition', 'definition general', 'general feature', 'feature depression', 'depression according', 'according diagnostic', 'diagnostic statistical', 'statistical manual', 'manual mental', 'mental disorder', 'disorder list', 'list word', 'word used', 'used english', 'english translation', 'translation shown', 'shown textbox', 'textbox june', 'june tweet', 'tweet including', 'including occurrence', 'occurrence word', 'word listed', 'listed textbox', 'textbox collected', 'collected collection', 'collection tweet', 'tweet select', 'select user', 'user publicly', 'publicly stated', 'stated textual', 'textual description', 'description associated', 'associated profile', 'profile suffered', 'suffered depression', 'depression profile', 'profile description', 'description including', 'including occurrence', 'occurrence word', 'word depr', 'depr possible', 'possible derivation', 'derivation related', 'related word', 'word depression', 'depression spanish', 'spanish depre', 'depre depresin', 'depresin depresivo', 'depresivo depresiva', 'depresiva deprimido', 'deprimido deprimida', 'deprimida considered', 'considered user', 'user included', 'included word', 'word description', 'description profile', 'profile user', 'user stated', 'stated suffered', 'suffered depression', 'depression receiving', 'receiving treatment', 'treatment depression', 'depression selected', 'selected analysis', 'analysis selection', 'selection wa', 'wa performed', 'performed psychologist', 'psychologist verifying', 'verifying statement', 'statement related', 'related real', 'real expression', 'expression depression', 'depression excluding', 'excluding quote', 'quote joke', 'joke fake', 'fake one', 'one depressed', 'depressed twitter', 'twitter user', 'user collected', 'collected recent', 'recent tweet', 'tweet timeline', 'timeline maximum', 'maximum tweet', 'tweet thus', 'thus total', 'total tweet', 'tweet collected', 'collected figure', 'figure wa', 'wa reduced', 'reduced discarding', 'discarding retweets', 'retweets tweet', 'tweet constituted', 'constituted depressive', 'depressive user', 'user dataset', 'dataset example', 'example sentence', 'sentence appearing', 'appearing user', 'user profile', 'profile used', 'used selecting', 'selecting depressive', 'depressive user', 'user paciente', 'paciente psiquitrico', 'psiquitrico con', 'con depresin', 'depresin crnica', 'crnica psychiatric', 'psychiatric patient', 'patient chronic', 'chronic depression', 'depression example', 'example profile', 'profile sentence', 'sentence indicates', 'indicates depression', 'depression colecciono', 'colecciono errores', 'errores traducidos', 'traducidos tweet', 'tweet depresivos', 'depresivos uno', 'uno que', 'que otro', 'otro impulso', 'impulso de', 'de amor', 'amor gather', 'gather error', 'error translated', 'translated depressing', 'depressing tweet', 'tweet one', 'one another', 'another love', 'love impulse', 'impulse example', 'example profile', 'profile sentence', 'sentence doe', 'doe indicate', 'indicate depression', 'depression user', 'user profile', 'profile sentence', 'sentence indicating', 'indicating depression', 'depression retrieved', 'retrieved twitter', 'twitter timeline', 'timeline collected', 'collected user', 'user timeline', 'timeline least', 'least tweet', 'tweet suggested', 'suggested sign', 'sign depression', 'depression retained', 'retained analysis', 'analysis user', 'user selection', 'selection tweet', 'tweet wa', 'wa performed', 'performed manually', 'manually inspecting', 'inspecting tweet', 'tweet user', 'user complete', 'complete timeline', 'timeline reverse', 'reverse temporal', 'temporal order', 'order starting', 'starting recent', 'recent one', 'one oldest', 'oldest tweet', 'tweet timeline', 'timeline retrieved', 'retrieved mean', 'mean twitter', 'twitter api', 'api finally', 'finally total', 'total number', 'number tweet', 'tweet issued', 'issued depressive', 'depressive user', 'user suggesting', 'suggesting sign', 'sign depression', 'depression detected', 'detected used', 'used analysis', 'analysis set', 'set tweet', 'tweet provided', 'provided u', 'u depressive', 'depressive tweet', 'tweet dataset', 'dataset wa', 'wa used', 'used analyze', 'analyze linguistic', 'linguistic feature', 'feature tweet', 'tweet showing', 'showing sign', 'sign depression', 'depression ha', 'ha mentioned', 'mentioned tweet', 'tweet included', 'included depressive', 'depressive user', 'user dataset', 'dataset see', 'see figure', 'figure time', 'time tweet', 'tweet also', 'also collected', 'collected june', 'june tweet', 'tweet gathered', 'gathered listening', 'listening public', 'public twitter', 'twitter stream', 'stream time', 'time span', 'span considering', 'considering tweet', 'tweet spanish', 'spanish textual', 'textual content', 'content detected', 'detected twitter', 'twitter language', 'language identification', 'identification support', 'support given', 'given twitter', 'twitter requires', 'requires restrictive', 'restrictive filter', 'filter language', 'language tweet', 'tweet used', 'used list', 'list frequently', 'frequently used', 'used spanish', 'spanish word', 'word stopwords', 'stopwords retrieve', 'retrieve tweet', 'tweet included', 'included word', 'word vast', 'vast majority', 'majority spanish', 'spanish tweet', 'tweet match', 'match criterion', 'criterion sample', 'sample user', 'user mention', 'mention profile', 'profile word', 'word depression', 'depression derivation', 'derivation selected', 'selected randomly', 'randomly tweet', 'tweet complete', 'complete timeline', 'timeline user', 'user compiled', 'compiled tweet', 'tweet reduced', 'reduced retweets', 'retweets removed', 'removed tweet', 'tweet constituted', 'constituted control', 'control dataset', 'dataset identify', 'identify language', 'language tweet', 'tweet relied', 'relied language', 'language automatically', 'automatically identified', 'identified twitter', 'twitter tweet', 'tweet selecting', 'selecting tweet', 'tweet spanish', 'spanish ha', 'ha noted', 'noted data', 'data contain', 'contain tweet', 'tweet unidentified', 'unidentified depressive', 'depressive user']","['selection tweet user', 'tweet user wa', 'user wa based', 'wa based filtered', 'based filtered realtime', 'filtered realtime streaming', 'realtime streaming support', 'streaming support provided', 'support provided twitter', 'provided twitter api', 'twitter api first', 'api first step', 'first step selected', 'step selected user', 'selected user showed', 'user showed potential', 'showed potential sign', 'potential sign depression', 'sign depression twitter', 'depression twitter basis', 'twitter basis frequent', 'basis frequent word', 'frequent word spanish', 'word spanish expressed', 'spanish expressed patient', 'expressed patient suffering', 'patient suffering depression', 'suffering depression clinical', 'depression clinical setting', 'clinical setting word', 'setting word jointly', 'word jointly identified', 'jointly identified selected', 'identified selected psychologist', 'selected psychologist family', 'psychologist family physician', 'family physician clinical', 'physician clinical experience', 'clinical experience based', 'experience based definition', 'based definition general', 'definition general feature', 'general feature depression', 'feature depression according', 'depression according diagnostic', 'according diagnostic statistical', 'diagnostic statistical manual', 'statistical manual mental', 'manual mental disorder', 'mental disorder list', 'disorder list word', 'list word used', 'word used english', 'used english translation', 'english translation shown', 'translation shown textbox', 'shown textbox june', 'textbox june tweet', 'june tweet including', 'tweet including occurrence', 'including occurrence word', 'occurrence word listed', 'word listed textbox', 'listed textbox collected', 'textbox collected collection', 'collected collection tweet', 'collection tweet select', 'tweet select user', 'select user publicly', 'user publicly stated', 'publicly stated textual', 'stated textual description', 'textual description associated', 'description associated profile', 'associated profile suffered', 'profile suffered depression', 'suffered depression profile', 'depression profile description', 'profile description including', 'description including occurrence', 'including occurrence word', 'occurrence word depr', 'word depr possible', 'depr possible derivation', 'possible derivation related', 'derivation related word', 'related word depression', 'word depression spanish', 'depression spanish depre', 'spanish depre depresin', 'depre depresin depresivo', 'depresin depresivo depresiva', 'depresivo depresiva deprimido', 'depresiva deprimido deprimida', 'deprimido deprimida considered', 'deprimida considered user', 'considered user included', 'user included word', 'included word description', 'word description profile', 'description profile user', 'profile user stated', 'user stated suffered', 'stated suffered depression', 'suffered depression receiving', 'depression receiving treatment', 'receiving treatment depression', 'treatment depression selected', 'depression selected analysis', 'selected analysis selection', 'analysis selection wa', 'selection wa performed', 'wa performed psychologist', 'performed psychologist verifying', 'psychologist verifying statement', 'verifying statement related', 'statement related real', 'related real expression', 'real expression depression', 'expression depression excluding', 'depression excluding quote', 'excluding quote joke', 'quote joke fake', 'joke fake one', 'fake one depressed', 'one depressed twitter', 'depressed twitter user', 'twitter user collected', 'user collected recent', 'collected recent tweet', 'recent tweet timeline', 'tweet timeline maximum', 'timeline maximum tweet', 'maximum tweet thus', 'tweet thus total', 'thus total tweet', 'total tweet collected', 'tweet collected figure', 'collected figure wa', 'figure wa reduced', 'wa reduced discarding', 'reduced discarding retweets', 'discarding retweets tweet', 'retweets tweet constituted', 'tweet constituted depressive', 'constituted depressive user', 'depressive user dataset', 'user dataset example', 'dataset example sentence', 'example sentence appearing', 'sentence appearing user', 'appearing user profile', 'user profile used', 'profile used selecting', 'used selecting depressive', 'selecting depressive user', 'depressive user paciente', 'user paciente psiquitrico', 'paciente psiquitrico con', 'psiquitrico con depresin', 'con depresin crnica', 'depresin crnica psychiatric', 'crnica psychiatric patient', 'psychiatric patient chronic', 'patient chronic depression', 'chronic depression example', 'depression example profile', 'example profile sentence', 'profile sentence indicates', 'sentence indicates depression', 'indicates depression colecciono', 'depression colecciono errores', 'colecciono errores traducidos', 'errores traducidos tweet', 'traducidos tweet depresivos', 'tweet depresivos uno', 'depresivos uno que', 'uno que otro', 'que otro impulso', 'otro impulso de', 'impulso de amor', 'de amor gather', 'amor gather error', 'gather error translated', 'error translated depressing', 'translated depressing tweet', 'depressing tweet one', 'tweet one another', 'one another love', 'another love impulse', 'love impulse example', 'impulse example profile', 'example profile sentence', 'profile sentence doe', 'sentence doe indicate', 'doe indicate depression', 'indicate depression user', 'depression user profile', 'user profile sentence', 'profile sentence indicating', 'sentence indicating depression', 'indicating depression retrieved', 'depression retrieved twitter', 'retrieved twitter timeline', 'twitter timeline collected', 'timeline collected user', 'collected user timeline', 'user timeline least', 'timeline least tweet', 'least tweet suggested', 'tweet suggested sign', 'suggested sign depression', 'sign depression retained', 'depression retained analysis', 'retained analysis user', 'analysis user selection', 'user selection tweet', 'selection tweet wa', 'tweet wa performed', 'wa performed manually', 'performed manually inspecting', 'manually inspecting tweet', 'inspecting tweet user', 'tweet user complete', 'user complete timeline', 'complete timeline reverse', 'timeline reverse temporal', 'reverse temporal order', 'temporal order starting', 'order starting recent', 'starting recent one', 'recent one oldest', 'one oldest tweet', 'oldest tweet timeline', 'tweet timeline retrieved', 'timeline retrieved mean', 'retrieved mean twitter', 'mean twitter api', 'twitter api finally', 'api finally total', 'finally total number', 'total number tweet', 'number tweet issued', 'tweet issued depressive', 'issued depressive user', 'depressive user suggesting', 'user suggesting sign', 'suggesting sign depression', 'sign depression detected', 'depression detected used', 'detected used analysis', 'used analysis set', 'analysis set tweet', 'set tweet provided', 'tweet provided u', 'provided u depressive', 'u depressive tweet', 'depressive tweet dataset', 'tweet dataset wa', 'dataset wa used', 'wa used analyze', 'used analyze linguistic', 'analyze linguistic feature', 'linguistic feature tweet', 'feature tweet showing', 'tweet showing sign', 'showing sign depression', 'sign depression ha', 'depression ha mentioned', 'ha mentioned tweet', 'mentioned tweet included', 'tweet included depressive', 'included depressive user', 'depressive user dataset', 'user dataset see', 'dataset see figure', 'see figure time', 'figure time tweet', 'time tweet also', 'tweet also collected', 'also collected june', 'collected june tweet', 'june tweet gathered', 'tweet gathered listening', 'gathered listening public', 'listening public twitter', 'public twitter stream', 'twitter stream time', 'stream time span', 'time span considering', 'span considering tweet', 'considering tweet spanish', 'tweet spanish textual', 'spanish textual content', 'textual content detected', 'content detected twitter', 'detected twitter language', 'twitter language identification', 'language identification support', 'identification support given', 'support given twitter', 'given twitter requires', 'twitter requires restrictive', 'requires restrictive filter', 'restrictive filter language', 'filter language tweet', 'language tweet used', 'tweet used list', 'used list frequently', 'list frequently used', 'frequently used spanish', 'used spanish word', 'spanish word stopwords', 'word stopwords retrieve', 'stopwords retrieve tweet', 'retrieve tweet included', 'tweet included word', 'included word vast', 'word vast majority', 'vast majority spanish', 'majority spanish tweet', 'spanish tweet match', 'tweet match criterion', 'match criterion sample', 'criterion sample user', 'sample user mention', 'user mention profile', 'mention profile word', 'profile word depression', 'word depression derivation', 'depression derivation selected', 'derivation selected randomly', 'selected randomly tweet', 'randomly tweet complete', 'tweet complete timeline', 'complete timeline user', 'timeline user compiled', 'user compiled tweet', 'compiled tweet reduced', 'tweet reduced retweets', 'reduced retweets removed', 'retweets removed tweet', 'removed tweet constituted', 'tweet constituted control', 'constituted control dataset', 'control dataset identify', 'dataset identify language', 'identify language tweet', 'language tweet relied', 'tweet relied language', 'relied language automatically', 'language automatically identified', 'automatically identified twitter', 'identified twitter tweet', 'twitter tweet selecting', 'tweet selecting tweet', 'selecting tweet spanish', 'tweet spanish ha', 'spanish ha noted', 'ha noted data', 'noted data contain', 'data contain tweet', 'contain tweet unidentified', 'tweet unidentified depressive', 'unidentified depressive user']"
https://aclanthology.org/W18-0608.pdf,0,Data was collected from 7 Cups of Tea an anonymous online chat-based peer support community for emotional distress1 . Users agree at signup that their data may be used for the purposes of research. All the data used for the current study was anonymous and securely stored. This research was performed in line with the ethical and privacy protocols outlined in detail in (Benton et al. 2017). Data from 7 Cups takes the form of written dialogue between users of the service and volunteers who are trained as “active listeners”. A fragment of an exchange between the user of the service (U) and the volunteer (V) might go as follows: For the analyses reported in this paper we used only text generated by users of the service not the volunteers providing peer support. Users who reported depression as their primary concern at sign up were eligible for inclusion in analyses. Our original sample was comprised of 23048 conversations involving 1937 unique users. Users were excluded from the sample if they did not indicate their culture or if they selected ‘Other’. This resulted in the exclusion of 199 and 130 users respectively. The original sample also included users identifying as Native American or American Indian. This group was excluded from analyses since the majority of the data among these users was not English. This resulted in the removal of 15 users leaving a total sample size of 1593.,data wa collected from cup of tea an anonymous online chatbased peer support community for emotional distress user agree at signup that their data may be used for the purpose of research all the data used for the current study wa anonymous and securely stored this research wa performed in line with the ethical and privacy protocol outlined in detail in benton et al data from cup take the form of written dialogue between user of the service and volunteer who are trained a active listener a fragment of an exchange between the user of the service u and the volunteer v might go a follows for the analysis reported in this paper we used only text generated by user of the service not the volunteer providing peer support user who reported depression a their primary concern at sign up were eligible for inclusion in analysis our original sample wa comprised of conversation involving unique user user were excluded from the sample if they did not indicate their culture or if they selected other this resulted in the exclusion of and user respectively the original sample also included user identifying a native american or american indian this group wa excluded from analysis since the majority of the data among these user wa not english this resulted in the removal of user leaving a total sample size of,"['data', 'wa', 'collected', 'cup', 'tea', 'anonymous', 'online', 'chatbased', 'peer', 'support', 'community', 'emotional', 'distress', 'user', 'agree', 'signup', 'data', 'may', 'used', 'purpose', 'research', 'data', 'used', 'current', 'study', 'wa', 'anonymous', 'securely', 'stored', 'research', 'wa', 'performed', 'line', 'ethical', 'privacy', 'protocol', 'outlined', 'detail', 'benton', 'et', 'al', 'data', 'cup', 'take', 'form', 'written', 'dialogue', 'user', 'service', 'volunteer', 'trained', 'active', 'listener', 'fragment', 'exchange', 'user', 'service', 'u', 'volunteer', 'v', 'might', 'go', 'follows', 'analysis', 'reported', 'paper', 'used', 'text', 'generated', 'user', 'service', 'volunteer', 'providing', 'peer', 'support', 'user', 'reported', 'depression', 'primary', 'concern', 'sign', 'eligible', 'inclusion', 'analysis', 'original', 'sample', 'wa', 'comprised', 'conversation', 'involving', 'unique', 'user', 'user', 'excluded', 'sample', 'indicate', 'culture', 'selected', 'resulted', 'exclusion', 'user', 'respectively', 'original', 'sample', 'also', 'included', 'user', 'identifying', 'native', 'american', 'american', 'indian', 'group', 'wa', 'excluded', 'analysis', 'since', 'majority', 'data', 'among', 'user', 'wa', 'english', 'resulted', 'removal', 'user', 'leaving', 'total', 'sample', 'size']","['data wa', 'wa collected', 'collected cup', 'cup tea', 'tea anonymous', 'anonymous online', 'online chatbased', 'chatbased peer', 'peer support', 'support community', 'community emotional', 'emotional distress', 'distress user', 'user agree', 'agree signup', 'signup data', 'data may', 'may used', 'used purpose', 'purpose research', 'research data', 'data used', 'used current', 'current study', 'study wa', 'wa anonymous', 'anonymous securely', 'securely stored', 'stored research', 'research wa', 'wa performed', 'performed line', 'line ethical', 'ethical privacy', 'privacy protocol', 'protocol outlined', 'outlined detail', 'detail benton', 'benton et', 'et al', 'al data', 'data cup', 'cup take', 'take form', 'form written', 'written dialogue', 'dialogue user', 'user service', 'service volunteer', 'volunteer trained', 'trained active', 'active listener', 'listener fragment', 'fragment exchange', 'exchange user', 'user service', 'service u', 'u volunteer', 'volunteer v', 'v might', 'might go', 'go follows', 'follows analysis', 'analysis reported', 'reported paper', 'paper used', 'used text', 'text generated', 'generated user', 'user service', 'service volunteer', 'volunteer providing', 'providing peer', 'peer support', 'support user', 'user reported', 'reported depression', 'depression primary', 'primary concern', 'concern sign', 'sign eligible', 'eligible inclusion', 'inclusion analysis', 'analysis original', 'original sample', 'sample wa', 'wa comprised', 'comprised conversation', 'conversation involving', 'involving unique', 'unique user', 'user user', 'user excluded', 'excluded sample', 'sample indicate', 'indicate culture', 'culture selected', 'selected resulted', 'resulted exclusion', 'exclusion user', 'user respectively', 'respectively original', 'original sample', 'sample also', 'also included', 'included user', 'user identifying', 'identifying native', 'native american', 'american american', 'american indian', 'indian group', 'group wa', 'wa excluded', 'excluded analysis', 'analysis since', 'since majority', 'majority data', 'data among', 'among user', 'user wa', 'wa english', 'english resulted', 'resulted removal', 'removal user', 'user leaving', 'leaving total', 'total sample', 'sample size']","['data wa collected', 'wa collected cup', 'collected cup tea', 'cup tea anonymous', 'tea anonymous online', 'anonymous online chatbased', 'online chatbased peer', 'chatbased peer support', 'peer support community', 'support community emotional', 'community emotional distress', 'emotional distress user', 'distress user agree', 'user agree signup', 'agree signup data', 'signup data may', 'data may used', 'may used purpose', 'used purpose research', 'purpose research data', 'research data used', 'data used current', 'used current study', 'current study wa', 'study wa anonymous', 'wa anonymous securely', 'anonymous securely stored', 'securely stored research', 'stored research wa', 'research wa performed', 'wa performed line', 'performed line ethical', 'line ethical privacy', 'ethical privacy protocol', 'privacy protocol outlined', 'protocol outlined detail', 'outlined detail benton', 'detail benton et', 'benton et al', 'et al data', 'al data cup', 'data cup take', 'cup take form', 'take form written', 'form written dialogue', 'written dialogue user', 'dialogue user service', 'user service volunteer', 'service volunteer trained', 'volunteer trained active', 'trained active listener', 'active listener fragment', 'listener fragment exchange', 'fragment exchange user', 'exchange user service', 'user service u', 'service u volunteer', 'u volunteer v', 'volunteer v might', 'v might go', 'might go follows', 'go follows analysis', 'follows analysis reported', 'analysis reported paper', 'reported paper used', 'paper used text', 'used text generated', 'text generated user', 'generated user service', 'user service volunteer', 'service volunteer providing', 'volunteer providing peer', 'providing peer support', 'peer support user', 'support user reported', 'user reported depression', 'reported depression primary', 'depression primary concern', 'primary concern sign', 'concern sign eligible', 'sign eligible inclusion', 'eligible inclusion analysis', 'inclusion analysis original', 'analysis original sample', 'original sample wa', 'sample wa comprised', 'wa comprised conversation', 'comprised conversation involving', 'conversation involving unique', 'involving unique user', 'unique user user', 'user user excluded', 'user excluded sample', 'excluded sample indicate', 'sample indicate culture', 'indicate culture selected', 'culture selected resulted', 'selected resulted exclusion', 'resulted exclusion user', 'exclusion user respectively', 'user respectively original', 'respectively original sample', 'original sample also', 'sample also included', 'also included user', 'included user identifying', 'user identifying native', 'identifying native american', 'native american american', 'american american indian', 'american indian group', 'indian group wa', 'group wa excluded', 'wa excluded analysis', 'excluded analysis since', 'analysis since majority', 'since majority data', 'majority data among', 'data among user', 'among user wa', 'user wa english', 'wa english resulted', 'english resulted removal', 'resulted removal user', 'removal user leaving', 'user leaving total', 'leaving total sample', 'total sample size']"
https://dl.acm.org/doi/pdf/10.1145/3359169,0,"Selection Criteria and Data Scope. To understand the impact of cultural differences on how individuals use online mental health platforms we begin our analysis by creating a dataset of users from different national communities on Talklife a support platform with over half a million users [91]. For this analysis due to the fact that most research in CSCW on mental health online has been done either agnostic of cultural context [12 34] or in a Western context [60 67 88] we choose to focus on users from non-Western countries following Zhang et al. [103]. As researchers located in the Global South and with lived experience interacting with the health system and diverse explanatory models [52] of mental illness we believe that moving the focus of CSCW and CSCWadjacent mental health research away from the West is crucial to better meet the needs of people often underserved by the medical system [70]. To create these subgroups of users we choose the three non-Western countries with the highest user populations on Talklife or India Malaysia and the Philippines. Guided by the rich amount of literature on the unique nuances to mental health expression for each country [35 62 77 80] we examine the national identity linguistic and behavior-based differences of use between each user subgroup. In particular this research notes that as a result of cultural norms around the sharing of distress and alternative conceptualizations of mental illness in India Malaysia and the Philippines symptoms are often expressed in somatic and religious terms as opposed to traditionally clinical or psychiatric terms. We choose to analyze each subgroup at the national level for both theoretical and practical reasons. On a theoretical level in past work in the medical anthropology of mental health national identity has commonly been used for a approximate level of analysis for cultural identity [31 33 52]. Additionally on a more practical level each user’s country was determined using their IP address by Talklife and shared with us in an user-anonymized dataset. Inferring a more precise location could potentially compromise user anonymity as discussed in past work [47] and did not seem to have any more significant value for our analysis of cultural differences than analysis at the national level. We analyze data from 10532 Indian users 3370 Malaysian users and 3370 Filipino users as shown in Table 2. Collectively we refer to these countries as the minority sample. As a comparison set we construct a random sample of all threads on Talklife and refer to it as the majority sample. Due to the relative prevalence of users from Western English-speaking countries in Talklife most of the threads in the majority sample include posts from countries such as the USA UK and Canada. Indians are the largest non-Western minority subgroup on Talklife. Data was sampled from May 2012 to June 2018. Following this cross-national analysis to see if our broader results on Talklife generalize to a differently structured online mental health community we picked the largest Western country (the United States) and the largest non-Western country (India) represented on 7Cups a similar support platform with more than 15000 users actively using the platform each week [7]. Using 7Cups data we repeat our analysis testing for the same cultural differences we found in our Talklife sample. For this analysis we were provided a sample of data on activity from 6055 Indian users and 18581 American users as shown in Table 2. Unlike our sample of Talklife users this dataset is not a random sample. There is an upsampling of Indian users to ensure that we have data from a sufficient number of Indians in the dataset. Like on Talklife Indians are the largest non-Western minority subgroup on 7Cups. We focus on Indian users due to a lack of sufficient data on users from Malaysia or the Philippines. Data was sampled from March 2014 - August 2018. 3.1.2 Defining Cultural Identity and Use of Clinical Language. In this work we examine the relationship between cultural identity and use of online mental health support forums. To do so we leverage Tomlinson’s definition of cultural identity as “self and communal definitions based around specific usually politically inflected differentiations: gender sexuality class religion race and ethnicity nationality"" [94] particularly looking at the aspect that of modern cultural identity that runs along national lines as delineated by Hall et al. [41]. As a diverse and amorphous form of identity cultural identity can often intersect and interact with other forms of identity including religious or ethnic identity. However in the absence of direct information about religious or ethnic identity based on the data available we use national identity as a proxy for cultural identity. Additionally following Schlesinger et al’s [83] call for more intersectional analyses and methods within HCI we also include analyses of adjacent and intersecting identities when relevant including religious identity. To analyze clinical language we use a broader definition of clinical language than just specific medical diagnoses. Following methods used in past work to analyze antidepressant related language [30] we create a dataset of clinical mental health language including unigrams bigrams and trigrams from a list of mental disorders as defined by the International Classification of Diseases (ICD-10) and Diagnostic and Statistical Manual of Mental Disorders (DSM-5) [100]. We also included all unigrams from the MacMillan Dictionary list of words used to describe illnesses and diseases both specifically for mental illness and general illness [1–3]. As a result we include unigrams like “night"" (from night terrors) or sleep (from “sleep disorder"") as these are often correlated with specific symptoms of mental illness or distress such as sleep issues or being awake at night [30]. This included any clinically common abbreviations for mental disorders such as OCD for “obsessive compulsive disorder"" or BPD for “borderline personality disorder."" Shorthand for disorders commonly used by online communities such as “pro-ana” (as used in pro-eating disorder communities) [22] were not included due to the difficulty in finding an exhaustive list of these terms across disorders. We choose to use terms from and associated with DSM and ICD categorized disorders as a result of the common usage of these frameworks globally [99]. Throughout our analysis of these varied factors we use µ to represent means and σ to represent standard deviations. 3.1.3 Constraints Limitations and Tradeoffs. Cultural identity can exist at many different and intersecting levels including subcultures and subcommunities within the larger umbrella of a cultural identity. As a result for the purpose of this analysis we had to adopt some constraints in order to do a meaningful and specific analysis. One large limiting constraint that we chose for this study is to use national identity at the state level as a proxy for cultural identity. Though a major and formative part of modern cultural identity as argued by both Hall [41] and Tomlinson [94] each country we analyze is incredibly diverse with many individual cultural identities that both intersect and diverge from a greater national identity [54 64 89]. A more rich analysis of these other forms of cultural identity is beyond the scope of this work but could lead to richer conclusions about the nature of cultural identity in online mental health support communities particularly with regard to cultural differences between users with the same national identity. Additionally to stay consistent between analyses as a result of a lack of data on users from Malaysia and the Philippines we only analyze users in India on 7Cups and extend these findings to the experience of being part of a minority group on an online mental health forum. We draw validity for these exploratory findings from similar consistent patterns we observe between Indian Malaysian and Filipino users but a deeper analysis with a larger dataset is likely necessary to determine when and for which minority communities these conclusions do not hold true. Additionally while we construct clinical language through use of the commonly used DSM and ICD both frameworks of illness categorization have significant limitations particularly in the countries we have selected. For example there are both mental health disorders that are culturebound [74] as well as mental health language that is used in different ways within the specific countries we analyze such as depression often being an umbrella term for all mental illnesses [53]. Additionally it is clear that online support communities often develop their own cultural norms and language around mental health [21 72] and a deeper understanding of how this plays out on Talklife and 7Cups is neither the focus nor within the scope of this work. In this work we intentionally use standard clinical and medical terms for mental health disorders in our analysis of clinical language. As detailed in past anthropological research [52] it is theorized that the use of medical and clinical language is representative of a medicalized explanatory model of illness and we frame use of this language across cultures as a approximate signifier of a greater awareness of the presence of a mental disorder as opposed to conceptualizing distress as “stress"" “tension"" or “depression"" [25 53 98]. For our analysis we strictly analyzed posts that were in the Latin alphabet with almost all posts on both Talklife and 7Cups being in English. However as both Malay [8] and Tagalog [82] are most commonly written in the Latin script and since it is common for users from India speakers to use romanized versions of Indian languages online [79] it is possible that a small minority of posts in our analysis were text in a different language. However as confirmed by only seeing English words used in our analysis of the top n-grams among each user subgroup it is clear that English is the predominant language on both platforms. Though beyond the immediate scope of this work a greater analysis of non-English code-switching on these platforms could lead to a deeper understanding of the impact of interactions on expression between users with the same national identity but different language preferences.",selection criterion and data scope to understand the impact of cultural difference on how individual use online mental health platform we begin our analysis by creating a dataset of user from different national community on talklife a support platform with over half a million user for this analysis due to the fact that most research in cscw on mental health online ha been done either agnostic of cultural context or in a western context we choose to focus on user from nonwestern country following zhang et al a researcher located in the global south and with lived experience interacting with the health system and diverse explanatory model of mental illness we believe that moving the focus of cscw and cscwadjacent mental health research away from the west is crucial to better meet the need of people often underserved by the medical system to create these subgroup of user we choose the three nonwestern country with the highest user population on talklife or india malaysia and the philippine guided by the rich amount of literature on the unique nuance to mental health expression for each country we examine the national identity linguistic and behaviorbased difference of use between each user subgroup in particular this research note that a a result of cultural norm around the sharing of distress and alternative conceptualization of mental illness in india malaysia and the philippine symptom are often expressed in somatic and religious term a opposed to traditionally clinical or psychiatric term we choose to analyze each subgroup at the national level for both theoretical and practical reason on a theoretical level in past work in the medical anthropology of mental health national identity ha commonly been used for a approximate level of analysis for cultural identity additionally on a more practical level each user country wa determined using their ip address by talklife and shared with u in an useranonymized dataset inferring a more precise location could potentially compromise user anonymity a discussed in past work and did not seem to have any more significant value for our analysis of cultural difference than analysis at the national level we analyze data from indian user malaysian user and filipino user a shown in table collectively we refer to these country a the minority sample a a comparison set we construct a random sample of all thread on talklife and refer to it a the majority sample due to the relative prevalence of user from western englishspeaking country in talklife most of the thread in the majority sample include post from country such a the usa uk and canada indian are the largest nonwestern minority subgroup on talklife data wa sampled from may to june following this crossnational analysis to see if our broader result on talklife generalize to a differently structured online mental health community we picked the largest western country the united state and the largest nonwestern country india represented on cup a similar support platform with more than user actively using the platform each week using cup data we repeat our analysis testing for the same cultural difference we found in our talklife sample for this analysis we were provided a sample of data on activity from indian user and american user a shown in table unlike our sample of talklife user this dataset is not a random sample there is an upsampling of indian user to ensure that we have data from a sufficient number of indian in the dataset like on talklife indian are the largest nonwestern minority subgroup on cup we focus on indian user due to a lack of sufficient data on user from malaysia or the philippine data wa sampled from march august defining cultural identity and use of clinical language in this work we examine the relationship between cultural identity and use of online mental health support forum to do so we leverage tomlinsons definition of cultural identity a self and communal definition based around specific usually politically inflected differentiation gender sexuality class religion race and ethnicity nationality particularly looking at the aspect that of modern cultural identity that run along national line a delineated by hall et al a a diverse and amorphous form of identity cultural identity can often intersect and interact with other form of identity including religious or ethnic identity however in the absence of direct information about religious or ethnic identity based on the data available we use national identity a a proxy for cultural identity additionally following schlesinger et al call for more intersectional analysis and method within hci we also include analysis of adjacent and intersecting identity when relevant including religious identity to analyze clinical language we use a broader definition of clinical language than just specific medical diagnosis following method used in past work to analyze antidepressant related language we create a dataset of clinical mental health language including unigrams bigram and trigram from a list of mental disorder a defined by the international classification of disease icd and diagnostic and statistical manual of mental disorder dsm we also included all unigrams from the macmillan dictionary list of word used to describe illness and disease both specifically for mental illness and general illness a a result we include unigrams like night from night terror or sleep from sleep disorder a these are often correlated with specific symptom of mental illness or distress such a sleep issue or being awake at night this included any clinically common abbreviation for mental disorder such a ocd for obsessive compulsive disorder or bpd for borderline personality disorder shorthand for disorder commonly used by online community such a proana a used in proeating disorder community were not included due to the difficulty in finding an exhaustive list of these term across disorder we choose to use term from and associated with dsm and icd categorized disorder a a result of the common usage of these framework globally throughout our analysis of these varied factor we use to represent mean and to represent standard deviation constraint limitation and tradeoff cultural identity can exist at many different and intersecting level including subculture and subcommunities within the larger umbrella of a cultural identity a a result for the purpose of this analysis we had to adopt some constraint in order to do a meaningful and specific analysis one large limiting constraint that we chose for this study is to use national identity at the state level a a proxy for cultural identity though a major and formative part of modern cultural identity a argued by both hall and tomlinson each country we analyze is incredibly diverse with many individual cultural identity that both intersect and diverge from a greater national identity a more rich analysis of these other form of cultural identity is beyond the scope of this work but could lead to richer conclusion about the nature of cultural identity in online mental health support community particularly with regard to cultural difference between user with the same national identity additionally to stay consistent between analysis a a result of a lack of data on user from malaysia and the philippine we only analyze user in india on cup and extend these finding to the experience of being part of a minority group on an online mental health forum we draw validity for these exploratory finding from similar consistent pattern we observe between indian malaysian and filipino user but a deeper analysis with a larger dataset is likely necessary to determine when and for which minority community these conclusion do not hold true additionally while we construct clinical language through use of the commonly used dsm and icd both framework of illness categorization have significant limitation particularly in the country we have selected for example there are both mental health disorder that are culturebound a well a mental health language that is used in different way within the specific country we analyze such a depression often being an umbrella term for all mental illness additionally it is clear that online support community often develop their own cultural norm and language around mental health and a deeper understanding of how this play out on talklife and cup is neither the focus nor within the scope of this work in this work we intentionally use standard clinical and medical term for mental health disorder in our analysis of clinical language a detailed in past anthropological research it is theorized that the use of medical and clinical language is representative of a medicalized explanatory model of illness and we frame use of this language across culture a a approximate signifier of a greater awareness of the presence of a mental disorder a opposed to conceptualizing distress a stress tension or depression for our analysis we strictly analyzed post that were in the latin alphabet with almost all post on both talklife and cup being in english however a both malay and tagalog are most commonly written in the latin script and since it is common for user from india speaker to use romanized version of indian language online it is possible that a small minority of post in our analysis were text in a different language however a confirmed by only seeing english word used in our analysis of the top ngrams among each user subgroup it is clear that english is the predominant language on both platform though beyond the immediate scope of this work a greater analysis of nonenglish codeswitching on these platform could lead to a deeper understanding of the impact of interaction on expression between user with the same national identity but different language preference,"['selection', 'criterion', 'data', 'scope', 'understand', 'impact', 'cultural', 'difference', 'individual', 'use', 'online', 'mental', 'health', 'platform', 'begin', 'analysis', 'creating', 'dataset', 'user', 'different', 'national', 'community', 'talklife', 'support', 'platform', 'half', 'million', 'user', 'analysis', 'due', 'fact', 'research', 'cscw', 'mental', 'health', 'online', 'ha', 'done', 'either', 'agnostic', 'cultural', 'context', 'western', 'context', 'choose', 'focus', 'user', 'nonwestern', 'country', 'following', 'zhang', 'et', 'al', 'researcher', 'located', 'global', 'south', 'lived', 'experience', 'interacting', 'health', 'system', 'diverse', 'explanatory', 'model', 'mental', 'illness', 'believe', 'moving', 'focus', 'cscw', 'cscwadjacent', 'mental', 'health', 'research', 'away', 'west', 'crucial', 'better', 'meet', 'need', 'people', 'often', 'underserved', 'medical', 'system', 'create', 'subgroup', 'user', 'choose', 'three', 'nonwestern', 'country', 'highest', 'user', 'population', 'talklife', 'india', 'malaysia', 'philippine', 'guided', 'rich', 'amount', 'literature', 'unique', 'nuance', 'mental', 'health', 'expression', 'country', 'examine', 'national', 'identity', 'linguistic', 'behaviorbased', 'difference', 'use', 'user', 'subgroup', 'particular', 'research', 'note', 'result', 'cultural', 'norm', 'around', 'sharing', 'distress', 'alternative', 'conceptualization', 'mental', 'illness', 'india', 'malaysia', 'philippine', 'symptom', 'often', 'expressed', 'somatic', 'religious', 'term', 'opposed', 'traditionally', 'clinical', 'psychiatric', 'term', 'choose', 'analyze', 'subgroup', 'national', 'level', 'theoretical', 'practical', 'reason', 'theoretical', 'level', 'past', 'work', 'medical', 'anthropology', 'mental', 'health', 'national', 'identity', 'ha', 'commonly', 'used', 'approximate', 'level', 'analysis', 'cultural', 'identity', 'additionally', 'practical', 'level', 'user', 'country', 'wa', 'determined', 'using', 'ip', 'address', 'talklife', 'shared', 'u', 'useranonymized', 'dataset', 'inferring', 'precise', 'location', 'could', 'potentially', 'compromise', 'user', 'anonymity', 'discussed', 'past', 'work', 'seem', 'significant', 'value', 'analysis', 'cultural', 'difference', 'analysis', 'national', 'level', 'analyze', 'data', 'indian', 'user', 'malaysian', 'user', 'filipino', 'user', 'shown', 'table', 'collectively', 'refer', 'country', 'minority', 'sample', 'comparison', 'set', 'construct', 'random', 'sample', 'thread', 'talklife', 'refer', 'majority', 'sample', 'due', 'relative', 'prevalence', 'user', 'western', 'englishspeaking', 'country', 'talklife', 'thread', 'majority', 'sample', 'include', 'post', 'country', 'usa', 'uk', 'canada', 'indian', 'largest', 'nonwestern', 'minority', 'subgroup', 'talklife', 'data', 'wa', 'sampled', 'may', 'june', 'following', 'crossnational', 'analysis', 'see', 'broader', 'result', 'talklife', 'generalize', 'differently', 'structured', 'online', 'mental', 'health', 'community', 'picked', 'largest', 'western', 'country', 'united', 'state', 'largest', 'nonwestern', 'country', 'india', 'represented', 'cup', 'similar', 'support', 'platform', 'user', 'actively', 'using', 'platform', 'week', 'using', 'cup', 'data', 'repeat', 'analysis', 'testing', 'cultural', 'difference', 'found', 'talklife', 'sample', 'analysis', 'provided', 'sample', 'data', 'activity', 'indian', 'user', 'american', 'user', 'shown', 'table', 'unlike', 'sample', 'talklife', 'user', 'dataset', 'random', 'sample', 'upsampling', 'indian', 'user', 'ensure', 'data', 'sufficient', 'number', 'indian', 'dataset', 'like', 'talklife', 'indian', 'largest', 'nonwestern', 'minority', 'subgroup', 'cup', 'focus', 'indian', 'user', 'due', 'lack', 'sufficient', 'data', 'user', 'malaysia', 'philippine', 'data', 'wa', 'sampled', 'march', 'august', 'defining', 'cultural', 'identity', 'use', 'clinical', 'language', 'work', 'examine', 'relationship', 'cultural', 'identity', 'use', 'online', 'mental', 'health', 'support', 'forum', 'leverage', 'tomlinsons', 'definition', 'cultural', 'identity', 'self', 'communal', 'definition', 'based', 'around', 'specific', 'usually', 'politically', 'inflected', 'differentiation', 'gender', 'sexuality', 'class', 'religion', 'race', 'ethnicity', 'nationality', 'particularly', 'looking', 'aspect', 'modern', 'cultural', 'identity', 'run', 'along', 'national', 'line', 'delineated', 'hall', 'et', 'al', 'diverse', 'amorphous', 'form', 'identity', 'cultural', 'identity', 'often', 'intersect', 'interact', 'form', 'identity', 'including', 'religious', 'ethnic', 'identity', 'however', 'absence', 'direct', 'information', 'religious', 'ethnic', 'identity', 'based', 'data', 'available', 'use', 'national', 'identity', 'proxy', 'cultural', 'identity', 'additionally', 'following', 'schlesinger', 'et', 'al', 'call', 'intersectional', 'analysis', 'method', 'within', 'hci', 'also', 'include', 'analysis', 'adjacent', 'intersecting', 'identity', 'relevant', 'including', 'religious', 'identity', 'analyze', 'clinical', 'language', 'use', 'broader', 'definition', 'clinical', 'language', 'specific', 'medical', 'diagnosis', 'following', 'method', 'used', 'past', 'work', 'analyze', 'antidepressant', 'related', 'language', 'create', 'dataset', 'clinical', 'mental', 'health', 'language', 'including', 'unigrams', 'bigram', 'trigram', 'list', 'mental', 'disorder', 'defined', 'international', 'classification', 'disease', 'icd', 'diagnostic', 'statistical', 'manual', 'mental', 'disorder', 'dsm', 'also', 'included', 'unigrams', 'macmillan', 'dictionary', 'list', 'word', 'used', 'describe', 'illness', 'disease', 'specifically', 'mental', 'illness', 'general', 'illness', 'result', 'include', 'unigrams', 'like', 'night', 'night', 'terror', 'sleep', 'sleep', 'disorder', 'often', 'correlated', 'specific', 'symptom', 'mental', 'illness', 'distress', 'sleep', 'issue', 'awake', 'night', 'included', 'clinically', 'common', 'abbreviation', 'mental', 'disorder', 'ocd', 'obsessive', 'compulsive', 'disorder', 'bpd', 'borderline', 'personality', 'disorder', 'shorthand', 'disorder', 'commonly', 'used', 'online', 'community', 'proana', 'used', 'proeating', 'disorder', 'community', 'included', 'due', 'difficulty', 'finding', 'exhaustive', 'list', 'term', 'across', 'disorder', 'choose', 'use', 'term', 'associated', 'dsm', 'icd', 'categorized', 'disorder', 'result', 'common', 'usage', 'framework', 'globally', 'throughout', 'analysis', 'varied', 'factor', 'use', 'represent', 'mean', 'represent', 'standard', 'deviation', 'constraint', 'limitation', 'tradeoff', 'cultural', 'identity', 'exist', 'many', 'different', 'intersecting', 'level', 'including', 'subculture', 'subcommunities', 'within', 'larger', 'umbrella', 'cultural', 'identity', 'result', 'purpose', 'analysis', 'adopt', 'constraint', 'order', 'meaningful', 'specific', 'analysis', 'one', 'large', 'limiting', 'constraint', 'chose', 'study', 'use', 'national', 'identity', 'state', 'level', 'proxy', 'cultural', 'identity', 'though', 'major', 'formative', 'part', 'modern', 'cultural', 'identity', 'argued', 'hall', 'tomlinson', 'country', 'analyze', 'incredibly', 'diverse', 'many', 'individual', 'cultural', 'identity', 'intersect', 'diverge', 'greater', 'national', 'identity', 'rich', 'analysis', 'form', 'cultural', 'identity', 'beyond', 'scope', 'work', 'could', 'lead', 'richer', 'conclusion', 'nature', 'cultural', 'identity', 'online', 'mental', 'health', 'support', 'community', 'particularly', 'regard', 'cultural', 'difference', 'user', 'national', 'identity', 'additionally', 'stay', 'consistent', 'analysis', 'result', 'lack', 'data', 'user', 'malaysia', 'philippine', 'analyze', 'user', 'india', 'cup', 'extend', 'finding', 'experience', 'part', 'minority', 'group', 'online', 'mental', 'health', 'forum', 'draw', 'validity', 'exploratory', 'finding', 'similar', 'consistent', 'pattern', 'observe', 'indian', 'malaysian', 'filipino', 'user', 'deeper', 'analysis', 'larger', 'dataset', 'likely', 'necessary', 'determine', 'minority', 'community', 'conclusion', 'hold', 'true', 'additionally', 'construct', 'clinical', 'language', 'use', 'commonly', 'used', 'dsm', 'icd', 'framework', 'illness', 'categorization', 'significant', 'limitation', 'particularly', 'country', 'selected', 'example', 'mental', 'health', 'disorder', 'culturebound', 'well', 'mental', 'health', 'language', 'used', 'different', 'way', 'within', 'specific', 'country', 'analyze', 'depression', 'often', 'umbrella', 'term', 'mental', 'illness', 'additionally', 'clear', 'online', 'support', 'community', 'often', 'develop', 'cultural', 'norm', 'language', 'around', 'mental', 'health', 'deeper', 'understanding', 'play', 'talklife', 'cup', 'neither', 'focus', 'within', 'scope', 'work', 'work', 'intentionally', 'use', 'standard', 'clinical', 'medical', 'term', 'mental', 'health', 'disorder', 'analysis', 'clinical', 'language', 'detailed', 'past', 'anthropological', 'research', 'theorized', 'use', 'medical', 'clinical', 'language', 'representative', 'medicalized', 'explanatory', 'model', 'illness', 'frame', 'use', 'language', 'across', 'culture', 'approximate', 'signifier', 'greater', 'awareness', 'presence', 'mental', 'disorder', 'opposed', 'conceptualizing', 'distress', 'stress', 'tension', 'depression', 'analysis', 'strictly', 'analyzed', 'post', 'latin', 'alphabet', 'almost', 'post', 'talklife', 'cup', 'english', 'however', 'malay', 'tagalog', 'commonly', 'written', 'latin', 'script', 'since', 'common', 'user', 'india', 'speaker', 'use', 'romanized', 'version', 'indian', 'language', 'online', 'possible', 'small', 'minority', 'post', 'analysis', 'text', 'different', 'language', 'however', 'confirmed', 'seeing', 'english', 'word', 'used', 'analysis', 'top', 'ngrams', 'among', 'user', 'subgroup', 'clear', 'english', 'predominant', 'language', 'platform', 'though', 'beyond', 'immediate', 'scope', 'work', 'greater', 'analysis', 'nonenglish', 'codeswitching', 'platform', 'could', 'lead', 'deeper', 'understanding', 'impact', 'interaction', 'expression', 'user', 'national', 'identity', 'different', 'language', 'preference']","['selection criterion', 'criterion data', 'data scope', 'scope understand', 'understand impact', 'impact cultural', 'cultural difference', 'difference individual', 'individual use', 'use online', 'online mental', 'mental health', 'health platform', 'platform begin', 'begin analysis', 'analysis creating', 'creating dataset', 'dataset user', 'user different', 'different national', 'national community', 'community talklife', 'talklife support', 'support platform', 'platform half', 'half million', 'million user', 'user analysis', 'analysis due', 'due fact', 'fact research', 'research cscw', 'cscw mental', 'mental health', 'health online', 'online ha', 'ha done', 'done either', 'either agnostic', 'agnostic cultural', 'cultural context', 'context western', 'western context', 'context choose', 'choose focus', 'focus user', 'user nonwestern', 'nonwestern country', 'country following', 'following zhang', 'zhang et', 'et al', 'al researcher', 'researcher located', 'located global', 'global south', 'south lived', 'lived experience', 'experience interacting', 'interacting health', 'health system', 'system diverse', 'diverse explanatory', 'explanatory model', 'model mental', 'mental illness', 'illness believe', 'believe moving', 'moving focus', 'focus cscw', 'cscw cscwadjacent', 'cscwadjacent mental', 'mental health', 'health research', 'research away', 'away west', 'west crucial', 'crucial better', 'better meet', 'meet need', 'need people', 'people often', 'often underserved', 'underserved medical', 'medical system', 'system create', 'create subgroup', 'subgroup user', 'user choose', 'choose three', 'three nonwestern', 'nonwestern country', 'country highest', 'highest user', 'user population', 'population talklife', 'talklife india', 'india malaysia', 'malaysia philippine', 'philippine guided', 'guided rich', 'rich amount', 'amount literature', 'literature unique', 'unique nuance', 'nuance mental', 'mental health', 'health expression', 'expression country', 'country examine', 'examine national', 'national identity', 'identity linguistic', 'linguistic behaviorbased', 'behaviorbased difference', 'difference use', 'use user', 'user subgroup', 'subgroup particular', 'particular research', 'research note', 'note result', 'result cultural', 'cultural norm', 'norm around', 'around sharing', 'sharing distress', 'distress alternative', 'alternative conceptualization', 'conceptualization mental', 'mental illness', 'illness india', 'india malaysia', 'malaysia philippine', 'philippine symptom', 'symptom often', 'often expressed', 'expressed somatic', 'somatic religious', 'religious term', 'term opposed', 'opposed traditionally', 'traditionally clinical', 'clinical psychiatric', 'psychiatric term', 'term choose', 'choose analyze', 'analyze subgroup', 'subgroup national', 'national level', 'level theoretical', 'theoretical practical', 'practical reason', 'reason theoretical', 'theoretical level', 'level past', 'past work', 'work medical', 'medical anthropology', 'anthropology mental', 'mental health', 'health national', 'national identity', 'identity ha', 'ha commonly', 'commonly used', 'used approximate', 'approximate level', 'level analysis', 'analysis cultural', 'cultural identity', 'identity additionally', 'additionally practical', 'practical level', 'level user', 'user country', 'country wa', 'wa determined', 'determined using', 'using ip', 'ip address', 'address talklife', 'talklife shared', 'shared u', 'u useranonymized', 'useranonymized dataset', 'dataset inferring', 'inferring precise', 'precise location', 'location could', 'could potentially', 'potentially compromise', 'compromise user', 'user anonymity', 'anonymity discussed', 'discussed past', 'past work', 'work seem', 'seem significant', 'significant value', 'value analysis', 'analysis cultural', 'cultural difference', 'difference analysis', 'analysis national', 'national level', 'level analyze', 'analyze data', 'data indian', 'indian user', 'user malaysian', 'malaysian user', 'user filipino', 'filipino user', 'user shown', 'shown table', 'table collectively', 'collectively refer', 'refer country', 'country minority', 'minority sample', 'sample comparison', 'comparison set', 'set construct', 'construct random', 'random sample', 'sample thread', 'thread talklife', 'talklife refer', 'refer majority', 'majority sample', 'sample due', 'due relative', 'relative prevalence', 'prevalence user', 'user western', 'western englishspeaking', 'englishspeaking country', 'country talklife', 'talklife thread', 'thread majority', 'majority sample', 'sample include', 'include post', 'post country', 'country usa', 'usa uk', 'uk canada', 'canada indian', 'indian largest', 'largest nonwestern', 'nonwestern minority', 'minority subgroup', 'subgroup talklife', 'talklife data', 'data wa', 'wa sampled', 'sampled may', 'may june', 'june following', 'following crossnational', 'crossnational analysis', 'analysis see', 'see broader', 'broader result', 'result talklife', 'talklife generalize', 'generalize differently', 'differently structured', 'structured online', 'online mental', 'mental health', 'health community', 'community picked', 'picked largest', 'largest western', 'western country', 'country united', 'united state', 'state largest', 'largest nonwestern', 'nonwestern country', 'country india', 'india represented', 'represented cup', 'cup similar', 'similar support', 'support platform', 'platform user', 'user actively', 'actively using', 'using platform', 'platform week', 'week using', 'using cup', 'cup data', 'data repeat', 'repeat analysis', 'analysis testing', 'testing cultural', 'cultural difference', 'difference found', 'found talklife', 'talklife sample', 'sample analysis', 'analysis provided', 'provided sample', 'sample data', 'data activity', 'activity indian', 'indian user', 'user american', 'american user', 'user shown', 'shown table', 'table unlike', 'unlike sample', 'sample talklife', 'talklife user', 'user dataset', 'dataset random', 'random sample', 'sample upsampling', 'upsampling indian', 'indian user', 'user ensure', 'ensure data', 'data sufficient', 'sufficient number', 'number indian', 'indian dataset', 'dataset like', 'like talklife', 'talklife indian', 'indian largest', 'largest nonwestern', 'nonwestern minority', 'minority subgroup', 'subgroup cup', 'cup focus', 'focus indian', 'indian user', 'user due', 'due lack', 'lack sufficient', 'sufficient data', 'data user', 'user malaysia', 'malaysia philippine', 'philippine data', 'data wa', 'wa sampled', 'sampled march', 'march august', 'august defining', 'defining cultural', 'cultural identity', 'identity use', 'use clinical', 'clinical language', 'language work', 'work examine', 'examine relationship', 'relationship cultural', 'cultural identity', 'identity use', 'use online', 'online mental', 'mental health', 'health support', 'support forum', 'forum leverage', 'leverage tomlinsons', 'tomlinsons definition', 'definition cultural', 'cultural identity', 'identity self', 'self communal', 'communal definition', 'definition based', 'based around', 'around specific', 'specific usually', 'usually politically', 'politically inflected', 'inflected differentiation', 'differentiation gender', 'gender sexuality', 'sexuality class', 'class religion', 'religion race', 'race ethnicity', 'ethnicity nationality', 'nationality particularly', 'particularly looking', 'looking aspect', 'aspect modern', 'modern cultural', 'cultural identity', 'identity run', 'run along', 'along national', 'national line', 'line delineated', 'delineated hall', 'hall et', 'et al', 'al diverse', 'diverse amorphous', 'amorphous form', 'form identity', 'identity cultural', 'cultural identity', 'identity often', 'often intersect', 'intersect interact', 'interact form', 'form identity', 'identity including', 'including religious', 'religious ethnic', 'ethnic identity', 'identity however', 'however absence', 'absence direct', 'direct information', 'information religious', 'religious ethnic', 'ethnic identity', 'identity based', 'based data', 'data available', 'available use', 'use national', 'national identity', 'identity proxy', 'proxy cultural', 'cultural identity', 'identity additionally', 'additionally following', 'following schlesinger', 'schlesinger et', 'et al', 'al call', 'call intersectional', 'intersectional analysis', 'analysis method', 'method within', 'within hci', 'hci also', 'also include', 'include analysis', 'analysis adjacent', 'adjacent intersecting', 'intersecting identity', 'identity relevant', 'relevant including', 'including religious', 'religious identity', 'identity analyze', 'analyze clinical', 'clinical language', 'language use', 'use broader', 'broader definition', 'definition clinical', 'clinical language', 'language specific', 'specific medical', 'medical diagnosis', 'diagnosis following', 'following method', 'method used', 'used past', 'past work', 'work analyze', 'analyze antidepressant', 'antidepressant related', 'related language', 'language create', 'create dataset', 'dataset clinical', 'clinical mental', 'mental health', 'health language', 'language including', 'including unigrams', 'unigrams bigram', 'bigram trigram', 'trigram list', 'list mental', 'mental disorder', 'disorder defined', 'defined international', 'international classification', 'classification disease', 'disease icd', 'icd diagnostic', 'diagnostic statistical', 'statistical manual', 'manual mental', 'mental disorder', 'disorder dsm', 'dsm also', 'also included', 'included unigrams', 'unigrams macmillan', 'macmillan dictionary', 'dictionary list', 'list word', 'word used', 'used describe', 'describe illness', 'illness disease', 'disease specifically', 'specifically mental', 'mental illness', 'illness general', 'general illness', 'illness result', 'result include', 'include unigrams', 'unigrams like', 'like night', 'night night', 'night terror', 'terror sleep', 'sleep sleep', 'sleep disorder', 'disorder often', 'often correlated', 'correlated specific', 'specific symptom', 'symptom mental', 'mental illness', 'illness distress', 'distress sleep', 'sleep issue', 'issue awake', 'awake night', 'night included', 'included clinically', 'clinically common', 'common abbreviation', 'abbreviation mental', 'mental disorder', 'disorder ocd', 'ocd obsessive', 'obsessive compulsive', 'compulsive disorder', 'disorder bpd', 'bpd borderline', 'borderline personality', 'personality disorder', 'disorder shorthand', 'shorthand disorder', 'disorder commonly', 'commonly used', 'used online', 'online community', 'community proana', 'proana used', 'used proeating', 'proeating disorder', 'disorder community', 'community included', 'included due', 'due difficulty', 'difficulty finding', 'finding exhaustive', 'exhaustive list', 'list term', 'term across', 'across disorder', 'disorder choose', 'choose use', 'use term', 'term associated', 'associated dsm', 'dsm icd', 'icd categorized', 'categorized disorder', 'disorder result', 'result common', 'common usage', 'usage framework', 'framework globally', 'globally throughout', 'throughout analysis', 'analysis varied', 'varied factor', 'factor use', 'use represent', 'represent mean', 'mean represent', 'represent standard', 'standard deviation', 'deviation constraint', 'constraint limitation', 'limitation tradeoff', 'tradeoff cultural', 'cultural identity', 'identity exist', 'exist many', 'many different', 'different intersecting', 'intersecting level', 'level including', 'including subculture', 'subculture subcommunities', 'subcommunities within', 'within larger', 'larger umbrella', 'umbrella cultural', 'cultural identity', 'identity result', 'result purpose', 'purpose analysis', 'analysis adopt', 'adopt constraint', 'constraint order', 'order meaningful', 'meaningful specific', 'specific analysis', 'analysis one', 'one large', 'large limiting', 'limiting constraint', 'constraint chose', 'chose study', 'study use', 'use national', 'national identity', 'identity state', 'state level', 'level proxy', 'proxy cultural', 'cultural identity', 'identity though', 'though major', 'major formative', 'formative part', 'part modern', 'modern cultural', 'cultural identity', 'identity argued', 'argued hall', 'hall tomlinson', 'tomlinson country', 'country analyze', 'analyze incredibly', 'incredibly diverse', 'diverse many', 'many individual', 'individual cultural', 'cultural identity', 'identity intersect', 'intersect diverge', 'diverge greater', 'greater national', 'national identity', 'identity rich', 'rich analysis', 'analysis form', 'form cultural', 'cultural identity', 'identity beyond', 'beyond scope', 'scope work', 'work could', 'could lead', 'lead richer', 'richer conclusion', 'conclusion nature', 'nature cultural', 'cultural identity', 'identity online', 'online mental', 'mental health', 'health support', 'support community', 'community particularly', 'particularly regard', 'regard cultural', 'cultural difference', 'difference user', 'user national', 'national identity', 'identity additionally', 'additionally stay', 'stay consistent', 'consistent analysis', 'analysis result', 'result lack', 'lack data', 'data user', 'user malaysia', 'malaysia philippine', 'philippine analyze', 'analyze user', 'user india', 'india cup', 'cup extend', 'extend finding', 'finding experience', 'experience part', 'part minority', 'minority group', 'group online', 'online mental', 'mental health', 'health forum', 'forum draw', 'draw validity', 'validity exploratory', 'exploratory finding', 'finding similar', 'similar consistent', 'consistent pattern', 'pattern observe', 'observe indian', 'indian malaysian', 'malaysian filipino', 'filipino user', 'user deeper', 'deeper analysis', 'analysis larger', 'larger dataset', 'dataset likely', 'likely necessary', 'necessary determine', 'determine minority', 'minority community', 'community conclusion', 'conclusion hold', 'hold true', 'true additionally', 'additionally construct', 'construct clinical', 'clinical language', 'language use', 'use commonly', 'commonly used', 'used dsm', 'dsm icd', 'icd framework', 'framework illness', 'illness categorization', 'categorization significant', 'significant limitation', 'limitation particularly', 'particularly country', 'country selected', 'selected example', 'example mental', 'mental health', 'health disorder', 'disorder culturebound', 'culturebound well', 'well mental', 'mental health', 'health language', 'language used', 'used different', 'different way', 'way within', 'within specific', 'specific country', 'country analyze', 'analyze depression', 'depression often', 'often umbrella', 'umbrella term', 'term mental', 'mental illness', 'illness additionally', 'additionally clear', 'clear online', 'online support', 'support community', 'community often', 'often develop', 'develop cultural', 'cultural norm', 'norm language', 'language around', 'around mental', 'mental health', 'health deeper', 'deeper understanding', 'understanding play', 'play talklife', 'talklife cup', 'cup neither', 'neither focus', 'focus within', 'within scope', 'scope work', 'work work', 'work intentionally', 'intentionally use', 'use standard', 'standard clinical', 'clinical medical', 'medical term', 'term mental', 'mental health', 'health disorder', 'disorder analysis', 'analysis clinical', 'clinical language', 'language detailed', 'detailed past', 'past anthropological', 'anthropological research', 'research theorized', 'theorized use', 'use medical', 'medical clinical', 'clinical language', 'language representative', 'representative medicalized', 'medicalized explanatory', 'explanatory model', 'model illness', 'illness frame', 'frame use', 'use language', 'language across', 'across culture', 'culture approximate', 'approximate signifier', 'signifier greater', 'greater awareness', 'awareness presence', 'presence mental', 'mental disorder', 'disorder opposed', 'opposed conceptualizing', 'conceptualizing distress', 'distress stress', 'stress tension', 'tension depression', 'depression analysis', 'analysis strictly', 'strictly analyzed', 'analyzed post', 'post latin', 'latin alphabet', 'alphabet almost', 'almost post', 'post talklife', 'talklife cup', 'cup english', 'english however', 'however malay', 'malay tagalog', 'tagalog commonly', 'commonly written', 'written latin', 'latin script', 'script since', 'since common', 'common user', 'user india', 'india speaker', 'speaker use', 'use romanized', 'romanized version', 'version indian', 'indian language', 'language online', 'online possible', 'possible small', 'small minority', 'minority post', 'post analysis', 'analysis text', 'text different', 'different language', 'language however', 'however confirmed', 'confirmed seeing', 'seeing english', 'english word', 'word used', 'used analysis', 'analysis top', 'top ngrams', 'ngrams among', 'among user', 'user subgroup', 'subgroup clear', 'clear english', 'english predominant', 'predominant language', 'language platform', 'platform though', 'though beyond', 'beyond immediate', 'immediate scope', 'scope work', 'work greater', 'greater analysis', 'analysis nonenglish', 'nonenglish codeswitching', 'codeswitching platform', 'platform could', 'could lead', 'lead deeper', 'deeper understanding', 'understanding impact', 'impact interaction', 'interaction expression', 'expression user', 'user national', 'national identity', 'identity different', 'different language', 'language preference']","['selection criterion data', 'criterion data scope', 'data scope understand', 'scope understand impact', 'understand impact cultural', 'impact cultural difference', 'cultural difference individual', 'difference individual use', 'individual use online', 'use online mental', 'online mental health', 'mental health platform', 'health platform begin', 'platform begin analysis', 'begin analysis creating', 'analysis creating dataset', 'creating dataset user', 'dataset user different', 'user different national', 'different national community', 'national community talklife', 'community talklife support', 'talklife support platform', 'support platform half', 'platform half million', 'half million user', 'million user analysis', 'user analysis due', 'analysis due fact', 'due fact research', 'fact research cscw', 'research cscw mental', 'cscw mental health', 'mental health online', 'health online ha', 'online ha done', 'ha done either', 'done either agnostic', 'either agnostic cultural', 'agnostic cultural context', 'cultural context western', 'context western context', 'western context choose', 'context choose focus', 'choose focus user', 'focus user nonwestern', 'user nonwestern country', 'nonwestern country following', 'country following zhang', 'following zhang et', 'zhang et al', 'et al researcher', 'al researcher located', 'researcher located global', 'located global south', 'global south lived', 'south lived experience', 'lived experience interacting', 'experience interacting health', 'interacting health system', 'health system diverse', 'system diverse explanatory', 'diverse explanatory model', 'explanatory model mental', 'model mental illness', 'mental illness believe', 'illness believe moving', 'believe moving focus', 'moving focus cscw', 'focus cscw cscwadjacent', 'cscw cscwadjacent mental', 'cscwadjacent mental health', 'mental health research', 'health research away', 'research away west', 'away west crucial', 'west crucial better', 'crucial better meet', 'better meet need', 'meet need people', 'need people often', 'people often underserved', 'often underserved medical', 'underserved medical system', 'medical system create', 'system create subgroup', 'create subgroup user', 'subgroup user choose', 'user choose three', 'choose three nonwestern', 'three nonwestern country', 'nonwestern country highest', 'country highest user', 'highest user population', 'user population talklife', 'population talklife india', 'talklife india malaysia', 'india malaysia philippine', 'malaysia philippine guided', 'philippine guided rich', 'guided rich amount', 'rich amount literature', 'amount literature unique', 'literature unique nuance', 'unique nuance mental', 'nuance mental health', 'mental health expression', 'health expression country', 'expression country examine', 'country examine national', 'examine national identity', 'national identity linguistic', 'identity linguistic behaviorbased', 'linguistic behaviorbased difference', 'behaviorbased difference use', 'difference use user', 'use user subgroup', 'user subgroup particular', 'subgroup particular research', 'particular research note', 'research note result', 'note result cultural', 'result cultural norm', 'cultural norm around', 'norm around sharing', 'around sharing distress', 'sharing distress alternative', 'distress alternative conceptualization', 'alternative conceptualization mental', 'conceptualization mental illness', 'mental illness india', 'illness india malaysia', 'india malaysia philippine', 'malaysia philippine symptom', 'philippine symptom often', 'symptom often expressed', 'often expressed somatic', 'expressed somatic religious', 'somatic religious term', 'religious term opposed', 'term opposed traditionally', 'opposed traditionally clinical', 'traditionally clinical psychiatric', 'clinical psychiatric term', 'psychiatric term choose', 'term choose analyze', 'choose analyze subgroup', 'analyze subgroup national', 'subgroup national level', 'national level theoretical', 'level theoretical practical', 'theoretical practical reason', 'practical reason theoretical', 'reason theoretical level', 'theoretical level past', 'level past work', 'past work medical', 'work medical anthropology', 'medical anthropology mental', 'anthropology mental health', 'mental health national', 'health national identity', 'national identity ha', 'identity ha commonly', 'ha commonly used', 'commonly used approximate', 'used approximate level', 'approximate level analysis', 'level analysis cultural', 'analysis cultural identity', 'cultural identity additionally', 'identity additionally practical', 'additionally practical level', 'practical level user', 'level user country', 'user country wa', 'country wa determined', 'wa determined using', 'determined using ip', 'using ip address', 'ip address talklife', 'address talklife shared', 'talklife shared u', 'shared u useranonymized', 'u useranonymized dataset', 'useranonymized dataset inferring', 'dataset inferring precise', 'inferring precise location', 'precise location could', 'location could potentially', 'could potentially compromise', 'potentially compromise user', 'compromise user anonymity', 'user anonymity discussed', 'anonymity discussed past', 'discussed past work', 'past work seem', 'work seem significant', 'seem significant value', 'significant value analysis', 'value analysis cultural', 'analysis cultural difference', 'cultural difference analysis', 'difference analysis national', 'analysis national level', 'national level analyze', 'level analyze data', 'analyze data indian', 'data indian user', 'indian user malaysian', 'user malaysian user', 'malaysian user filipino', 'user filipino user', 'filipino user shown', 'user shown table', 'shown table collectively', 'table collectively refer', 'collectively refer country', 'refer country minority', 'country minority sample', 'minority sample comparison', 'sample comparison set', 'comparison set construct', 'set construct random', 'construct random sample', 'random sample thread', 'sample thread talklife', 'thread talklife refer', 'talklife refer majority', 'refer majority sample', 'majority sample due', 'sample due relative', 'due relative prevalence', 'relative prevalence user', 'prevalence user western', 'user western englishspeaking', 'western englishspeaking country', 'englishspeaking country talklife', 'country talklife thread', 'talklife thread majority', 'thread majority sample', 'majority sample include', 'sample include post', 'include post country', 'post country usa', 'country usa uk', 'usa uk canada', 'uk canada indian', 'canada indian largest', 'indian largest nonwestern', 'largest nonwestern minority', 'nonwestern minority subgroup', 'minority subgroup talklife', 'subgroup talklife data', 'talklife data wa', 'data wa sampled', 'wa sampled may', 'sampled may june', 'may june following', 'june following crossnational', 'following crossnational analysis', 'crossnational analysis see', 'analysis see broader', 'see broader result', 'broader result talklife', 'result talklife generalize', 'talklife generalize differently', 'generalize differently structured', 'differently structured online', 'structured online mental', 'online mental health', 'mental health community', 'health community picked', 'community picked largest', 'picked largest western', 'largest western country', 'western country united', 'country united state', 'united state largest', 'state largest nonwestern', 'largest nonwestern country', 'nonwestern country india', 'country india represented', 'india represented cup', 'represented cup similar', 'cup similar support', 'similar support platform', 'support platform user', 'platform user actively', 'user actively using', 'actively using platform', 'using platform week', 'platform week using', 'week using cup', 'using cup data', 'cup data repeat', 'data repeat analysis', 'repeat analysis testing', 'analysis testing cultural', 'testing cultural difference', 'cultural difference found', 'difference found talklife', 'found talklife sample', 'talklife sample analysis', 'sample analysis provided', 'analysis provided sample', 'provided sample data', 'sample data activity', 'data activity indian', 'activity indian user', 'indian user american', 'user american user', 'american user shown', 'user shown table', 'shown table unlike', 'table unlike sample', 'unlike sample talklife', 'sample talklife user', 'talklife user dataset', 'user dataset random', 'dataset random sample', 'random sample upsampling', 'sample upsampling indian', 'upsampling indian user', 'indian user ensure', 'user ensure data', 'ensure data sufficient', 'data sufficient number', 'sufficient number indian', 'number indian dataset', 'indian dataset like', 'dataset like talklife', 'like talklife indian', 'talklife indian largest', 'indian largest nonwestern', 'largest nonwestern minority', 'nonwestern minority subgroup', 'minority subgroup cup', 'subgroup cup focus', 'cup focus indian', 'focus indian user', 'indian user due', 'user due lack', 'due lack sufficient', 'lack sufficient data', 'sufficient data user', 'data user malaysia', 'user malaysia philippine', 'malaysia philippine data', 'philippine data wa', 'data wa sampled', 'wa sampled march', 'sampled march august', 'march august defining', 'august defining cultural', 'defining cultural identity', 'cultural identity use', 'identity use clinical', 'use clinical language', 'clinical language work', 'language work examine', 'work examine relationship', 'examine relationship cultural', 'relationship cultural identity', 'cultural identity use', 'identity use online', 'use online mental', 'online mental health', 'mental health support', 'health support forum', 'support forum leverage', 'forum leverage tomlinsons', 'leverage tomlinsons definition', 'tomlinsons definition cultural', 'definition cultural identity', 'cultural identity self', 'identity self communal', 'self communal definition', 'communal definition based', 'definition based around', 'based around specific', 'around specific usually', 'specific usually politically', 'usually politically inflected', 'politically inflected differentiation', 'inflected differentiation gender', 'differentiation gender sexuality', 'gender sexuality class', 'sexuality class religion', 'class religion race', 'religion race ethnicity', 'race ethnicity nationality', 'ethnicity nationality particularly', 'nationality particularly looking', 'particularly looking aspect', 'looking aspect modern', 'aspect modern cultural', 'modern cultural identity', 'cultural identity run', 'identity run along', 'run along national', 'along national line', 'national line delineated', 'line delineated hall', 'delineated hall et', 'hall et al', 'et al diverse', 'al diverse amorphous', 'diverse amorphous form', 'amorphous form identity', 'form identity cultural', 'identity cultural identity', 'cultural identity often', 'identity often intersect', 'often intersect interact', 'intersect interact form', 'interact form identity', 'form identity including', 'identity including religious', 'including religious ethnic', 'religious ethnic identity', 'ethnic identity however', 'identity however absence', 'however absence direct', 'absence direct information', 'direct information religious', 'information religious ethnic', 'religious ethnic identity', 'ethnic identity based', 'identity based data', 'based data available', 'data available use', 'available use national', 'use national identity', 'national identity proxy', 'identity proxy cultural', 'proxy cultural identity', 'cultural identity additionally', 'identity additionally following', 'additionally following schlesinger', 'following schlesinger et', 'schlesinger et al', 'et al call', 'al call intersectional', 'call intersectional analysis', 'intersectional analysis method', 'analysis method within', 'method within hci', 'within hci also', 'hci also include', 'also include analysis', 'include analysis adjacent', 'analysis adjacent intersecting', 'adjacent intersecting identity', 'intersecting identity relevant', 'identity relevant including', 'relevant including religious', 'including religious identity', 'religious identity analyze', 'identity analyze clinical', 'analyze clinical language', 'clinical language use', 'language use broader', 'use broader definition', 'broader definition clinical', 'definition clinical language', 'clinical language specific', 'language specific medical', 'specific medical diagnosis', 'medical diagnosis following', 'diagnosis following method', 'following method used', 'method used past', 'used past work', 'past work analyze', 'work analyze antidepressant', 'analyze antidepressant related', 'antidepressant related language', 'related language create', 'language create dataset', 'create dataset clinical', 'dataset clinical mental', 'clinical mental health', 'mental health language', 'health language including', 'language including unigrams', 'including unigrams bigram', 'unigrams bigram trigram', 'bigram trigram list', 'trigram list mental', 'list mental disorder', 'mental disorder defined', 'disorder defined international', 'defined international classification', 'international classification disease', 'classification disease icd', 'disease icd diagnostic', 'icd diagnostic statistical', 'diagnostic statistical manual', 'statistical manual mental', 'manual mental disorder', 'mental disorder dsm', 'disorder dsm also', 'dsm also included', 'also included unigrams', 'included unigrams macmillan', 'unigrams macmillan dictionary', 'macmillan dictionary list', 'dictionary list word', 'list word used', 'word used describe', 'used describe illness', 'describe illness disease', 'illness disease specifically', 'disease specifically mental', 'specifically mental illness', 'mental illness general', 'illness general illness', 'general illness result', 'illness result include', 'result include unigrams', 'include unigrams like', 'unigrams like night', 'like night night', 'night night terror', 'night terror sleep', 'terror sleep sleep', 'sleep sleep disorder', 'sleep disorder often', 'disorder often correlated', 'often correlated specific', 'correlated specific symptom', 'specific symptom mental', 'symptom mental illness', 'mental illness distress', 'illness distress sleep', 'distress sleep issue', 'sleep issue awake', 'issue awake night', 'awake night included', 'night included clinically', 'included clinically common', 'clinically common abbreviation', 'common abbreviation mental', 'abbreviation mental disorder', 'mental disorder ocd', 'disorder ocd obsessive', 'ocd obsessive compulsive', 'obsessive compulsive disorder', 'compulsive disorder bpd', 'disorder bpd borderline', 'bpd borderline personality', 'borderline personality disorder', 'personality disorder shorthand', 'disorder shorthand disorder', 'shorthand disorder commonly', 'disorder commonly used', 'commonly used online', 'used online community', 'online community proana', 'community proana used', 'proana used proeating', 'used proeating disorder', 'proeating disorder community', 'disorder community included', 'community included due', 'included due difficulty', 'due difficulty finding', 'difficulty finding exhaustive', 'finding exhaustive list', 'exhaustive list term', 'list term across', 'term across disorder', 'across disorder choose', 'disorder choose use', 'choose use term', 'use term associated', 'term associated dsm', 'associated dsm icd', 'dsm icd categorized', 'icd categorized disorder', 'categorized disorder result', 'disorder result common', 'result common usage', 'common usage framework', 'usage framework globally', 'framework globally throughout', 'globally throughout analysis', 'throughout analysis varied', 'analysis varied factor', 'varied factor use', 'factor use represent', 'use represent mean', 'represent mean represent', 'mean represent standard', 'represent standard deviation', 'standard deviation constraint', 'deviation constraint limitation', 'constraint limitation tradeoff', 'limitation tradeoff cultural', 'tradeoff cultural identity', 'cultural identity exist', 'identity exist many', 'exist many different', 'many different intersecting', 'different intersecting level', 'intersecting level including', 'level including subculture', 'including subculture subcommunities', 'subculture subcommunities within', 'subcommunities within larger', 'within larger umbrella', 'larger umbrella cultural', 'umbrella cultural identity', 'cultural identity result', 'identity result purpose', 'result purpose analysis', 'purpose analysis adopt', 'analysis adopt constraint', 'adopt constraint order', 'constraint order meaningful', 'order meaningful specific', 'meaningful specific analysis', 'specific analysis one', 'analysis one large', 'one large limiting', 'large limiting constraint', 'limiting constraint chose', 'constraint chose study', 'chose study use', 'study use national', 'use national identity', 'national identity state', 'identity state level', 'state level proxy', 'level proxy cultural', 'proxy cultural identity', 'cultural identity though', 'identity though major', 'though major formative', 'major formative part', 'formative part modern', 'part modern cultural', 'modern cultural identity', 'cultural identity argued', 'identity argued hall', 'argued hall tomlinson', 'hall tomlinson country', 'tomlinson country analyze', 'country analyze incredibly', 'analyze incredibly diverse', 'incredibly diverse many', 'diverse many individual', 'many individual cultural', 'individual cultural identity', 'cultural identity intersect', 'identity intersect diverge', 'intersect diverge greater', 'diverge greater national', 'greater national identity', 'national identity rich', 'identity rich analysis', 'rich analysis form', 'analysis form cultural', 'form cultural identity', 'cultural identity beyond', 'identity beyond scope', 'beyond scope work', 'scope work could', 'work could lead', 'could lead richer', 'lead richer conclusion', 'richer conclusion nature', 'conclusion nature cultural', 'nature cultural identity', 'cultural identity online', 'identity online mental', 'online mental health', 'mental health support', 'health support community', 'support community particularly', 'community particularly regard', 'particularly regard cultural', 'regard cultural difference', 'cultural difference user', 'difference user national', 'user national identity', 'national identity additionally', 'identity additionally stay', 'additionally stay consistent', 'stay consistent analysis', 'consistent analysis result', 'analysis result lack', 'result lack data', 'lack data user', 'data user malaysia', 'user malaysia philippine', 'malaysia philippine analyze', 'philippine analyze user', 'analyze user india', 'user india cup', 'india cup extend', 'cup extend finding', 'extend finding experience', 'finding experience part', 'experience part minority', 'part minority group', 'minority group online', 'group online mental', 'online mental health', 'mental health forum', 'health forum draw', 'forum draw validity', 'draw validity exploratory', 'validity exploratory finding', 'exploratory finding similar', 'finding similar consistent', 'similar consistent pattern', 'consistent pattern observe', 'pattern observe indian', 'observe indian malaysian', 'indian malaysian filipino', 'malaysian filipino user', 'filipino user deeper', 'user deeper analysis', 'deeper analysis larger', 'analysis larger dataset', 'larger dataset likely', 'dataset likely necessary', 'likely necessary determine', 'necessary determine minority', 'determine minority community', 'minority community conclusion', 'community conclusion hold', 'conclusion hold true', 'hold true additionally', 'true additionally construct', 'additionally construct clinical', 'construct clinical language', 'clinical language use', 'language use commonly', 'use commonly used', 'commonly used dsm', 'used dsm icd', 'dsm icd framework', 'icd framework illness', 'framework illness categorization', 'illness categorization significant', 'categorization significant limitation', 'significant limitation particularly', 'limitation particularly country', 'particularly country selected', 'country selected example', 'selected example mental', 'example mental health', 'mental health disorder', 'health disorder culturebound', 'disorder culturebound well', 'culturebound well mental', 'well mental health', 'mental health language', 'health language used', 'language used different', 'used different way', 'different way within', 'way within specific', 'within specific country', 'specific country analyze', 'country analyze depression', 'analyze depression often', 'depression often umbrella', 'often umbrella term', 'umbrella term mental', 'term mental illness', 'mental illness additionally', 'illness additionally clear', 'additionally clear online', 'clear online support', 'online support community', 'support community often', 'community often develop', 'often develop cultural', 'develop cultural norm', 'cultural norm language', 'norm language around', 'language around mental', 'around mental health', 'mental health deeper', 'health deeper understanding', 'deeper understanding play', 'understanding play talklife', 'play talklife cup', 'talklife cup neither', 'cup neither focus', 'neither focus within', 'focus within scope', 'within scope work', 'scope work work', 'work work intentionally', 'work intentionally use', 'intentionally use standard', 'use standard clinical', 'standard clinical medical', 'clinical medical term', 'medical term mental', 'term mental health', 'mental health disorder', 'health disorder analysis', 'disorder analysis clinical', 'analysis clinical language', 'clinical language detailed', 'language detailed past', 'detailed past anthropological', 'past anthropological research', 'anthropological research theorized', 'research theorized use', 'theorized use medical', 'use medical clinical', 'medical clinical language', 'clinical language representative', 'language representative medicalized', 'representative medicalized explanatory', 'medicalized explanatory model', 'explanatory model illness', 'model illness frame', 'illness frame use', 'frame use language', 'use language across', 'language across culture', 'across culture approximate', 'culture approximate signifier', 'approximate signifier greater', 'signifier greater awareness', 'greater awareness presence', 'awareness presence mental', 'presence mental disorder', 'mental disorder opposed', 'disorder opposed conceptualizing', 'opposed conceptualizing distress', 'conceptualizing distress stress', 'distress stress tension', 'stress tension depression', 'tension depression analysis', 'depression analysis strictly', 'analysis strictly analyzed', 'strictly analyzed post', 'analyzed post latin', 'post latin alphabet', 'latin alphabet almost', 'alphabet almost post', 'almost post talklife', 'post talklife cup', 'talklife cup english', 'cup english however', 'english however malay', 'however malay tagalog', 'malay tagalog commonly', 'tagalog commonly written', 'commonly written latin', 'written latin script', 'latin script since', 'script since common', 'since common user', 'common user india', 'user india speaker', 'india speaker use', 'speaker use romanized', 'use romanized version', 'romanized version indian', 'version indian language', 'indian language online', 'language online possible', 'online possible small', 'possible small minority', 'small minority post', 'minority post analysis', 'post analysis text', 'analysis text different', 'text different language', 'different language however', 'language however confirmed', 'however confirmed seeing', 'confirmed seeing english', 'seeing english word', 'english word used', 'word used analysis', 'used analysis top', 'analysis top ngrams', 'top ngrams among', 'ngrams among user', 'among user subgroup', 'user subgroup clear', 'subgroup clear english', 'clear english predominant', 'english predominant language', 'predominant language platform', 'language platform though', 'platform though beyond', 'though beyond immediate', 'beyond immediate scope', 'immediate scope work', 'scope work greater', 'work greater analysis', 'greater analysis nonenglish', 'analysis nonenglish codeswitching', 'nonenglish codeswitching platform', 'codeswitching platform could', 'platform could lead', 'could lead deeper', 'lead deeper understanding', 'deeper understanding impact', 'understanding impact interaction', 'impact interaction expression', 'interaction expression user', 'expression user national', 'user national identity', 'national identity different', 'identity different language', 'different language preference']"
https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-018-2197-z,1,Many topic modeling algorithms exist including latent semantic indexing latent Dirichlet allocation and non-negative matrix factorization. In this work we turn our attention to Word2Vec which has been argued to have many advantages over these earlier algorithms [19 20]. Word2Vec describes two implementations of a shallow neural network the continuous bag of words (CBOW) model and the skip-gram model. We focus on the skip-gram model in this work which learns vector representations of words by predicting neighboring words in a text. See Fig. 2.,many topic modeling algorithm exist including latent semantic indexing latent dirichlet allocation and nonnegative matrix factorization in this work we turn our attention to wordvec which ha been argued to have many advantage over these earlier algorithm wordvec describes two implementation of a shallow neural network the continuous bag of word cbow model and the skipgram model we focus on the skipgram model in this work which learns vector representation of word by predicting neighboring word in a text see fig,"['many', 'topic', 'modeling', 'algorithm', 'exist', 'including', 'latent', 'semantic', 'indexing', 'latent', 'dirichlet', 'allocation', 'nonnegative', 'matrix', 'factorization', 'work', 'turn', 'attention', 'wordvec', 'ha', 'argued', 'many', 'advantage', 'earlier', 'algorithm', 'wordvec', 'describes', 'two', 'implementation', 'shallow', 'neural', 'network', 'continuous', 'bag', 'word', 'cbow', 'model', 'skipgram', 'model', 'focus', 'skipgram', 'model', 'work', 'learns', 'vector', 'representation', 'word', 'predicting', 'neighboring', 'word', 'text', 'see', 'fig']","['many topic', 'topic modeling', 'modeling algorithm', 'algorithm exist', 'exist including', 'including latent', 'latent semantic', 'semantic indexing', 'indexing latent', 'latent dirichlet', 'dirichlet allocation', 'allocation nonnegative', 'nonnegative matrix', 'matrix factorization', 'factorization work', 'work turn', 'turn attention', 'attention wordvec', 'wordvec ha', 'ha argued', 'argued many', 'many advantage', 'advantage earlier', 'earlier algorithm', 'algorithm wordvec', 'wordvec describes', 'describes two', 'two implementation', 'implementation shallow', 'shallow neural', 'neural network', 'network continuous', 'continuous bag', 'bag word', 'word cbow', 'cbow model', 'model skipgram', 'skipgram model', 'model focus', 'focus skipgram', 'skipgram model', 'model work', 'work learns', 'learns vector', 'vector representation', 'representation word', 'word predicting', 'predicting neighboring', 'neighboring word', 'word text', 'text see', 'see fig']","['many topic modeling', 'topic modeling algorithm', 'modeling algorithm exist', 'algorithm exist including', 'exist including latent', 'including latent semantic', 'latent semantic indexing', 'semantic indexing latent', 'indexing latent dirichlet', 'latent dirichlet allocation', 'dirichlet allocation nonnegative', 'allocation nonnegative matrix', 'nonnegative matrix factorization', 'matrix factorization work', 'factorization work turn', 'work turn attention', 'turn attention wordvec', 'attention wordvec ha', 'wordvec ha argued', 'ha argued many', 'argued many advantage', 'many advantage earlier', 'advantage earlier algorithm', 'earlier algorithm wordvec', 'algorithm wordvec describes', 'wordvec describes two', 'describes two implementation', 'two implementation shallow', 'implementation shallow neural', 'shallow neural network', 'neural network continuous', 'network continuous bag', 'continuous bag word', 'bag word cbow', 'word cbow model', 'cbow model skipgram', 'model skipgram model', 'skipgram model focus', 'model focus skipgram', 'focus skipgram model', 'skipgram model work', 'model work learns', 'work learns vector', 'learns vector representation', 'vector representation word', 'representation word predicting', 'word predicting neighboring', 'predicting neighboring word', 'neighboring word text', 'word text see', 'text see fig']"
https://ieeexplore.ieee.org/abstract/document/8609647,0,RMN is a recursive neural network designed to model relationships between pairs of entities from text [13]. Each relationship is represented at a given point in time as a vector of weights over K descriptors. Entities that form a relationship do not need to be of the same class: for example in this work we model the relationships between a user and the community in which she interacts. Each post or comment corresponds to a different instant. Words are represented as embeddings of dimension P that is each word w of a vocabulary V is a vector in RP . Users and communities are represented by embeddings of dimension U and C respectively. As in previous works we generated embeddings using GloVe [15]. The descriptors obtained from RMN are vectors in RP  allowing us to find the closest words to each descriptor. The post’s (or comment’s) representation is denoted by vpost ∈ RP . This vector is the average of the word embeddings contained in the post. The representations of users and communities are denoted by vuser and vcomm. For each post or comment RMN takes as input a vector v ∈ RP +U+C obtained by concatenating vpost vuser and vcomm. These vectors are combined through the weights of the neural network to obtain a representation dt ∈ RK of the relationship between the user and the community at that particular time. RMN uses a smoothing parameter α ∈ (0 1) to avoid abrupt changes in the representation of the same relation in consecutive instants dt and dt−1. The descriptor array R ∈ RK×P is used for attempting to reconstruct the post vpost by making rt = Rdt. RMN parameters (weights and matrix of descriptors) are trained in order to maximize an objective function that aims to approximate rt and vpost while retaining some distance between rt and other randomly sampled posts. See [13] for more details on RMN. The input data for RMN was preprocessed as follows. First we removed all posts and comments marked as [deleted] or [removed] standard stop-words (using the NLTK library) punctuation and accents. Second for each subreddit we removed posts and comments from users who performed less than 50 activities (posts or comments) following the methodology presented in [16]. Finally we selected the words that appear at least once in each of the four subreddits analyzed seeking to find similarities in the way people express themselves when discussing mental health disorders. The final subset of data analyzed by RMN is composed of 18020 unique words 25101 posts and 401428 comments.,rmn is a recursive neural network designed to model relationship between pair of entity from text each relationship is represented at a given point in time a a vector of weight over k descriptor entity that form a relationship do not need to be of the same class for example in this work we model the relationship between a user and the community in which she interacts each post or comment corresponds to a different instant word are represented a embeddings of dimension p that is each word w of a vocabulary v is a vector in rp user and community are represented by embeddings of dimension u and c respectively a in previous work we generated embeddings using glove the descriptor obtained from rmn are vector in rp allowing u to find the closest word to each descriptor the post or comment representation is denoted by vpost rp this vector is the average of the word embeddings contained in the post the representation of user and community are denoted by vuser and vcomm for each post or comment rmn take a input a vector v rp uc obtained by concatenating vpost vuser and vcomm these vector are combined through the weight of the neural network to obtain a representation dt rk of the relationship between the user and the community at that particular time rmn us a smoothing parameter to avoid abrupt change in the representation of the same relation in consecutive instant dt and dt the descriptor array r rkp is used for attempting to reconstruct the post vpost by making rt rdt rmn parameter weight and matrix of descriptor are trained in order to maximize an objective function that aim to approximate rt and vpost while retaining some distance between rt and other randomly sampled post see for more detail on rmn the input data for rmn wa preprocessed a follows first we removed all post and comment marked a deleted or removed standard stopwords using the nltk library punctuation and accent second for each subreddit we removed post and comment from user who performed le than activity post or comment following the methodology presented in finally we selected the word that appear at least once in each of the four subreddits analyzed seeking to find similarity in the way people express themselves when discussing mental health disorder the final subset of data analyzed by rmn is composed of unique word post and comment,"['rmn', 'recursive', 'neural', 'network', 'designed', 'model', 'relationship', 'pair', 'entity', 'text', 'relationship', 'represented', 'given', 'point', 'time', 'vector', 'weight', 'k', 'descriptor', 'entity', 'form', 'relationship', 'need', 'class', 'example', 'work', 'model', 'relationship', 'user', 'community', 'interacts', 'post', 'comment', 'corresponds', 'different', 'instant', 'word', 'represented', 'embeddings', 'dimension', 'p', 'word', 'w', 'vocabulary', 'v', 'vector', 'rp', 'user', 'community', 'represented', 'embeddings', 'dimension', 'u', 'c', 'respectively', 'previous', 'work', 'generated', 'embeddings', 'using', 'glove', 'descriptor', 'obtained', 'rmn', 'vector', 'rp', 'allowing', 'u', 'find', 'closest', 'word', 'descriptor', 'post', 'comment', 'representation', 'denoted', 'vpost', 'rp', 'vector', 'average', 'word', 'embeddings', 'contained', 'post', 'representation', 'user', 'community', 'denoted', 'vuser', 'vcomm', 'post', 'comment', 'rmn', 'take', 'input', 'vector', 'v', 'rp', 'uc', 'obtained', 'concatenating', 'vpost', 'vuser', 'vcomm', 'vector', 'combined', 'weight', 'neural', 'network', 'obtain', 'representation', 'dt', 'rk', 'relationship', 'user', 'community', 'particular', 'time', 'rmn', 'us', 'smoothing', 'parameter', 'avoid', 'abrupt', 'change', 'representation', 'relation', 'consecutive', 'instant', 'dt', 'dt', 'descriptor', 'array', 'r', 'rkp', 'used', 'attempting', 'reconstruct', 'post', 'vpost', 'making', 'rt', 'rdt', 'rmn', 'parameter', 'weight', 'matrix', 'descriptor', 'trained', 'order', 'maximize', 'objective', 'function', 'aim', 'approximate', 'rt', 'vpost', 'retaining', 'distance', 'rt', 'randomly', 'sampled', 'post', 'see', 'detail', 'rmn', 'input', 'data', 'rmn', 'wa', 'preprocessed', 'follows', 'first', 'removed', 'post', 'comment', 'marked', 'deleted', 'removed', 'standard', 'stopwords', 'using', 'nltk', 'library', 'punctuation', 'accent', 'second', 'subreddit', 'removed', 'post', 'comment', 'user', 'performed', 'le', 'activity', 'post', 'comment', 'following', 'methodology', 'presented', 'finally', 'selected', 'word', 'appear', 'least', 'four', 'subreddits', 'analyzed', 'seeking', 'find', 'similarity', 'way', 'people', 'express', 'discussing', 'mental', 'health', 'disorder', 'final', 'subset', 'data', 'analyzed', 'rmn', 'composed', 'unique', 'word', 'post', 'comment']","['rmn recursive', 'recursive neural', 'neural network', 'network designed', 'designed model', 'model relationship', 'relationship pair', 'pair entity', 'entity text', 'text relationship', 'relationship represented', 'represented given', 'given point', 'point time', 'time vector', 'vector weight', 'weight k', 'k descriptor', 'descriptor entity', 'entity form', 'form relationship', 'relationship need', 'need class', 'class example', 'example work', 'work model', 'model relationship', 'relationship user', 'user community', 'community interacts', 'interacts post', 'post comment', 'comment corresponds', 'corresponds different', 'different instant', 'instant word', 'word represented', 'represented embeddings', 'embeddings dimension', 'dimension p', 'p word', 'word w', 'w vocabulary', 'vocabulary v', 'v vector', 'vector rp', 'rp user', 'user community', 'community represented', 'represented embeddings', 'embeddings dimension', 'dimension u', 'u c', 'c respectively', 'respectively previous', 'previous work', 'work generated', 'generated embeddings', 'embeddings using', 'using glove', 'glove descriptor', 'descriptor obtained', 'obtained rmn', 'rmn vector', 'vector rp', 'rp allowing', 'allowing u', 'u find', 'find closest', 'closest word', 'word descriptor', 'descriptor post', 'post comment', 'comment representation', 'representation denoted', 'denoted vpost', 'vpost rp', 'rp vector', 'vector average', 'average word', 'word embeddings', 'embeddings contained', 'contained post', 'post representation', 'representation user', 'user community', 'community denoted', 'denoted vuser', 'vuser vcomm', 'vcomm post', 'post comment', 'comment rmn', 'rmn take', 'take input', 'input vector', 'vector v', 'v rp', 'rp uc', 'uc obtained', 'obtained concatenating', 'concatenating vpost', 'vpost vuser', 'vuser vcomm', 'vcomm vector', 'vector combined', 'combined weight', 'weight neural', 'neural network', 'network obtain', 'obtain representation', 'representation dt', 'dt rk', 'rk relationship', 'relationship user', 'user community', 'community particular', 'particular time', 'time rmn', 'rmn us', 'us smoothing', 'smoothing parameter', 'parameter avoid', 'avoid abrupt', 'abrupt change', 'change representation', 'representation relation', 'relation consecutive', 'consecutive instant', 'instant dt', 'dt dt', 'dt descriptor', 'descriptor array', 'array r', 'r rkp', 'rkp used', 'used attempting', 'attempting reconstruct', 'reconstruct post', 'post vpost', 'vpost making', 'making rt', 'rt rdt', 'rdt rmn', 'rmn parameter', 'parameter weight', 'weight matrix', 'matrix descriptor', 'descriptor trained', 'trained order', 'order maximize', 'maximize objective', 'objective function', 'function aim', 'aim approximate', 'approximate rt', 'rt vpost', 'vpost retaining', 'retaining distance', 'distance rt', 'rt randomly', 'randomly sampled', 'sampled post', 'post see', 'see detail', 'detail rmn', 'rmn input', 'input data', 'data rmn', 'rmn wa', 'wa preprocessed', 'preprocessed follows', 'follows first', 'first removed', 'removed post', 'post comment', 'comment marked', 'marked deleted', 'deleted removed', 'removed standard', 'standard stopwords', 'stopwords using', 'using nltk', 'nltk library', 'library punctuation', 'punctuation accent', 'accent second', 'second subreddit', 'subreddit removed', 'removed post', 'post comment', 'comment user', 'user performed', 'performed le', 'le activity', 'activity post', 'post comment', 'comment following', 'following methodology', 'methodology presented', 'presented finally', 'finally selected', 'selected word', 'word appear', 'appear least', 'least four', 'four subreddits', 'subreddits analyzed', 'analyzed seeking', 'seeking find', 'find similarity', 'similarity way', 'way people', 'people express', 'express discussing', 'discussing mental', 'mental health', 'health disorder', 'disorder final', 'final subset', 'subset data', 'data analyzed', 'analyzed rmn', 'rmn composed', 'composed unique', 'unique word', 'word post', 'post comment']","['rmn recursive neural', 'recursive neural network', 'neural network designed', 'network designed model', 'designed model relationship', 'model relationship pair', 'relationship pair entity', 'pair entity text', 'entity text relationship', 'text relationship represented', 'relationship represented given', 'represented given point', 'given point time', 'point time vector', 'time vector weight', 'vector weight k', 'weight k descriptor', 'k descriptor entity', 'descriptor entity form', 'entity form relationship', 'form relationship need', 'relationship need class', 'need class example', 'class example work', 'example work model', 'work model relationship', 'model relationship user', 'relationship user community', 'user community interacts', 'community interacts post', 'interacts post comment', 'post comment corresponds', 'comment corresponds different', 'corresponds different instant', 'different instant word', 'instant word represented', 'word represented embeddings', 'represented embeddings dimension', 'embeddings dimension p', 'dimension p word', 'p word w', 'word w vocabulary', 'w vocabulary v', 'vocabulary v vector', 'v vector rp', 'vector rp user', 'rp user community', 'user community represented', 'community represented embeddings', 'represented embeddings dimension', 'embeddings dimension u', 'dimension u c', 'u c respectively', 'c respectively previous', 'respectively previous work', 'previous work generated', 'work generated embeddings', 'generated embeddings using', 'embeddings using glove', 'using glove descriptor', 'glove descriptor obtained', 'descriptor obtained rmn', 'obtained rmn vector', 'rmn vector rp', 'vector rp allowing', 'rp allowing u', 'allowing u find', 'u find closest', 'find closest word', 'closest word descriptor', 'word descriptor post', 'descriptor post comment', 'post comment representation', 'comment representation denoted', 'representation denoted vpost', 'denoted vpost rp', 'vpost rp vector', 'rp vector average', 'vector average word', 'average word embeddings', 'word embeddings contained', 'embeddings contained post', 'contained post representation', 'post representation user', 'representation user community', 'user community denoted', 'community denoted vuser', 'denoted vuser vcomm', 'vuser vcomm post', 'vcomm post comment', 'post comment rmn', 'comment rmn take', 'rmn take input', 'take input vector', 'input vector v', 'vector v rp', 'v rp uc', 'rp uc obtained', 'uc obtained concatenating', 'obtained concatenating vpost', 'concatenating vpost vuser', 'vpost vuser vcomm', 'vuser vcomm vector', 'vcomm vector combined', 'vector combined weight', 'combined weight neural', 'weight neural network', 'neural network obtain', 'network obtain representation', 'obtain representation dt', 'representation dt rk', 'dt rk relationship', 'rk relationship user', 'relationship user community', 'user community particular', 'community particular time', 'particular time rmn', 'time rmn us', 'rmn us smoothing', 'us smoothing parameter', 'smoothing parameter avoid', 'parameter avoid abrupt', 'avoid abrupt change', 'abrupt change representation', 'change representation relation', 'representation relation consecutive', 'relation consecutive instant', 'consecutive instant dt', 'instant dt dt', 'dt dt descriptor', 'dt descriptor array', 'descriptor array r', 'array r rkp', 'r rkp used', 'rkp used attempting', 'used attempting reconstruct', 'attempting reconstruct post', 'reconstruct post vpost', 'post vpost making', 'vpost making rt', 'making rt rdt', 'rt rdt rmn', 'rdt rmn parameter', 'rmn parameter weight', 'parameter weight matrix', 'weight matrix descriptor', 'matrix descriptor trained', 'descriptor trained order', 'trained order maximize', 'order maximize objective', 'maximize objective function', 'objective function aim', 'function aim approximate', 'aim approximate rt', 'approximate rt vpost', 'rt vpost retaining', 'vpost retaining distance', 'retaining distance rt', 'distance rt randomly', 'rt randomly sampled', 'randomly sampled post', 'sampled post see', 'post see detail', 'see detail rmn', 'detail rmn input', 'rmn input data', 'input data rmn', 'data rmn wa', 'rmn wa preprocessed', 'wa preprocessed follows', 'preprocessed follows first', 'follows first removed', 'first removed post', 'removed post comment', 'post comment marked', 'comment marked deleted', 'marked deleted removed', 'deleted removed standard', 'removed standard stopwords', 'standard stopwords using', 'stopwords using nltk', 'using nltk library', 'nltk library punctuation', 'library punctuation accent', 'punctuation accent second', 'accent second subreddit', 'second subreddit removed', 'subreddit removed post', 'removed post comment', 'post comment user', 'comment user performed', 'user performed le', 'performed le activity', 'le activity post', 'activity post comment', 'post comment following', 'comment following methodology', 'following methodology presented', 'methodology presented finally', 'presented finally selected', 'finally selected word', 'selected word appear', 'word appear least', 'appear least four', 'least four subreddits', 'four subreddits analyzed', 'subreddits analyzed seeking', 'analyzed seeking find', 'seeking find similarity', 'find similarity way', 'similarity way people', 'way people express', 'people express discussing', 'express discussing mental', 'discussing mental health', 'mental health disorder', 'health disorder final', 'disorder final subset', 'final subset data', 'subset data analyzed', 'data analyzed rmn', 'analyzed rmn composed', 'rmn composed unique', 'composed unique word', 'unique word post', 'word post comment']"
