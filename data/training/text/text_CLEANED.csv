Link to paper,Score,Text,Cleaned,unigrams,bigrams,trigrams
https://link.springer.com/content/pdf/10.1140/epjds/s13688-017-0110-z.pdf,0,Several different types of information were extracted from the collected Instagram data. We used total posts per user per day as a measure of user activity. We gauged community reaction by counting the number of comments and ‘likes’ each posted photograph received. Face detection software was used to determine whether or not a photograph contained a human face as well as count the total number of faces in each photo as a proxy measure for participants’ social activity levels. Pixel-level averages were computed for Hue Saturation and Value (HSV) three color properties commonly used in image analysis. Hue describes an image’s coloring on the light spectrum (ranging from red to blue/purple). Lower hue values indicate more red and higher hue values indicate more blue. Saturation refers to the vividness of an image. Low saturation makes an image appear grey and faded. Value refers to image brightness. Lower brightness scores indicate a darker image. See Figure  for a comparison of high and low HSV values. We also checked metadata to assess whether an Instagram-provided filter was applied to alter the appearance of a photograph. Collectively these measures served as the feature set in our primary model. For the separate model fit on ratings data we used only the four ratings categories (happy sad likable interesting) as predictors.,several different type of information were extracted from the collected instagram data we used total post per user per day a a measure of user activity we gauged community reaction by counting the number of comment and like each posted photograph received face detection software wa used to determine whether or not a photograph contained a human face a well a count the total number of face in each photo a a proxy measure for participant social activity level pixellevel average were computed for hue saturation and value hsv three color property commonly used in image analysis hue describes an image coloring on the light spectrum ranging from red to bluepurple lower hue value indicate more red and higher hue value indicate more blue saturation refers to the vividness of an image low saturation make an image appear grey and faded value refers to image brightness lower brightness score indicate a darker image see figure for a comparison of high and low hsv value we also checked metadata to ass whether an instagramprovided filter wa applied to alter the appearance of a photograph collectively these measure served a the feature set in our primary model for the separate model fit on rating data we used only the four rating category happy sad likable interesting a predictor,"['several', 'different', 'type', 'information', 'extracted', 'collected', 'instagram', 'total', 'per', 'per', 'day', 'measure', 'activity', 'gauged', 'community', 'reaction', 'counting', 'number', 'comment', 'like', 'posted', 'photograph', 'received', 'face', 'detection', 'software', 'determine', 'whether', 'photograph', 'contained', 'human', 'face', 'well', 'count', 'total', 'number', 'face', 'photo', 'proxy', 'measure', 'social', 'activity', 'level', 'pixellevel', 'average', 'computed', 'hue', 'saturation', 'value', 'hsv', 'three', 'color', 'property', 'commonly', 'image', 'analysis', 'hue', 'describes', 'image', 'coloring', 'light', 'spectrum', 'ranging', 'red', 'bluepurple', 'lower', 'hue', 'value', 'indicate', 'red', 'higher', 'hue', 'value', 'indicate', 'blue', 'saturation', 'refers', 'vividness', 'image', 'low', 'saturation', 'make', 'image', 'appear', 'grey', 'faded', 'value', 'refers', 'image', 'brightness', 'lower', 'brightness', 'score', 'indicate', 'darker', 'image', 'see', 'figure', 'comparison', 'high', 'low', 'hsv', 'value', 'also', 'checked', 'metadata', 'ass', 'whether', 'instagramprovided', 'filter', 'applied', 'alter', 'appearance', 'photograph', 'collectively', 'measure', 'served', 'feature', 'set', 'primary', 'model', 'separate', 'model', 'fit', 'rating', 'four', 'rating', 'category', 'happy', 'sad', 'likable', 'interesting', 'predictor']","['several different', 'different type', 'type information', 'information extracted', 'extracted collected', 'collected instagram', 'instagram total', 'total per', 'per per', 'per day', 'day measure', 'measure activity', 'activity gauged', 'gauged community', 'community reaction', 'reaction counting', 'counting number', 'number comment', 'comment like', 'like posted', 'posted photograph', 'photograph received', 'received face', 'face detection', 'detection software', 'software determine', 'determine whether', 'whether photograph', 'photograph contained', 'contained human', 'human face', 'face well', 'well count', 'count total', 'total number', 'number face', 'face photo', 'photo proxy', 'proxy measure', 'measure social', 'social activity', 'activity level', 'level pixellevel', 'pixellevel average', 'average computed', 'computed hue', 'hue saturation', 'saturation value', 'value hsv', 'hsv three', 'three color', 'color property', 'property commonly', 'commonly image', 'image analysis', 'analysis hue', 'hue describes', 'describes image', 'image coloring', 'coloring light', 'light spectrum', 'spectrum ranging', 'ranging red', 'red bluepurple', 'bluepurple lower', 'lower hue', 'hue value', 'value indicate', 'indicate red', 'red higher', 'higher hue', 'hue value', 'value indicate', 'indicate blue', 'blue saturation', 'saturation refers', 'refers vividness', 'vividness image', 'image low', 'low saturation', 'saturation make', 'make image', 'image appear', 'appear grey', 'grey faded', 'faded value', 'value refers', 'refers image', 'image brightness', 'brightness lower', 'lower brightness', 'brightness score', 'score indicate', 'indicate darker', 'darker image', 'image see', 'see figure', 'figure comparison', 'comparison high', 'high low', 'low hsv', 'hsv value', 'value also', 'also checked', 'checked metadata', 'metadata ass', 'ass whether', 'whether instagramprovided', 'instagramprovided filter', 'filter applied', 'applied alter', 'alter appearance', 'appearance photograph', 'photograph collectively', 'collectively measure', 'measure served', 'served feature', 'feature set', 'set primary', 'primary model', 'model separate', 'separate model', 'model fit', 'fit rating', 'rating four', 'four rating', 'rating category', 'category happy', 'happy sad', 'sad likable', 'likable interesting', 'interesting predictor']","['several different type', 'different type information', 'type information extracted', 'information extracted collected', 'extracted collected instagram', 'collected instagram total', 'instagram total per', 'total per per', 'per per day', 'per day measure', 'day measure activity', 'measure activity gauged', 'activity gauged community', 'gauged community reaction', 'community reaction counting', 'reaction counting number', 'counting number comment', 'number comment like', 'comment like posted', 'like posted photograph', 'posted photograph received', 'photograph received face', 'received face detection', 'face detection software', 'detection software determine', 'software determine whether', 'determine whether photograph', 'whether photograph contained', 'photograph contained human', 'contained human face', 'human face well', 'face well count', 'well count total', 'count total number', 'total number face', 'number face photo', 'face photo proxy', 'photo proxy measure', 'proxy measure social', 'measure social activity', 'social activity level', 'activity level pixellevel', 'level pixellevel average', 'pixellevel average computed', 'average computed hue', 'computed hue saturation', 'hue saturation value', 'saturation value hsv', 'value hsv three', 'hsv three color', 'three color property', 'color property commonly', 'property commonly image', 'commonly image analysis', 'image analysis hue', 'analysis hue describes', 'hue describes image', 'describes image coloring', 'image coloring light', 'coloring light spectrum', 'light spectrum ranging', 'spectrum ranging red', 'ranging red bluepurple', 'red bluepurple lower', 'bluepurple lower hue', 'lower hue value', 'hue value indicate', 'value indicate red', 'indicate red higher', 'red higher hue', 'higher hue value', 'hue value indicate', 'value indicate blue', 'indicate blue saturation', 'blue saturation refers', 'saturation refers vividness', 'refers vividness image', 'vividness image low', 'image low saturation', 'low saturation make', 'saturation make image', 'make image appear', 'image appear grey', 'appear grey faded', 'grey faded value', 'faded value refers', 'value refers image', 'refers image brightness', 'image brightness lower', 'brightness lower brightness', 'lower brightness score', 'brightness score indicate', 'score indicate darker', 'indicate darker image', 'darker image see', 'image see figure', 'see figure comparison', 'figure comparison high', 'comparison high low', 'high low hsv', 'low hsv value', 'hsv value also', 'value also checked', 'also checked metadata', 'checked metadata ass', 'metadata ass whether', 'ass whether instagramprovided', 'whether instagramprovided filter', 'instagramprovided filter applied', 'filter applied alter', 'applied alter appearance', 'alter appearance photograph', 'appearance photograph collectively', 'photograph collectively measure', 'collectively measure served', 'measure served feature', 'served feature set', 'feature set primary', 'set primary model', 'primary model separate', 'model separate model', 'separate model fit', 'model fit rating', 'fit rating four', 'rating four rating', 'four rating category', 'rating category happy', 'category happy sad', 'happy sad likable', 'sad likable interesting', 'likable interesting predictor']"
https://dl.acm.org/doi/abs/10.1145/2858036.2858207 ,0,Our first set of methods include developing three sets of measures spanning: linguistic structure interpersonal awareness and interaction. The choice of these measures is motivated by literature that examines associations between the behavioral expression of individuals and their responses to crises including vulnerability due to mental illness [13 23]. Each of these measure categories consists of the following variables: Linguistic Structure. For this measure we compute the fraction of nouns verbs2  and adverbs in posts and comments; automated readability index a measure to gauge the understandability of text [62]; and linguistic accommodation a process by which individuals in a conversation adjust their language style according to that of others [20]3 . Together these variables characterize the text shared by the user classes beyond their informational content. Per literature in psycholinguistics such structure is known to relate to an individual’s underlying psychological and cognitive state and can reveal cues about their social coordination [54]. Interpersonal Awareness. This measure category includes: proportion of first person singular (indicating pre-occupation with self) first person plural (indicating collective attention) second person and third person pronouns (indicating social interactivity and reference to people or objects in the environment). Literature has indicated that pronoun use can quantify an individual’s self and social awareness and can reveal mental well-being including that manifested in social media [21]. Interaction. Variables corresponding to this measure category include: volume of posts and comments authored post length length of comments authored volume of comments received on shared posts length of comments received mean vote difference (difference between upvotes and downvotes on posts authored) and response velocity (in minutes) given by the time elapsed between the first comment and the time the corresponding post was shared,our first set of method include developing three set of measure spanning linguistic structure interpersonal awareness and interaction the choice of these measure is motivated by literature that examines association between the behavioral expression of individual and their response to crisis including vulnerability due to mental illness each of these measure category consists of the following variable linguistic structure for this measure we compute the fraction of noun verb and adverb in post and comment automated readability index a measure to gauge the understandability of text and linguistic accommodation a process by which individual in a conversation adjust their language style according to that of others together these variable characterize the text shared by the user class beyond their informational content per literature in psycholinguistics such structure is known to relate to an individual underlying psychological and cognitive state and can reveal cue about their social coordination interpersonal awareness this measure category includes proportion of first person singular indicating preoccupation with self first person plural indicating collective attention second person and third person pronoun indicating social interactivity and reference to people or object in the environment literature ha indicated that pronoun use can quantify an individual self and social awareness and can reveal mental wellbeing including that manifested in social medium interaction variable corresponding to this measure category include volume of post and comment authored post length length of comment authored volume of comment received on shared post length of comment received mean vote difference difference between upvotes and downvotes on post authored and response velocity in minute given by the time elapsed between the first comment and the time the corresponding post wa shared,"['first', 'set', 'method', 'include', 'developing', 'three', 'set', 'measure', 'spanning', 'linguistic', 'structure', 'interpersonal', 'awareness', 'interaction', 'choice', 'measure', 'motivated', 'literature', 'examines', 'association', 'behavioral', 'expression', 'individual', 'response', 'crisis', 'including', 'vulnerability', 'due', 'illness', 'measure', 'category', 'consists', 'following', 'variable', 'linguistic', 'structure', 'measure', 'compute', 'fraction', 'noun', 'verb', 'adverb', 'comment', 'automated', 'readability', 'index', 'measure', 'gauge', 'understandability', 'text', 'linguistic', 'accommodation', 'process', 'individual', 'conversation', 'adjust', 'language', 'style', 'according', 'others', 'together', 'variable', 'characterize', 'text', 'shared', 'class', 'beyond', 'informational', 'content', 'per', 'literature', 'psycholinguistics', 'structure', 'known', 'relate', 'individual', 'underlying', 'psychological', 'cognitive', 'state', 'reveal', 'cue', 'social', 'coordination', 'interpersonal', 'awareness', 'measure', 'category', 'includes', 'proportion', 'first', 'person', 'singular', 'indicating', 'preoccupation', 'self', 'first', 'person', 'plural', 'indicating', 'collective', 'attention', 'second', 'person', 'third', 'person', 'pronoun', 'indicating', 'social', 'interactivity', 'reference', 'people', 'object', 'environment', 'literature', 'ha', 'indicated', 'pronoun', 'use', 'quantify', 'individual', 'self', 'social', 'awareness', 'reveal', 'wellbeing', 'including', 'manifested', 'social', 'medium', 'interaction', 'variable', 'corresponding', 'measure', 'category', 'include', 'volume', 'comment', 'authored', 'length', 'length', 'comment', 'authored', 'volume', 'comment', 'received', 'shared', 'length', 'comment', 'received', 'mean', 'vote', 'difference', 'difference', 'upvotes', 'downvotes', 'authored', 'response', 'velocity', 'minute', 'given', 'time', 'elapsed', 'first', 'comment', 'time', 'corresponding', 'shared']","['first set', 'set method', 'method include', 'include developing', 'developing three', 'three set', 'set measure', 'measure spanning', 'spanning linguistic', 'linguistic structure', 'structure interpersonal', 'interpersonal awareness', 'awareness interaction', 'interaction choice', 'choice measure', 'measure motivated', 'motivated literature', 'literature examines', 'examines association', 'association behavioral', 'behavioral expression', 'expression individual', 'individual response', 'response crisis', 'crisis including', 'including vulnerability', 'vulnerability due', 'due illness', 'illness measure', 'measure category', 'category consists', 'consists following', 'following variable', 'variable linguistic', 'linguistic structure', 'structure measure', 'measure compute', 'compute fraction', 'fraction noun', 'noun verb', 'verb adverb', 'adverb comment', 'comment automated', 'automated readability', 'readability index', 'index measure', 'measure gauge', 'gauge understandability', 'understandability text', 'text linguistic', 'linguistic accommodation', 'accommodation process', 'process individual', 'individual conversation', 'conversation adjust', 'adjust language', 'language style', 'style according', 'according others', 'others together', 'together variable', 'variable characterize', 'characterize text', 'text shared', 'shared class', 'class beyond', 'beyond informational', 'informational content', 'content per', 'per literature', 'literature psycholinguistics', 'psycholinguistics structure', 'structure known', 'known relate', 'relate individual', 'individual underlying', 'underlying psychological', 'psychological cognitive', 'cognitive state', 'state reveal', 'reveal cue', 'cue social', 'social coordination', 'coordination interpersonal', 'interpersonal awareness', 'awareness measure', 'measure category', 'category includes', 'includes proportion', 'proportion first', 'first person', 'person singular', 'singular indicating', 'indicating preoccupation', 'preoccupation self', 'self first', 'first person', 'person plural', 'plural indicating', 'indicating collective', 'collective attention', 'attention second', 'second person', 'person third', 'third person', 'person pronoun', 'pronoun indicating', 'indicating social', 'social interactivity', 'interactivity reference', 'reference people', 'people object', 'object environment', 'environment literature', 'literature ha', 'ha indicated', 'indicated pronoun', 'pronoun use', 'use quantify', 'quantify individual', 'individual self', 'self social', 'social awareness', 'awareness reveal', 'reveal wellbeing', 'wellbeing including', 'including manifested', 'manifested social', 'social medium', 'medium interaction', 'interaction variable', 'variable corresponding', 'corresponding measure', 'measure category', 'category include', 'include volume', 'volume comment', 'comment authored', 'authored length', 'length length', 'length comment', 'comment authored', 'authored volume', 'volume comment', 'comment received', 'received shared', 'shared length', 'length comment', 'comment received', 'received mean', 'mean vote', 'vote difference', 'difference difference', 'difference upvotes', 'upvotes downvotes', 'downvotes authored', 'authored response', 'response velocity', 'velocity minute', 'minute given', 'given time', 'time elapsed', 'elapsed first', 'first comment', 'comment time', 'time corresponding', 'corresponding shared']","['first set method', 'set method include', 'method include developing', 'include developing three', 'developing three set', 'three set measure', 'set measure spanning', 'measure spanning linguistic', 'spanning linguistic structure', 'linguistic structure interpersonal', 'structure interpersonal awareness', 'interpersonal awareness interaction', 'awareness interaction choice', 'interaction choice measure', 'choice measure motivated', 'measure motivated literature', 'motivated literature examines', 'literature examines association', 'examines association behavioral', 'association behavioral expression', 'behavioral expression individual', 'expression individual response', 'individual response crisis', 'response crisis including', 'crisis including vulnerability', 'including vulnerability due', 'vulnerability due illness', 'due illness measure', 'illness measure category', 'measure category consists', 'category consists following', 'consists following variable', 'following variable linguistic', 'variable linguistic structure', 'linguistic structure measure', 'structure measure compute', 'measure compute fraction', 'compute fraction noun', 'fraction noun verb', 'noun verb adverb', 'verb adverb comment', 'adverb comment automated', 'comment automated readability', 'automated readability index', 'readability index measure', 'index measure gauge', 'measure gauge understandability', 'gauge understandability text', 'understandability text linguistic', 'text linguistic accommodation', 'linguistic accommodation process', 'accommodation process individual', 'process individual conversation', 'individual conversation adjust', 'conversation adjust language', 'adjust language style', 'language style according', 'style according others', 'according others together', 'others together variable', 'together variable characterize', 'variable characterize text', 'characterize text shared', 'text shared class', 'shared class beyond', 'class beyond informational', 'beyond informational content', 'informational content per', 'content per literature', 'per literature psycholinguistics', 'literature psycholinguistics structure', 'psycholinguistics structure known', 'structure known relate', 'known relate individual', 'relate individual underlying', 'individual underlying psychological', 'underlying psychological cognitive', 'psychological cognitive state', 'cognitive state reveal', 'state reveal cue', 'reveal cue social', 'cue social coordination', 'social coordination interpersonal', 'coordination interpersonal awareness', 'interpersonal awareness measure', 'awareness measure category', 'measure category includes', 'category includes proportion', 'includes proportion first', 'proportion first person', 'first person singular', 'person singular indicating', 'singular indicating preoccupation', 'indicating preoccupation self', 'preoccupation self first', 'self first person', 'first person plural', 'person plural indicating', 'plural indicating collective', 'indicating collective attention', 'collective attention second', 'attention second person', 'second person third', 'person third person', 'third person pronoun', 'person pronoun indicating', 'pronoun indicating social', 'indicating social interactivity', 'social interactivity reference', 'interactivity reference people', 'reference people object', 'people object environment', 'object environment literature', 'environment literature ha', 'literature ha indicated', 'ha indicated pronoun', 'indicated pronoun use', 'pronoun use quantify', 'use quantify individual', 'quantify individual self', 'individual self social', 'self social awareness', 'social awareness reveal', 'awareness reveal wellbeing', 'reveal wellbeing including', 'wellbeing including manifested', 'including manifested social', 'manifested social medium', 'social medium interaction', 'medium interaction variable', 'interaction variable corresponding', 'variable corresponding measure', 'corresponding measure category', 'measure category include', 'category include volume', 'include volume comment', 'volume comment authored', 'comment authored length', 'authored length length', 'length length comment', 'length comment authored', 'comment authored volume', 'authored volume comment', 'volume comment received', 'comment received shared', 'received shared length', 'shared length comment', 'length comment received', 'comment received mean', 'received mean vote', 'mean vote difference', 'vote difference difference', 'difference difference upvotes', 'difference upvotes downvotes', 'upvotes downvotes authored', 'downvotes authored response', 'authored response velocity', 'response velocity minute', 'velocity minute given', 'minute given time', 'given time elapsed', 'time elapsed first', 'elapsed first comment', 'first comment time', 'comment time corresponding', 'time corresponding shared']"
https://dl.acm.org/doi/abs/10.1145/2702613.2732733,1,Based on the training data thus created we pursued the use of supervised learning to develop a classifier which would indicate whether a post is of high low or no self-disclosure. We tested a variety of different classification techniques (decision trees k Nearest Neighbor naive Bayes). The best performing classifier was found to be a perceptron classifier with adaptive boosting used to amplify performance [17] whose results will be used in the remainder of this paper. We used the following feature generation rules: First we eliminated stopwords from each post based on standard list provided by Python’s NLTK library. Next we performed stemming using Porter Stemmer. We extracted uni- bi- and tri-grams from each post and considered those with five or more occurrences. We also computed two additional features – length of each post and whether the author of the post is an exclusive poster on mental health forums or is observed in our dataset to post on other forums as well. Thus each post was characterized by 1070 features. We used standard 10-fold cross validation (CV) to evaluate the classifier and ran our model over 100 random 10-fold CV assignments for generalizability of the results. We report the average accuracy precision recall F1 specificity as metrics of performance. We find that our classifier based on the perception model yields an average accuracy of 78.4% in detecting high or low self-disclosure with .74 precision and .86 recall (see Table 4 for details). Other methods like k-NN (k=5) give higher precision but at the expense of very low recall. Figure 1 gives the ROC (receiver operating characteristic) curves for all the models. Per the ROC curve corresponding to the perceptron model we find it to yield the maximum area under curve (.81) hence best performance. We further identify in Table 5 the n-grams (or features) with the highest weights given by the perceptron – it implies these features were the most significant in the classification task. We provide some brief qualitative examinations of these n-grams in the light of prior psychology literature on selfdisclosure and mental health [10 11]. We find that the ngrams primarily are associated with vulnerable and selfloathing thoughts (e.g. thoughts of suicide) bear a negative tone or depict confessional experiences. Based on prior research [10 11 12] and our own work on mental health discourse on Reddit [4] we find that these are the topical dimensions along which high self-disclosure and low/no selfdisclosure posts vary. In essence high self-disclosure posts share extensively their personal beliefs and fear for instance their vital constructs and private sensitive informational attributes. The post excerpts below have been classified to be of high self-disclosure and through them we demonstrate the use of some of the n-grams in Table 7: “I don’t want to kill myself I haven’t felt suicidal in a long time but I just want to stop life for a while you know?” “My dad would beat the living shit out of me […]. I’ve been to the hospital so many times I’ve lost track” “I hate this. I hate myself. I don't want to f****** be this person anymore. I'm unmotivated unfocused immature.”,based on the training data thus created we pursued the use of supervised learning to develop a classifier which would indicate whether a post is of high low or no selfdisclosure we tested a variety of different classification technique decision tree k nearest neighbor naive bayes the best performing classifier wa found to be a perceptron classifier with adaptive boosting used to amplify performance whose result will be used in the remainder of this paper we used the following feature generation rule first we eliminated stopwords from each post based on standard list provided by python nltk library next we performed stemming using porter stemmer we extracted uni bi and trigram from each post and considered those with five or more occurrence we also computed two additional feature length of each post and whether the author of the post is an exclusive poster on mental health forum or is observed in our dataset to post on other forum a well thus each post wa characterized by feature we used standard fold cross validation cv to evaluate the classifier and ran our model over random fold cv assignment for generalizability of the result we report the average accuracy precision recall f specificity a metric of performance we find that our classifier based on the perception model yield an average accuracy of in detecting high or low selfdisclosure with precision and recall see table for detail other method like knn k give higher precision but at the expense of very low recall figure give the roc receiver operating characteristic curve for all the model per the roc curve corresponding to the perceptron model we find it to yield the maximum area under curve hence best performance we further identify in table the ngrams or feature with the highest weight given by the perceptron it implies these feature were the most significant in the classification task we provide some brief qualitative examination of these ngrams in the light of prior psychology literature on selfdisclosure and mental health we find that the ngrams primarily are associated with vulnerable and selfloathing thought eg thought of suicide bear a negative tone or depict confessional experience based on prior research and our own work on mental health discourse on reddit we find that these are the topical dimension along which high selfdisclosure and lowno selfdisclosure post vary in essence high selfdisclosure post share extensively their personal belief and fear for instance their vital construct and private sensitive informational attribute the post excerpt below have been classified to be of high selfdisclosure and through them we demonstrate the use of some of the ngrams in table i dont want to kill myself i havent felt suicidal in a long time but i just want to stop life for a while you know my dad would beat the living shit out of me ive been to the hospital so many time ive lost track i hate this i hate myself i dont want to f be this person anymore im unmotivated unfocused immature,"['based', 'training', 'thus', 'created', 'pursued', 'use', 'supervised', 'learning', 'develop', 'classifier', 'would', 'indicate', 'whether', 'high', 'low', 'selfdisclosure', 'tested', 'variety', 'different', 'classification', 'technique', 'decision', 'tree', 'k', 'nearest', 'neighbor', 'naive', 'bayes', 'best', 'performing', 'classifier', 'found', 'perceptron', 'classifier', 'adaptive', 'boosting', 'amplify', 'performance', 'whose', 'result', 'remainder', 'paper', 'following', 'feature', 'generation', 'rule', 'first', 'eliminated', 'stopwords', 'based', 'standard', 'list', 'provided', 'python', 'nltk', 'library', 'next', 'performed', 'stemming', 'using', 'porter', 'stemmer', 'extracted', 'uni', 'bi', 'trigram', 'considered', 'five', 'occurrence', 'also', 'computed', 'two', 'additional', 'feature', 'length', 'whether', 'author', 'exclusive', 'poster', 'forum', 'observed', 'dataset', 'forum', 'well', 'thus', 'characterized', 'feature', 'standard', 'fold', 'cross', 'validation', 'cv', 'evaluate', 'classifier', 'ran', 'model', 'random', 'fold', 'cv', 'assignment', 'generalizability', 'result', 'report', 'average', 'accuracy', 'precision', 'recall', 'f', 'specificity', 'metric', 'performance', 'find', 'classifier', 'based', 'perception', 'model', 'yield', 'average', 'accuracy', 'detecting', 'high', 'low', 'selfdisclosure', 'precision', 'recall', 'see', 'table', 'detail', 'method', 'like', 'knn', 'k', 'give', 'higher', 'precision', 'expense', 'low', 'recall', 'figure', 'give', 'roc', 'receiver', 'operating', 'characteristic', 'curve', 'model', 'per', 'roc', 'curve', 'corresponding', 'perceptron', 'model', 'find', 'yield', 'maximum', 'area', 'curve', 'hence', 'best', 'performance', 'identify', 'table', 'ngrams', 'feature', 'highest', 'weight', 'given', 'perceptron', 'implies', 'feature', 'significant', 'classification', 'task', 'provide', 'brief', 'qualitative', 'examination', 'ngrams', 'light', 'prior', 'psychology', 'literature', 'selfdisclosure', 'find', 'ngrams', 'primarily', 'associated', 'vulnerable', 'selfloathing', 'thought', 'eg', 'thought', 'suicide', 'bear', 'negative', 'tone', 'depict', 'confessional', 'experience', 'based', 'prior', 'research', 'work', 'discourse', 'reddit', 'find', 'topical', 'dimension', 'along', 'high', 'selfdisclosure', 'lowno', 'selfdisclosure', 'vary', 'essence', 'high', 'selfdisclosure', 'share', 'extensively', 'personal', 'belief', 'fear', 'instance', 'vital', 'construct', 'private', 'sensitive', 'informational', 'attribute', 'excerpt', 'classified', 'high', 'selfdisclosure', 'demonstrate', 'use', 'ngrams', 'table', 'dont', 'want', 'kill', 'havent', 'felt', 'suicidal', 'long', 'time', 'want', 'stop', 'life', 'know', 'dad', 'would', 'beat', 'living', 'shit', 'ive', 'hospital', 'many', 'time', 'ive', 'lost', 'track', 'hate', 'hate', 'dont', 'want', 'f', 'person', 'anymore', 'im', 'unmotivated', 'unfocused', 'immature']","['based training', 'training thus', 'thus created', 'created pursued', 'pursued use', 'use supervised', 'supervised learning', 'learning develop', 'develop classifier', 'classifier would', 'would indicate', 'indicate whether', 'whether high', 'high low', 'low selfdisclosure', 'selfdisclosure tested', 'tested variety', 'variety different', 'different classification', 'classification technique', 'technique decision', 'decision tree', 'tree k', 'k nearest', 'nearest neighbor', 'neighbor naive', 'naive bayes', 'bayes best', 'best performing', 'performing classifier', 'classifier found', 'found perceptron', 'perceptron classifier', 'classifier adaptive', 'adaptive boosting', 'boosting amplify', 'amplify performance', 'performance whose', 'whose result', 'result remainder', 'remainder paper', 'paper following', 'following feature', 'feature generation', 'generation rule', 'rule first', 'first eliminated', 'eliminated stopwords', 'stopwords based', 'based standard', 'standard list', 'list provided', 'provided python', 'python nltk', 'nltk library', 'library next', 'next performed', 'performed stemming', 'stemming using', 'using porter', 'porter stemmer', 'stemmer extracted', 'extracted uni', 'uni bi', 'bi trigram', 'trigram considered', 'considered five', 'five occurrence', 'occurrence also', 'also computed', 'computed two', 'two additional', 'additional feature', 'feature length', 'length whether', 'whether author', 'author exclusive', 'exclusive poster', 'poster forum', 'forum observed', 'observed dataset', 'dataset forum', 'forum well', 'well thus', 'thus characterized', 'characterized feature', 'feature standard', 'standard fold', 'fold cross', 'cross validation', 'validation cv', 'cv evaluate', 'evaluate classifier', 'classifier ran', 'ran model', 'model random', 'random fold', 'fold cv', 'cv assignment', 'assignment generalizability', 'generalizability result', 'result report', 'report average', 'average accuracy', 'accuracy precision', 'precision recall', 'recall f', 'f specificity', 'specificity metric', 'metric performance', 'performance find', 'find classifier', 'classifier based', 'based perception', 'perception model', 'model yield', 'yield average', 'average accuracy', 'accuracy detecting', 'detecting high', 'high low', 'low selfdisclosure', 'selfdisclosure precision', 'precision recall', 'recall see', 'see table', 'table detail', 'detail method', 'method like', 'like knn', 'knn k', 'k give', 'give higher', 'higher precision', 'precision expense', 'expense low', 'low recall', 'recall figure', 'figure give', 'give roc', 'roc receiver', 'receiver operating', 'operating characteristic', 'characteristic curve', 'curve model', 'model per', 'per roc', 'roc curve', 'curve corresponding', 'corresponding perceptron', 'perceptron model', 'model find', 'find yield', 'yield maximum', 'maximum area', 'area curve', 'curve hence', 'hence best', 'best performance', 'performance identify', 'identify table', 'table ngrams', 'ngrams feature', 'feature highest', 'highest weight', 'weight given', 'given perceptron', 'perceptron implies', 'implies feature', 'feature significant', 'significant classification', 'classification task', 'task provide', 'provide brief', 'brief qualitative', 'qualitative examination', 'examination ngrams', 'ngrams light', 'light prior', 'prior psychology', 'psychology literature', 'literature selfdisclosure', 'selfdisclosure find', 'find ngrams', 'ngrams primarily', 'primarily associated', 'associated vulnerable', 'vulnerable selfloathing', 'selfloathing thought', 'thought eg', 'eg thought', 'thought suicide', 'suicide bear', 'bear negative', 'negative tone', 'tone depict', 'depict confessional', 'confessional experience', 'experience based', 'based prior', 'prior research', 'research work', 'work discourse', 'discourse reddit', 'reddit find', 'find topical', 'topical dimension', 'dimension along', 'along high', 'high selfdisclosure', 'selfdisclosure lowno', 'lowno selfdisclosure', 'selfdisclosure vary', 'vary essence', 'essence high', 'high selfdisclosure', 'selfdisclosure share', 'share extensively', 'extensively personal', 'personal belief', 'belief fear', 'fear instance', 'instance vital', 'vital construct', 'construct private', 'private sensitive', 'sensitive informational', 'informational attribute', 'attribute excerpt', 'excerpt classified', 'classified high', 'high selfdisclosure', 'selfdisclosure demonstrate', 'demonstrate use', 'use ngrams', 'ngrams table', 'table dont', 'dont want', 'want kill', 'kill havent', 'havent felt', 'felt suicidal', 'suicidal long', 'long time', 'time want', 'want stop', 'stop life', 'life know', 'know dad', 'dad would', 'would beat', 'beat living', 'living shit', 'shit ive', 'ive hospital', 'hospital many', 'many time', 'time ive', 'ive lost', 'lost track', 'track hate', 'hate hate', 'hate dont', 'dont want', 'want f', 'f person', 'person anymore', 'anymore im', 'im unmotivated', 'unmotivated unfocused', 'unfocused immature']","['based training thus', 'training thus created', 'thus created pursued', 'created pursued use', 'pursued use supervised', 'use supervised learning', 'supervised learning develop', 'learning develop classifier', 'develop classifier would', 'classifier would indicate', 'would indicate whether', 'indicate whether high', 'whether high low', 'high low selfdisclosure', 'low selfdisclosure tested', 'selfdisclosure tested variety', 'tested variety different', 'variety different classification', 'different classification technique', 'classification technique decision', 'technique decision tree', 'decision tree k', 'tree k nearest', 'k nearest neighbor', 'nearest neighbor naive', 'neighbor naive bayes', 'naive bayes best', 'bayes best performing', 'best performing classifier', 'performing classifier found', 'classifier found perceptron', 'found perceptron classifier', 'perceptron classifier adaptive', 'classifier adaptive boosting', 'adaptive boosting amplify', 'boosting amplify performance', 'amplify performance whose', 'performance whose result', 'whose result remainder', 'result remainder paper', 'remainder paper following', 'paper following feature', 'following feature generation', 'feature generation rule', 'generation rule first', 'rule first eliminated', 'first eliminated stopwords', 'eliminated stopwords based', 'stopwords based standard', 'based standard list', 'standard list provided', 'list provided python', 'provided python nltk', 'python nltk library', 'nltk library next', 'library next performed', 'next performed stemming', 'performed stemming using', 'stemming using porter', 'using porter stemmer', 'porter stemmer extracted', 'stemmer extracted uni', 'extracted uni bi', 'uni bi trigram', 'bi trigram considered', 'trigram considered five', 'considered five occurrence', 'five occurrence also', 'occurrence also computed', 'also computed two', 'computed two additional', 'two additional feature', 'additional feature length', 'feature length whether', 'length whether author', 'whether author exclusive', 'author exclusive poster', 'exclusive poster forum', 'poster forum observed', 'forum observed dataset', 'observed dataset forum', 'dataset forum well', 'forum well thus', 'well thus characterized', 'thus characterized feature', 'characterized feature standard', 'feature standard fold', 'standard fold cross', 'fold cross validation', 'cross validation cv', 'validation cv evaluate', 'cv evaluate classifier', 'evaluate classifier ran', 'classifier ran model', 'ran model random', 'model random fold', 'random fold cv', 'fold cv assignment', 'cv assignment generalizability', 'assignment generalizability result', 'generalizability result report', 'result report average', 'report average accuracy', 'average accuracy precision', 'accuracy precision recall', 'precision recall f', 'recall f specificity', 'f specificity metric', 'specificity metric performance', 'metric performance find', 'performance find classifier', 'find classifier based', 'classifier based perception', 'based perception model', 'perception model yield', 'model yield average', 'yield average accuracy', 'average accuracy detecting', 'accuracy detecting high', 'detecting high low', 'high low selfdisclosure', 'low selfdisclosure precision', 'selfdisclosure precision recall', 'precision recall see', 'recall see table', 'see table detail', 'table detail method', 'detail method like', 'method like knn', 'like knn k', 'knn k give', 'k give higher', 'give higher precision', 'higher precision expense', 'precision expense low', 'expense low recall', 'low recall figure', 'recall figure give', 'figure give roc', 'give roc receiver', 'roc receiver operating', 'receiver operating characteristic', 'operating characteristic curve', 'characteristic curve model', 'curve model per', 'model per roc', 'per roc curve', 'roc curve corresponding', 'curve corresponding perceptron', 'corresponding perceptron model', 'perceptron model find', 'model find yield', 'find yield maximum', 'yield maximum area', 'maximum area curve', 'area curve hence', 'curve hence best', 'hence best performance', 'best performance identify', 'performance identify table', 'identify table ngrams', 'table ngrams feature', 'ngrams feature highest', 'feature highest weight', 'highest weight given', 'weight given perceptron', 'given perceptron implies', 'perceptron implies feature', 'implies feature significant', 'feature significant classification', 'significant classification task', 'classification task provide', 'task provide brief', 'provide brief qualitative', 'brief qualitative examination', 'qualitative examination ngrams', 'examination ngrams light', 'ngrams light prior', 'light prior psychology', 'prior psychology literature', 'psychology literature selfdisclosure', 'literature selfdisclosure find', 'selfdisclosure find ngrams', 'find ngrams primarily', 'ngrams primarily associated', 'primarily associated vulnerable', 'associated vulnerable selfloathing', 'vulnerable selfloathing thought', 'selfloathing thought eg', 'thought eg thought', 'eg thought suicide', 'thought suicide bear', 'suicide bear negative', 'bear negative tone', 'negative tone depict', 'tone depict confessional', 'depict confessional experience', 'confessional experience based', 'experience based prior', 'based prior research', 'prior research work', 'research work discourse', 'work discourse reddit', 'discourse reddit find', 'reddit find topical', 'find topical dimension', 'topical dimension along', 'dimension along high', 'along high selfdisclosure', 'high selfdisclosure lowno', 'selfdisclosure lowno selfdisclosure', 'lowno selfdisclosure vary', 'selfdisclosure vary essence', 'vary essence high', 'essence high selfdisclosure', 'high selfdisclosure share', 'selfdisclosure share extensively', 'share extensively personal', 'extensively personal belief', 'personal belief fear', 'belief fear instance', 'fear instance vital', 'instance vital construct', 'vital construct private', 'construct private sensitive', 'private sensitive informational', 'sensitive informational attribute', 'informational attribute excerpt', 'attribute excerpt classified', 'excerpt classified high', 'classified high selfdisclosure', 'high selfdisclosure demonstrate', 'selfdisclosure demonstrate use', 'demonstrate use ngrams', 'use ngrams table', 'ngrams table dont', 'table dont want', 'dont want kill', 'want kill havent', 'kill havent felt', 'havent felt suicidal', 'felt suicidal long', 'suicidal long time', 'long time want', 'time want stop', 'want stop life', 'stop life know', 'life know dad', 'know dad would', 'dad would beat', 'would beat living', 'beat living shit', 'living shit ive', 'shit ive hospital', 'ive hospital many', 'hospital many time', 'many time ive', 'time ive lost', 'ive lost track', 'lost track hate', 'track hate hate', 'hate hate dont', 'hate dont want', 'dont want f', 'want f person', 'f person anymore', 'person anymore im', 'anymore im unmotivated', 'im unmotivated unfocused', 'unmotivated unfocused immature']"
https://dl.acm.org/doi/abs/10.1145/2702123.2702280,1,It is possible to extract various features from the activity histories of Twitter users. This section explains what kinds of features are used to estimate degree of depression and the way in which these quantities are extracted. Table 2 shows the features used in this study. A detailed explanation of each feature follows. The frequencies of words in a tweet (i.e. its bag of words) are used as a basic feature relating to the content of the tweet. Tsugawa et al. showed that the word frequencies are useful for identifying depression [37]. MeCab [20] was used to for morphological stemming and categorization of the Japanese tweet text to obtain accurate word frequencies. Particles auxiliary verbs adnominal adjectives and visual symbols were excluded for extracting content words. Words used by only one participant were also excluded resulting in a total of 84255 distinct words. However most of these words were rarely used and the distribution of word frequencies is extremely biased (see Fig. 4). Because words with a low rate of use were regarded as unlikely to be associated with depression for most users the frequencies of only the 20000 words with the highest rate of use (corresponding to 25 or more uses across all participants) were used as a feature in this study. Furthermore because the number and length of tweets differed by participant the word frequencies were normalized by the total number of words in the tweets. The topics of the tweets of each user as estimated by using a representative topic model LDA [5] were used as a second feature relating to the content of the tweets. With LDA the distribution of topics in each document is estimated from the word frequencies in each text through unsupervised learning on the assumption that the text and the words in it are generated according to a particular topic [5]. In LDA the number of topics to identify and a set of documents (as bags of words) are used as input and a topic distribution is output for each document. As mentioned in Related Work Section the topics of essays written by university students were estimated by using LDA and found to be useful in evaluating degree of depression [31]. From that study topics are expected to be a useful feature. A set of all tweets of each user was used as the user document for input in LDA and the 20000 words selected as described above were used as the words. We used LDA with collapsed Gibbs sampling [16]. As the parameters of LDA we used α = 50/K and β = 0.1 where K is the number of topics [16]. All extracted topics were used as the features. The ratio of positive words and the ratio of negative words used in the tweet text are used as the final features relating to tweet content. Users with depression are intuitively expected to use negative words more frequently than users without depression do. To categorize words a dictionary of affective words [19] which is compiled by manual evaluation of a dictionary of positive and negative words extracted according to a technique proposed in the literature [35] is used. The dictionary contains 760 positive words and 862 negative words. The user’s timing of tweets frequency of tweets average number of words retweet rate (rate of republishing other users’ tweets) mention rate (rate of directly referencing at least one other user) ratio of tweets containing a uniform resource locator (URL) number of users being followed and number of users following are used as features independent of the content of the tweet. The relative ratios of tweets posted during each hour of the day were used to characterize the timing of tweets; the number of posts per day was used as the posting frequency; and the ratio of qualifying tweets to all tweets were used for the retweet ratio mention ratio and ratio of tweets containing a URL. These features are used in prior research [14].,it is possible to extract various feature from the activity history of twitter user this section explains what kind of feature are used to estimate degree of depression and the way in which these quantity are extracted table show the feature used in this study a detailed explanation of each feature follows the frequency of word in a tweet ie it bag of word are used a a basic feature relating to the content of the tweet tsugawa et al showed that the word frequency are useful for identifying depression mecab wa used to for morphological stemming and categorization of the japanese tweet text to obtain accurate word frequency particle auxiliary verb adnominal adjective and visual symbol were excluded for extracting content word word used by only one participant were also excluded resulting in a total of distinct word however most of these word were rarely used and the distribution of word frequency is extremely biased see fig because word with a low rate of use were regarded a unlikely to be associated with depression for most user the frequency of only the word with the highest rate of use corresponding to or more us across all participant were used a a feature in this study furthermore because the number and length of tweet differed by participant the word frequency were normalized by the total number of word in the tweet the topic of the tweet of each user a estimated by using a representative topic model lda were used a a second feature relating to the content of the tweet with lda the distribution of topic in each document is estimated from the word frequency in each text through unsupervised learning on the assumption that the text and the word in it are generated according to a particular topic in lda the number of topic to identify and a set of document a bag of word are used a input and a topic distribution is output for each document a mentioned in related work section the topic of essay written by university student were estimated by using lda and found to be useful in evaluating degree of depression from that study topic are expected to be a useful feature a set of all tweet of each user wa used a the user document for input in lda and the word selected a described above were used a the word we used lda with collapsed gibbs sampling a the parameter of lda we used k and where k is the number of topic all extracted topic were used a the feature the ratio of positive word and the ratio of negative word used in the tweet text are used a the final feature relating to tweet content user with depression are intuitively expected to use negative word more frequently than user without depression do to categorize word a dictionary of affective word which is compiled by manual evaluation of a dictionary of positive and negative word extracted according to a technique proposed in the literature is used the dictionary contains positive word and negative word the user timing of tweet frequency of tweet average number of word retweet rate rate of republishing other user tweet mention rate rate of directly referencing at least one other user ratio of tweet containing a uniform resource locator url number of user being followed and number of user following are used a feature independent of the content of the tweet the relative ratio of tweet posted during each hour of the day were used to characterize the timing of tweet the number of post per day wa used a the posting frequency and the ratio of qualifying tweet to all tweet were used for the retweet ratio mention ratio and ratio of tweet containing a url these feature are used in prior research,"['possible', 'extract', 'various', 'feature', 'activity', 'history', 'section', 'explains', 'kind', 'feature', 'estimate', 'degree', 'way', 'quantity', 'extracted', 'table', 'show', 'feature', 'study', 'detailed', 'explanation', 'feature', 'follows', 'frequency', 'ie', 'bag', 'basic', 'feature', 'relating', 'content', 'tsugawa', 'et', 'al', 'showed', 'frequency', 'useful', 'identifying', 'mecab', 'morphological', 'stemming', 'categorization', 'japanese', 'text', 'obtain', 'accurate', 'frequency', 'particle', 'auxiliary', 'verb', 'adnominal', 'adjective', 'visual', 'symbol', 'excluded', 'extracting', 'content', 'one', 'also', 'excluded', 'resulting', 'total', 'distinct', 'however', 'rarely', 'distribution', 'frequency', 'extremely', 'biased', 'see', 'fig', 'low', 'rate', 'use', 'regarded', 'unlikely', 'associated', 'frequency', 'highest', 'rate', 'use', 'corresponding', 'us', 'across', 'feature', 'study', 'furthermore', 'number', 'length', 'differed', 'frequency', 'normalized', 'total', 'number', 'topic', 'estimated', 'using', 'representative', 'topic', 'model', 'lda', 'second', 'feature', 'relating', 'content', 'lda', 'distribution', 'topic', 'document', 'estimated', 'frequency', 'text', 'unsupervised', 'learning', 'assumption', 'text', 'generated', 'according', 'particular', 'topic', 'lda', 'number', 'topic', 'identify', 'set', 'document', 'bag', 'input', 'topic', 'distribution', 'output', 'document', 'mentioned', 'related', 'work', 'section', 'topic', 'essay', 'written', 'university', 'student', 'estimated', 'using', 'lda', 'found', 'useful', 'evaluating', 'degree', 'study', 'topic', 'expected', 'useful', 'feature', 'set', 'document', 'input', 'lda', 'selected', 'described', 'lda', 'collapsed', 'gibbs', 'sampling', 'parameter', 'lda', 'k', 'k', 'number', 'topic', 'extracted', 'topic', 'feature', 'ratio', 'positive', 'ratio', 'negative', 'text', 'final', 'feature', 'relating', 'content', 'intuitively', 'expected', 'use', 'negative', 'frequently', 'without', 'categorize', 'dictionary', 'affective', 'compiled', 'manual', 'evaluation', 'dictionary', 'positive', 'negative', 'extracted', 'according', 'technique', 'proposed', 'literature', 'dictionary', 'contains', 'positive', 'negative', 'timing', 'frequency', 'average', 'number', 'retweet', 'rate', 'rate', 'republishing', 'mention', 'rate', 'rate', 'directly', 'referencing', 'least', 'one', 'ratio', 'containing', 'uniform', 'resource', 'locator', 'url', 'number', 'followed', 'number', 'following', 'feature', 'independent', 'content', 'relative', 'ratio', 'posted', 'hour', 'day', 'characterize', 'timing', 'number', 'per', 'day', 'posting', 'frequency', 'ratio', 'qualifying', 'retweet', 'ratio', 'mention', 'ratio', 'ratio', 'containing', 'url', 'feature', 'prior', 'research']","['possible extract', 'extract various', 'various feature', 'feature activity', 'activity history', 'history section', 'section explains', 'explains kind', 'kind feature', 'feature estimate', 'estimate degree', 'degree way', 'way quantity', 'quantity extracted', 'extracted table', 'table show', 'show feature', 'feature study', 'study detailed', 'detailed explanation', 'explanation feature', 'feature follows', 'follows frequency', 'frequency ie', 'ie bag', 'bag basic', 'basic feature', 'feature relating', 'relating content', 'content tsugawa', 'tsugawa et', 'et al', 'al showed', 'showed frequency', 'frequency useful', 'useful identifying', 'identifying mecab', 'mecab morphological', 'morphological stemming', 'stemming categorization', 'categorization japanese', 'japanese text', 'text obtain', 'obtain accurate', 'accurate frequency', 'frequency particle', 'particle auxiliary', 'auxiliary verb', 'verb adnominal', 'adnominal adjective', 'adjective visual', 'visual symbol', 'symbol excluded', 'excluded extracting', 'extracting content', 'content one', 'one also', 'also excluded', 'excluded resulting', 'resulting total', 'total distinct', 'distinct however', 'however rarely', 'rarely distribution', 'distribution frequency', 'frequency extremely', 'extremely biased', 'biased see', 'see fig', 'fig low', 'low rate', 'rate use', 'use regarded', 'regarded unlikely', 'unlikely associated', 'associated frequency', 'frequency highest', 'highest rate', 'rate use', 'use corresponding', 'corresponding us', 'us across', 'across feature', 'feature study', 'study furthermore', 'furthermore number', 'number length', 'length differed', 'differed frequency', 'frequency normalized', 'normalized total', 'total number', 'number topic', 'topic estimated', 'estimated using', 'using representative', 'representative topic', 'topic model', 'model lda', 'lda second', 'second feature', 'feature relating', 'relating content', 'content lda', 'lda distribution', 'distribution topic', 'topic document', 'document estimated', 'estimated frequency', 'frequency text', 'text unsupervised', 'unsupervised learning', 'learning assumption', 'assumption text', 'text generated', 'generated according', 'according particular', 'particular topic', 'topic lda', 'lda number', 'number topic', 'topic identify', 'identify set', 'set document', 'document bag', 'bag input', 'input topic', 'topic distribution', 'distribution output', 'output document', 'document mentioned', 'mentioned related', 'related work', 'work section', 'section topic', 'topic essay', 'essay written', 'written university', 'university student', 'student estimated', 'estimated using', 'using lda', 'lda found', 'found useful', 'useful evaluating', 'evaluating degree', 'degree study', 'study topic', 'topic expected', 'expected useful', 'useful feature', 'feature set', 'set document', 'document input', 'input lda', 'lda selected', 'selected described', 'described lda', 'lda collapsed', 'collapsed gibbs', 'gibbs sampling', 'sampling parameter', 'parameter lda', 'lda k', 'k k', 'k number', 'number topic', 'topic extracted', 'extracted topic', 'topic feature', 'feature ratio', 'ratio positive', 'positive ratio', 'ratio negative', 'negative text', 'text final', 'final feature', 'feature relating', 'relating content', 'content intuitively', 'intuitively expected', 'expected use', 'use negative', 'negative frequently', 'frequently without', 'without categorize', 'categorize dictionary', 'dictionary affective', 'affective compiled', 'compiled manual', 'manual evaluation', 'evaluation dictionary', 'dictionary positive', 'positive negative', 'negative extracted', 'extracted according', 'according technique', 'technique proposed', 'proposed literature', 'literature dictionary', 'dictionary contains', 'contains positive', 'positive negative', 'negative timing', 'timing frequency', 'frequency average', 'average number', 'number retweet', 'retweet rate', 'rate rate', 'rate republishing', 'republishing mention', 'mention rate', 'rate rate', 'rate directly', 'directly referencing', 'referencing least', 'least one', 'one ratio', 'ratio containing', 'containing uniform', 'uniform resource', 'resource locator', 'locator url', 'url number', 'number followed', 'followed number', 'number following', 'following feature', 'feature independent', 'independent content', 'content relative', 'relative ratio', 'ratio posted', 'posted hour', 'hour day', 'day characterize', 'characterize timing', 'timing number', 'number per', 'per day', 'day posting', 'posting frequency', 'frequency ratio', 'ratio qualifying', 'qualifying retweet', 'retweet ratio', 'ratio mention', 'mention ratio', 'ratio ratio', 'ratio containing', 'containing url', 'url feature', 'feature prior', 'prior research']","['possible extract various', 'extract various feature', 'various feature activity', 'feature activity history', 'activity history section', 'history section explains', 'section explains kind', 'explains kind feature', 'kind feature estimate', 'feature estimate degree', 'estimate degree way', 'degree way quantity', 'way quantity extracted', 'quantity extracted table', 'extracted table show', 'table show feature', 'show feature study', 'feature study detailed', 'study detailed explanation', 'detailed explanation feature', 'explanation feature follows', 'feature follows frequency', 'follows frequency ie', 'frequency ie bag', 'ie bag basic', 'bag basic feature', 'basic feature relating', 'feature relating content', 'relating content tsugawa', 'content tsugawa et', 'tsugawa et al', 'et al showed', 'al showed frequency', 'showed frequency useful', 'frequency useful identifying', 'useful identifying mecab', 'identifying mecab morphological', 'mecab morphological stemming', 'morphological stemming categorization', 'stemming categorization japanese', 'categorization japanese text', 'japanese text obtain', 'text obtain accurate', 'obtain accurate frequency', 'accurate frequency particle', 'frequency particle auxiliary', 'particle auxiliary verb', 'auxiliary verb adnominal', 'verb adnominal adjective', 'adnominal adjective visual', 'adjective visual symbol', 'visual symbol excluded', 'symbol excluded extracting', 'excluded extracting content', 'extracting content one', 'content one also', 'one also excluded', 'also excluded resulting', 'excluded resulting total', 'resulting total distinct', 'total distinct however', 'distinct however rarely', 'however rarely distribution', 'rarely distribution frequency', 'distribution frequency extremely', 'frequency extremely biased', 'extremely biased see', 'biased see fig', 'see fig low', 'fig low rate', 'low rate use', 'rate use regarded', 'use regarded unlikely', 'regarded unlikely associated', 'unlikely associated frequency', 'associated frequency highest', 'frequency highest rate', 'highest rate use', 'rate use corresponding', 'use corresponding us', 'corresponding us across', 'us across feature', 'across feature study', 'feature study furthermore', 'study furthermore number', 'furthermore number length', 'number length differed', 'length differed frequency', 'differed frequency normalized', 'frequency normalized total', 'normalized total number', 'total number topic', 'number topic estimated', 'topic estimated using', 'estimated using representative', 'using representative topic', 'representative topic model', 'topic model lda', 'model lda second', 'lda second feature', 'second feature relating', 'feature relating content', 'relating content lda', 'content lda distribution', 'lda distribution topic', 'distribution topic document', 'topic document estimated', 'document estimated frequency', 'estimated frequency text', 'frequency text unsupervised', 'text unsupervised learning', 'unsupervised learning assumption', 'learning assumption text', 'assumption text generated', 'text generated according', 'generated according particular', 'according particular topic', 'particular topic lda', 'topic lda number', 'lda number topic', 'number topic identify', 'topic identify set', 'identify set document', 'set document bag', 'document bag input', 'bag input topic', 'input topic distribution', 'topic distribution output', 'distribution output document', 'output document mentioned', 'document mentioned related', 'mentioned related work', 'related work section', 'work section topic', 'section topic essay', 'topic essay written', 'essay written university', 'written university student', 'university student estimated', 'student estimated using', 'estimated using lda', 'using lda found', 'lda found useful', 'found useful evaluating', 'useful evaluating degree', 'evaluating degree study', 'degree study topic', 'study topic expected', 'topic expected useful', 'expected useful feature', 'useful feature set', 'feature set document', 'set document input', 'document input lda', 'input lda selected', 'lda selected described', 'selected described lda', 'described lda collapsed', 'lda collapsed gibbs', 'collapsed gibbs sampling', 'gibbs sampling parameter', 'sampling parameter lda', 'parameter lda k', 'lda k k', 'k k number', 'k number topic', 'number topic extracted', 'topic extracted topic', 'extracted topic feature', 'topic feature ratio', 'feature ratio positive', 'ratio positive ratio', 'positive ratio negative', 'ratio negative text', 'negative text final', 'text final feature', 'final feature relating', 'feature relating content', 'relating content intuitively', 'content intuitively expected', 'intuitively expected use', 'expected use negative', 'use negative frequently', 'negative frequently without', 'frequently without categorize', 'without categorize dictionary', 'categorize dictionary affective', 'dictionary affective compiled', 'affective compiled manual', 'compiled manual evaluation', 'manual evaluation dictionary', 'evaluation dictionary positive', 'dictionary positive negative', 'positive negative extracted', 'negative extracted according', 'extracted according technique', 'according technique proposed', 'technique proposed literature', 'proposed literature dictionary', 'literature dictionary contains', 'dictionary contains positive', 'contains positive negative', 'positive negative timing', 'negative timing frequency', 'timing frequency average', 'frequency average number', 'average number retweet', 'number retweet rate', 'retweet rate rate', 'rate rate republishing', 'rate republishing mention', 'republishing mention rate', 'mention rate rate', 'rate rate directly', 'rate directly referencing', 'directly referencing least', 'referencing least one', 'least one ratio', 'one ratio containing', 'ratio containing uniform', 'containing uniform resource', 'uniform resource locator', 'resource locator url', 'locator url number', 'url number followed', 'number followed number', 'followed number following', 'number following feature', 'following feature independent', 'feature independent content', 'independent content relative', 'content relative ratio', 'relative ratio posted', 'ratio posted hour', 'posted hour day', 'hour day characterize', 'day characterize timing', 'characterize timing number', 'timing number per', 'number per day', 'per day posting', 'day posting frequency', 'posting frequency ratio', 'frequency ratio qualifying', 'ratio qualifying retweet', 'qualifying retweet ratio', 'retweet ratio mention', 'ratio mention ratio', 'mention ratio ratio', 'ratio ratio containing', 'ratio containing url', 'containing url feature', 'url feature prior', 'feature prior research']"
https://www.nature.com/articles/s41598-017-12961-9,0,In an effort to minimize noisy and unreliable data we applied several quality assurance measures in our data collection process. MTurk workers who have completed at least 100 tasks with a minimum 95% approval rating have been found to provide reliable valid survey responses33. We restricted survey visibility only to workers with these qualifications. Survey access was also restricted to U.S. IP addresses as MTurk data collected from outside the United States are generally of poorer quality34. All participants were only permitted to take the survey once. We excluded participants with a total of fewer than five Twitter posts. We also excluded participants with CES-D scores of 21 or lower (depression) or TSQ scores of 5 or lower (PTSD). Studies have indicated that a CES-D score of 22 represents an optimal cutoff for identifying clinically relevant depression3536; an equivalent TSQ cutoff of 6 has been found to be optimal in the case of PTSD32. We note here that in the study that inspired the present work De Choudhury et al.8 used two depression scales (CES-D and BDI) and filtered individuals whose depression score did not correlate across the both scales. This additional criteria is a methodological strength of De Choudhury et al.8 with respect to the present work.,in an effort to minimize noisy and unreliable data we applied several quality assurance measure in our data collection process mturk worker who have completed at least task with a minimum approval rating have been found to provide reliable valid survey response we restricted survey visibility only to worker with these qualification survey access wa also restricted to u ip address a mturk data collected from outside the united state are generally of poorer quality all participant were only permitted to take the survey once we excluded participant with a total of fewer than five twitter post we also excluded participant with cesd score of or lower depression or tsq score of or lower ptsd study have indicated that a cesd score of represents an optimal cutoff for identifying clinically relevant depression an equivalent tsq cutoff of ha been found to be optimal in the case of ptsd we note here that in the study that inspired the present work de choudhury et al used two depression scale cesd and bdi and filtered individual whose depression score did not correlate across the both scale this additional criterion is a methodological strength of de choudhury et al with respect to the present work,"['effort', 'minimize', 'noisy', 'unreliable', 'applied', 'several', 'quality', 'assurance', 'measure', 'collection', 'process', 'mturk', 'worker', 'completed', 'least', 'task', 'minimum', 'approval', 'rating', 'found', 'provide', 'reliable', 'valid', 'survey', 'response', 'restricted', 'survey', 'visibility', 'worker', 'qualification', 'survey', 'access', 'also', 'restricted', 'u', 'ip', 'address', 'mturk', 'collected', 'outside', 'united', 'state', 'generally', 'poorer', 'quality', 'permitted', 'take', 'survey', 'excluded', 'total', 'fewer', 'five', 'also', 'excluded', 'cesd', 'score', 'lower', 'tsq', 'score', 'lower', 'ptsd', 'study', 'indicated', 'cesd', 'score', 'represents', 'optimal', 'cutoff', 'identifying', 'clinically', 'relevant', 'equivalent', 'tsq', 'cutoff', 'ha', 'found', 'optimal', 'case', 'ptsd', 'note', 'study', 'inspired', 'present', 'work', 'de', 'choudhury', 'et', 'al', 'two', 'scale', 'cesd', 'bdi', 'filtered', 'individual', 'whose', 'score', 'correlate', 'across', 'scale', 'additional', 'criterion', 'methodological', 'strength', 'de', 'choudhury', 'et', 'al', 'respect', 'present', 'work']","['effort minimize', 'minimize noisy', 'noisy unreliable', 'unreliable applied', 'applied several', 'several quality', 'quality assurance', 'assurance measure', 'measure collection', 'collection process', 'process mturk', 'mturk worker', 'worker completed', 'completed least', 'least task', 'task minimum', 'minimum approval', 'approval rating', 'rating found', 'found provide', 'provide reliable', 'reliable valid', 'valid survey', 'survey response', 'response restricted', 'restricted survey', 'survey visibility', 'visibility worker', 'worker qualification', 'qualification survey', 'survey access', 'access also', 'also restricted', 'restricted u', 'u ip', 'ip address', 'address mturk', 'mturk collected', 'collected outside', 'outside united', 'united state', 'state generally', 'generally poorer', 'poorer quality', 'quality permitted', 'permitted take', 'take survey', 'survey excluded', 'excluded total', 'total fewer', 'fewer five', 'five also', 'also excluded', 'excluded cesd', 'cesd score', 'score lower', 'lower tsq', 'tsq score', 'score lower', 'lower ptsd', 'ptsd study', 'study indicated', 'indicated cesd', 'cesd score', 'score represents', 'represents optimal', 'optimal cutoff', 'cutoff identifying', 'identifying clinically', 'clinically relevant', 'relevant equivalent', 'equivalent tsq', 'tsq cutoff', 'cutoff ha', 'ha found', 'found optimal', 'optimal case', 'case ptsd', 'ptsd note', 'note study', 'study inspired', 'inspired present', 'present work', 'work de', 'de choudhury', 'choudhury et', 'et al', 'al two', 'two scale', 'scale cesd', 'cesd bdi', 'bdi filtered', 'filtered individual', 'individual whose', 'whose score', 'score correlate', 'correlate across', 'across scale', 'scale additional', 'additional criterion', 'criterion methodological', 'methodological strength', 'strength de', 'de choudhury', 'choudhury et', 'et al', 'al respect', 'respect present', 'present work']","['effort minimize noisy', 'minimize noisy unreliable', 'noisy unreliable applied', 'unreliable applied several', 'applied several quality', 'several quality assurance', 'quality assurance measure', 'assurance measure collection', 'measure collection process', 'collection process mturk', 'process mturk worker', 'mturk worker completed', 'worker completed least', 'completed least task', 'least task minimum', 'task minimum approval', 'minimum approval rating', 'approval rating found', 'rating found provide', 'found provide reliable', 'provide reliable valid', 'reliable valid survey', 'valid survey response', 'survey response restricted', 'response restricted survey', 'restricted survey visibility', 'survey visibility worker', 'visibility worker qualification', 'worker qualification survey', 'qualification survey access', 'survey access also', 'access also restricted', 'also restricted u', 'restricted u ip', 'u ip address', 'ip address mturk', 'address mturk collected', 'mturk collected outside', 'collected outside united', 'outside united state', 'united state generally', 'state generally poorer', 'generally poorer quality', 'poorer quality permitted', 'quality permitted take', 'permitted take survey', 'take survey excluded', 'survey excluded total', 'excluded total fewer', 'total fewer five', 'fewer five also', 'five also excluded', 'also excluded cesd', 'excluded cesd score', 'cesd score lower', 'score lower tsq', 'lower tsq score', 'tsq score lower', 'score lower ptsd', 'lower ptsd study', 'ptsd study indicated', 'study indicated cesd', 'indicated cesd score', 'cesd score represents', 'score represents optimal', 'represents optimal cutoff', 'optimal cutoff identifying', 'cutoff identifying clinically', 'identifying clinically relevant', 'clinically relevant equivalent', 'relevant equivalent tsq', 'equivalent tsq cutoff', 'tsq cutoff ha', 'cutoff ha found', 'ha found optimal', 'found optimal case', 'optimal case ptsd', 'case ptsd note', 'ptsd note study', 'note study inspired', 'study inspired present', 'inspired present work', 'present work de', 'work de choudhury', 'de choudhury et', 'choudhury et al', 'et al two', 'al two scale', 'two scale cesd', 'scale cesd bdi', 'cesd bdi filtered', 'bdi filtered individual', 'filtered individual whose', 'individual whose score', 'whose score correlate', 'score correlate across', 'correlate across scale', 'across scale additional', 'scale additional criterion', 'additional criterion methodological', 'criterion methodological strength', 'methodological strength de', 'strength de choudhury', 'de choudhury et', 'choudhury et al', 'et al respect', 'al respect present', 'respect present work']"
https://aclanthology.org/W14-3214.pdf,1,Ngrams of order to 1 to 3 found via HappierFunTokenizer and restricted to those used by at least 5% of users (resulting in 10450 ngrams). The features were encoded as relative frequency of mentioning each ngram (ng):,ngrams of order to to found via happierfuntokenizer and restricted to those used by at least of user resulting in ngrams the feature were encoded a relative frequency of mentioning each ngram ng,"['ngrams', 'order', 'found', 'via', 'happierfuntokenizer', 'restricted', 'least', 'resulting', 'ngrams', 'feature', 'encoded', 'relative', 'frequency', 'mentioning', 'ngram', 'ng']","['ngrams order', 'order found', 'found via', 'via happierfuntokenizer', 'happierfuntokenizer restricted', 'restricted least', 'least resulting', 'resulting ngrams', 'ngrams feature', 'feature encoded', 'encoded relative', 'relative frequency', 'frequency mentioning', 'mentioning ngram', 'ngram ng']","['ngrams order found', 'order found via', 'found via happierfuntokenizer', 'via happierfuntokenizer restricted', 'happierfuntokenizer restricted least', 'restricted least resulting', 'least resulting ngrams', 'resulting ngrams feature', 'ngrams feature encoded', 'feature encoded relative', 'encoded relative frequency', 'relative frequency mentioning', 'frequency mentioning ngram', 'mentioning ngram ng']"
https://ieeexplore.ieee.org/abstract/document/6784326,0,To characterize the difference between CLINICAL and CONTROL communities a variety of features are extracted: Affective features: We use the lexicon—Affective Norms for English Words (ANEW) [5]—to extract the sentiment conveyed in the content. This lexicon consists of 1034 words rated in terms of valence and arousal and is thus suitable for a quantitative estimation. The valence of ANEW words is on a scale of 1 (very unpleasant) to 9 (very pleasant). The arousal is measured on the same scale—1 (least active) to 9 (most active). A cloud visualization of ANEW words used in the blog posts made by CLINICAL and CONTROL groups is illustrated in Fig. 1. Mood tags: LiveJournal provides a mechanism for users to tag their posts from a list of 132 pre-defined mood labels.4 Thus in addition to the emotion expressed in the text of posts the mood tag produced allows us direct access to the user sentiment. A cloud visualization of moods tagged on blog posts made by CLINICAL and CONTROL communities is llustrated in Fig. 2. LIWC features: We examine the proportions of words in psycholinguistic categories as defined in the LIWC package [27]: linguistic social affective cognitive perceptual biological relativity personal concerns and spoken.5 Table 3 presents the mean of these LIWC psycholinguistic processes for the CLINICAL and CONTROL communities. Whilst similar in the use words with positive emotion people in the CLINICAL communities tend to use words with more negative emotion—as examples anxiety anger and sadness. Further they discuss more issues about health and death in comparison with the CONTROL group. On the other hand the users in the CONTROL group discuss more neutral life related topics—ingestion home and leisure words. Topics: For extracting topics latent Dirichlet allocation (LDA) [4] is used as a Bayesian probabilistic modelling framework. LDA extracts the probabilities —that is words in a topic and then assigns a topic to each word in a document. For the inference part we implemented Gibbs inference detailed in [10]. We set the number of topics to 50 run the Gibbs for 5000 samples and use the last Gibbs sample to interpret the results.,to characterize the difference between clinical and control community a variety of feature are extracted affective feature we use the lexiconaffective norm for english word anew to extract the sentiment conveyed in the content this lexicon consists of word rated in term of valence and arousal and is thus suitable for a quantitative estimation the valence of anew word is on a scale of very unpleasant to very pleasant the arousal is measured on the same scale least active to most active a cloud visualization of anew word used in the blog post made by clinical and control group is illustrated in fig mood tag livejournal provides a mechanism for user to tag their post from a list of predefined mood label thus in addition to the emotion expressed in the text of post the mood tag produced allows u direct access to the user sentiment a cloud visualization of mood tagged on blog post made by clinical and control community is llustrated in fig liwc feature we examine the proportion of word in psycholinguistic category a defined in the liwc package linguistic social affective cognitive perceptual biological relativity personal concern and spoken table present the mean of these liwc psycholinguistic process for the clinical and control community whilst similar in the use word with positive emotion people in the clinical community tend to use word with more negative emotionas example anxiety anger and sadness further they discus more issue about health and death in comparison with the control group on the other hand the user in the control group discus more neutral life related topicsingestion home and leisure word topic for extracting topic latent dirichlet allocation lda is used a a bayesian probabilistic modelling framework lda extract the probability that is word in a topic and then assigns a topic to each word in a document for the inference part we implemented gibbs inference detailed in we set the number of topic to run the gibbs for sample and use the last gibbs sample to interpret the result,"['characterize', 'difference', 'clinical', 'community', 'variety', 'feature', 'extracted', 'affective', 'feature', 'use', 'lexiconaffective', 'norm', 'english', 'anew', 'extract', 'sentiment', 'conveyed', 'content', 'lexicon', 'consists', 'rated', 'term', 'valence', 'arousal', 'thus', 'suitable', 'quantitative', 'estimation', 'valence', 'anew', 'scale', 'unpleasant', 'pleasant', 'arousal', 'measured', 'scale', 'least', 'active', 'active', 'cloud', 'visualization', 'anew', 'blog', 'made', 'clinical', 'group', 'illustrated', 'fig', 'mood', 'tag', 'livejournal', 'provides', 'mechanism', 'tag', 'list', 'predefined', 'mood', 'label', 'thus', 'addition', 'emotion', 'expressed', 'text', 'mood', 'tag', 'produced', 'allows', 'u', 'direct', 'access', 'sentiment', 'cloud', 'visualization', 'mood', 'tagged', 'blog', 'made', 'clinical', 'community', 'llustrated', 'fig', 'liwc', 'feature', 'examine', 'proportion', 'psycholinguistic', 'category', 'defined', 'liwc', 'package', 'linguistic', 'social', 'affective', 'cognitive', 'perceptual', 'biological', 'relativity', 'personal', 'concern', 'spoken', 'table', 'present', 'mean', 'liwc', 'psycholinguistic', 'process', 'clinical', 'community', 'whilst', 'similar', 'use', 'positive', 'emotion', 'people', 'clinical', 'community', 'tend', 'use', 'negative', 'emotionas', 'example', 'anxiety', 'anger', 'sadness', 'discus', 'issue', 'death', 'comparison', 'group', 'hand', 'group', 'discus', 'neutral', 'life', 'related', 'topicsingestion', 'home', 'leisure', 'topic', 'extracting', 'topic', 'latent', 'dirichlet', 'allocation', 'lda', 'bayesian', 'probabilistic', 'modelling', 'framework', 'lda', 'extract', 'probability', 'topic', 'assigns', 'topic', 'document', 'inference', 'part', 'implemented', 'gibbs', 'inference', 'detailed', 'set', 'number', 'topic', 'run', 'gibbs', 'sample', 'use', 'last', 'gibbs', 'sample', 'interpret', 'result']","['characterize difference', 'difference clinical', 'clinical community', 'community variety', 'variety feature', 'feature extracted', 'extracted affective', 'affective feature', 'feature use', 'use lexiconaffective', 'lexiconaffective norm', 'norm english', 'english anew', 'anew extract', 'extract sentiment', 'sentiment conveyed', 'conveyed content', 'content lexicon', 'lexicon consists', 'consists rated', 'rated term', 'term valence', 'valence arousal', 'arousal thus', 'thus suitable', 'suitable quantitative', 'quantitative estimation', 'estimation valence', 'valence anew', 'anew scale', 'scale unpleasant', 'unpleasant pleasant', 'pleasant arousal', 'arousal measured', 'measured scale', 'scale least', 'least active', 'active active', 'active cloud', 'cloud visualization', 'visualization anew', 'anew blog', 'blog made', 'made clinical', 'clinical group', 'group illustrated', 'illustrated fig', 'fig mood', 'mood tag', 'tag livejournal', 'livejournal provides', 'provides mechanism', 'mechanism tag', 'tag list', 'list predefined', 'predefined mood', 'mood label', 'label thus', 'thus addition', 'addition emotion', 'emotion expressed', 'expressed text', 'text mood', 'mood tag', 'tag produced', 'produced allows', 'allows u', 'u direct', 'direct access', 'access sentiment', 'sentiment cloud', 'cloud visualization', 'visualization mood', 'mood tagged', 'tagged blog', 'blog made', 'made clinical', 'clinical community', 'community llustrated', 'llustrated fig', 'fig liwc', 'liwc feature', 'feature examine', 'examine proportion', 'proportion psycholinguistic', 'psycholinguistic category', 'category defined', 'defined liwc', 'liwc package', 'package linguistic', 'linguistic social', 'social affective', 'affective cognitive', 'cognitive perceptual', 'perceptual biological', 'biological relativity', 'relativity personal', 'personal concern', 'concern spoken', 'spoken table', 'table present', 'present mean', 'mean liwc', 'liwc psycholinguistic', 'psycholinguistic process', 'process clinical', 'clinical community', 'community whilst', 'whilst similar', 'similar use', 'use positive', 'positive emotion', 'emotion people', 'people clinical', 'clinical community', 'community tend', 'tend use', 'use negative', 'negative emotionas', 'emotionas example', 'example anxiety', 'anxiety anger', 'anger sadness', 'sadness discus', 'discus issue', 'issue death', 'death comparison', 'comparison group', 'group hand', 'hand group', 'group discus', 'discus neutral', 'neutral life', 'life related', 'related topicsingestion', 'topicsingestion home', 'home leisure', 'leisure topic', 'topic extracting', 'extracting topic', 'topic latent', 'latent dirichlet', 'dirichlet allocation', 'allocation lda', 'lda bayesian', 'bayesian probabilistic', 'probabilistic modelling', 'modelling framework', 'framework lda', 'lda extract', 'extract probability', 'probability topic', 'topic assigns', 'assigns topic', 'topic document', 'document inference', 'inference part', 'part implemented', 'implemented gibbs', 'gibbs inference', 'inference detailed', 'detailed set', 'set number', 'number topic', 'topic run', 'run gibbs', 'gibbs sample', 'sample use', 'use last', 'last gibbs', 'gibbs sample', 'sample interpret', 'interpret result']","['characterize difference clinical', 'difference clinical community', 'clinical community variety', 'community variety feature', 'variety feature extracted', 'feature extracted affective', 'extracted affective feature', 'affective feature use', 'feature use lexiconaffective', 'use lexiconaffective norm', 'lexiconaffective norm english', 'norm english anew', 'english anew extract', 'anew extract sentiment', 'extract sentiment conveyed', 'sentiment conveyed content', 'conveyed content lexicon', 'content lexicon consists', 'lexicon consists rated', 'consists rated term', 'rated term valence', 'term valence arousal', 'valence arousal thus', 'arousal thus suitable', 'thus suitable quantitative', 'suitable quantitative estimation', 'quantitative estimation valence', 'estimation valence anew', 'valence anew scale', 'anew scale unpleasant', 'scale unpleasant pleasant', 'unpleasant pleasant arousal', 'pleasant arousal measured', 'arousal measured scale', 'measured scale least', 'scale least active', 'least active active', 'active active cloud', 'active cloud visualization', 'cloud visualization anew', 'visualization anew blog', 'anew blog made', 'blog made clinical', 'made clinical group', 'clinical group illustrated', 'group illustrated fig', 'illustrated fig mood', 'fig mood tag', 'mood tag livejournal', 'tag livejournal provides', 'livejournal provides mechanism', 'provides mechanism tag', 'mechanism tag list', 'tag list predefined', 'list predefined mood', 'predefined mood label', 'mood label thus', 'label thus addition', 'thus addition emotion', 'addition emotion expressed', 'emotion expressed text', 'expressed text mood', 'text mood tag', 'mood tag produced', 'tag produced allows', 'produced allows u', 'allows u direct', 'u direct access', 'direct access sentiment', 'access sentiment cloud', 'sentiment cloud visualization', 'cloud visualization mood', 'visualization mood tagged', 'mood tagged blog', 'tagged blog made', 'blog made clinical', 'made clinical community', 'clinical community llustrated', 'community llustrated fig', 'llustrated fig liwc', 'fig liwc feature', 'liwc feature examine', 'feature examine proportion', 'examine proportion psycholinguistic', 'proportion psycholinguistic category', 'psycholinguistic category defined', 'category defined liwc', 'defined liwc package', 'liwc package linguistic', 'package linguistic social', 'linguistic social affective', 'social affective cognitive', 'affective cognitive perceptual', 'cognitive perceptual biological', 'perceptual biological relativity', 'biological relativity personal', 'relativity personal concern', 'personal concern spoken', 'concern spoken table', 'spoken table present', 'table present mean', 'present mean liwc', 'mean liwc psycholinguistic', 'liwc psycholinguistic process', 'psycholinguistic process clinical', 'process clinical community', 'clinical community whilst', 'community whilst similar', 'whilst similar use', 'similar use positive', 'use positive emotion', 'positive emotion people', 'emotion people clinical', 'people clinical community', 'clinical community tend', 'community tend use', 'tend use negative', 'use negative emotionas', 'negative emotionas example', 'emotionas example anxiety', 'example anxiety anger', 'anxiety anger sadness', 'anger sadness discus', 'sadness discus issue', 'discus issue death', 'issue death comparison', 'death comparison group', 'comparison group hand', 'group hand group', 'hand group discus', 'group discus neutral', 'discus neutral life', 'neutral life related', 'life related topicsingestion', 'related topicsingestion home', 'topicsingestion home leisure', 'home leisure topic', 'leisure topic extracting', 'topic extracting topic', 'extracting topic latent', 'topic latent dirichlet', 'latent dirichlet allocation', 'dirichlet allocation lda', 'allocation lda bayesian', 'lda bayesian probabilistic', 'bayesian probabilistic modelling', 'probabilistic modelling framework', 'modelling framework lda', 'framework lda extract', 'lda extract probability', 'extract probability topic', 'probability topic assigns', 'topic assigns topic', 'assigns topic document', 'topic document inference', 'document inference part', 'inference part implemented', 'part implemented gibbs', 'implemented gibbs inference', 'gibbs inference detailed', 'inference detailed set', 'detailed set number', 'set number topic', 'number topic run', 'topic run gibbs', 'run gibbs sample', 'gibbs sample use', 'sample use last', 'use last gibbs', 'last gibbs sample', 'gibbs sample interpret', 'sample interpret result']"
https://www.jmir.org/2017/7/e243/,0,Weibo posts were segmented using the Stanford word segmenter [39] that resulted in 349374 words and phrases. Thereafter the SC-LIWC [33] dictionary was applied to count the appearance of each category of words in every respondents’ Weibo posts. The SC-LIWC dictionary includes 7450 words that are grouped into 71 categories including 7 main linguistic or psychological categories and 64 subcategories. In addition the total number of words or phrases that each respondent published in the 12 months was counted as the 72nd category. Scores of the SC-LIWC categories were counted as percentages of the total number of words.,weibo post were segmented using the stanford word segmenter that resulted in word and phrase thereafter the scliwc dictionary wa applied to count the appearance of each category of word in every respondent weibo post the scliwc dictionary includes word that are grouped into category including main linguistic or psychological category and subcategories in addition the total number of word or phrase that each respondent published in the month wa counted a the nd category score of the scliwc category were counted a percentage of the total number of word,"['weibo', 'segmented', 'using', 'stanford', 'segmenter', 'resulted', 'phrase', 'thereafter', 'scliwc', 'dictionary', 'applied', 'count', 'appearance', 'category', 'every', 'respondent', 'weibo', 'scliwc', 'dictionary', 'includes', 'grouped', 'category', 'including', 'main', 'linguistic', 'psychological', 'category', 'subcategories', 'addition', 'total', 'number', 'phrase', 'respondent', 'published', 'month', 'counted', 'nd', 'category', 'score', 'scliwc', 'category', 'counted', 'percentage', 'total', 'number']","['weibo segmented', 'segmented using', 'using stanford', 'stanford segmenter', 'segmenter resulted', 'resulted phrase', 'phrase thereafter', 'thereafter scliwc', 'scliwc dictionary', 'dictionary applied', 'applied count', 'count appearance', 'appearance category', 'category every', 'every respondent', 'respondent weibo', 'weibo scliwc', 'scliwc dictionary', 'dictionary includes', 'includes grouped', 'grouped category', 'category including', 'including main', 'main linguistic', 'linguistic psychological', 'psychological category', 'category subcategories', 'subcategories addition', 'addition total', 'total number', 'number phrase', 'phrase respondent', 'respondent published', 'published month', 'month counted', 'counted nd', 'nd category', 'category score', 'score scliwc', 'scliwc category', 'category counted', 'counted percentage', 'percentage total', 'total number']","['weibo segmented using', 'segmented using stanford', 'using stanford segmenter', 'stanford segmenter resulted', 'segmenter resulted phrase', 'resulted phrase thereafter', 'phrase thereafter scliwc', 'thereafter scliwc dictionary', 'scliwc dictionary applied', 'dictionary applied count', 'applied count appearance', 'count appearance category', 'appearance category every', 'category every respondent', 'every respondent weibo', 'respondent weibo scliwc', 'weibo scliwc dictionary', 'scliwc dictionary includes', 'dictionary includes grouped', 'includes grouped category', 'grouped category including', 'category including main', 'including main linguistic', 'main linguistic psychological', 'linguistic psychological category', 'psychological category subcategories', 'category subcategories addition', 'subcategories addition total', 'addition total number', 'total number phrase', 'number phrase respondent', 'phrase respondent published', 'respondent published month', 'published month counted', 'month counted nd', 'counted nd category', 'nd category score', 'category score scliwc', 'score scliwc category', 'scliwc category counted', 'category counted percentage', 'counted percentage total', 'percentage total number']"
https://dl.acm.org/doi/abs/10.1145/3025453.3025909,0,We now present a methodology of identifying posts shared in university subreddits that that are likely to be mental health expressions. Note that our Reddit data does not contain any gold standard information around whether a post shared in a university subreddit is about one’s mental health experience or condition. Our proposed method overcomes this challenge by employing an inductive transfer learning approach [16]. First we include (as ground truth data) Reddit posts made on various mental health support communities. Prior work has established that in these communities individuals selfdisclose a variety of mental health challenges explicitly [50]. Parallelly we utilize another set of Reddit posts made on generic subreddits unrelated to mental health to be a control. Next we build a machine learning classifier to distinguish between these two types of posts. Then we learn features that could detect whether an post shared in a university subreddit could be an expression of some mental health concern. We discuss these steps in detail in the following subsections.,we now present a methodology of identifying post shared in university subreddits that that are likely to be mental health expression note that our reddit data doe not contain any gold standard information around whether a post shared in a university subreddit is about one mental health experience or condition our proposed method overcomes this challenge by employing an inductive transfer learning approach first we include a ground truth data reddit post made on various mental health support community prior work ha established that in these community individual selfdisclose a variety of mental health challenge explicitly parallelly we utilize another set of reddit post made on generic subreddits unrelated to mental health to be a control next we build a machine learning classifier to distinguish between these two type of post then we learn feature that could detect whether an post shared in a university subreddit could be an expression of some mental health concern we discus these step in detail in the following subsection,"['present', 'methodology', 'identifying', 'shared', 'university', 'subreddits', 'likely', 'expression', 'note', 'reddit', 'doe', 'contain', 'gold', 'standard', 'information', 'around', 'whether', 'shared', 'university', 'subreddit', 'one', 'experience', 'condition', 'proposed', 'method', 'overcomes', 'challenge', 'employing', 'inductive', 'transfer', 'learning', 'approach', 'first', 'include', 'ground', 'truth', 'reddit', 'made', 'various', 'support', 'community', 'prior', 'work', 'ha', 'established', 'community', 'individual', 'selfdisclose', 'variety', 'challenge', 'explicitly', 'parallelly', 'utilize', 'another', 'set', 'reddit', 'made', 'generic', 'subreddits', 'unrelated', 'next', 'build', 'machine', 'learning', 'classifier', 'distinguish', 'two', 'type', 'learn', 'feature', 'could', 'detect', 'whether', 'shared', 'university', 'subreddit', 'could', 'expression', 'concern', 'discus', 'step', 'detail', 'following', 'subsection']","['present methodology', 'methodology identifying', 'identifying shared', 'shared university', 'university subreddits', 'subreddits likely', 'likely expression', 'expression note', 'note reddit', 'reddit doe', 'doe contain', 'contain gold', 'gold standard', 'standard information', 'information around', 'around whether', 'whether shared', 'shared university', 'university subreddit', 'subreddit one', 'one experience', 'experience condition', 'condition proposed', 'proposed method', 'method overcomes', 'overcomes challenge', 'challenge employing', 'employing inductive', 'inductive transfer', 'transfer learning', 'learning approach', 'approach first', 'first include', 'include ground', 'ground truth', 'truth reddit', 'reddit made', 'made various', 'various support', 'support community', 'community prior', 'prior work', 'work ha', 'ha established', 'established community', 'community individual', 'individual selfdisclose', 'selfdisclose variety', 'variety challenge', 'challenge explicitly', 'explicitly parallelly', 'parallelly utilize', 'utilize another', 'another set', 'set reddit', 'reddit made', 'made generic', 'generic subreddits', 'subreddits unrelated', 'unrelated next', 'next build', 'build machine', 'machine learning', 'learning classifier', 'classifier distinguish', 'distinguish two', 'two type', 'type learn', 'learn feature', 'feature could', 'could detect', 'detect whether', 'whether shared', 'shared university', 'university subreddit', 'subreddit could', 'could expression', 'expression concern', 'concern discus', 'discus step', 'step detail', 'detail following', 'following subsection']","['present methodology identifying', 'methodology identifying shared', 'identifying shared university', 'shared university subreddits', 'university subreddits likely', 'subreddits likely expression', 'likely expression note', 'expression note reddit', 'note reddit doe', 'reddit doe contain', 'doe contain gold', 'contain gold standard', 'gold standard information', 'standard information around', 'information around whether', 'around whether shared', 'whether shared university', 'shared university subreddit', 'university subreddit one', 'subreddit one experience', 'one experience condition', 'experience condition proposed', 'condition proposed method', 'proposed method overcomes', 'method overcomes challenge', 'overcomes challenge employing', 'challenge employing inductive', 'employing inductive transfer', 'inductive transfer learning', 'transfer learning approach', 'learning approach first', 'approach first include', 'first include ground', 'include ground truth', 'ground truth reddit', 'truth reddit made', 'reddit made various', 'made various support', 'various support community', 'support community prior', 'community prior work', 'prior work ha', 'work ha established', 'ha established community', 'established community individual', 'community individual selfdisclose', 'individual selfdisclose variety', 'selfdisclose variety challenge', 'variety challenge explicitly', 'challenge explicitly parallelly', 'explicitly parallelly utilize', 'parallelly utilize another', 'utilize another set', 'another set reddit', 'set reddit made', 'reddit made generic', 'made generic subreddits', 'generic subreddits unrelated', 'subreddits unrelated next', 'unrelated next build', 'next build machine', 'build machine learning', 'machine learning classifier', 'learning classifier distinguish', 'classifier distinguish two', 'distinguish two type', 'two type learn', 'type learn feature', 'learn feature could', 'feature could detect', 'could detect whether', 'detect whether shared', 'whether shared university', 'shared university subreddit', 'university subreddit could', 'subreddit could expression', 'could expression concern', 'expression concern discus', 'concern discus step', 'discus step detail', 'step detail following', 'detail following subsection']"
https://ieeexplore.ieee.org/abstract/document/7752434,0,In this work we are focused on two main type of features (linguistic and behavioral). TF-IDF is adopted to model the linguist features of patients and Pattern of Life Features (PLF) adopted from the work of Coppersmith et al. [1] is used to model the behavioral style of patients. TF-IDF Features To capture the frequent and representative words used by the patients TF-IDF is applied on the unigram and bigrams collected from all the patients' tweets. Pattern of Life Features (PLF) These features reveal the emotional patterns and behavioral tendency of users by measuring polarity emotion and social interactions. In order to fully compose the PLF we combined the following list of features: Age and Gender: Twitter does not publicly provide information about the age and gender of its users mainly due to privacy concerns so we adopted the work of Sap et al. [5] to fill in this information. Polarity Features: The Sentiment140 API 3 was used to label each tweet as either positive negative or neutral. The polarity is furthermore transformed into five different values to capture the affective traits of each user: 1) Positive Ratio: the percentage of positive tweets 2) Negative Ratio: the percentage of negative tweets 3) Positive Combo: captures the mania and hypomania traits of patients which is determined by the number of continuous positive posts appearing more than x amount of times within a period of time in minutes T. 4) Negative Combos: captures the depression traits of patients and is determined by the number of continuous negative posts appearing more than x amount of times within a period of time in minutes T. 5) Flips Ratio: quantifies the emotional unstableness and is determined by counting how frequently two continuous tweets with different polarity (either positive to negative or negative to positive) appear together within a period of time in minutes T. In our work x is set to 2 and T is set to 30 minutes. Social Features: These features can demonstrate how users are behaving with respect to their environment. The following are the social features designed for each user: 1) Tweeting Frequency; the frequency of daily posts; 2) Mention Ratio: the percentage of posts which contain at least one mention of another user; 3) Frequent Mentions: the number of Twitter users mentioned more than three times which is a measurement of how many close friends a particular user may have; 4) Unique Mentions: the number of unique users mentioned which is a measure of the width of a user's social network.,in this work we are focused on two main type of feature linguistic and behavioral tfidf is adopted to model the linguist feature of patient and pattern of life feature plf adopted from the work of coppersmith et al is used to model the behavioral style of patient tfidf feature to capture the frequent and representative word used by the patient tfidf is applied on the unigram and bigram collected from all the patient tweet pattern of life feature plf these feature reveal the emotional pattern and behavioral tendency of user by measuring polarity emotion and social interaction in order to fully compose the plf we combined the following list of feature age and gender twitter doe not publicly provide information about the age and gender of it user mainly due to privacy concern so we adopted the work of sap et al to fill in this information polarity feature the sentiment api wa used to label each tweet a either positive negative or neutral the polarity is furthermore transformed into five different value to capture the affective trait of each user positive ratio the percentage of positive tweet negative ratio the percentage of negative tweet positive combo capture the mania and hypomania trait of patient which is determined by the number of continuous positive post appearing more than x amount of time within a period of time in minute t negative combo capture the depression trait of patient and is determined by the number of continuous negative post appearing more than x amount of time within a period of time in minute t flip ratio quantifies the emotional unstableness and is determined by counting how frequently two continuous tweet with different polarity either positive to negative or negative to positive appear together within a period of time in minute t in our work x is set to and t is set to minute social feature these feature can demonstrate how user are behaving with respect to their environment the following are the social feature designed for each user tweeting frequency the frequency of daily post mention ratio the percentage of post which contain at least one mention of another user frequent mention the number of twitter user mentioned more than three time which is a measurement of how many close friend a particular user may have unique mention the number of unique user mentioned which is a measure of the width of a user social network,"['work', 'focused', 'two', 'main', 'type', 'feature', 'linguistic', 'behavioral', 'tfidf', 'adopted', 'model', 'linguist', 'feature', 'patient', 'pattern', 'life', 'feature', 'plf', 'adopted', 'work', 'coppersmith', 'et', 'al', 'model', 'behavioral', 'style', 'patient', 'tfidf', 'feature', 'capture', 'frequent', 'representative', 'patient', 'tfidf', 'applied', 'unigram', 'bigram', 'collected', 'patient', 'pattern', 'life', 'feature', 'plf', 'feature', 'reveal', 'emotional', 'pattern', 'behavioral', 'tendency', 'measuring', 'polarity', 'emotion', 'social', 'interaction', 'order', 'fully', 'compose', 'plf', 'combined', 'following', 'list', 'feature', 'age', 'gender', 'doe', 'publicly', 'provide', 'information', 'age', 'gender', 'mainly', 'due', 'privacy', 'concern', 'adopted', 'work', 'sap', 'et', 'al', 'fill', 'information', 'polarity', 'feature', 'sentiment', 'api', 'label', 'either', 'positive', 'negative', 'neutral', 'polarity', 'furthermore', 'transformed', 'five', 'different', 'value', 'capture', 'affective', 'trait', 'positive', 'ratio', 'percentage', 'positive', 'negative', 'ratio', 'percentage', 'negative', 'positive', 'combo', 'capture', 'mania', 'hypomania', 'trait', 'patient', 'determined', 'number', 'continuous', 'positive', 'appearing', 'x', 'amount', 'time', 'within', 'period', 'time', 'minute', 'negative', 'combo', 'capture', 'trait', 'patient', 'determined', 'number', 'continuous', 'negative', 'appearing', 'x', 'amount', 'time', 'within', 'period', 'time', 'minute', 'flip', 'ratio', 'quantifies', 'emotional', 'unstableness', 'determined', 'counting', 'frequently', 'two', 'continuous', 'different', 'polarity', 'either', 'positive', 'negative', 'negative', 'positive', 'appear', 'together', 'within', 'period', 'time', 'minute', 'work', 'x', 'set', 'set', 'minute', 'social', 'feature', 'feature', 'demonstrate', 'behaving', 'respect', 'environment', 'following', 'social', 'feature', 'designed', 'tweeting', 'frequency', 'frequency', 'daily', 'mention', 'ratio', 'percentage', 'contain', 'least', 'one', 'mention', 'another', 'frequent', 'mention', 'number', 'mentioned', 'three', 'time', 'measurement', 'many', 'close', 'friend', 'particular', 'may', 'unique', 'mention', 'number', 'unique', 'mentioned', 'measure', 'width', 'social', 'network']","['work focused', 'focused two', 'two main', 'main type', 'type feature', 'feature linguistic', 'linguistic behavioral', 'behavioral tfidf', 'tfidf adopted', 'adopted model', 'model linguist', 'linguist feature', 'feature patient', 'patient pattern', 'pattern life', 'life feature', 'feature plf', 'plf adopted', 'adopted work', 'work coppersmith', 'coppersmith et', 'et al', 'al model', 'model behavioral', 'behavioral style', 'style patient', 'patient tfidf', 'tfidf feature', 'feature capture', 'capture frequent', 'frequent representative', 'representative patient', 'patient tfidf', 'tfidf applied', 'applied unigram', 'unigram bigram', 'bigram collected', 'collected patient', 'patient pattern', 'pattern life', 'life feature', 'feature plf', 'plf feature', 'feature reveal', 'reveal emotional', 'emotional pattern', 'pattern behavioral', 'behavioral tendency', 'tendency measuring', 'measuring polarity', 'polarity emotion', 'emotion social', 'social interaction', 'interaction order', 'order fully', 'fully compose', 'compose plf', 'plf combined', 'combined following', 'following list', 'list feature', 'feature age', 'age gender', 'gender doe', 'doe publicly', 'publicly provide', 'provide information', 'information age', 'age gender', 'gender mainly', 'mainly due', 'due privacy', 'privacy concern', 'concern adopted', 'adopted work', 'work sap', 'sap et', 'et al', 'al fill', 'fill information', 'information polarity', 'polarity feature', 'feature sentiment', 'sentiment api', 'api label', 'label either', 'either positive', 'positive negative', 'negative neutral', 'neutral polarity', 'polarity furthermore', 'furthermore transformed', 'transformed five', 'five different', 'different value', 'value capture', 'capture affective', 'affective trait', 'trait positive', 'positive ratio', 'ratio percentage', 'percentage positive', 'positive negative', 'negative ratio', 'ratio percentage', 'percentage negative', 'negative positive', 'positive combo', 'combo capture', 'capture mania', 'mania hypomania', 'hypomania trait', 'trait patient', 'patient determined', 'determined number', 'number continuous', 'continuous positive', 'positive appearing', 'appearing x', 'x amount', 'amount time', 'time within', 'within period', 'period time', 'time minute', 'minute negative', 'negative combo', 'combo capture', 'capture trait', 'trait patient', 'patient determined', 'determined number', 'number continuous', 'continuous negative', 'negative appearing', 'appearing x', 'x amount', 'amount time', 'time within', 'within period', 'period time', 'time minute', 'minute flip', 'flip ratio', 'ratio quantifies', 'quantifies emotional', 'emotional unstableness', 'unstableness determined', 'determined counting', 'counting frequently', 'frequently two', 'two continuous', 'continuous different', 'different polarity', 'polarity either', 'either positive', 'positive negative', 'negative negative', 'negative positive', 'positive appear', 'appear together', 'together within', 'within period', 'period time', 'time minute', 'minute work', 'work x', 'x set', 'set set', 'set minute', 'minute social', 'social feature', 'feature feature', 'feature demonstrate', 'demonstrate behaving', 'behaving respect', 'respect environment', 'environment following', 'following social', 'social feature', 'feature designed', 'designed tweeting', 'tweeting frequency', 'frequency frequency', 'frequency daily', 'daily mention', 'mention ratio', 'ratio percentage', 'percentage contain', 'contain least', 'least one', 'one mention', 'mention another', 'another frequent', 'frequent mention', 'mention number', 'number mentioned', 'mentioned three', 'three time', 'time measurement', 'measurement many', 'many close', 'close friend', 'friend particular', 'particular may', 'may unique', 'unique mention', 'mention number', 'number unique', 'unique mentioned', 'mentioned measure', 'measure width', 'width social', 'social network']","['work focused two', 'focused two main', 'two main type', 'main type feature', 'type feature linguistic', 'feature linguistic behavioral', 'linguistic behavioral tfidf', 'behavioral tfidf adopted', 'tfidf adopted model', 'adopted model linguist', 'model linguist feature', 'linguist feature patient', 'feature patient pattern', 'patient pattern life', 'pattern life feature', 'life feature plf', 'feature plf adopted', 'plf adopted work', 'adopted work coppersmith', 'work coppersmith et', 'coppersmith et al', 'et al model', 'al model behavioral', 'model behavioral style', 'behavioral style patient', 'style patient tfidf', 'patient tfidf feature', 'tfidf feature capture', 'feature capture frequent', 'capture frequent representative', 'frequent representative patient', 'representative patient tfidf', 'patient tfidf applied', 'tfidf applied unigram', 'applied unigram bigram', 'unigram bigram collected', 'bigram collected patient', 'collected patient pattern', 'patient pattern life', 'pattern life feature', 'life feature plf', 'feature plf feature', 'plf feature reveal', 'feature reveal emotional', 'reveal emotional pattern', 'emotional pattern behavioral', 'pattern behavioral tendency', 'behavioral tendency measuring', 'tendency measuring polarity', 'measuring polarity emotion', 'polarity emotion social', 'emotion social interaction', 'social interaction order', 'interaction order fully', 'order fully compose', 'fully compose plf', 'compose plf combined', 'plf combined following', 'combined following list', 'following list feature', 'list feature age', 'feature age gender', 'age gender doe', 'gender doe publicly', 'doe publicly provide', 'publicly provide information', 'provide information age', 'information age gender', 'age gender mainly', 'gender mainly due', 'mainly due privacy', 'due privacy concern', 'privacy concern adopted', 'concern adopted work', 'adopted work sap', 'work sap et', 'sap et al', 'et al fill', 'al fill information', 'fill information polarity', 'information polarity feature', 'polarity feature sentiment', 'feature sentiment api', 'sentiment api label', 'api label either', 'label either positive', 'either positive negative', 'positive negative neutral', 'negative neutral polarity', 'neutral polarity furthermore', 'polarity furthermore transformed', 'furthermore transformed five', 'transformed five different', 'five different value', 'different value capture', 'value capture affective', 'capture affective trait', 'affective trait positive', 'trait positive ratio', 'positive ratio percentage', 'ratio percentage positive', 'percentage positive negative', 'positive negative ratio', 'negative ratio percentage', 'ratio percentage negative', 'percentage negative positive', 'negative positive combo', 'positive combo capture', 'combo capture mania', 'capture mania hypomania', 'mania hypomania trait', 'hypomania trait patient', 'trait patient determined', 'patient determined number', 'determined number continuous', 'number continuous positive', 'continuous positive appearing', 'positive appearing x', 'appearing x amount', 'x amount time', 'amount time within', 'time within period', 'within period time', 'period time minute', 'time minute negative', 'minute negative combo', 'negative combo capture', 'combo capture trait', 'capture trait patient', 'trait patient determined', 'patient determined number', 'determined number continuous', 'number continuous negative', 'continuous negative appearing', 'negative appearing x', 'appearing x amount', 'x amount time', 'amount time within', 'time within period', 'within period time', 'period time minute', 'time minute flip', 'minute flip ratio', 'flip ratio quantifies', 'ratio quantifies emotional', 'quantifies emotional unstableness', 'emotional unstableness determined', 'unstableness determined counting', 'determined counting frequently', 'counting frequently two', 'frequently two continuous', 'two continuous different', 'continuous different polarity', 'different polarity either', 'polarity either positive', 'either positive negative', 'positive negative negative', 'negative negative positive', 'negative positive appear', 'positive appear together', 'appear together within', 'together within period', 'within period time', 'period time minute', 'time minute work', 'minute work x', 'work x set', 'x set set', 'set set minute', 'set minute social', 'minute social feature', 'social feature feature', 'feature feature demonstrate', 'feature demonstrate behaving', 'demonstrate behaving respect', 'behaving respect environment', 'respect environment following', 'environment following social', 'following social feature', 'social feature designed', 'feature designed tweeting', 'designed tweeting frequency', 'tweeting frequency frequency', 'frequency frequency daily', 'frequency daily mention', 'daily mention ratio', 'mention ratio percentage', 'ratio percentage contain', 'percentage contain least', 'contain least one', 'least one mention', 'one mention another', 'mention another frequent', 'another frequent mention', 'frequent mention number', 'mention number mentioned', 'number mentioned three', 'mentioned three time', 'three time measurement', 'time measurement many', 'measurement many close', 'many close friend', 'close friend particular', 'friend particular may', 'particular may unique', 'may unique mention', 'unique mention number', 'mention number unique', 'number unique mentioned', 'unique mentioned measure', 'mentioned measure width', 'measure width social', 'width social network']"
https://www.nature.com/articles/s41598-020-68764-y,1,The data pre-processing procedure for the collected post data is presented in Fig. 1. After collecting the data each title was combined with its corresponding post. We removed unnecessary punctuation marks and white spaces for each post. Then we used the natural language toolkit (NLTK) implemented in Python to tokenize users’ posts and filter frequently employed words (stop words). Porter Stemmer a tool used to define a series of guidelines for exploring word meaning and source was employed on the tokenized words to convert a word to its root meaning and to decrease the number of word corpus. After this procedure data from 228060 users with 488472 posts in total were employed for the analysis.,the data preprocessing procedure for the collected post data is presented in fig after collecting the data each title wa combined with it corresponding post we removed unnecessary punctuation mark and white space for each post then we used the natural language toolkit nltk implemented in python to tokenize user post and filter frequently employed word stop word porter stemmer a tool used to define a series of guideline for exploring word meaning and source wa employed on the tokenized word to convert a word to it root meaning and to decrease the number of word corpus after this procedure data from user with post in total were employed for the analysis,"['preprocessing', 'procedure', 'collected', 'presented', 'fig', 'collecting', 'title', 'combined', 'corresponding', 'removed', 'unnecessary', 'punctuation', 'mark', 'white', 'space', 'natural', 'language', 'toolkit', 'nltk', 'implemented', 'python', 'tokenize', 'filter', 'frequently', 'employed', 'stop', 'porter', 'stemmer', 'tool', 'define', 'series', 'guideline', 'exploring', 'meaning', 'source', 'employed', 'tokenized', 'convert', 'root', 'meaning', 'decrease', 'number', 'corpus', 'procedure', 'total', 'employed', 'analysis']","['preprocessing procedure', 'procedure collected', 'collected presented', 'presented fig', 'fig collecting', 'collecting title', 'title combined', 'combined corresponding', 'corresponding removed', 'removed unnecessary', 'unnecessary punctuation', 'punctuation mark', 'mark white', 'white space', 'space natural', 'natural language', 'language toolkit', 'toolkit nltk', 'nltk implemented', 'implemented python', 'python tokenize', 'tokenize filter', 'filter frequently', 'frequently employed', 'employed stop', 'stop porter', 'porter stemmer', 'stemmer tool', 'tool define', 'define series', 'series guideline', 'guideline exploring', 'exploring meaning', 'meaning source', 'source employed', 'employed tokenized', 'tokenized convert', 'convert root', 'root meaning', 'meaning decrease', 'decrease number', 'number corpus', 'corpus procedure', 'procedure total', 'total employed', 'employed analysis']","['preprocessing procedure collected', 'procedure collected presented', 'collected presented fig', 'presented fig collecting', 'fig collecting title', 'collecting title combined', 'title combined corresponding', 'combined corresponding removed', 'corresponding removed unnecessary', 'removed unnecessary punctuation', 'unnecessary punctuation mark', 'punctuation mark white', 'mark white space', 'white space natural', 'space natural language', 'natural language toolkit', 'language toolkit nltk', 'toolkit nltk implemented', 'nltk implemented python', 'implemented python tokenize', 'python tokenize filter', 'tokenize filter frequently', 'filter frequently employed', 'frequently employed stop', 'employed stop porter', 'stop porter stemmer', 'porter stemmer tool', 'stemmer tool define', 'tool define series', 'define series guideline', 'series guideline exploring', 'guideline exploring meaning', 'exploring meaning source', 'meaning source employed', 'source employed tokenized', 'employed tokenized convert', 'tokenized convert root', 'convert root meaning', 'root meaning decrease', 'meaning decrease number', 'decrease number corpus', 'number corpus procedure', 'corpus procedure total', 'procedure total employed', 'total employed analysis']"
https://aclanthology.org/W19-3013.pdf,0,We now briefly describe our approach for cohortbased studies over social media. A more detailed description of the proposed methodology will appear in a forthcoming publication. Most works on social media analysis estimate trends by aggregating document-level signals inferred from arbitrary (and biased) data samples selected to match a predefined outcome. While some recent work has begun incorporating demographic information to contextualize analyses (Mandel et al. 2012; Mitchell et al. 2013; Huang et al. 2017 2019) and to improve representativeness of the data (Coppersmith et al. 2015b; Dos Reis and Culotta 2015) these studies still select on specific outcomes. We depart from these works by constructing a demographically representative digital cohort of social media users prior to the analyses and then conducting cohort-based studies over this preselected population. While a significant undertaking in most medical studies the vast quantities of available social media data make assembling social media cohorts feasible. Such cohorts can be used to support longitudinal and cross-sectional studies allowing experts to contextualize the outcomes produce externally valid trends from inherently biased samples and extrapolate those trends to a broader population. Similar strategies have been utilized in online surveys which can have comparable validity to other survey modalities simply by controlling for basic demographic features such as the location age ethnicity and gender (Duffy et al. 2005).,we now briefly describe our approach for cohortbased study over social medium a more detailed description of the proposed methodology will appear in a forthcoming publication most work on social medium analysis estimate trend by aggregating documentlevel signal inferred from arbitrary and biased data sample selected to match a predefined outcome while some recent work ha begun incorporating demographic information to contextualize analysis mandel et al mitchell et al huang et al and to improve representativeness of the data coppersmith et al b do real and culotta these study still select on specific outcome we depart from these work by constructing a demographically representative digital cohort of social medium user prior to the analysis and then conducting cohortbased study over this preselected population while a significant undertaking in most medical study the vast quantity of available social medium data make assembling social medium cohort feasible such cohort can be used to support longitudinal and crosssectional study allowing expert to contextualize the outcome produce externally valid trend from inherently biased sample and extrapolate those trend to a broader population similar strategy have been utilized in online survey which can have comparable validity to other survey modality simply by controlling for basic demographic feature such a the location age ethnicity and gender duffy et al,"['briefly', 'describe', 'approach', 'cohortbased', 'study', 'social', 'medium', 'detailed', 'description', 'proposed', 'methodology', 'appear', 'forthcoming', 'publication', 'work', 'social', 'medium', 'analysis', 'estimate', 'trend', 'aggregating', 'documentlevel', 'signal', 'inferred', 'arbitrary', 'biased', 'sample', 'selected', 'match', 'predefined', 'outcome', 'recent', 'work', 'ha', 'begun', 'incorporating', 'demographic', 'information', 'contextualize', 'analysis', 'mandel', 'et', 'al', 'mitchell', 'et', 'al', 'huang', 'et', 'al', 'improve', 'representativeness', 'coppersmith', 'et', 'al', 'b', 'real', 'culotta', 'study', 'still', 'select', 'specific', 'outcome', 'depart', 'work', 'constructing', 'demographically', 'representative', 'digital', 'cohort', 'social', 'medium', 'prior', 'analysis', 'conducting', 'cohortbased', 'study', 'preselected', 'population', 'significant', 'undertaking', 'medical', 'study', 'vast', 'quantity', 'available', 'social', 'medium', 'make', 'assembling', 'social', 'medium', 'cohort', 'feasible', 'cohort', 'support', 'longitudinal', 'crosssectional', 'study', 'allowing', 'expert', 'contextualize', 'outcome', 'produce', 'externally', 'valid', 'trend', 'inherently', 'biased', 'sample', 'extrapolate', 'trend', 'broader', 'population', 'similar', 'strategy', 'utilized', 'online', 'survey', 'comparable', 'validity', 'survey', 'modality', 'simply', 'controlling', 'basic', 'demographic', 'feature', 'location', 'age', 'ethnicity', 'gender', 'duffy', 'et', 'al']","['briefly describe', 'describe approach', 'approach cohortbased', 'cohortbased study', 'study social', 'social medium', 'medium detailed', 'detailed description', 'description proposed', 'proposed methodology', 'methodology appear', 'appear forthcoming', 'forthcoming publication', 'publication work', 'work social', 'social medium', 'medium analysis', 'analysis estimate', 'estimate trend', 'trend aggregating', 'aggregating documentlevel', 'documentlevel signal', 'signal inferred', 'inferred arbitrary', 'arbitrary biased', 'biased sample', 'sample selected', 'selected match', 'match predefined', 'predefined outcome', 'outcome recent', 'recent work', 'work ha', 'ha begun', 'begun incorporating', 'incorporating demographic', 'demographic information', 'information contextualize', 'contextualize analysis', 'analysis mandel', 'mandel et', 'et al', 'al mitchell', 'mitchell et', 'et al', 'al huang', 'huang et', 'et al', 'al improve', 'improve representativeness', 'representativeness coppersmith', 'coppersmith et', 'et al', 'al b', 'b real', 'real culotta', 'culotta study', 'study still', 'still select', 'select specific', 'specific outcome', 'outcome depart', 'depart work', 'work constructing', 'constructing demographically', 'demographically representative', 'representative digital', 'digital cohort', 'cohort social', 'social medium', 'medium prior', 'prior analysis', 'analysis conducting', 'conducting cohortbased', 'cohortbased study', 'study preselected', 'preselected population', 'population significant', 'significant undertaking', 'undertaking medical', 'medical study', 'study vast', 'vast quantity', 'quantity available', 'available social', 'social medium', 'medium make', 'make assembling', 'assembling social', 'social medium', 'medium cohort', 'cohort feasible', 'feasible cohort', 'cohort support', 'support longitudinal', 'longitudinal crosssectional', 'crosssectional study', 'study allowing', 'allowing expert', 'expert contextualize', 'contextualize outcome', 'outcome produce', 'produce externally', 'externally valid', 'valid trend', 'trend inherently', 'inherently biased', 'biased sample', 'sample extrapolate', 'extrapolate trend', 'trend broader', 'broader population', 'population similar', 'similar strategy', 'strategy utilized', 'utilized online', 'online survey', 'survey comparable', 'comparable validity', 'validity survey', 'survey modality', 'modality simply', 'simply controlling', 'controlling basic', 'basic demographic', 'demographic feature', 'feature location', 'location age', 'age ethnicity', 'ethnicity gender', 'gender duffy', 'duffy et', 'et al']","['briefly describe approach', 'describe approach cohortbased', 'approach cohortbased study', 'cohortbased study social', 'study social medium', 'social medium detailed', 'medium detailed description', 'detailed description proposed', 'description proposed methodology', 'proposed methodology appear', 'methodology appear forthcoming', 'appear forthcoming publication', 'forthcoming publication work', 'publication work social', 'work social medium', 'social medium analysis', 'medium analysis estimate', 'analysis estimate trend', 'estimate trend aggregating', 'trend aggregating documentlevel', 'aggregating documentlevel signal', 'documentlevel signal inferred', 'signal inferred arbitrary', 'inferred arbitrary biased', 'arbitrary biased sample', 'biased sample selected', 'sample selected match', 'selected match predefined', 'match predefined outcome', 'predefined outcome recent', 'outcome recent work', 'recent work ha', 'work ha begun', 'ha begun incorporating', 'begun incorporating demographic', 'incorporating demographic information', 'demographic information contextualize', 'information contextualize analysis', 'contextualize analysis mandel', 'analysis mandel et', 'mandel et al', 'et al mitchell', 'al mitchell et', 'mitchell et al', 'et al huang', 'al huang et', 'huang et al', 'et al improve', 'al improve representativeness', 'improve representativeness coppersmith', 'representativeness coppersmith et', 'coppersmith et al', 'et al b', 'al b real', 'b real culotta', 'real culotta study', 'culotta study still', 'study still select', 'still select specific', 'select specific outcome', 'specific outcome depart', 'outcome depart work', 'depart work constructing', 'work constructing demographically', 'constructing demographically representative', 'demographically representative digital', 'representative digital cohort', 'digital cohort social', 'cohort social medium', 'social medium prior', 'medium prior analysis', 'prior analysis conducting', 'analysis conducting cohortbased', 'conducting cohortbased study', 'cohortbased study preselected', 'study preselected population', 'preselected population significant', 'population significant undertaking', 'significant undertaking medical', 'undertaking medical study', 'medical study vast', 'study vast quantity', 'vast quantity available', 'quantity available social', 'available social medium', 'social medium make', 'medium make assembling', 'make assembling social', 'assembling social medium', 'social medium cohort', 'medium cohort feasible', 'cohort feasible cohort', 'feasible cohort support', 'cohort support longitudinal', 'support longitudinal crosssectional', 'longitudinal crosssectional study', 'crosssectional study allowing', 'study allowing expert', 'allowing expert contextualize', 'expert contextualize outcome', 'contextualize outcome produce', 'outcome produce externally', 'produce externally valid', 'externally valid trend', 'valid trend inherently', 'trend inherently biased', 'inherently biased sample', 'biased sample extrapolate', 'sample extrapolate trend', 'extrapolate trend broader', 'trend broader population', 'broader population similar', 'population similar strategy', 'similar strategy utilized', 'strategy utilized online', 'utilized online survey', 'online survey comparable', 'survey comparable validity', 'comparable validity survey', 'validity survey modality', 'survey modality simply', 'modality simply controlling', 'simply controlling basic', 'controlling basic demographic', 'basic demographic feature', 'demographic feature location', 'feature location age', 'location age ethnicity', 'age ethnicity gender', 'ethnicity gender duffy', 'gender duffy et', 'duffy et al']"
https://link.springer.com/chapter/10.1007/978-3-319-67186-4_6,1,First we removed journals with no text and those with fewer than 20 characters1 leaving 1.1 million journals for topic modelling. Next we pre-processed the text using the Stanford Tweet Tokenizer which is a “Twitter-aware” tokenizer designed to handle short informal text [1]. We used the option that truncates characters repeating 3 or more times converting phrases such as “I’m sooooo happyy” to “I’m soo happyy”. On average the number of tokens per journal was 27.7. Since we are interested in topics we removed stopwords and tokens with fewer than two letters and we only retained nouns which appear in the WordNet corpus [10]. After this filtering the average number of nouns per journal was 7. Examples of frequently appearing nouns in alphabetical order include “anxiety” “class” “dinner” “family” “god” “job” “lunch” “miss” “school” “sick” “sleep” and “work”. We then iteratively clustered the journals into topics (details below) and removed nouns that do not refer to topics such as numbers timings (e.g. “today” “yesterday”) general feelings (e.g. “feel” “like”) proper nouns and nouns that have ambiguous meanings (e.g. “overall” “true”). Lastly we only retained nouns that appeared more than ten times in the dataset. This process resulted in a vocabulary of 8386 words for topic modelling. Each journal is represented as a 8386-dimensional term frequency vector with each component denoting the term-frequency/ inverse-document-frequency (TF-IDF) of the corresponding term. Algorithm 1 summarizes our topic modelling methodology. Given a TF-IDF term frequency vector for each journal we run non-negative matrix factorization (NMF) [8] implemented in Python’s scikit-learn package [12]. The objective of NMF is to find two matrices whose product approximates the original matrix. In our case one matrix is the weighted set of topics in each journal and the other is the weighted set of words that belong to each topic. Hence each journal is represented as a combination of topics which are themselves composed of a weighted combination of words. We chose NMF because its non-negativity constraint aids with interpretability. In the context of analyzing word frequencies negative presence of a word would not be interpretable. This is because we only track word occurrences and not semantics or syntax. Unlike other matrix factorization methods NMF reconstructs each document from a sum of positive parts which enables us to easily manually label the discovered topics. Iterating from 4 to 40 topics we derived 37 different topic matrices (steps 1 and 2 of Algorithm 1). Each matrix consists of one topic per row. Each topic has a positive weight for each word in the vocabulary. Stronger weights indicate higher relevance to the topic. The final topic matrix we used has 14 topics and is shown in Table 1. We show the first six words in this table for simplicity where we sorted the words associated with each topic from highest relevance to lowest. When judging the topic matrices we considered the top twenty most important words per topic. Using this information we manually labeled each row in the matrix with a corresponding topic. Furthermore we manually evaluated each matrix based on the distinctness between topics consistency within topics and interpretability. During this process we compiled a custom list of removed words that we mentioned earlier in this section. The groups of words we removed appeared as stand-alone topics that did not offer information about what the journal was about. For example proper nouns appeared as a stand-alone topic. Other words which we deemed too general or ambiguous appeared across several topics and hence did not provide discriminative information. We tested different levels of regularization to enforce sparseness in our models (see [8] for a discussion) but did not find significant differences. However one important modification we made to regularize each topic was to make their first words only as strong as their second ones (by default first words are stronger than second words which are stronger than third words and so on). This is since the most relevant word for each topic tended to be too strong of a signal regardless of how we changed the number of topics pre-processing procedure or regularization in the objective function. For example the word “love” in a journal about sports would be so strong that the journal would be labeled as relating to romantic love. Lowering the importance of first words was sufficient to eliminate the false positives we identified. Given the final topic matrix (summarized in Table 1) the next step is to use it to assign labels to journals (steps 3 and 4 of Algorithm 1). We plotted the distribution of how important each topic was to all journals in the dataset with importance ranging from zero to one. Each distribution had a similar shape with a clear inflection point between 0.05 to 0.15 importance. Figure 4 shows an example importance distribution for the topic “Work” where the inflection point occurs at 0.1 importance.,first we removed journal with no text and those with fewer than character leaving million journal for topic modelling next we preprocessed the text using the stanford tweet tokenizer which is a twitteraware tokenizer designed to handle short informal text we used the option that truncates character repeating or more time converting phrase such a im sooooo happyy to im soo happyy on average the number of token per journal wa since we are interested in topic we removed stopwords and token with fewer than two letter and we only retained noun which appear in the wordnet corpus after this filtering the average number of noun per journal wa example of frequently appearing noun in alphabetical order include anxiety class dinner family god job lunch miss school sick sleep and work we then iteratively clustered the journal into topic detail below and removed noun that do not refer to topic such a number timing eg today yesterday general feeling eg feel like proper noun and noun that have ambiguous meaning eg overall true lastly we only retained noun that appeared more than ten time in the dataset this process resulted in a vocabulary of word for topic modelling each journal is represented a a dimensional term frequency vector with each component denoting the termfrequency inversedocumentfrequency tfidf of the corresponding term algorithm summarizes our topic modelling methodology given a tfidf term frequency vector for each journal we run nonnegative matrix factorization nmf implemented in python scikitlearn package the objective of nmf is to find two matrix whose product approximates the original matrix in our case one matrix is the weighted set of topic in each journal and the other is the weighted set of word that belong to each topic hence each journal is represented a a combination of topic which are themselves composed of a weighted combination of word we chose nmf because it nonnegativity constraint aid with interpretability in the context of analyzing word frequency negative presence of a word would not be interpretable this is because we only track word occurrence and not semantics or syntax unlike other matrix factorization method nmf reconstructs each document from a sum of positive part which enables u to easily manually label the discovered topic iterating from to topic we derived different topic matrix step and of algorithm each matrix consists of one topic per row each topic ha a positive weight for each word in the vocabulary stronger weight indicate higher relevance to the topic the final topic matrix we used ha topic and is shown in table we show the first six word in this table for simplicity where we sorted the word associated with each topic from highest relevance to lowest when judging the topic matrix we considered the top twenty most important word per topic using this information we manually labeled each row in the matrix with a corresponding topic furthermore we manually evaluated each matrix based on the distinctness between topic consistency within topic and interpretability during this process we compiled a custom list of removed word that we mentioned earlier in this section the group of word we removed appeared a standalone topic that did not offer information about what the journal wa about for example proper noun appeared a a standalone topic other word which we deemed too general or ambiguous appeared across several topic and hence did not provide discriminative information we tested different level of regularization to enforce sparseness in our model see for a discussion but did not find significant difference however one important modification we made to regularize each topic wa to make their first word only a strong a their second one by default first word are stronger than second word which are stronger than third word and so on this is since the most relevant word for each topic tended to be too strong of a signal regardless of how we changed the number of topic preprocessing procedure or regularization in the objective function for example the word love in a journal about sport would be so strong that the journal would be labeled a relating to romantic love lowering the importance of first word wa sufficient to eliminate the false positive we identified given the final topic matrix summarized in table the next step is to use it to assign label to journal step and of algorithm we plotted the distribution of how important each topic wa to all journal in the dataset with importance ranging from zero to one each distribution had a similar shape with a clear inflection point between to importance figure show an example importance distribution for the topic work where the inflection point occurs at importance,"['first', 'removed', 'journal', 'text', 'fewer', 'character', 'leaving', 'million', 'journal', 'topic', 'modelling', 'next', 'preprocessed', 'text', 'using', 'stanford', 'tokenizer', 'twitteraware', 'tokenizer', 'designed', 'handle', 'short', 'informal', 'text', 'option', 'truncates', 'character', 'repeating', 'time', 'converting', 'phrase', 'im', 'sooooo', 'happyy', 'im', 'soo', 'happyy', 'average', 'number', 'token', 'per', 'journal', 'since', 'interested', 'topic', 'removed', 'stopwords', 'token', 'fewer', 'two', 'letter', 'retained', 'noun', 'appear', 'wordnet', 'corpus', 'filtering', 'average', 'number', 'noun', 'per', 'journal', 'example', 'frequently', 'appearing', 'noun', 'alphabetical', 'order', 'include', 'anxiety', 'class', 'dinner', 'family', 'god', 'job', 'lunch', 'miss', 'school', 'sick', 'sleep', 'work', 'iteratively', 'clustered', 'journal', 'topic', 'detail', 'removed', 'noun', 'refer', 'topic', 'number', 'timing', 'eg', 'today', 'yesterday', 'general', 'feeling', 'eg', 'feel', 'like', 'proper', 'noun', 'noun', 'ambiguous', 'meaning', 'eg', 'overall', 'true', 'lastly', 'retained', 'noun', 'appeared', 'ten', 'time', 'dataset', 'process', 'resulted', 'vocabulary', 'topic', 'modelling', 'journal', 'represented', 'dimensional', 'term', 'frequency', 'vector', 'component', 'denoting', 'termfrequency', 'inversedocumentfrequency', 'tfidf', 'corresponding', 'term', 'algorithm', 'summarizes', 'topic', 'modelling', 'methodology', 'given', 'tfidf', 'term', 'frequency', 'vector', 'journal', 'run', 'nonnegative', 'matrix', 'factorization', 'nmf', 'implemented', 'python', 'scikitlearn', 'package', 'objective', 'nmf', 'find', 'two', 'matrix', 'whose', 'product', 'approximates', 'original', 'matrix', 'case', 'one', 'matrix', 'weighted', 'set', 'topic', 'journal', 'weighted', 'set', 'belong', 'topic', 'hence', 'journal', 'represented', 'combination', 'topic', 'composed', 'weighted', 'combination', 'chose', 'nmf', 'nonnegativity', 'constraint', 'aid', 'interpretability', 'context', 'analyzing', 'frequency', 'negative', 'presence', 'would', 'interpretable', 'track', 'occurrence', 'semantics', 'syntax', 'unlike', 'matrix', 'factorization', 'method', 'nmf', 'reconstructs', 'document', 'sum', 'positive', 'part', 'enables', 'u', 'easily', 'manually', 'label', 'discovered', 'topic', 'iterating', 'topic', 'derived', 'different', 'topic', 'matrix', 'step', 'algorithm', 'matrix', 'consists', 'one', 'topic', 'per', 'row', 'topic', 'ha', 'positive', 'weight', 'vocabulary', 'stronger', 'weight', 'indicate', 'higher', 'relevance', 'topic', 'final', 'topic', 'matrix', 'ha', 'topic', 'shown', 'table', 'show', 'first', 'six', 'table', 'simplicity', 'sorted', 'associated', 'topic', 'highest', 'relevance', 'lowest', 'judging', 'topic', 'matrix', 'considered', 'top', 'twenty', 'important', 'per', 'topic', 'using', 'information', 'manually', 'labeled', 'row', 'matrix', 'corresponding', 'topic', 'furthermore', 'manually', 'evaluated', 'matrix', 'based', 'distinctness', 'topic', 'consistency', 'within', 'topic', 'interpretability', 'process', 'compiled', 'custom', 'list', 'removed', 'mentioned', 'earlier', 'section', 'group', 'removed', 'appeared', 'standalone', 'topic', 'offer', 'information', 'journal', 'example', 'proper', 'noun', 'appeared', 'standalone', 'topic', 'deemed', 'general', 'ambiguous', 'appeared', 'across', 'several', 'topic', 'hence', 'provide', 'discriminative', 'information', 'tested', 'different', 'level', 'regularization', 'enforce', 'sparseness', 'model', 'see', 'discussion', 'find', 'significant', 'difference', 'however', 'one', 'important', 'modification', 'made', 'regularize', 'topic', 'make', 'first', 'strong', 'second', 'one', 'default', 'first', 'stronger', 'second', 'stronger', 'third', 'since', 'relevant', 'topic', 'tended', 'strong', 'signal', 'regardless', 'changed', 'number', 'topic', 'preprocessing', 'procedure', 'regularization', 'objective', 'function', 'example', 'love', 'journal', 'sport', 'would', 'strong', 'journal', 'would', 'labeled', 'relating', 'romantic', 'love', 'lowering', 'importance', 'first', 'sufficient', 'eliminate', 'false', 'positive', 'identified', 'given', 'final', 'topic', 'matrix', 'summarized', 'table', 'next', 'step', 'use', 'assign', 'label', 'journal', 'step', 'algorithm', 'plotted', 'distribution', 'important', 'topic', 'journal', 'dataset', 'importance', 'ranging', 'zero', 'one', 'distribution', 'similar', 'shape', 'clear', 'inflection', 'point', 'importance', 'figure', 'show', 'example', 'importance', 'distribution', 'topic', 'work', 'inflection', 'point', 'occurs', 'importance']","['first removed', 'removed journal', 'journal text', 'text fewer', 'fewer character', 'character leaving', 'leaving million', 'million journal', 'journal topic', 'topic modelling', 'modelling next', 'next preprocessed', 'preprocessed text', 'text using', 'using stanford', 'stanford tokenizer', 'tokenizer twitteraware', 'twitteraware tokenizer', 'tokenizer designed', 'designed handle', 'handle short', 'short informal', 'informal text', 'text option', 'option truncates', 'truncates character', 'character repeating', 'repeating time', 'time converting', 'converting phrase', 'phrase im', 'im sooooo', 'sooooo happyy', 'happyy im', 'im soo', 'soo happyy', 'happyy average', 'average number', 'number token', 'token per', 'per journal', 'journal since', 'since interested', 'interested topic', 'topic removed', 'removed stopwords', 'stopwords token', 'token fewer', 'fewer two', 'two letter', 'letter retained', 'retained noun', 'noun appear', 'appear wordnet', 'wordnet corpus', 'corpus filtering', 'filtering average', 'average number', 'number noun', 'noun per', 'per journal', 'journal example', 'example frequently', 'frequently appearing', 'appearing noun', 'noun alphabetical', 'alphabetical order', 'order include', 'include anxiety', 'anxiety class', 'class dinner', 'dinner family', 'family god', 'god job', 'job lunch', 'lunch miss', 'miss school', 'school sick', 'sick sleep', 'sleep work', 'work iteratively', 'iteratively clustered', 'clustered journal', 'journal topic', 'topic detail', 'detail removed', 'removed noun', 'noun refer', 'refer topic', 'topic number', 'number timing', 'timing eg', 'eg today', 'today yesterday', 'yesterday general', 'general feeling', 'feeling eg', 'eg feel', 'feel like', 'like proper', 'proper noun', 'noun noun', 'noun ambiguous', 'ambiguous meaning', 'meaning eg', 'eg overall', 'overall true', 'true lastly', 'lastly retained', 'retained noun', 'noun appeared', 'appeared ten', 'ten time', 'time dataset', 'dataset process', 'process resulted', 'resulted vocabulary', 'vocabulary topic', 'topic modelling', 'modelling journal', 'journal represented', 'represented dimensional', 'dimensional term', 'term frequency', 'frequency vector', 'vector component', 'component denoting', 'denoting termfrequency', 'termfrequency inversedocumentfrequency', 'inversedocumentfrequency tfidf', 'tfidf corresponding', 'corresponding term', 'term algorithm', 'algorithm summarizes', 'summarizes topic', 'topic modelling', 'modelling methodology', 'methodology given', 'given tfidf', 'tfidf term', 'term frequency', 'frequency vector', 'vector journal', 'journal run', 'run nonnegative', 'nonnegative matrix', 'matrix factorization', 'factorization nmf', 'nmf implemented', 'implemented python', 'python scikitlearn', 'scikitlearn package', 'package objective', 'objective nmf', 'nmf find', 'find two', 'two matrix', 'matrix whose', 'whose product', 'product approximates', 'approximates original', 'original matrix', 'matrix case', 'case one', 'one matrix', 'matrix weighted', 'weighted set', 'set topic', 'topic journal', 'journal weighted', 'weighted set', 'set belong', 'belong topic', 'topic hence', 'hence journal', 'journal represented', 'represented combination', 'combination topic', 'topic composed', 'composed weighted', 'weighted combination', 'combination chose', 'chose nmf', 'nmf nonnegativity', 'nonnegativity constraint', 'constraint aid', 'aid interpretability', 'interpretability context', 'context analyzing', 'analyzing frequency', 'frequency negative', 'negative presence', 'presence would', 'would interpretable', 'interpretable track', 'track occurrence', 'occurrence semantics', 'semantics syntax', 'syntax unlike', 'unlike matrix', 'matrix factorization', 'factorization method', 'method nmf', 'nmf reconstructs', 'reconstructs document', 'document sum', 'sum positive', 'positive part', 'part enables', 'enables u', 'u easily', 'easily manually', 'manually label', 'label discovered', 'discovered topic', 'topic iterating', 'iterating topic', 'topic derived', 'derived different', 'different topic', 'topic matrix', 'matrix step', 'step algorithm', 'algorithm matrix', 'matrix consists', 'consists one', 'one topic', 'topic per', 'per row', 'row topic', 'topic ha', 'ha positive', 'positive weight', 'weight vocabulary', 'vocabulary stronger', 'stronger weight', 'weight indicate', 'indicate higher', 'higher relevance', 'relevance topic', 'topic final', 'final topic', 'topic matrix', 'matrix ha', 'ha topic', 'topic shown', 'shown table', 'table show', 'show first', 'first six', 'six table', 'table simplicity', 'simplicity sorted', 'sorted associated', 'associated topic', 'topic highest', 'highest relevance', 'relevance lowest', 'lowest judging', 'judging topic', 'topic matrix', 'matrix considered', 'considered top', 'top twenty', 'twenty important', 'important per', 'per topic', 'topic using', 'using information', 'information manually', 'manually labeled', 'labeled row', 'row matrix', 'matrix corresponding', 'corresponding topic', 'topic furthermore', 'furthermore manually', 'manually evaluated', 'evaluated matrix', 'matrix based', 'based distinctness', 'distinctness topic', 'topic consistency', 'consistency within', 'within topic', 'topic interpretability', 'interpretability process', 'process compiled', 'compiled custom', 'custom list', 'list removed', 'removed mentioned', 'mentioned earlier', 'earlier section', 'section group', 'group removed', 'removed appeared', 'appeared standalone', 'standalone topic', 'topic offer', 'offer information', 'information journal', 'journal example', 'example proper', 'proper noun', 'noun appeared', 'appeared standalone', 'standalone topic', 'topic deemed', 'deemed general', 'general ambiguous', 'ambiguous appeared', 'appeared across', 'across several', 'several topic', 'topic hence', 'hence provide', 'provide discriminative', 'discriminative information', 'information tested', 'tested different', 'different level', 'level regularization', 'regularization enforce', 'enforce sparseness', 'sparseness model', 'model see', 'see discussion', 'discussion find', 'find significant', 'significant difference', 'difference however', 'however one', 'one important', 'important modification', 'modification made', 'made regularize', 'regularize topic', 'topic make', 'make first', 'first strong', 'strong second', 'second one', 'one default', 'default first', 'first stronger', 'stronger second', 'second stronger', 'stronger third', 'third since', 'since relevant', 'relevant topic', 'topic tended', 'tended strong', 'strong signal', 'signal regardless', 'regardless changed', 'changed number', 'number topic', 'topic preprocessing', 'preprocessing procedure', 'procedure regularization', 'regularization objective', 'objective function', 'function example', 'example love', 'love journal', 'journal sport', 'sport would', 'would strong', 'strong journal', 'journal would', 'would labeled', 'labeled relating', 'relating romantic', 'romantic love', 'love lowering', 'lowering importance', 'importance first', 'first sufficient', 'sufficient eliminate', 'eliminate false', 'false positive', 'positive identified', 'identified given', 'given final', 'final topic', 'topic matrix', 'matrix summarized', 'summarized table', 'table next', 'next step', 'step use', 'use assign', 'assign label', 'label journal', 'journal step', 'step algorithm', 'algorithm plotted', 'plotted distribution', 'distribution important', 'important topic', 'topic journal', 'journal dataset', 'dataset importance', 'importance ranging', 'ranging zero', 'zero one', 'one distribution', 'distribution similar', 'similar shape', 'shape clear', 'clear inflection', 'inflection point', 'point importance', 'importance figure', 'figure show', 'show example', 'example importance', 'importance distribution', 'distribution topic', 'topic work', 'work inflection', 'inflection point', 'point occurs', 'occurs importance']","['first removed journal', 'removed journal text', 'journal text fewer', 'text fewer character', 'fewer character leaving', 'character leaving million', 'leaving million journal', 'million journal topic', 'journal topic modelling', 'topic modelling next', 'modelling next preprocessed', 'next preprocessed text', 'preprocessed text using', 'text using stanford', 'using stanford tokenizer', 'stanford tokenizer twitteraware', 'tokenizer twitteraware tokenizer', 'twitteraware tokenizer designed', 'tokenizer designed handle', 'designed handle short', 'handle short informal', 'short informal text', 'informal text option', 'text option truncates', 'option truncates character', 'truncates character repeating', 'character repeating time', 'repeating time converting', 'time converting phrase', 'converting phrase im', 'phrase im sooooo', 'im sooooo happyy', 'sooooo happyy im', 'happyy im soo', 'im soo happyy', 'soo happyy average', 'happyy average number', 'average number token', 'number token per', 'token per journal', 'per journal since', 'journal since interested', 'since interested topic', 'interested topic removed', 'topic removed stopwords', 'removed stopwords token', 'stopwords token fewer', 'token fewer two', 'fewer two letter', 'two letter retained', 'letter retained noun', 'retained noun appear', 'noun appear wordnet', 'appear wordnet corpus', 'wordnet corpus filtering', 'corpus filtering average', 'filtering average number', 'average number noun', 'number noun per', 'noun per journal', 'per journal example', 'journal example frequently', 'example frequently appearing', 'frequently appearing noun', 'appearing noun alphabetical', 'noun alphabetical order', 'alphabetical order include', 'order include anxiety', 'include anxiety class', 'anxiety class dinner', 'class dinner family', 'dinner family god', 'family god job', 'god job lunch', 'job lunch miss', 'lunch miss school', 'miss school sick', 'school sick sleep', 'sick sleep work', 'sleep work iteratively', 'work iteratively clustered', 'iteratively clustered journal', 'clustered journal topic', 'journal topic detail', 'topic detail removed', 'detail removed noun', 'removed noun refer', 'noun refer topic', 'refer topic number', 'topic number timing', 'number timing eg', 'timing eg today', 'eg today yesterday', 'today yesterday general', 'yesterday general feeling', 'general feeling eg', 'feeling eg feel', 'eg feel like', 'feel like proper', 'like proper noun', 'proper noun noun', 'noun noun ambiguous', 'noun ambiguous meaning', 'ambiguous meaning eg', 'meaning eg overall', 'eg overall true', 'overall true lastly', 'true lastly retained', 'lastly retained noun', 'retained noun appeared', 'noun appeared ten', 'appeared ten time', 'ten time dataset', 'time dataset process', 'dataset process resulted', 'process resulted vocabulary', 'resulted vocabulary topic', 'vocabulary topic modelling', 'topic modelling journal', 'modelling journal represented', 'journal represented dimensional', 'represented dimensional term', 'dimensional term frequency', 'term frequency vector', 'frequency vector component', 'vector component denoting', 'component denoting termfrequency', 'denoting termfrequency inversedocumentfrequency', 'termfrequency inversedocumentfrequency tfidf', 'inversedocumentfrequency tfidf corresponding', 'tfidf corresponding term', 'corresponding term algorithm', 'term algorithm summarizes', 'algorithm summarizes topic', 'summarizes topic modelling', 'topic modelling methodology', 'modelling methodology given', 'methodology given tfidf', 'given tfidf term', 'tfidf term frequency', 'term frequency vector', 'frequency vector journal', 'vector journal run', 'journal run nonnegative', 'run nonnegative matrix', 'nonnegative matrix factorization', 'matrix factorization nmf', 'factorization nmf implemented', 'nmf implemented python', 'implemented python scikitlearn', 'python scikitlearn package', 'scikitlearn package objective', 'package objective nmf', 'objective nmf find', 'nmf find two', 'find two matrix', 'two matrix whose', 'matrix whose product', 'whose product approximates', 'product approximates original', 'approximates original matrix', 'original matrix case', 'matrix case one', 'case one matrix', 'one matrix weighted', 'matrix weighted set', 'weighted set topic', 'set topic journal', 'topic journal weighted', 'journal weighted set', 'weighted set belong', 'set belong topic', 'belong topic hence', 'topic hence journal', 'hence journal represented', 'journal represented combination', 'represented combination topic', 'combination topic composed', 'topic composed weighted', 'composed weighted combination', 'weighted combination chose', 'combination chose nmf', 'chose nmf nonnegativity', 'nmf nonnegativity constraint', 'nonnegativity constraint aid', 'constraint aid interpretability', 'aid interpretability context', 'interpretability context analyzing', 'context analyzing frequency', 'analyzing frequency negative', 'frequency negative presence', 'negative presence would', 'presence would interpretable', 'would interpretable track', 'interpretable track occurrence', 'track occurrence semantics', 'occurrence semantics syntax', 'semantics syntax unlike', 'syntax unlike matrix', 'unlike matrix factorization', 'matrix factorization method', 'factorization method nmf', 'method nmf reconstructs', 'nmf reconstructs document', 'reconstructs document sum', 'document sum positive', 'sum positive part', 'positive part enables', 'part enables u', 'enables u easily', 'u easily manually', 'easily manually label', 'manually label discovered', 'label discovered topic', 'discovered topic iterating', 'topic iterating topic', 'iterating topic derived', 'topic derived different', 'derived different topic', 'different topic matrix', 'topic matrix step', 'matrix step algorithm', 'step algorithm matrix', 'algorithm matrix consists', 'matrix consists one', 'consists one topic', 'one topic per', 'topic per row', 'per row topic', 'row topic ha', 'topic ha positive', 'ha positive weight', 'positive weight vocabulary', 'weight vocabulary stronger', 'vocabulary stronger weight', 'stronger weight indicate', 'weight indicate higher', 'indicate higher relevance', 'higher relevance topic', 'relevance topic final', 'topic final topic', 'final topic matrix', 'topic matrix ha', 'matrix ha topic', 'ha topic shown', 'topic shown table', 'shown table show', 'table show first', 'show first six', 'first six table', 'six table simplicity', 'table simplicity sorted', 'simplicity sorted associated', 'sorted associated topic', 'associated topic highest', 'topic highest relevance', 'highest relevance lowest', 'relevance lowest judging', 'lowest judging topic', 'judging topic matrix', 'topic matrix considered', 'matrix considered top', 'considered top twenty', 'top twenty important', 'twenty important per', 'important per topic', 'per topic using', 'topic using information', 'using information manually', 'information manually labeled', 'manually labeled row', 'labeled row matrix', 'row matrix corresponding', 'matrix corresponding topic', 'corresponding topic furthermore', 'topic furthermore manually', 'furthermore manually evaluated', 'manually evaluated matrix', 'evaluated matrix based', 'matrix based distinctness', 'based distinctness topic', 'distinctness topic consistency', 'topic consistency within', 'consistency within topic', 'within topic interpretability', 'topic interpretability process', 'interpretability process compiled', 'process compiled custom', 'compiled custom list', 'custom list removed', 'list removed mentioned', 'removed mentioned earlier', 'mentioned earlier section', 'earlier section group', 'section group removed', 'group removed appeared', 'removed appeared standalone', 'appeared standalone topic', 'standalone topic offer', 'topic offer information', 'offer information journal', 'information journal example', 'journal example proper', 'example proper noun', 'proper noun appeared', 'noun appeared standalone', 'appeared standalone topic', 'standalone topic deemed', 'topic deemed general', 'deemed general ambiguous', 'general ambiguous appeared', 'ambiguous appeared across', 'appeared across several', 'across several topic', 'several topic hence', 'topic hence provide', 'hence provide discriminative', 'provide discriminative information', 'discriminative information tested', 'information tested different', 'tested different level', 'different level regularization', 'level regularization enforce', 'regularization enforce sparseness', 'enforce sparseness model', 'sparseness model see', 'model see discussion', 'see discussion find', 'discussion find significant', 'find significant difference', 'significant difference however', 'difference however one', 'however one important', 'one important modification', 'important modification made', 'modification made regularize', 'made regularize topic', 'regularize topic make', 'topic make first', 'make first strong', 'first strong second', 'strong second one', 'second one default', 'one default first', 'default first stronger', 'first stronger second', 'stronger second stronger', 'second stronger third', 'stronger third since', 'third since relevant', 'since relevant topic', 'relevant topic tended', 'topic tended strong', 'tended strong signal', 'strong signal regardless', 'signal regardless changed', 'regardless changed number', 'changed number topic', 'number topic preprocessing', 'topic preprocessing procedure', 'preprocessing procedure regularization', 'procedure regularization objective', 'regularization objective function', 'objective function example', 'function example love', 'example love journal', 'love journal sport', 'journal sport would', 'sport would strong', 'would strong journal', 'strong journal would', 'journal would labeled', 'would labeled relating', 'labeled relating romantic', 'relating romantic love', 'romantic love lowering', 'love lowering importance', 'lowering importance first', 'importance first sufficient', 'first sufficient eliminate', 'sufficient eliminate false', 'eliminate false positive', 'false positive identified', 'positive identified given', 'identified given final', 'given final topic', 'final topic matrix', 'topic matrix summarized', 'matrix summarized table', 'summarized table next', 'table next step', 'next step use', 'step use assign', 'use assign label', 'assign label journal', 'label journal step', 'journal step algorithm', 'step algorithm plotted', 'algorithm plotted distribution', 'plotted distribution important', 'distribution important topic', 'important topic journal', 'topic journal dataset', 'journal dataset importance', 'dataset importance ranging', 'importance ranging zero', 'ranging zero one', 'zero one distribution', 'one distribution similar', 'distribution similar shape', 'similar shape clear', 'shape clear inflection', 'clear inflection point', 'inflection point importance', 'point importance figure', 'importance figure show', 'figure show example', 'show example importance', 'example importance distribution', 'importance distribution topic', 'distribution topic work', 'topic work inflection', 'work inflection point', 'inflection point occurs', 'point occurs importance']"
https://dl.acm.org/doi/abs/10.1145/2556288.2557214,0,We focus on the social media platform Twitter a popular microblogging service used by 18% of U.S. Internet users and whose popularity continues to increase [28]. Twitter is particularly interesting to study since nearly all posts are public; the public nature of tweets provides an interesting counterpoint to the private nature of search engine activity. We gathered a 15-month sample of Twitter’s Firehose stream (which includes all public tweets) between November 1 2011 and March 31 2013 made available to us under contract focusing on English-language tweets. Twitter post count and unique user count were computed for each condition and aggregated over the full time period. Specifically we considered a post to belong to a certain health condition if there was a regular expression match of the condition to the text of the post (this would not permit substring matches within terms). To reduce noise we excluded posts that were retweets or contained hyperlinks since they were likely related to general news and not a user’s personal health. Using this method we obtained 125166549 tweets on the 165 health conditions from 62269225 users in the time period of interest. The median number of posts was 51687 per condition from a median of 40152 users per condition.,we focus on the social medium platform twitter a popular microblogging service used by of u internet user and whose popularity continues to increase twitter is particularly interesting to study since nearly all post are public the public nature of tweet provides an interesting counterpoint to the private nature of search engine activity we gathered a month sample of twitter firehose stream which includes all public tweet between november and march made available to u under contract focusing on englishlanguage tweet twitter post count and unique user count were computed for each condition and aggregated over the full time period specifically we considered a post to belong to a certain health condition if there wa a regular expression match of the condition to the text of the post this would not permit substring match within term to reduce noise we excluded post that were retweets or contained hyperlink since they were likely related to general news and not a user personal health using this method we obtained tweet on the health condition from user in the time period of interest the median number of post wa per condition from a median of user per condition,"['focus', 'social', 'medium', 'platform', 'popular', 'microblogging', 'service', 'u', 'internet', 'whose', 'popularity', 'continues', 'increase', 'particularly', 'interesting', 'study', 'since', 'nearly', 'public', 'public', 'nature', 'provides', 'interesting', 'counterpoint', 'private', 'nature', 'search', 'engine', 'activity', 'gathered', 'month', 'sample', 'firehose', 'stream', 'includes', 'public', 'november', 'march', 'made', 'available', 'u', 'contract', 'focusing', 'englishlanguage', 'count', 'unique', 'count', 'computed', 'condition', 'aggregated', 'full', 'time', 'period', 'specifically', 'considered', 'belong', 'certain', 'condition', 'regular', 'expression', 'match', 'condition', 'text', 'would', 'permit', 'substring', 'match', 'within', 'term', 'reduce', 'noise', 'excluded', 'retweets', 'contained', 'hyperlink', 'since', 'likely', 'related', 'general', 'news', 'personal', 'using', 'method', 'obtained', 'condition', 'time', 'period', 'interest', 'median', 'number', 'per', 'condition', 'median', 'per', 'condition']","['focus social', 'social medium', 'medium platform', 'platform popular', 'popular microblogging', 'microblogging service', 'service u', 'u internet', 'internet whose', 'whose popularity', 'popularity continues', 'continues increase', 'increase particularly', 'particularly interesting', 'interesting study', 'study since', 'since nearly', 'nearly public', 'public public', 'public nature', 'nature provides', 'provides interesting', 'interesting counterpoint', 'counterpoint private', 'private nature', 'nature search', 'search engine', 'engine activity', 'activity gathered', 'gathered month', 'month sample', 'sample firehose', 'firehose stream', 'stream includes', 'includes public', 'public november', 'november march', 'march made', 'made available', 'available u', 'u contract', 'contract focusing', 'focusing englishlanguage', 'englishlanguage count', 'count unique', 'unique count', 'count computed', 'computed condition', 'condition aggregated', 'aggregated full', 'full time', 'time period', 'period specifically', 'specifically considered', 'considered belong', 'belong certain', 'certain condition', 'condition regular', 'regular expression', 'expression match', 'match condition', 'condition text', 'text would', 'would permit', 'permit substring', 'substring match', 'match within', 'within term', 'term reduce', 'reduce noise', 'noise excluded', 'excluded retweets', 'retweets contained', 'contained hyperlink', 'hyperlink since', 'since likely', 'likely related', 'related general', 'general news', 'news personal', 'personal using', 'using method', 'method obtained', 'obtained condition', 'condition time', 'time period', 'period interest', 'interest median', 'median number', 'number per', 'per condition', 'condition median', 'median per', 'per condition']","['focus social medium', 'social medium platform', 'medium platform popular', 'platform popular microblogging', 'popular microblogging service', 'microblogging service u', 'service u internet', 'u internet whose', 'internet whose popularity', 'whose popularity continues', 'popularity continues increase', 'continues increase particularly', 'increase particularly interesting', 'particularly interesting study', 'interesting study since', 'study since nearly', 'since nearly public', 'nearly public public', 'public public nature', 'public nature provides', 'nature provides interesting', 'provides interesting counterpoint', 'interesting counterpoint private', 'counterpoint private nature', 'private nature search', 'nature search engine', 'search engine activity', 'engine activity gathered', 'activity gathered month', 'gathered month sample', 'month sample firehose', 'sample firehose stream', 'firehose stream includes', 'stream includes public', 'includes public november', 'public november march', 'november march made', 'march made available', 'made available u', 'available u contract', 'u contract focusing', 'contract focusing englishlanguage', 'focusing englishlanguage count', 'englishlanguage count unique', 'count unique count', 'unique count computed', 'count computed condition', 'computed condition aggregated', 'condition aggregated full', 'aggregated full time', 'full time period', 'time period specifically', 'period specifically considered', 'specifically considered belong', 'considered belong certain', 'belong certain condition', 'certain condition regular', 'condition regular expression', 'regular expression match', 'expression match condition', 'match condition text', 'condition text would', 'text would permit', 'would permit substring', 'permit substring match', 'substring match within', 'match within term', 'within term reduce', 'term reduce noise', 'reduce noise excluded', 'noise excluded retweets', 'excluded retweets contained', 'retweets contained hyperlink', 'contained hyperlink since', 'hyperlink since likely', 'since likely related', 'likely related general', 'related general news', 'general news personal', 'news personal using', 'personal using method', 'using method obtained', 'method obtained condition', 'obtained condition time', 'condition time period', 'time period interest', 'period interest median', 'interest median number', 'median number per', 'number per condition', 'per condition median', 'condition median per', 'median per condition']"
https://www.aaai.org/ocs/index.php/ICWSM/ICWSM14/paper/viewPaper/8075,1,To understand the nature of self-disclosure in reddit posts we first examine the general linguistic attributes manifested in their content. In Table 3 we first present a list of the most popular (stopword eliminated) unigrams that appear in reddit postings. We intended to look at these highly shared unigrams more deeply and systematically hence we organized these unigrams (stopword inclusive) in various semantic categories provided by the psycholinguistic lexicon LIWC (http://www.liwc.net/). We find that among the unigrams in Table 3 there are words that extensively span emotional or affective expressions (happy love bad anxiety good hate) e.g.: “I've been recently wondering if my love to numb the world around me has turned me into an alcoholic…” “Has anyone else battled numbness loss of feeling during recovery? Does it ever get better? i've been sober for about 2 years and still have pretty severe anxiety at times”. We observe presence of relationships and social life words too (family friends people person parents) e.g.: “i get really anxious out when i go home for big events.” “i do love my family they're just really loud and argumentative sometimes”. Temporal indicators in reddit discourse is also visible— e.g. time day years months: “hi all i'm ten weeks sober today and while i wish i could say i'm physically and mentally in great shape the truth is i judge my days by how i feel less bad as oppose to good”. Work and daily grind oriented words are common as well because lifestyle irregularities are often associated with the psychopathology of mental illness (Prigerson et al. 1995)—e.g. life school work job: “I am completely broke can't afford rehab and can't take time off work”. We also find a fair number of cognitive words in these highly used unigrams (felt hard feeling lot) e.g.: “I'm new here but having anxiety like I haven't felt in a long time” “I find a lot of strength in going to a concert. I have been understanding of my anxiety and depression since i was about 8 and i hated it”. These observations are supported by psychology literature where cognitive biases as manifested through dysfunctional attitudes depressive attributional biases and negative automatic thoughts were found to be characteristic of mental illness (Eaves & Rush 1984). Further inhibition words like avoid deny safe demonstrate that redditors are perhaps using the platform to broadcast their thoughts to an audience of strangers or weak ties on issues and topics they might consider to be socially stigmatic to be discussed elsewhere: “i can't escape the feeling of fright i have at all times. i don't feel safe in my own home” “ive been denying my (assumed) depression symptoms for close to two years now writing them off …” Comparing across different LIWC semantic categories over all posts we observe noticeable differences— KruskalWallis one-way analysis of variance indicated the differences across categories to be significant (χ2 (39; N=20411)=9.24; p<10-4). Table 4 reports the top 8 most common LIWC categories the mean proportion of words from each category in the posts and the corresponding standard deviation. Note that the percentages over all categories sum to greater than 100% since a word could belong to multiple categories. Observing closely many of the categories whose corresponding unigrams appeared in Table 3 are also highly prominent categories globally over all posts as given in Table 4. Not shown in Table 4 perhaps intuitively negative emotion anger and sadness words were considerably more prominent than positive emotion words (a Wilcoxon signed rank test reveals that the differences are statistically significant (z=-6.08 p<.001)). Likely these redditors experience several negative emotions: hence mental instability helplessness loneliness restlessness manifest in their postings (Rude et al. 2004). the health and social issues they are facing. In fact high selfattentional focus is a known psychological attribute of mental illness sufferers (Chung & Pennebaker 2007).,to understand the nature of selfdisclosure in reddit post we first examine the general linguistic attribute manifested in their content in table we first present a list of the most popular stopword eliminated unigrams that appear in reddit posting we intended to look at these highly shared unigrams more deeply and systematically hence we organized these unigrams stopword inclusive in various semantic category provided by the psycholinguistic lexicon liwc httpwwwliwcnet we find that among the unigrams in table there are word that extensively span emotional or affective expression happy love bad anxiety good hate eg ive been recently wondering if my love to numb the world around me ha turned me into an alcoholic ha anyone else battled numbness loss of feeling during recovery doe it ever get better ive been sober for about year and still have pretty severe anxiety at time we observe presence of relationship and social life word too family friend people person parent eg i get really anxious out when i go home for big event i do love my family theyre just really loud and argumentative sometimes temporal indicator in reddit discourse is also visible eg time day year month hi all im ten week sober today and while i wish i could say im physically and mentally in great shape the truth is i judge my day by how i feel le bad a oppose to good work and daily grind oriented word are common a well because lifestyle irregularity are often associated with the psychopathology of mental illness prigerson et al eg life school work job i am completely broke cant afford rehab and cant take time off work we also find a fair number of cognitive word in these highly used unigrams felt hard feeling lot eg im new here but having anxiety like i havent felt in a long time i find a lot of strength in going to a concert i have been understanding of my anxiety and depression since i wa about and i hated it these observation are supported by psychology literature where cognitive bias a manifested through dysfunctional attitude depressive attributional bias and negative automatic thought were found to be characteristic of mental illness eaves rush further inhibition word like avoid deny safe demonstrate that redditors are perhaps using the platform to broadcast their thought to an audience of stranger or weak tie on issue and topic they might consider to be socially stigmatic to be discussed elsewhere i cant escape the feeling of fright i have at all time i dont feel safe in my own home ive been denying my assumed depression symptom for close to two year now writing them off comparing across different liwc semantic category over all post we observe noticeable difference kruskalwallis oneway analysis of variance indicated the difference across category to be significant n p table report the top most common liwc category the mean proportion of word from each category in the post and the corresponding standard deviation note that the percentage over all category sum to greater than since a word could belong to multiple category observing closely many of the category whose corresponding unigrams appeared in table are also highly prominent category globally over all post a given in table not shown in table perhaps intuitively negative emotion anger and sadness word were considerably more prominent than positive emotion word a wilcoxon signed rank test reveals that the difference are statistically significant z p likely these redditors experience several negative emotion hence mental instability helplessness loneliness restlessness manifest in their posting rude et al the health and social issue they are facing in fact high selfattentional focus is a known psychological attribute of mental illness sufferer chung pennebaker,"['understand', 'nature', 'selfdisclosure', 'reddit', 'first', 'examine', 'general', 'linguistic', 'attribute', 'manifested', 'content', 'table', 'first', 'present', 'list', 'popular', 'stopword', 'eliminated', 'unigrams', 'appear', 'reddit', 'posting', 'intended', 'look', 'highly', 'shared', 'unigrams', 'deeply', 'systematically', 'hence', 'organized', 'unigrams', 'stopword', 'inclusive', 'various', 'semantic', 'category', 'provided', 'psycholinguistic', 'lexicon', 'liwc', 'httpwwwliwcnet', 'find', 'among', 'unigrams', 'table', 'extensively', 'span', 'emotional', 'affective', 'expression', 'happy', 'love', 'bad', 'anxiety', 'good', 'hate', 'eg', 'ive', 'recently', 'wondering', 'love', 'numb', 'world', 'around', 'ha', 'turned', 'alcoholic', 'ha', 'anyone', 'else', 'battled', 'numbness', 'loss', 'feeling', 'recovery', 'doe', 'ever', 'get', 'better', 'ive', 'sober', 'year', 'still', 'pretty', 'severe', 'anxiety', 'time', 'observe', 'presence', 'relationship', 'social', 'life', 'family', 'friend', 'people', 'person', 'parent', 'eg', 'get', 'really', 'anxious', 'go', 'home', 'big', 'event', 'love', 'family', 'theyre', 'really', 'loud', 'argumentative', 'sometimes', 'temporal', 'indicator', 'reddit', 'discourse', 'also', 'visible', 'eg', 'time', 'day', 'year', 'month', 'hi', 'im', 'ten', 'week', 'sober', 'today', 'wish', 'could', 'say', 'im', 'physically', 'mentally', 'great', 'shape', 'truth', 'judge', 'day', 'feel', 'le', 'bad', 'oppose', 'good', 'work', 'daily', 'grind', 'oriented', 'common', 'well', 'lifestyle', 'irregularity', 'often', 'associated', 'psychopathology', 'illness', 'prigerson', 'et', 'al', 'eg', 'life', 'school', 'work', 'job', 'completely', 'broke', 'cant', 'afford', 'rehab', 'cant', 'take', 'time', 'work', 'also', 'find', 'fair', 'number', 'cognitive', 'highly', 'unigrams', 'felt', 'hard', 'feeling', 'lot', 'eg', 'im', 'new', 'anxiety', 'like', 'havent', 'felt', 'long', 'time', 'find', 'lot', 'strength', 'going', 'concert', 'understanding', 'anxiety', 'since', 'hated', 'observation', 'supported', 'psychology', 'literature', 'cognitive', 'bias', 'manifested', 'dysfunctional', 'attitude', 'depressive', 'attributional', 'bias', 'negative', 'automatic', 'thought', 'found', 'characteristic', 'illness', 'eaves', 'rush', 'inhibition', 'like', 'avoid', 'deny', 'safe', 'demonstrate', 'redditors', 'perhaps', 'using', 'platform', 'broadcast', 'thought', 'audience', 'stranger', 'weak', 'tie', 'issue', 'topic', 'might', 'consider', 'socially', 'stigmatic', 'discussed', 'elsewhere', 'cant', 'escape', 'feeling', 'fright', 'time', 'dont', 'feel', 'safe', 'home', 'ive', 'denying', 'assumed', 'symptom', 'close', 'two', 'year', 'writing', 'comparing', 'across', 'different', 'liwc', 'semantic', 'category', 'observe', 'noticeable', 'difference', 'kruskalwallis', 'oneway', 'analysis', 'variance', 'indicated', 'difference', 'across', 'category', 'significant', 'n', 'p', 'table', 'report', 'top', 'common', 'liwc', 'category', 'mean', 'proportion', 'category', 'corresponding', 'standard', 'deviation', 'note', 'percentage', 'category', 'sum', 'greater', 'since', 'could', 'belong', 'multiple', 'category', 'observing', 'closely', 'many', 'category', 'whose', 'corresponding', 'unigrams', 'appeared', 'table', 'also', 'highly', 'prominent', 'category', 'globally', 'given', 'table', 'shown', 'table', 'perhaps', 'intuitively', 'negative', 'emotion', 'anger', 'sadness', 'considerably', 'prominent', 'positive', 'emotion', 'wilcoxon', 'signed', 'rank', 'test', 'reveals', 'difference', 'statistically', 'significant', 'z', 'p', 'likely', 'redditors', 'experience', 'several', 'negative', 'emotion', 'hence', 'instability', 'helplessness', 'loneliness', 'restlessness', 'manifest', 'posting', 'rude', 'et', 'al', 'social', 'issue', 'facing', 'fact', 'high', 'selfattentional', 'focus', 'known', 'psychological', 'attribute', 'illness', 'sufferer', 'chung', 'pennebaker']","['understand nature', 'nature selfdisclosure', 'selfdisclosure reddit', 'reddit first', 'first examine', 'examine general', 'general linguistic', 'linguistic attribute', 'attribute manifested', 'manifested content', 'content table', 'table first', 'first present', 'present list', 'list popular', 'popular stopword', 'stopword eliminated', 'eliminated unigrams', 'unigrams appear', 'appear reddit', 'reddit posting', 'posting intended', 'intended look', 'look highly', 'highly shared', 'shared unigrams', 'unigrams deeply', 'deeply systematically', 'systematically hence', 'hence organized', 'organized unigrams', 'unigrams stopword', 'stopword inclusive', 'inclusive various', 'various semantic', 'semantic category', 'category provided', 'provided psycholinguistic', 'psycholinguistic lexicon', 'lexicon liwc', 'liwc httpwwwliwcnet', 'httpwwwliwcnet find', 'find among', 'among unigrams', 'unigrams table', 'table extensively', 'extensively span', 'span emotional', 'emotional affective', 'affective expression', 'expression happy', 'happy love', 'love bad', 'bad anxiety', 'anxiety good', 'good hate', 'hate eg', 'eg ive', 'ive recently', 'recently wondering', 'wondering love', 'love numb', 'numb world', 'world around', 'around ha', 'ha turned', 'turned alcoholic', 'alcoholic ha', 'ha anyone', 'anyone else', 'else battled', 'battled numbness', 'numbness loss', 'loss feeling', 'feeling recovery', 'recovery doe', 'doe ever', 'ever get', 'get better', 'better ive', 'ive sober', 'sober year', 'year still', 'still pretty', 'pretty severe', 'severe anxiety', 'anxiety time', 'time observe', 'observe presence', 'presence relationship', 'relationship social', 'social life', 'life family', 'family friend', 'friend people', 'people person', 'person parent', 'parent eg', 'eg get', 'get really', 'really anxious', 'anxious go', 'go home', 'home big', 'big event', 'event love', 'love family', 'family theyre', 'theyre really', 'really loud', 'loud argumentative', 'argumentative sometimes', 'sometimes temporal', 'temporal indicator', 'indicator reddit', 'reddit discourse', 'discourse also', 'also visible', 'visible eg', 'eg time', 'time day', 'day year', 'year month', 'month hi', 'hi im', 'im ten', 'ten week', 'week sober', 'sober today', 'today wish', 'wish could', 'could say', 'say im', 'im physically', 'physically mentally', 'mentally great', 'great shape', 'shape truth', 'truth judge', 'judge day', 'day feel', 'feel le', 'le bad', 'bad oppose', 'oppose good', 'good work', 'work daily', 'daily grind', 'grind oriented', 'oriented common', 'common well', 'well lifestyle', 'lifestyle irregularity', 'irregularity often', 'often associated', 'associated psychopathology', 'psychopathology illness', 'illness prigerson', 'prigerson et', 'et al', 'al eg', 'eg life', 'life school', 'school work', 'work job', 'job completely', 'completely broke', 'broke cant', 'cant afford', 'afford rehab', 'rehab cant', 'cant take', 'take time', 'time work', 'work also', 'also find', 'find fair', 'fair number', 'number cognitive', 'cognitive highly', 'highly unigrams', 'unigrams felt', 'felt hard', 'hard feeling', 'feeling lot', 'lot eg', 'eg im', 'im new', 'new anxiety', 'anxiety like', 'like havent', 'havent felt', 'felt long', 'long time', 'time find', 'find lot', 'lot strength', 'strength going', 'going concert', 'concert understanding', 'understanding anxiety', 'anxiety since', 'since hated', 'hated observation', 'observation supported', 'supported psychology', 'psychology literature', 'literature cognitive', 'cognitive bias', 'bias manifested', 'manifested dysfunctional', 'dysfunctional attitude', 'attitude depressive', 'depressive attributional', 'attributional bias', 'bias negative', 'negative automatic', 'automatic thought', 'thought found', 'found characteristic', 'characteristic illness', 'illness eaves', 'eaves rush', 'rush inhibition', 'inhibition like', 'like avoid', 'avoid deny', 'deny safe', 'safe demonstrate', 'demonstrate redditors', 'redditors perhaps', 'perhaps using', 'using platform', 'platform broadcast', 'broadcast thought', 'thought audience', 'audience stranger', 'stranger weak', 'weak tie', 'tie issue', 'issue topic', 'topic might', 'might consider', 'consider socially', 'socially stigmatic', 'stigmatic discussed', 'discussed elsewhere', 'elsewhere cant', 'cant escape', 'escape feeling', 'feeling fright', 'fright time', 'time dont', 'dont feel', 'feel safe', 'safe home', 'home ive', 'ive denying', 'denying assumed', 'assumed symptom', 'symptom close', 'close two', 'two year', 'year writing', 'writing comparing', 'comparing across', 'across different', 'different liwc', 'liwc semantic', 'semantic category', 'category observe', 'observe noticeable', 'noticeable difference', 'difference kruskalwallis', 'kruskalwallis oneway', 'oneway analysis', 'analysis variance', 'variance indicated', 'indicated difference', 'difference across', 'across category', 'category significant', 'significant n', 'n p', 'p table', 'table report', 'report top', 'top common', 'common liwc', 'liwc category', 'category mean', 'mean proportion', 'proportion category', 'category corresponding', 'corresponding standard', 'standard deviation', 'deviation note', 'note percentage', 'percentage category', 'category sum', 'sum greater', 'greater since', 'since could', 'could belong', 'belong multiple', 'multiple category', 'category observing', 'observing closely', 'closely many', 'many category', 'category whose', 'whose corresponding', 'corresponding unigrams', 'unigrams appeared', 'appeared table', 'table also', 'also highly', 'highly prominent', 'prominent category', 'category globally', 'globally given', 'given table', 'table shown', 'shown table', 'table perhaps', 'perhaps intuitively', 'intuitively negative', 'negative emotion', 'emotion anger', 'anger sadness', 'sadness considerably', 'considerably prominent', 'prominent positive', 'positive emotion', 'emotion wilcoxon', 'wilcoxon signed', 'signed rank', 'rank test', 'test reveals', 'reveals difference', 'difference statistically', 'statistically significant', 'significant z', 'z p', 'p likely', 'likely redditors', 'redditors experience', 'experience several', 'several negative', 'negative emotion', 'emotion hence', 'hence instability', 'instability helplessness', 'helplessness loneliness', 'loneliness restlessness', 'restlessness manifest', 'manifest posting', 'posting rude', 'rude et', 'et al', 'al social', 'social issue', 'issue facing', 'facing fact', 'fact high', 'high selfattentional', 'selfattentional focus', 'focus known', 'known psychological', 'psychological attribute', 'attribute illness', 'illness sufferer', 'sufferer chung', 'chung pennebaker']","['understand nature selfdisclosure', 'nature selfdisclosure reddit', 'selfdisclosure reddit first', 'reddit first examine', 'first examine general', 'examine general linguistic', 'general linguistic attribute', 'linguistic attribute manifested', 'attribute manifested content', 'manifested content table', 'content table first', 'table first present', 'first present list', 'present list popular', 'list popular stopword', 'popular stopword eliminated', 'stopword eliminated unigrams', 'eliminated unigrams appear', 'unigrams appear reddit', 'appear reddit posting', 'reddit posting intended', 'posting intended look', 'intended look highly', 'look highly shared', 'highly shared unigrams', 'shared unigrams deeply', 'unigrams deeply systematically', 'deeply systematically hence', 'systematically hence organized', 'hence organized unigrams', 'organized unigrams stopword', 'unigrams stopword inclusive', 'stopword inclusive various', 'inclusive various semantic', 'various semantic category', 'semantic category provided', 'category provided psycholinguistic', 'provided psycholinguistic lexicon', 'psycholinguistic lexicon liwc', 'lexicon liwc httpwwwliwcnet', 'liwc httpwwwliwcnet find', 'httpwwwliwcnet find among', 'find among unigrams', 'among unigrams table', 'unigrams table extensively', 'table extensively span', 'extensively span emotional', 'span emotional affective', 'emotional affective expression', 'affective expression happy', 'expression happy love', 'happy love bad', 'love bad anxiety', 'bad anxiety good', 'anxiety good hate', 'good hate eg', 'hate eg ive', 'eg ive recently', 'ive recently wondering', 'recently wondering love', 'wondering love numb', 'love numb world', 'numb world around', 'world around ha', 'around ha turned', 'ha turned alcoholic', 'turned alcoholic ha', 'alcoholic ha anyone', 'ha anyone else', 'anyone else battled', 'else battled numbness', 'battled numbness loss', 'numbness loss feeling', 'loss feeling recovery', 'feeling recovery doe', 'recovery doe ever', 'doe ever get', 'ever get better', 'get better ive', 'better ive sober', 'ive sober year', 'sober year still', 'year still pretty', 'still pretty severe', 'pretty severe anxiety', 'severe anxiety time', 'anxiety time observe', 'time observe presence', 'observe presence relationship', 'presence relationship social', 'relationship social life', 'social life family', 'life family friend', 'family friend people', 'friend people person', 'people person parent', 'person parent eg', 'parent eg get', 'eg get really', 'get really anxious', 'really anxious go', 'anxious go home', 'go home big', 'home big event', 'big event love', 'event love family', 'love family theyre', 'family theyre really', 'theyre really loud', 'really loud argumentative', 'loud argumentative sometimes', 'argumentative sometimes temporal', 'sometimes temporal indicator', 'temporal indicator reddit', 'indicator reddit discourse', 'reddit discourse also', 'discourse also visible', 'also visible eg', 'visible eg time', 'eg time day', 'time day year', 'day year month', 'year month hi', 'month hi im', 'hi im ten', 'im ten week', 'ten week sober', 'week sober today', 'sober today wish', 'today wish could', 'wish could say', 'could say im', 'say im physically', 'im physically mentally', 'physically mentally great', 'mentally great shape', 'great shape truth', 'shape truth judge', 'truth judge day', 'judge day feel', 'day feel le', 'feel le bad', 'le bad oppose', 'bad oppose good', 'oppose good work', 'good work daily', 'work daily grind', 'daily grind oriented', 'grind oriented common', 'oriented common well', 'common well lifestyle', 'well lifestyle irregularity', 'lifestyle irregularity often', 'irregularity often associated', 'often associated psychopathology', 'associated psychopathology illness', 'psychopathology illness prigerson', 'illness prigerson et', 'prigerson et al', 'et al eg', 'al eg life', 'eg life school', 'life school work', 'school work job', 'work job completely', 'job completely broke', 'completely broke cant', 'broke cant afford', 'cant afford rehab', 'afford rehab cant', 'rehab cant take', 'cant take time', 'take time work', 'time work also', 'work also find', 'also find fair', 'find fair number', 'fair number cognitive', 'number cognitive highly', 'cognitive highly unigrams', 'highly unigrams felt', 'unigrams felt hard', 'felt hard feeling', 'hard feeling lot', 'feeling lot eg', 'lot eg im', 'eg im new', 'im new anxiety', 'new anxiety like', 'anxiety like havent', 'like havent felt', 'havent felt long', 'felt long time', 'long time find', 'time find lot', 'find lot strength', 'lot strength going', 'strength going concert', 'going concert understanding', 'concert understanding anxiety', 'understanding anxiety since', 'anxiety since hated', 'since hated observation', 'hated observation supported', 'observation supported psychology', 'supported psychology literature', 'psychology literature cognitive', 'literature cognitive bias', 'cognitive bias manifested', 'bias manifested dysfunctional', 'manifested dysfunctional attitude', 'dysfunctional attitude depressive', 'attitude depressive attributional', 'depressive attributional bias', 'attributional bias negative', 'bias negative automatic', 'negative automatic thought', 'automatic thought found', 'thought found characteristic', 'found characteristic illness', 'characteristic illness eaves', 'illness eaves rush', 'eaves rush inhibition', 'rush inhibition like', 'inhibition like avoid', 'like avoid deny', 'avoid deny safe', 'deny safe demonstrate', 'safe demonstrate redditors', 'demonstrate redditors perhaps', 'redditors perhaps using', 'perhaps using platform', 'using platform broadcast', 'platform broadcast thought', 'broadcast thought audience', 'thought audience stranger', 'audience stranger weak', 'stranger weak tie', 'weak tie issue', 'tie issue topic', 'issue topic might', 'topic might consider', 'might consider socially', 'consider socially stigmatic', 'socially stigmatic discussed', 'stigmatic discussed elsewhere', 'discussed elsewhere cant', 'elsewhere cant escape', 'cant escape feeling', 'escape feeling fright', 'feeling fright time', 'fright time dont', 'time dont feel', 'dont feel safe', 'feel safe home', 'safe home ive', 'home ive denying', 'ive denying assumed', 'denying assumed symptom', 'assumed symptom close', 'symptom close two', 'close two year', 'two year writing', 'year writing comparing', 'writing comparing across', 'comparing across different', 'across different liwc', 'different liwc semantic', 'liwc semantic category', 'semantic category observe', 'category observe noticeable', 'observe noticeable difference', 'noticeable difference kruskalwallis', 'difference kruskalwallis oneway', 'kruskalwallis oneway analysis', 'oneway analysis variance', 'analysis variance indicated', 'variance indicated difference', 'indicated difference across', 'difference across category', 'across category significant', 'category significant n', 'significant n p', 'n p table', 'p table report', 'table report top', 'report top common', 'top common liwc', 'common liwc category', 'liwc category mean', 'category mean proportion', 'mean proportion category', 'proportion category corresponding', 'category corresponding standard', 'corresponding standard deviation', 'standard deviation note', 'deviation note percentage', 'note percentage category', 'percentage category sum', 'category sum greater', 'sum greater since', 'greater since could', 'since could belong', 'could belong multiple', 'belong multiple category', 'multiple category observing', 'category observing closely', 'observing closely many', 'closely many category', 'many category whose', 'category whose corresponding', 'whose corresponding unigrams', 'corresponding unigrams appeared', 'unigrams appeared table', 'appeared table also', 'table also highly', 'also highly prominent', 'highly prominent category', 'prominent category globally', 'category globally given', 'globally given table', 'given table shown', 'table shown table', 'shown table perhaps', 'table perhaps intuitively', 'perhaps intuitively negative', 'intuitively negative emotion', 'negative emotion anger', 'emotion anger sadness', 'anger sadness considerably', 'sadness considerably prominent', 'considerably prominent positive', 'prominent positive emotion', 'positive emotion wilcoxon', 'emotion wilcoxon signed', 'wilcoxon signed rank', 'signed rank test', 'rank test reveals', 'test reveals difference', 'reveals difference statistically', 'difference statistically significant', 'statistically significant z', 'significant z p', 'z p likely', 'p likely redditors', 'likely redditors experience', 'redditors experience several', 'experience several negative', 'several negative emotion', 'negative emotion hence', 'emotion hence instability', 'hence instability helplessness', 'instability helplessness loneliness', 'helplessness loneliness restlessness', 'loneliness restlessness manifest', 'restlessness manifest posting', 'manifest posting rude', 'posting rude et', 'rude et al', 'et al social', 'al social issue', 'social issue facing', 'issue facing fact', 'facing fact high', 'fact high selfattentional', 'high selfattentional focus', 'selfattentional focus known', 'focus known psychological', 'known psychological attribute', 'psychological attribute illness', 'attribute illness sufferer', 'illness sufferer chung', 'sufferer chung pennebaker']"
https://dl.acm.org/doi/abs/10.1145/3025453.3025932,1,Finally to complement the visual themes (our research goal RQ 3) we identify themes from the captions and hashtags (textual data) associated with the Instagram images in our dataset. We refer to these latent topics as linguistic themes. Existing literature [42] emphasizes the importance of studying language since it reflects a variety of thoughts functions as a signal of identity and emphasizes the social distance. We believe the linguistic themes may therefore help us contrast the visual themes around how individuals engage in mental health disclosure on Instagram. We used TwitterLDA4 to extract these linguistic themes. This method was developed for topic modeling of short text corpora for mining the latent topics. As typically done in topic modeling we pre-processed the data by removing a standard list of stop words words with very high frequency and words that occur fewer than five times. Since LDA is an unsupervised learning approach identifying the correct number of topics is challenging. We used the default hyper-parameter settings and 10 topics which we determined based on the value of average corpus likelihood over ten runs. These 10 topics constituted what is known as lifted forms of linguistic vocabulary. On these extracted linguistic vocabulary to arrive at interpretive descriptions (we call them linguistic themes) we adopted a similar semi-open coding approach as the visual themes that involved the same two researchers as above. The raters referred to the mental health literature and identified the best possible description that characterized the tokens in the linguistic vocabulary corresponding to each of the 10 linguistic topics. To characterize and represent each of the visual and the linguistic themes we propose a measure of visual diversity. This measure estimates how coherent images are with respect to the each other in a theme. To measure the diversity in terms of the latent visual features images are expressed in terms of their principle components within a theme. In the component space distance between a pair of images are computed by employing the cosine theta similarity function. To perform these set of operations we utilize the Python scikit-learn library.,finally to complement the visual theme our research goal rq we identify theme from the caption and hashtags textual data associated with the instagram image in our dataset we refer to these latent topic a linguistic theme existing literature emphasizes the importance of studying language since it reflects a variety of thought function a a signal of identity and emphasizes the social distance we believe the linguistic theme may therefore help u contrast the visual theme around how individual engage in mental health disclosure on instagram we used twitterlda to extract these linguistic theme this method wa developed for topic modeling of short text corpus for mining the latent topic a typically done in topic modeling we preprocessed the data by removing a standard list of stop word word with very high frequency and word that occur fewer than five time since lda is an unsupervised learning approach identifying the correct number of topic is challenging we used the default hyperparameter setting and topic which we determined based on the value of average corpus likelihood over ten run these topic constituted what is known a lifted form of linguistic vocabulary on these extracted linguistic vocabulary to arrive at interpretive description we call them linguistic theme we adopted a similar semiopen coding approach a the visual theme that involved the same two researcher a above the raters referred to the mental health literature and identified the best possible description that characterized the token in the linguistic vocabulary corresponding to each of the linguistic topic to characterize and represent each of the visual and the linguistic theme we propose a measure of visual diversity this measure estimate how coherent image are with respect to the each other in a theme to measure the diversity in term of the latent visual feature image are expressed in term of their principle component within a theme in the component space distance between a pair of image are computed by employing the cosine theta similarity function to perform these set of operation we utilize the python scikitlearn library,"['finally', 'complement', 'visual', 'theme', 'research', 'goal', 'rq', 'identify', 'theme', 'caption', 'hashtags', 'textual', 'associated', 'instagram', 'image', 'dataset', 'refer', 'latent', 'topic', 'linguistic', 'theme', 'existing', 'literature', 'emphasizes', 'importance', 'studying', 'language', 'since', 'reflects', 'variety', 'thought', 'function', 'signal', 'identity', 'emphasizes', 'social', 'distance', 'believe', 'linguistic', 'theme', 'may', 'therefore', 'help', 'u', 'contrast', 'visual', 'theme', 'around', 'individual', 'engage', 'disclosure', 'instagram', 'twitterlda', 'extract', 'linguistic', 'theme', 'method', 'developed', 'topic', 'modeling', 'short', 'text', 'corpus', 'mining', 'latent', 'topic', 'typically', 'done', 'topic', 'modeling', 'preprocessed', 'removing', 'standard', 'list', 'stop', 'high', 'frequency', 'occur', 'fewer', 'five', 'time', 'since', 'lda', 'unsupervised', 'learning', 'approach', 'identifying', 'correct', 'number', 'topic', 'challenging', 'default', 'hyperparameter', 'setting', 'topic', 'determined', 'based', 'value', 'average', 'corpus', 'likelihood', 'ten', 'run', 'topic', 'constituted', 'known', 'lifted', 'form', 'linguistic', 'vocabulary', 'extracted', 'linguistic', 'vocabulary', 'arrive', 'interpretive', 'description', 'call', 'linguistic', 'theme', 'adopted', 'similar', 'semiopen', 'coding', 'approach', 'visual', 'theme', 'involved', 'two', 'researcher', 'raters', 'referred', 'literature', 'identified', 'best', 'possible', 'description', 'characterized', 'token', 'linguistic', 'vocabulary', 'corresponding', 'linguistic', 'topic', 'characterize', 'represent', 'visual', 'linguistic', 'theme', 'propose', 'measure', 'visual', 'diversity', 'measure', 'estimate', 'coherent', 'image', 'respect', 'theme', 'measure', 'diversity', 'term', 'latent', 'visual', 'feature', 'image', 'expressed', 'term', 'principle', 'component', 'within', 'theme', 'component', 'space', 'distance', 'pair', 'image', 'computed', 'employing', 'cosine', 'theta', 'similarity', 'function', 'perform', 'set', 'operation', 'utilize', 'python', 'scikitlearn', 'library']","['finally complement', 'complement visual', 'visual theme', 'theme research', 'research goal', 'goal rq', 'rq identify', 'identify theme', 'theme caption', 'caption hashtags', 'hashtags textual', 'textual associated', 'associated instagram', 'instagram image', 'image dataset', 'dataset refer', 'refer latent', 'latent topic', 'topic linguistic', 'linguistic theme', 'theme existing', 'existing literature', 'literature emphasizes', 'emphasizes importance', 'importance studying', 'studying language', 'language since', 'since reflects', 'reflects variety', 'variety thought', 'thought function', 'function signal', 'signal identity', 'identity emphasizes', 'emphasizes social', 'social distance', 'distance believe', 'believe linguistic', 'linguistic theme', 'theme may', 'may therefore', 'therefore help', 'help u', 'u contrast', 'contrast visual', 'visual theme', 'theme around', 'around individual', 'individual engage', 'engage disclosure', 'disclosure instagram', 'instagram twitterlda', 'twitterlda extract', 'extract linguistic', 'linguistic theme', 'theme method', 'method developed', 'developed topic', 'topic modeling', 'modeling short', 'short text', 'text corpus', 'corpus mining', 'mining latent', 'latent topic', 'topic typically', 'typically done', 'done topic', 'topic modeling', 'modeling preprocessed', 'preprocessed removing', 'removing standard', 'standard list', 'list stop', 'stop high', 'high frequency', 'frequency occur', 'occur fewer', 'fewer five', 'five time', 'time since', 'since lda', 'lda unsupervised', 'unsupervised learning', 'learning approach', 'approach identifying', 'identifying correct', 'correct number', 'number topic', 'topic challenging', 'challenging default', 'default hyperparameter', 'hyperparameter setting', 'setting topic', 'topic determined', 'determined based', 'based value', 'value average', 'average corpus', 'corpus likelihood', 'likelihood ten', 'ten run', 'run topic', 'topic constituted', 'constituted known', 'known lifted', 'lifted form', 'form linguistic', 'linguistic vocabulary', 'vocabulary extracted', 'extracted linguistic', 'linguistic vocabulary', 'vocabulary arrive', 'arrive interpretive', 'interpretive description', 'description call', 'call linguistic', 'linguistic theme', 'theme adopted', 'adopted similar', 'similar semiopen', 'semiopen coding', 'coding approach', 'approach visual', 'visual theme', 'theme involved', 'involved two', 'two researcher', 'researcher raters', 'raters referred', 'referred literature', 'literature identified', 'identified best', 'best possible', 'possible description', 'description characterized', 'characterized token', 'token linguistic', 'linguistic vocabulary', 'vocabulary corresponding', 'corresponding linguistic', 'linguistic topic', 'topic characterize', 'characterize represent', 'represent visual', 'visual linguistic', 'linguistic theme', 'theme propose', 'propose measure', 'measure visual', 'visual diversity', 'diversity measure', 'measure estimate', 'estimate coherent', 'coherent image', 'image respect', 'respect theme', 'theme measure', 'measure diversity', 'diversity term', 'term latent', 'latent visual', 'visual feature', 'feature image', 'image expressed', 'expressed term', 'term principle', 'principle component', 'component within', 'within theme', 'theme component', 'component space', 'space distance', 'distance pair', 'pair image', 'image computed', 'computed employing', 'employing cosine', 'cosine theta', 'theta similarity', 'similarity function', 'function perform', 'perform set', 'set operation', 'operation utilize', 'utilize python', 'python scikitlearn', 'scikitlearn library']","['finally complement visual', 'complement visual theme', 'visual theme research', 'theme research goal', 'research goal rq', 'goal rq identify', 'rq identify theme', 'identify theme caption', 'theme caption hashtags', 'caption hashtags textual', 'hashtags textual associated', 'textual associated instagram', 'associated instagram image', 'instagram image dataset', 'image dataset refer', 'dataset refer latent', 'refer latent topic', 'latent topic linguistic', 'topic linguistic theme', 'linguistic theme existing', 'theme existing literature', 'existing literature emphasizes', 'literature emphasizes importance', 'emphasizes importance studying', 'importance studying language', 'studying language since', 'language since reflects', 'since reflects variety', 'reflects variety thought', 'variety thought function', 'thought function signal', 'function signal identity', 'signal identity emphasizes', 'identity emphasizes social', 'emphasizes social distance', 'social distance believe', 'distance believe linguistic', 'believe linguistic theme', 'linguistic theme may', 'theme may therefore', 'may therefore help', 'therefore help u', 'help u contrast', 'u contrast visual', 'contrast visual theme', 'visual theme around', 'theme around individual', 'around individual engage', 'individual engage disclosure', 'engage disclosure instagram', 'disclosure instagram twitterlda', 'instagram twitterlda extract', 'twitterlda extract linguistic', 'extract linguistic theme', 'linguistic theme method', 'theme method developed', 'method developed topic', 'developed topic modeling', 'topic modeling short', 'modeling short text', 'short text corpus', 'text corpus mining', 'corpus mining latent', 'mining latent topic', 'latent topic typically', 'topic typically done', 'typically done topic', 'done topic modeling', 'topic modeling preprocessed', 'modeling preprocessed removing', 'preprocessed removing standard', 'removing standard list', 'standard list stop', 'list stop high', 'stop high frequency', 'high frequency occur', 'frequency occur fewer', 'occur fewer five', 'fewer five time', 'five time since', 'time since lda', 'since lda unsupervised', 'lda unsupervised learning', 'unsupervised learning approach', 'learning approach identifying', 'approach identifying correct', 'identifying correct number', 'correct number topic', 'number topic challenging', 'topic challenging default', 'challenging default hyperparameter', 'default hyperparameter setting', 'hyperparameter setting topic', 'setting topic determined', 'topic determined based', 'determined based value', 'based value average', 'value average corpus', 'average corpus likelihood', 'corpus likelihood ten', 'likelihood ten run', 'ten run topic', 'run topic constituted', 'topic constituted known', 'constituted known lifted', 'known lifted form', 'lifted form linguistic', 'form linguistic vocabulary', 'linguistic vocabulary extracted', 'vocabulary extracted linguistic', 'extracted linguistic vocabulary', 'linguistic vocabulary arrive', 'vocabulary arrive interpretive', 'arrive interpretive description', 'interpretive description call', 'description call linguistic', 'call linguistic theme', 'linguistic theme adopted', 'theme adopted similar', 'adopted similar semiopen', 'similar semiopen coding', 'semiopen coding approach', 'coding approach visual', 'approach visual theme', 'visual theme involved', 'theme involved two', 'involved two researcher', 'two researcher raters', 'researcher raters referred', 'raters referred literature', 'referred literature identified', 'literature identified best', 'identified best possible', 'best possible description', 'possible description characterized', 'description characterized token', 'characterized token linguistic', 'token linguistic vocabulary', 'linguistic vocabulary corresponding', 'vocabulary corresponding linguistic', 'corresponding linguistic topic', 'linguistic topic characterize', 'topic characterize represent', 'characterize represent visual', 'represent visual linguistic', 'visual linguistic theme', 'linguistic theme propose', 'theme propose measure', 'propose measure visual', 'measure visual diversity', 'visual diversity measure', 'diversity measure estimate', 'measure estimate coherent', 'estimate coherent image', 'coherent image respect', 'image respect theme', 'respect theme measure', 'theme measure diversity', 'measure diversity term', 'diversity term latent', 'term latent visual', 'latent visual feature', 'visual feature image', 'feature image expressed', 'image expressed term', 'expressed term principle', 'term principle component', 'principle component within', 'component within theme', 'within theme component', 'theme component space', 'component space distance', 'space distance pair', 'distance pair image', 'pair image computed', 'image computed employing', 'computed employing cosine', 'employing cosine theta', 'cosine theta similarity', 'theta similarity function', 'similarity function perform', 'function perform set', 'perform set operation', 'set operation utilize', 'operation utilize python', 'utilize python scikitlearn', 'python scikitlearn library']"
https://www.sciencedirect.com/science/article/pii/S0747563215300996,0,Tweets about depression were collected by Simply Measured a company that specializes in social media measurement and analytics (Simply Measured 2014). Simply Measured has access to the Twitter “firehose” (or full volume of tweets) via Gnip a licensed company that can retrieve the full Twitter data stream. All tweets in the English language that contained at least either “depressed” “#depressed” “depression” or “#depression” were collected between April 11 and May 4 2014. We scanned a random sample of the tweets to identify common phrases that included our keywords of interest but were not about mental health. In SAS version 9.3 (SAS Institute Inc. Cary NC) we used the index function which searches a character expression (in this case the text of the tweet) for a specific string of characters to locate and remove such tweets from our sample. We removed tweets that included the following terms regardless of capitalization: “Great Depression” “economic depression” “during the depression” “depression era” “tropical depression” and “depressed real estate”. The popularity and influence of the Tweeters was described using the distribution of followers and Klout Scores. While number of followers is a measure of popularity Klout Score is a measure of influence. Klout Scores range from 0 to 100 with a higher score indicating higher influence. Klout Score is calculated based on an algorithm that considers over 400 signals from eight different online networks. Examples of signals include the amount of retweets a person generates in relation to the amount of tweets shared and the amount of engagement a user drives from unique individuals (e.g. lots of retweets from different individuals as opposed to lots of retweets from one person) (Klout Inc. 2014).,tweet about depression were collected by simply measured a company that specializes in social medium measurement and analytics simply measured simply measured ha access to the twitter firehose or full volume of tweet via gnip a licensed company that can retrieve the full twitter data stream all tweet in the english language that contained at least either depressed depressed depression or depression were collected between april and may we scanned a random sample of the tweet to identify common phrase that included our keywords of interest but were not about mental health in sa version sa institute inc cary nc we used the index function which search a character expression in this case the text of the tweet for a specific string of character to locate and remove such tweet from our sample we removed tweet that included the following term regardless of capitalization great depression economic depression during the depression depression era tropical depression and depressed real estate the popularity and influence of the tweeter wa described using the distribution of follower and klout score while number of follower is a measure of popularity klout score is a measure of influence klout score range from to with a higher score indicating higher influence klout score is calculated based on an algorithm that considers over signal from eight different online network example of signal include the amount of retweets a person generates in relation to the amount of tweet shared and the amount of engagement a user drive from unique individual eg lot of retweets from different individual a opposed to lot of retweets from one person klout inc,"['collected', 'simply', 'measured', 'company', 'specializes', 'social', 'medium', 'measurement', 'analytics', 'simply', 'measured', 'simply', 'measured', 'ha', 'access', 'firehose', 'full', 'volume', 'via', 'gnip', 'licensed', 'company', 'retrieve', 'full', 'stream', 'english', 'language', 'contained', 'least', 'either', 'depressed', 'depressed', 'collected', 'april', 'may', 'scanned', 'random', 'sample', 'identify', 'common', 'phrase', 'included', 'keywords', 'interest', 'sa', 'version', 'sa', 'institute', 'inc', 'cary', 'nc', 'index', 'function', 'search', 'character', 'expression', 'case', 'text', 'specific', 'string', 'character', 'locate', 'remove', 'sample', 'removed', 'included', 'following', 'term', 'regardless', 'capitalization', 'great', 'economic', 'era', 'tropical', 'depressed', 'real', 'estate', 'popularity', 'influence', 'tweeter', 'described', 'using', 'distribution', 'follower', 'klout', 'score', 'number', 'follower', 'measure', 'popularity', 'klout', 'score', 'measure', 'influence', 'klout', 'score', 'range', 'higher', 'score', 'indicating', 'higher', 'influence', 'klout', 'score', 'calculated', 'based', 'algorithm', 'considers', 'signal', 'eight', 'different', 'online', 'network', 'example', 'signal', 'include', 'amount', 'retweets', 'person', 'generates', 'relation', 'amount', 'shared', 'amount', 'engagement', 'drive', 'unique', 'individual', 'eg', 'lot', 'retweets', 'different', 'individual', 'opposed', 'lot', 'retweets', 'one', 'person', 'klout', 'inc']","['collected simply', 'simply measured', 'measured company', 'company specializes', 'specializes social', 'social medium', 'medium measurement', 'measurement analytics', 'analytics simply', 'simply measured', 'measured simply', 'simply measured', 'measured ha', 'ha access', 'access firehose', 'firehose full', 'full volume', 'volume via', 'via gnip', 'gnip licensed', 'licensed company', 'company retrieve', 'retrieve full', 'full stream', 'stream english', 'english language', 'language contained', 'contained least', 'least either', 'either depressed', 'depressed depressed', 'depressed collected', 'collected april', 'april may', 'may scanned', 'scanned random', 'random sample', 'sample identify', 'identify common', 'common phrase', 'phrase included', 'included keywords', 'keywords interest', 'interest sa', 'sa version', 'version sa', 'sa institute', 'institute inc', 'inc cary', 'cary nc', 'nc index', 'index function', 'function search', 'search character', 'character expression', 'expression case', 'case text', 'text specific', 'specific string', 'string character', 'character locate', 'locate remove', 'remove sample', 'sample removed', 'removed included', 'included following', 'following term', 'term regardless', 'regardless capitalization', 'capitalization great', 'great economic', 'economic era', 'era tropical', 'tropical depressed', 'depressed real', 'real estate', 'estate popularity', 'popularity influence', 'influence tweeter', 'tweeter described', 'described using', 'using distribution', 'distribution follower', 'follower klout', 'klout score', 'score number', 'number follower', 'follower measure', 'measure popularity', 'popularity klout', 'klout score', 'score measure', 'measure influence', 'influence klout', 'klout score', 'score range', 'range higher', 'higher score', 'score indicating', 'indicating higher', 'higher influence', 'influence klout', 'klout score', 'score calculated', 'calculated based', 'based algorithm', 'algorithm considers', 'considers signal', 'signal eight', 'eight different', 'different online', 'online network', 'network example', 'example signal', 'signal include', 'include amount', 'amount retweets', 'retweets person', 'person generates', 'generates relation', 'relation amount', 'amount shared', 'shared amount', 'amount engagement', 'engagement drive', 'drive unique', 'unique individual', 'individual eg', 'eg lot', 'lot retweets', 'retweets different', 'different individual', 'individual opposed', 'opposed lot', 'lot retweets', 'retweets one', 'one person', 'person klout', 'klout inc']","['collected simply measured', 'simply measured company', 'measured company specializes', 'company specializes social', 'specializes social medium', 'social medium measurement', 'medium measurement analytics', 'measurement analytics simply', 'analytics simply measured', 'simply measured simply', 'measured simply measured', 'simply measured ha', 'measured ha access', 'ha access firehose', 'access firehose full', 'firehose full volume', 'full volume via', 'volume via gnip', 'via gnip licensed', 'gnip licensed company', 'licensed company retrieve', 'company retrieve full', 'retrieve full stream', 'full stream english', 'stream english language', 'english language contained', 'language contained least', 'contained least either', 'least either depressed', 'either depressed depressed', 'depressed depressed collected', 'depressed collected april', 'collected april may', 'april may scanned', 'may scanned random', 'scanned random sample', 'random sample identify', 'sample identify common', 'identify common phrase', 'common phrase included', 'phrase included keywords', 'included keywords interest', 'keywords interest sa', 'interest sa version', 'sa version sa', 'version sa institute', 'sa institute inc', 'institute inc cary', 'inc cary nc', 'cary nc index', 'nc index function', 'index function search', 'function search character', 'search character expression', 'character expression case', 'expression case text', 'case text specific', 'text specific string', 'specific string character', 'string character locate', 'character locate remove', 'locate remove sample', 'remove sample removed', 'sample removed included', 'removed included following', 'included following term', 'following term regardless', 'term regardless capitalization', 'regardless capitalization great', 'capitalization great economic', 'great economic era', 'economic era tropical', 'era tropical depressed', 'tropical depressed real', 'depressed real estate', 'real estate popularity', 'estate popularity influence', 'popularity influence tweeter', 'influence tweeter described', 'tweeter described using', 'described using distribution', 'using distribution follower', 'distribution follower klout', 'follower klout score', 'klout score number', 'score number follower', 'number follower measure', 'follower measure popularity', 'measure popularity klout', 'popularity klout score', 'klout score measure', 'score measure influence', 'measure influence klout', 'influence klout score', 'klout score range', 'score range higher', 'range higher score', 'higher score indicating', 'score indicating higher', 'indicating higher influence', 'higher influence klout', 'influence klout score', 'klout score calculated', 'score calculated based', 'calculated based algorithm', 'based algorithm considers', 'algorithm considers signal', 'considers signal eight', 'signal eight different', 'eight different online', 'different online network', 'online network example', 'network example signal', 'example signal include', 'signal include amount', 'include amount retweets', 'amount retweets person', 'retweets person generates', 'person generates relation', 'generates relation amount', 'relation amount shared', 'amount shared amount', 'shared amount engagement', 'amount engagement drive', 'engagement drive unique', 'drive unique individual', 'unique individual eg', 'individual eg lot', 'eg lot retweets', 'lot retweets different', 'retweets different individual', 'different individual opposed', 'individual opposed lot', 'opposed lot retweets', 'lot retweets one', 'retweets one person', 'one person klout', 'person klout inc']"
https://aclanthology.org/W15-1202.pdf,1,We follow the data acquisition and curation process of Coppersmith et al. (2014a) summarizing the major points here: Social media such as Twitter contains frequent public statements by users reporting diagnoses for various medical conditions. Many talk about physical health conditions (e.g. cancer flu) but some also discuss mental illness including schizophrenia. There are a variety of motivations for users to share this information on social media: to offer or seek support to fight the stigma of mental illness or perhaps to offer an explanation for certain behaviors.4 We obtain messages with these self-reported diagnoses using the Twitter API and filtered via (caseinsensitive) regular expression to require “schizo” or a close phonetic approximation to be present; our expression matched “schizophrenia” its subtypes and various approximations: “schizo” “skitzo” “skitso” “schizotypal” “schizoid” etc. All data we collect are public posts made between 2008 and 2015 and exclude any message marked as ‘private’ by the author. All use of the data reported in this paper has been approved by the appropriate Institutional Review Board (IRB). Each self-stated diagnosis included in this study was examined by a human annotator (one of the authors) to verify that it appeared to be a genuine statement of a schizophrenia diagnosis excluding jokes quotes or disingenuous statements. We obtained 174 users with an apparently genuine selfstated diagnosis of a schizophrenia-related condition. Note that we cannot be certain that the Twitter user was actually diagnosed with schizophrenia only that their statement of being diagnosed appears to be genuine. Previous work indicates that interannotator agreement for this task is good: κ = 0.77 (Coppersmith et al. 2014a). For each user we obtained a set of their public Twitter posts via the Twitter API collecting up to 3200 tweets.5 As we wish to focus on user-authored content we exclude from analysis all retweets and any tweets that contain a URL (which often contain text that the user did not author). We lowercase all words and convert any non-standard characters (including emoji) to a systematic ASCII representation via Unidecode.6 For our community controls we used randomlyselected Twitter users who primarily tweet in English. Specifically during a two week period in early 2014 each Twitter user who was included in Twitter’s 1% “spritzer” sample had an equal chance for inclusion in our pool of community controls. We then collected some of their historic tweets and assessed the language(s) they tweeted in according to the Chromium Compact Language Detector.7 Users were excluded from our community controls if their tweets were less than 75% English.8,we follow the data acquisition and curation process of coppersmith et al a summarizing the major point here social medium such a twitter contains frequent public statement by user reporting diagnosis for various medical condition many talk about physical health condition eg cancer flu but some also discus mental illness including schizophrenia there are a variety of motivation for user to share this information on social medium to offer or seek support to fight the stigma of mental illness or perhaps to offer an explanation for certain behavior we obtain message with these selfreported diagnosis using the twitter api and filtered via caseinsensitive regular expression to require schizo or a close phonetic approximation to be present our expression matched schizophrenia it subtypes and various approximation schizo skitzo skitso schizotypal schizoid etc all data we collect are public post made between and and exclude any message marked a private by the author all use of the data reported in this paper ha been approved by the appropriate institutional review board irb each selfstated diagnosis included in this study wa examined by a human annotator one of the author to verify that it appeared to be a genuine statement of a schizophrenia diagnosis excluding joke quote or disingenuous statement we obtained user with an apparently genuine selfstated diagnosis of a schizophreniarelated condition note that we cannot be certain that the twitter user wa actually diagnosed with schizophrenia only that their statement of being diagnosed appears to be genuine previous work indicates that interannotator agreement for this task is good coppersmith et al a for each user we obtained a set of their public twitter post via the twitter api collecting up to tweet a we wish to focus on userauthored content we exclude from analysis all retweets and any tweet that contain a url which often contain text that the user did not author we lowercase all word and convert any nonstandard character including emoji to a systematic ascii representation via unidecode for our community control we used randomlyselected twitter user who primarily tweet in english specifically during a two week period in early each twitter user who wa included in twitter spritzer sample had an equal chance for inclusion in our pool of community control we then collected some of their historic tweet and assessed the language they tweeted in according to the chromium compact language detector user were excluded from our community control if their tweet were le than english,"['follow', 'acquisition', 'curation', 'process', 'coppersmith', 'et', 'al', 'summarizing', 'major', 'point', 'social', 'medium', 'contains', 'frequent', 'public', 'statement', 'reporting', 'diagnosis', 'various', 'medical', 'condition', 'many', 'talk', 'physical', 'condition', 'eg', 'cancer', 'flu', 'also', 'discus', 'illness', 'including', 'schizophrenia', 'variety', 'motivation', 'share', 'information', 'social', 'medium', 'offer', 'seek', 'support', 'fight', 'stigma', 'illness', 'perhaps', 'offer', 'explanation', 'certain', 'behavior', 'obtain', 'message', 'selfreported', 'diagnosis', 'using', 'api', 'filtered', 'via', 'caseinsensitive', 'regular', 'expression', 'require', 'schizo', 'close', 'phonetic', 'approximation', 'present', 'expression', 'matched', 'schizophrenia', 'subtypes', 'various', 'approximation', 'schizo', 'skitzo', 'skitso', 'schizotypal', 'schizoid', 'etc', 'collect', 'public', 'made', 'exclude', 'message', 'marked', 'private', 'author', 'use', 'reported', 'paper', 'ha', 'approved', 'appropriate', 'institutional', 'review', 'board', 'irb', 'selfstated', 'diagnosis', 'included', 'study', 'examined', 'human', 'annotator', 'one', 'author', 'verify', 'appeared', 'genuine', 'statement', 'schizophrenia', 'diagnosis', 'excluding', 'joke', 'quote', 'disingenuous', 'statement', 'obtained', 'apparently', 'genuine', 'selfstated', 'diagnosis', 'schizophreniarelated', 'condition', 'note', 'cannot', 'certain', 'actually', 'diagnosed', 'schizophrenia', 'statement', 'diagnosed', 'appears', 'genuine', 'previous', 'work', 'indicates', 'interannotator', 'agreement', 'task', 'good', 'coppersmith', 'et', 'al', 'obtained', 'set', 'public', 'via', 'api', 'collecting', 'wish', 'focus', 'userauthored', 'content', 'exclude', 'analysis', 'retweets', 'contain', 'url', 'often', 'contain', 'text', 'author', 'lowercase', 'convert', 'nonstandard', 'character', 'including', 'emoji', 'systematic', 'ascii', 'representation', 'via', 'unidecode', 'community', 'randomlyselected', 'primarily', 'english', 'specifically', 'two', 'week', 'period', 'early', 'included', 'spritzer', 'sample', 'equal', 'chance', 'inclusion', 'pool', 'community', 'collected', 'historic', 'assessed', 'language', 'tweeted', 'according', 'chromium', 'compact', 'language', 'detector', 'excluded', 'community', 'le', 'english']","['follow acquisition', 'acquisition curation', 'curation process', 'process coppersmith', 'coppersmith et', 'et al', 'al summarizing', 'summarizing major', 'major point', 'point social', 'social medium', 'medium contains', 'contains frequent', 'frequent public', 'public statement', 'statement reporting', 'reporting diagnosis', 'diagnosis various', 'various medical', 'medical condition', 'condition many', 'many talk', 'talk physical', 'physical condition', 'condition eg', 'eg cancer', 'cancer flu', 'flu also', 'also discus', 'discus illness', 'illness including', 'including schizophrenia', 'schizophrenia variety', 'variety motivation', 'motivation share', 'share information', 'information social', 'social medium', 'medium offer', 'offer seek', 'seek support', 'support fight', 'fight stigma', 'stigma illness', 'illness perhaps', 'perhaps offer', 'offer explanation', 'explanation certain', 'certain behavior', 'behavior obtain', 'obtain message', 'message selfreported', 'selfreported diagnosis', 'diagnosis using', 'using api', 'api filtered', 'filtered via', 'via caseinsensitive', 'caseinsensitive regular', 'regular expression', 'expression require', 'require schizo', 'schizo close', 'close phonetic', 'phonetic approximation', 'approximation present', 'present expression', 'expression matched', 'matched schizophrenia', 'schizophrenia subtypes', 'subtypes various', 'various approximation', 'approximation schizo', 'schizo skitzo', 'skitzo skitso', 'skitso schizotypal', 'schizotypal schizoid', 'schizoid etc', 'etc collect', 'collect public', 'public made', 'made exclude', 'exclude message', 'message marked', 'marked private', 'private author', 'author use', 'use reported', 'reported paper', 'paper ha', 'ha approved', 'approved appropriate', 'appropriate institutional', 'institutional review', 'review board', 'board irb', 'irb selfstated', 'selfstated diagnosis', 'diagnosis included', 'included study', 'study examined', 'examined human', 'human annotator', 'annotator one', 'one author', 'author verify', 'verify appeared', 'appeared genuine', 'genuine statement', 'statement schizophrenia', 'schizophrenia diagnosis', 'diagnosis excluding', 'excluding joke', 'joke quote', 'quote disingenuous', 'disingenuous statement', 'statement obtained', 'obtained apparently', 'apparently genuine', 'genuine selfstated', 'selfstated diagnosis', 'diagnosis schizophreniarelated', 'schizophreniarelated condition', 'condition note', 'note cannot', 'cannot certain', 'certain actually', 'actually diagnosed', 'diagnosed schizophrenia', 'schizophrenia statement', 'statement diagnosed', 'diagnosed appears', 'appears genuine', 'genuine previous', 'previous work', 'work indicates', 'indicates interannotator', 'interannotator agreement', 'agreement task', 'task good', 'good coppersmith', 'coppersmith et', 'et al', 'al obtained', 'obtained set', 'set public', 'public via', 'via api', 'api collecting', 'collecting wish', 'wish focus', 'focus userauthored', 'userauthored content', 'content exclude', 'exclude analysis', 'analysis retweets', 'retweets contain', 'contain url', 'url often', 'often contain', 'contain text', 'text author', 'author lowercase', 'lowercase convert', 'convert nonstandard', 'nonstandard character', 'character including', 'including emoji', 'emoji systematic', 'systematic ascii', 'ascii representation', 'representation via', 'via unidecode', 'unidecode community', 'community randomlyselected', 'randomlyselected primarily', 'primarily english', 'english specifically', 'specifically two', 'two week', 'week period', 'period early', 'early included', 'included spritzer', 'spritzer sample', 'sample equal', 'equal chance', 'chance inclusion', 'inclusion pool', 'pool community', 'community collected', 'collected historic', 'historic assessed', 'assessed language', 'language tweeted', 'tweeted according', 'according chromium', 'chromium compact', 'compact language', 'language detector', 'detector excluded', 'excluded community', 'community le', 'le english']","['follow acquisition curation', 'acquisition curation process', 'curation process coppersmith', 'process coppersmith et', 'coppersmith et al', 'et al summarizing', 'al summarizing major', 'summarizing major point', 'major point social', 'point social medium', 'social medium contains', 'medium contains frequent', 'contains frequent public', 'frequent public statement', 'public statement reporting', 'statement reporting diagnosis', 'reporting diagnosis various', 'diagnosis various medical', 'various medical condition', 'medical condition many', 'condition many talk', 'many talk physical', 'talk physical condition', 'physical condition eg', 'condition eg cancer', 'eg cancer flu', 'cancer flu also', 'flu also discus', 'also discus illness', 'discus illness including', 'illness including schizophrenia', 'including schizophrenia variety', 'schizophrenia variety motivation', 'variety motivation share', 'motivation share information', 'share information social', 'information social medium', 'social medium offer', 'medium offer seek', 'offer seek support', 'seek support fight', 'support fight stigma', 'fight stigma illness', 'stigma illness perhaps', 'illness perhaps offer', 'perhaps offer explanation', 'offer explanation certain', 'explanation certain behavior', 'certain behavior obtain', 'behavior obtain message', 'obtain message selfreported', 'message selfreported diagnosis', 'selfreported diagnosis using', 'diagnosis using api', 'using api filtered', 'api filtered via', 'filtered via caseinsensitive', 'via caseinsensitive regular', 'caseinsensitive regular expression', 'regular expression require', 'expression require schizo', 'require schizo close', 'schizo close phonetic', 'close phonetic approximation', 'phonetic approximation present', 'approximation present expression', 'present expression matched', 'expression matched schizophrenia', 'matched schizophrenia subtypes', 'schizophrenia subtypes various', 'subtypes various approximation', 'various approximation schizo', 'approximation schizo skitzo', 'schizo skitzo skitso', 'skitzo skitso schizotypal', 'skitso schizotypal schizoid', 'schizotypal schizoid etc', 'schizoid etc collect', 'etc collect public', 'collect public made', 'public made exclude', 'made exclude message', 'exclude message marked', 'message marked private', 'marked private author', 'private author use', 'author use reported', 'use reported paper', 'reported paper ha', 'paper ha approved', 'ha approved appropriate', 'approved appropriate institutional', 'appropriate institutional review', 'institutional review board', 'review board irb', 'board irb selfstated', 'irb selfstated diagnosis', 'selfstated diagnosis included', 'diagnosis included study', 'included study examined', 'study examined human', 'examined human annotator', 'human annotator one', 'annotator one author', 'one author verify', 'author verify appeared', 'verify appeared genuine', 'appeared genuine statement', 'genuine statement schizophrenia', 'statement schizophrenia diagnosis', 'schizophrenia diagnosis excluding', 'diagnosis excluding joke', 'excluding joke quote', 'joke quote disingenuous', 'quote disingenuous statement', 'disingenuous statement obtained', 'statement obtained apparently', 'obtained apparently genuine', 'apparently genuine selfstated', 'genuine selfstated diagnosis', 'selfstated diagnosis schizophreniarelated', 'diagnosis schizophreniarelated condition', 'schizophreniarelated condition note', 'condition note cannot', 'note cannot certain', 'cannot certain actually', 'certain actually diagnosed', 'actually diagnosed schizophrenia', 'diagnosed schizophrenia statement', 'schizophrenia statement diagnosed', 'statement diagnosed appears', 'diagnosed appears genuine', 'appears genuine previous', 'genuine previous work', 'previous work indicates', 'work indicates interannotator', 'indicates interannotator agreement', 'interannotator agreement task', 'agreement task good', 'task good coppersmith', 'good coppersmith et', 'coppersmith et al', 'et al obtained', 'al obtained set', 'obtained set public', 'set public via', 'public via api', 'via api collecting', 'api collecting wish', 'collecting wish focus', 'wish focus userauthored', 'focus userauthored content', 'userauthored content exclude', 'content exclude analysis', 'exclude analysis retweets', 'analysis retweets contain', 'retweets contain url', 'contain url often', 'url often contain', 'often contain text', 'contain text author', 'text author lowercase', 'author lowercase convert', 'lowercase convert nonstandard', 'convert nonstandard character', 'nonstandard character including', 'character including emoji', 'including emoji systematic', 'emoji systematic ascii', 'systematic ascii representation', 'ascii representation via', 'representation via unidecode', 'via unidecode community', 'unidecode community randomlyselected', 'community randomlyselected primarily', 'randomlyselected primarily english', 'primarily english specifically', 'english specifically two', 'specifically two week', 'two week period', 'week period early', 'period early included', 'early included spritzer', 'included spritzer sample', 'spritzer sample equal', 'sample equal chance', 'equal chance inclusion', 'chance inclusion pool', 'inclusion pool community', 'pool community collected', 'community collected historic', 'collected historic assessed', 'historic assessed language', 'assessed language tweeted', 'language tweeted according', 'tweeted according chromium', 'according chromium compact', 'chromium compact language', 'compact language detector', 'language detector excluded', 'detector excluded community', 'excluded community le', 'community le english']"
https://dl.acm.org/doi/abs/10.1145/2700171.2791026,0,We propose four categories of linguistic and non-linguistic attributes to examine preceding/succeeding celebrity suicides. These are: (1) affective attributes (2) cognitive attributes (3) linguistic style attributes and (4) social attributes. Measures belonging to all of these attribute categories are largely based on the psycholinguistic lexicon LIWC [40] and were motivated from prior literature that examine associations between the behavioral expression of individuals and their responses to traumatic context and crises including vulnerability due to mental illness [7 11]. Note that LIWC has been extensively validated to perform well on Internet language [7 18]. (1) We consider two measures of affect derived from LIWC: positive affect (PA) and negative affect (NA) and four other measures of emotional expression: anger anxiety sadness and swear. (2) We use LIWC to define the cognitive measures as well: (a) cognition comprising cognitive mech discrepancies inhibition negation death causation certainty and tentativeness; and (b) perception comprising set of words in LIWC around see hear feel percept insight and relative. (3) Next we consider four measures of linguistic style: (a) Lexical Density: consisting of words that are verbs auxiliary verbs nouns adjectives (identified using NLTK’s [2] POS tagger) and adverbs. (b) Temporal References: consisting of past present and future tenses. (c) Social/Personal Concerns: words belonging to family friends social work health humans religion bio body money achievement home and sexual. (d) Interpersonal Awareness and Focus: words that are 1st person singular 1st person plural 2nd person and 3rd person pronouns. (4) For social attributes we utilized a variety of content sharing social interaction and social support indicators. These are: post length number of comments vote difference (difference between upvotes and downvotes divided by total upvotes and downvotes) comment arrival rate (average time difference between any two subsequent comments in a post’s comment thread) time to first comment (time elapsed between the first comment and the timestamp of the corresponding post) and median comment length9 . We compute each of the above linguistic measures of behavior at the post level – the value of a measure is given by the ratio of the number of words in a post that match words belonging to the measure to the total number of words in the post. For each measure we take the average across all celebrities to ensure each suicide event is equally weighted i.e. to avoid skew due to a single suicide. For statistical comparison we used the Welch t-test; a negative tstatistic value means the measure increased after suicide.,we propose four category of linguistic and nonlinguistic attribute to examine precedingsucceeding celebrity suicide these are affective attribute cognitive attribute linguistic style attribute and social attribute measure belonging to all of these attribute category are largely based on the psycholinguistic lexicon liwc and were motivated from prior literature that examine association between the behavioral expression of individual and their response to traumatic context and crisis including vulnerability due to mental illness note that liwc ha been extensively validated to perform well on internet language we consider two measure of affect derived from liwc positive affect pa and negative affect na and four other measure of emotional expression anger anxiety sadness and swear we use liwc to define the cognitive measure a well a cognition comprising cognitive mech discrepancy inhibition negation death causation certainty and tentativeness and b perception comprising set of word in liwc around see hear feel percept insight and relative next we consider four measure of linguistic style a lexical density consisting of word that are verb auxiliary verb noun adjective identified using nltks po tagger and adverb b temporal reference consisting of past present and future tense c socialpersonal concern word belonging to family friend social work health human religion bio body money achievement home and sexual d interpersonal awareness and focus word that are st person singular st person plural nd person and rd person pronoun for social attribute we utilized a variety of content sharing social interaction and social support indicator these are post length number of comment vote difference difference between upvotes and downvotes divided by total upvotes and downvotes comment arrival rate average time difference between any two subsequent comment in a post comment thread time to first comment time elapsed between the first comment and the timestamp of the corresponding post and median comment length we compute each of the above linguistic measure of behavior at the post level the value of a measure is given by the ratio of the number of word in a post that match word belonging to the measure to the total number of word in the post for each measure we take the average across all celebrity to ensure each suicide event is equally weighted ie to avoid skew due to a single suicide for statistical comparison we used the welch ttest a negative tstatistic value mean the measure increased after suicide,"['propose', 'four', 'category', 'linguistic', 'nonlinguistic', 'attribute', 'examine', 'precedingsucceeding', 'celebrity', 'suicide', 'affective', 'attribute', 'cognitive', 'attribute', 'linguistic', 'style', 'attribute', 'social', 'attribute', 'measure', 'belonging', 'attribute', 'category', 'largely', 'based', 'psycholinguistic', 'lexicon', 'liwc', 'motivated', 'prior', 'literature', 'examine', 'association', 'behavioral', 'expression', 'individual', 'response', 'traumatic', 'context', 'crisis', 'including', 'vulnerability', 'due', 'illness', 'note', 'liwc', 'ha', 'extensively', 'validated', 'perform', 'well', 'internet', 'language', 'consider', 'two', 'measure', 'affect', 'derived', 'liwc', 'positive', 'affect', 'pa', 'negative', 'affect', 'na', 'four', 'measure', 'emotional', 'expression', 'anger', 'anxiety', 'sadness', 'swear', 'use', 'liwc', 'define', 'cognitive', 'measure', 'well', 'cognition', 'comprising', 'cognitive', 'mech', 'discrepancy', 'inhibition', 'negation', 'death', 'causation', 'certainty', 'tentativeness', 'b', 'perception', 'comprising', 'set', 'liwc', 'around', 'see', 'hear', 'feel', 'percept', 'insight', 'relative', 'next', 'consider', 'four', 'measure', 'linguistic', 'style', 'lexical', 'density', 'consisting', 'verb', 'auxiliary', 'verb', 'noun', 'adjective', 'identified', 'using', 'nltks', 'po', 'tagger', 'adverb', 'b', 'temporal', 'reference', 'consisting', 'past', 'present', 'future', 'tense', 'c', 'socialpersonal', 'concern', 'belonging', 'family', 'friend', 'social', 'work', 'human', 'religion', 'bio', 'body', 'money', 'achievement', 'home', 'sexual', 'interpersonal', 'awareness', 'focus', 'st', 'person', 'singular', 'st', 'person', 'plural', 'nd', 'person', 'rd', 'person', 'pronoun', 'social', 'attribute', 'utilized', 'variety', 'content', 'sharing', 'social', 'interaction', 'social', 'support', 'indicator', 'length', 'number', 'comment', 'vote', 'difference', 'difference', 'upvotes', 'downvotes', 'divided', 'total', 'upvotes', 'downvotes', 'comment', 'arrival', 'rate', 'average', 'time', 'difference', 'two', 'subsequent', 'comment', 'comment', 'thread', 'time', 'first', 'comment', 'time', 'elapsed', 'first', 'comment', 'timestamp', 'corresponding', 'median', 'comment', 'length', 'compute', 'linguistic', 'measure', 'behavior', 'level', 'value', 'measure', 'given', 'ratio', 'number', 'match', 'belonging', 'measure', 'total', 'number', 'measure', 'take', 'average', 'across', 'celebrity', 'ensure', 'suicide', 'event', 'equally', 'weighted', 'ie', 'avoid', 'skew', 'due', 'single', 'suicide', 'statistical', 'comparison', 'welch', 'ttest', 'negative', 'tstatistic', 'value', 'mean', 'measure', 'increased', 'suicide']","['propose four', 'four category', 'category linguistic', 'linguistic nonlinguistic', 'nonlinguistic attribute', 'attribute examine', 'examine precedingsucceeding', 'precedingsucceeding celebrity', 'celebrity suicide', 'suicide affective', 'affective attribute', 'attribute cognitive', 'cognitive attribute', 'attribute linguistic', 'linguistic style', 'style attribute', 'attribute social', 'social attribute', 'attribute measure', 'measure belonging', 'belonging attribute', 'attribute category', 'category largely', 'largely based', 'based psycholinguistic', 'psycholinguistic lexicon', 'lexicon liwc', 'liwc motivated', 'motivated prior', 'prior literature', 'literature examine', 'examine association', 'association behavioral', 'behavioral expression', 'expression individual', 'individual response', 'response traumatic', 'traumatic context', 'context crisis', 'crisis including', 'including vulnerability', 'vulnerability due', 'due illness', 'illness note', 'note liwc', 'liwc ha', 'ha extensively', 'extensively validated', 'validated perform', 'perform well', 'well internet', 'internet language', 'language consider', 'consider two', 'two measure', 'measure affect', 'affect derived', 'derived liwc', 'liwc positive', 'positive affect', 'affect pa', 'pa negative', 'negative affect', 'affect na', 'na four', 'four measure', 'measure emotional', 'emotional expression', 'expression anger', 'anger anxiety', 'anxiety sadness', 'sadness swear', 'swear use', 'use liwc', 'liwc define', 'define cognitive', 'cognitive measure', 'measure well', 'well cognition', 'cognition comprising', 'comprising cognitive', 'cognitive mech', 'mech discrepancy', 'discrepancy inhibition', 'inhibition negation', 'negation death', 'death causation', 'causation certainty', 'certainty tentativeness', 'tentativeness b', 'b perception', 'perception comprising', 'comprising set', 'set liwc', 'liwc around', 'around see', 'see hear', 'hear feel', 'feel percept', 'percept insight', 'insight relative', 'relative next', 'next consider', 'consider four', 'four measure', 'measure linguistic', 'linguistic style', 'style lexical', 'lexical density', 'density consisting', 'consisting verb', 'verb auxiliary', 'auxiliary verb', 'verb noun', 'noun adjective', 'adjective identified', 'identified using', 'using nltks', 'nltks po', 'po tagger', 'tagger adverb', 'adverb b', 'b temporal', 'temporal reference', 'reference consisting', 'consisting past', 'past present', 'present future', 'future tense', 'tense c', 'c socialpersonal', 'socialpersonal concern', 'concern belonging', 'belonging family', 'family friend', 'friend social', 'social work', 'work human', 'human religion', 'religion bio', 'bio body', 'body money', 'money achievement', 'achievement home', 'home sexual', 'sexual interpersonal', 'interpersonal awareness', 'awareness focus', 'focus st', 'st person', 'person singular', 'singular st', 'st person', 'person plural', 'plural nd', 'nd person', 'person rd', 'rd person', 'person pronoun', 'pronoun social', 'social attribute', 'attribute utilized', 'utilized variety', 'variety content', 'content sharing', 'sharing social', 'social interaction', 'interaction social', 'social support', 'support indicator', 'indicator length', 'length number', 'number comment', 'comment vote', 'vote difference', 'difference difference', 'difference upvotes', 'upvotes downvotes', 'downvotes divided', 'divided total', 'total upvotes', 'upvotes downvotes', 'downvotes comment', 'comment arrival', 'arrival rate', 'rate average', 'average time', 'time difference', 'difference two', 'two subsequent', 'subsequent comment', 'comment comment', 'comment thread', 'thread time', 'time first', 'first comment', 'comment time', 'time elapsed', 'elapsed first', 'first comment', 'comment timestamp', 'timestamp corresponding', 'corresponding median', 'median comment', 'comment length', 'length compute', 'compute linguistic', 'linguistic measure', 'measure behavior', 'behavior level', 'level value', 'value measure', 'measure given', 'given ratio', 'ratio number', 'number match', 'match belonging', 'belonging measure', 'measure total', 'total number', 'number measure', 'measure take', 'take average', 'average across', 'across celebrity', 'celebrity ensure', 'ensure suicide', 'suicide event', 'event equally', 'equally weighted', 'weighted ie', 'ie avoid', 'avoid skew', 'skew due', 'due single', 'single suicide', 'suicide statistical', 'statistical comparison', 'comparison welch', 'welch ttest', 'ttest negative', 'negative tstatistic', 'tstatistic value', 'value mean', 'mean measure', 'measure increased', 'increased suicide']","['propose four category', 'four category linguistic', 'category linguistic nonlinguistic', 'linguistic nonlinguistic attribute', 'nonlinguistic attribute examine', 'attribute examine precedingsucceeding', 'examine precedingsucceeding celebrity', 'precedingsucceeding celebrity suicide', 'celebrity suicide affective', 'suicide affective attribute', 'affective attribute cognitive', 'attribute cognitive attribute', 'cognitive attribute linguistic', 'attribute linguistic style', 'linguistic style attribute', 'style attribute social', 'attribute social attribute', 'social attribute measure', 'attribute measure belonging', 'measure belonging attribute', 'belonging attribute category', 'attribute category largely', 'category largely based', 'largely based psycholinguistic', 'based psycholinguistic lexicon', 'psycholinguistic lexicon liwc', 'lexicon liwc motivated', 'liwc motivated prior', 'motivated prior literature', 'prior literature examine', 'literature examine association', 'examine association behavioral', 'association behavioral expression', 'behavioral expression individual', 'expression individual response', 'individual response traumatic', 'response traumatic context', 'traumatic context crisis', 'context crisis including', 'crisis including vulnerability', 'including vulnerability due', 'vulnerability due illness', 'due illness note', 'illness note liwc', 'note liwc ha', 'liwc ha extensively', 'ha extensively validated', 'extensively validated perform', 'validated perform well', 'perform well internet', 'well internet language', 'internet language consider', 'language consider two', 'consider two measure', 'two measure affect', 'measure affect derived', 'affect derived liwc', 'derived liwc positive', 'liwc positive affect', 'positive affect pa', 'affect pa negative', 'pa negative affect', 'negative affect na', 'affect na four', 'na four measure', 'four measure emotional', 'measure emotional expression', 'emotional expression anger', 'expression anger anxiety', 'anger anxiety sadness', 'anxiety sadness swear', 'sadness swear use', 'swear use liwc', 'use liwc define', 'liwc define cognitive', 'define cognitive measure', 'cognitive measure well', 'measure well cognition', 'well cognition comprising', 'cognition comprising cognitive', 'comprising cognitive mech', 'cognitive mech discrepancy', 'mech discrepancy inhibition', 'discrepancy inhibition negation', 'inhibition negation death', 'negation death causation', 'death causation certainty', 'causation certainty tentativeness', 'certainty tentativeness b', 'tentativeness b perception', 'b perception comprising', 'perception comprising set', 'comprising set liwc', 'set liwc around', 'liwc around see', 'around see hear', 'see hear feel', 'hear feel percept', 'feel percept insight', 'percept insight relative', 'insight relative next', 'relative next consider', 'next consider four', 'consider four measure', 'four measure linguistic', 'measure linguistic style', 'linguistic style lexical', 'style lexical density', 'lexical density consisting', 'density consisting verb', 'consisting verb auxiliary', 'verb auxiliary verb', 'auxiliary verb noun', 'verb noun adjective', 'noun adjective identified', 'adjective identified using', 'identified using nltks', 'using nltks po', 'nltks po tagger', 'po tagger adverb', 'tagger adverb b', 'adverb b temporal', 'b temporal reference', 'temporal reference consisting', 'reference consisting past', 'consisting past present', 'past present future', 'present future tense', 'future tense c', 'tense c socialpersonal', 'c socialpersonal concern', 'socialpersonal concern belonging', 'concern belonging family', 'belonging family friend', 'family friend social', 'friend social work', 'social work human', 'work human religion', 'human religion bio', 'religion bio body', 'bio body money', 'body money achievement', 'money achievement home', 'achievement home sexual', 'home sexual interpersonal', 'sexual interpersonal awareness', 'interpersonal awareness focus', 'awareness focus st', 'focus st person', 'st person singular', 'person singular st', 'singular st person', 'st person plural', 'person plural nd', 'plural nd person', 'nd person rd', 'person rd person', 'rd person pronoun', 'person pronoun social', 'pronoun social attribute', 'social attribute utilized', 'attribute utilized variety', 'utilized variety content', 'variety content sharing', 'content sharing social', 'sharing social interaction', 'social interaction social', 'interaction social support', 'social support indicator', 'support indicator length', 'indicator length number', 'length number comment', 'number comment vote', 'comment vote difference', 'vote difference difference', 'difference difference upvotes', 'difference upvotes downvotes', 'upvotes downvotes divided', 'downvotes divided total', 'divided total upvotes', 'total upvotes downvotes', 'upvotes downvotes comment', 'downvotes comment arrival', 'comment arrival rate', 'arrival rate average', 'rate average time', 'average time difference', 'time difference two', 'difference two subsequent', 'two subsequent comment', 'subsequent comment comment', 'comment comment thread', 'comment thread time', 'thread time first', 'time first comment', 'first comment time', 'comment time elapsed', 'time elapsed first', 'elapsed first comment', 'first comment timestamp', 'comment timestamp corresponding', 'timestamp corresponding median', 'corresponding median comment', 'median comment length', 'comment length compute', 'length compute linguistic', 'compute linguistic measure', 'linguistic measure behavior', 'measure behavior level', 'behavior level value', 'level value measure', 'value measure given', 'measure given ratio', 'given ratio number', 'ratio number match', 'number match belonging', 'match belonging measure', 'belonging measure total', 'measure total number', 'total number measure', 'number measure take', 'measure take average', 'take average across', 'average across celebrity', 'across celebrity ensure', 'celebrity ensure suicide', 'ensure suicide event', 'suicide event equally', 'event equally weighted', 'equally weighted ie', 'weighted ie avoid', 'ie avoid skew', 'avoid skew due', 'skew due single', 'due single suicide', 'single suicide statistical', 'suicide statistical comparison', 'statistical comparison welch', 'comparison welch ttest', 'welch ttest negative', 'ttest negative tstatistic', 'negative tstatistic value', 'tstatistic value mean', 'value mean measure', 'mean measure increased', 'measure increased suicide']"
https://aclanthology.org/W17-3110.pdf,0,This study aimed to examine the prevalence of affective micropatterns in social media posts and highlight differences in micropattern occurrence that might be relevant to quantifying mental health. Primarily we do this through comparison of users with anxiety disorders eating disorders schizophrenia suicide attempt history and their matched controls. We use a straightforward and well-understood method for sentiment analysis VADER (Hutto and Gilbert 2014) to produce a trinary label for each message: positive neutral or negative. VADER outputs a [0 1] score for each sentiment label; we use the label with the maximum score. Specifically we examined trajectories of posted emotional content in three subsequent tweets no more than three hours from earliest to latest. The same tweet will be counted in more than one over lapping micropattern if more than three tweets occur in the three-hour time window – so if 5 tweets occur in 3 hours 3 micropatterns will be recorded from those 5 tweets likewise for 4 tweets 2 micropatterns will be recorded. The potential overlap exists for both patients and neurotypical users and subsequent analyses (e.g. classifying users based on proportion of micropatterns) were designed to be robust to this property of overlapping micropattern generation. The number of sequential tweets to examine was chosen to minimize the complexity of the analysis while allowing significant variability to be observed. Critically we aimed for the resulting dimensions (i.e. number of distinct micropatterns) to be small enough for meaningful interpretation by clinical psychologists.,this study aimed to examine the prevalence of affective micropatterns in social medium post and highlight difference in micropattern occurrence that might be relevant to quantifying mental health primarily we do this through comparison of user with anxiety disorder eating disorder schizophrenia suicide attempt history and their matched control we use a straightforward and wellunderstood method for sentiment analysis vader hutto and gilbert to produce a trinary label for each message positive neutral or negative vader output a score for each sentiment label we use the label with the maximum score specifically we examined trajectory of posted emotional content in three subsequent tweet no more than three hour from earliest to latest the same tweet will be counted in more than one over lapping micropattern if more than three tweet occur in the threehour time window so if tweet occur in hour micropatterns will be recorded from those tweet likewise for tweet micropatterns will be recorded the potential overlap exists for both patient and neurotypical user and subsequent analysis eg classifying user based on proportion of micropatterns were designed to be robust to this property of overlapping micropattern generation the number of sequential tweet to examine wa chosen to minimize the complexity of the analysis while allowing significant variability to be observed critically we aimed for the resulting dimension ie number of distinct micropatterns to be small enough for meaningful interpretation by clinical psychologist,"['study', 'aimed', 'examine', 'prevalence', 'affective', 'micropatterns', 'social', 'medium', 'highlight', 'difference', 'micropattern', 'occurrence', 'might', 'relevant', 'quantifying', 'primarily', 'comparison', 'anxiety', 'disorder', 'eating', 'disorder', 'schizophrenia', 'suicide', 'attempt', 'history', 'matched', 'use', 'straightforward', 'wellunderstood', 'method', 'sentiment', 'analysis', 'vader', 'hutto', 'gilbert', 'produce', 'trinary', 'label', 'message', 'positive', 'neutral', 'negative', 'vader', 'output', 'score', 'sentiment', 'label', 'use', 'label', 'maximum', 'score', 'specifically', 'examined', 'trajectory', 'posted', 'emotional', 'content', 'three', 'subsequent', 'three', 'hour', 'earliest', 'latest', 'counted', 'one', 'lapping', 'micropattern', 'three', 'occur', 'threehour', 'time', 'window', 'occur', 'hour', 'micropatterns', 'recorded', 'likewise', 'micropatterns', 'recorded', 'potential', 'overlap', 'exists', 'patient', 'neurotypical', 'subsequent', 'analysis', 'eg', 'classifying', 'based', 'proportion', 'micropatterns', 'designed', 'robust', 'property', 'overlapping', 'micropattern', 'generation', 'number', 'sequential', 'examine', 'chosen', 'minimize', 'complexity', 'analysis', 'allowing', 'significant', 'variability', 'observed', 'critically', 'aimed', 'resulting', 'dimension', 'ie', 'number', 'distinct', 'micropatterns', 'small', 'enough', 'meaningful', 'interpretation', 'clinical', 'psychologist']","['study aimed', 'aimed examine', 'examine prevalence', 'prevalence affective', 'affective micropatterns', 'micropatterns social', 'social medium', 'medium highlight', 'highlight difference', 'difference micropattern', 'micropattern occurrence', 'occurrence might', 'might relevant', 'relevant quantifying', 'quantifying primarily', 'primarily comparison', 'comparison anxiety', 'anxiety disorder', 'disorder eating', 'eating disorder', 'disorder schizophrenia', 'schizophrenia suicide', 'suicide attempt', 'attempt history', 'history matched', 'matched use', 'use straightforward', 'straightforward wellunderstood', 'wellunderstood method', 'method sentiment', 'sentiment analysis', 'analysis vader', 'vader hutto', 'hutto gilbert', 'gilbert produce', 'produce trinary', 'trinary label', 'label message', 'message positive', 'positive neutral', 'neutral negative', 'negative vader', 'vader output', 'output score', 'score sentiment', 'sentiment label', 'label use', 'use label', 'label maximum', 'maximum score', 'score specifically', 'specifically examined', 'examined trajectory', 'trajectory posted', 'posted emotional', 'emotional content', 'content three', 'three subsequent', 'subsequent three', 'three hour', 'hour earliest', 'earliest latest', 'latest counted', 'counted one', 'one lapping', 'lapping micropattern', 'micropattern three', 'three occur', 'occur threehour', 'threehour time', 'time window', 'window occur', 'occur hour', 'hour micropatterns', 'micropatterns recorded', 'recorded likewise', 'likewise micropatterns', 'micropatterns recorded', 'recorded potential', 'potential overlap', 'overlap exists', 'exists patient', 'patient neurotypical', 'neurotypical subsequent', 'subsequent analysis', 'analysis eg', 'eg classifying', 'classifying based', 'based proportion', 'proportion micropatterns', 'micropatterns designed', 'designed robust', 'robust property', 'property overlapping', 'overlapping micropattern', 'micropattern generation', 'generation number', 'number sequential', 'sequential examine', 'examine chosen', 'chosen minimize', 'minimize complexity', 'complexity analysis', 'analysis allowing', 'allowing significant', 'significant variability', 'variability observed', 'observed critically', 'critically aimed', 'aimed resulting', 'resulting dimension', 'dimension ie', 'ie number', 'number distinct', 'distinct micropatterns', 'micropatterns small', 'small enough', 'enough meaningful', 'meaningful interpretation', 'interpretation clinical', 'clinical psychologist']","['study aimed examine', 'aimed examine prevalence', 'examine prevalence affective', 'prevalence affective micropatterns', 'affective micropatterns social', 'micropatterns social medium', 'social medium highlight', 'medium highlight difference', 'highlight difference micropattern', 'difference micropattern occurrence', 'micropattern occurrence might', 'occurrence might relevant', 'might relevant quantifying', 'relevant quantifying primarily', 'quantifying primarily comparison', 'primarily comparison anxiety', 'comparison anxiety disorder', 'anxiety disorder eating', 'disorder eating disorder', 'eating disorder schizophrenia', 'disorder schizophrenia suicide', 'schizophrenia suicide attempt', 'suicide attempt history', 'attempt history matched', 'history matched use', 'matched use straightforward', 'use straightforward wellunderstood', 'straightforward wellunderstood method', 'wellunderstood method sentiment', 'method sentiment analysis', 'sentiment analysis vader', 'analysis vader hutto', 'vader hutto gilbert', 'hutto gilbert produce', 'gilbert produce trinary', 'produce trinary label', 'trinary label message', 'label message positive', 'message positive neutral', 'positive neutral negative', 'neutral negative vader', 'negative vader output', 'vader output score', 'output score sentiment', 'score sentiment label', 'sentiment label use', 'label use label', 'use label maximum', 'label maximum score', 'maximum score specifically', 'score specifically examined', 'specifically examined trajectory', 'examined trajectory posted', 'trajectory posted emotional', 'posted emotional content', 'emotional content three', 'content three subsequent', 'three subsequent three', 'subsequent three hour', 'three hour earliest', 'hour earliest latest', 'earliest latest counted', 'latest counted one', 'counted one lapping', 'one lapping micropattern', 'lapping micropattern three', 'micropattern three occur', 'three occur threehour', 'occur threehour time', 'threehour time window', 'time window occur', 'window occur hour', 'occur hour micropatterns', 'hour micropatterns recorded', 'micropatterns recorded likewise', 'recorded likewise micropatterns', 'likewise micropatterns recorded', 'micropatterns recorded potential', 'recorded potential overlap', 'potential overlap exists', 'overlap exists patient', 'exists patient neurotypical', 'patient neurotypical subsequent', 'neurotypical subsequent analysis', 'subsequent analysis eg', 'analysis eg classifying', 'eg classifying based', 'classifying based proportion', 'based proportion micropatterns', 'proportion micropatterns designed', 'micropatterns designed robust', 'designed robust property', 'robust property overlapping', 'property overlapping micropattern', 'overlapping micropattern generation', 'micropattern generation number', 'generation number sequential', 'number sequential examine', 'sequential examine chosen', 'examine chosen minimize', 'chosen minimize complexity', 'minimize complexity analysis', 'complexity analysis allowing', 'analysis allowing significant', 'allowing significant variability', 'significant variability observed', 'variability observed critically', 'observed critically aimed', 'critically aimed resulting', 'aimed resulting dimension', 'resulting dimension ie', 'dimension ie number', 'ie number distinct', 'number distinct micropatterns', 'distinct micropatterns small', 'micropatterns small enough', 'small enough meaningful', 'enough meaningful interpretation', 'meaningful interpretation clinical', 'interpretation clinical psychologist']"
https://dl.acm.org/doi/abs/10.1145/2998181.2998220,1,For inferring the country name corresponding to a user we adopted a stepwise approach as follows: First we cleaned the location strings reported in the location field of Twitter user profiles — including normalization of character case and removal of non-English word roots. Then we performed a location matching exercise wherein we split the cleaned location strings into single words iteratively created all possible 5-to-1-gram substrings and then matched each n-gram to a location database based on GeoNames (http://www.geonames.org) preferring larger n-grams over smaller ones (“New York City” to “York”). Third we performed disambiguation by computing geographic distances between matched text-adjacent places and assigned high likelihood to those matches that are close to each other geographically. We then sorted equally likely location alternatives by population size and choose the top one. We note that compared to geo-located Twitter posts this location field string lookup method has been known to yield better coverage in social media data [22].,for inferring the country name corresponding to a user we adopted a stepwise approach a follows first we cleaned the location string reported in the location field of twitter user profile including normalization of character case and removal of nonenglish word root then we performed a location matching exercise wherein we split the cleaned location string into single word iteratively created all possible togram substring and then matched each ngram to a location database based on geonames httpwwwgeonamesorg preferring larger ngrams over smaller one new york city to york third we performed disambiguation by computing geographic distance between matched textadjacent place and assigned high likelihood to those match that are close to each other geographically we then sorted equally likely location alternative by population size and choose the top one we note that compared to geolocated twitter post this location field string lookup method ha been known to yield better coverage in social medium data,"['inferring', 'country', 'name', 'corresponding', 'adopted', 'stepwise', 'approach', 'follows', 'first', 'cleaned', 'location', 'string', 'reported', 'location', 'field', 'profile', 'including', 'normalization', 'character', 'case', 'removal', 'nonenglish', 'root', 'performed', 'location', 'matching', 'exercise', 'wherein', 'split', 'cleaned', 'location', 'string', 'single', 'iteratively', 'created', 'possible', 'togram', 'substring', 'matched', 'ngram', 'location', 'database', 'based', 'geonames', 'httpwwwgeonamesorg', 'preferring', 'larger', 'ngrams', 'smaller', 'one', 'new', 'york', 'city', 'york', 'third', 'performed', 'disambiguation', 'computing', 'geographic', 'distance', 'matched', 'textadjacent', 'place', 'assigned', 'high', 'likelihood', 'match', 'close', 'geographically', 'sorted', 'equally', 'likely', 'location', 'alternative', 'population', 'size', 'choose', 'top', 'one', 'note', 'compared', 'geolocated', 'location', 'field', 'string', 'lookup', 'method', 'ha', 'known', 'yield', 'better', 'coverage', 'social', 'medium']","['inferring country', 'country name', 'name corresponding', 'corresponding adopted', 'adopted stepwise', 'stepwise approach', 'approach follows', 'follows first', 'first cleaned', 'cleaned location', 'location string', 'string reported', 'reported location', 'location field', 'field profile', 'profile including', 'including normalization', 'normalization character', 'character case', 'case removal', 'removal nonenglish', 'nonenglish root', 'root performed', 'performed location', 'location matching', 'matching exercise', 'exercise wherein', 'wherein split', 'split cleaned', 'cleaned location', 'location string', 'string single', 'single iteratively', 'iteratively created', 'created possible', 'possible togram', 'togram substring', 'substring matched', 'matched ngram', 'ngram location', 'location database', 'database based', 'based geonames', 'geonames httpwwwgeonamesorg', 'httpwwwgeonamesorg preferring', 'preferring larger', 'larger ngrams', 'ngrams smaller', 'smaller one', 'one new', 'new york', 'york city', 'city york', 'york third', 'third performed', 'performed disambiguation', 'disambiguation computing', 'computing geographic', 'geographic distance', 'distance matched', 'matched textadjacent', 'textadjacent place', 'place assigned', 'assigned high', 'high likelihood', 'likelihood match', 'match close', 'close geographically', 'geographically sorted', 'sorted equally', 'equally likely', 'likely location', 'location alternative', 'alternative population', 'population size', 'size choose', 'choose top', 'top one', 'one note', 'note compared', 'compared geolocated', 'geolocated location', 'location field', 'field string', 'string lookup', 'lookup method', 'method ha', 'ha known', 'known yield', 'yield better', 'better coverage', 'coverage social', 'social medium']","['inferring country name', 'country name corresponding', 'name corresponding adopted', 'corresponding adopted stepwise', 'adopted stepwise approach', 'stepwise approach follows', 'approach follows first', 'follows first cleaned', 'first cleaned location', 'cleaned location string', 'location string reported', 'string reported location', 'reported location field', 'location field profile', 'field profile including', 'profile including normalization', 'including normalization character', 'normalization character case', 'character case removal', 'case removal nonenglish', 'removal nonenglish root', 'nonenglish root performed', 'root performed location', 'performed location matching', 'location matching exercise', 'matching exercise wherein', 'exercise wherein split', 'wherein split cleaned', 'split cleaned location', 'cleaned location string', 'location string single', 'string single iteratively', 'single iteratively created', 'iteratively created possible', 'created possible togram', 'possible togram substring', 'togram substring matched', 'substring matched ngram', 'matched ngram location', 'ngram location database', 'location database based', 'database based geonames', 'based geonames httpwwwgeonamesorg', 'geonames httpwwwgeonamesorg preferring', 'httpwwwgeonamesorg preferring larger', 'preferring larger ngrams', 'larger ngrams smaller', 'ngrams smaller one', 'smaller one new', 'one new york', 'new york city', 'york city york', 'city york third', 'york third performed', 'third performed disambiguation', 'performed disambiguation computing', 'disambiguation computing geographic', 'computing geographic distance', 'geographic distance matched', 'distance matched textadjacent', 'matched textadjacent place', 'textadjacent place assigned', 'place assigned high', 'assigned high likelihood', 'high likelihood match', 'likelihood match close', 'match close geographically', 'close geographically sorted', 'geographically sorted equally', 'sorted equally likely', 'equally likely location', 'likely location alternative', 'location alternative population', 'alternative population size', 'population size choose', 'size choose top', 'choose top one', 'top one note', 'one note compared', 'note compared geolocated', 'compared geolocated location', 'geolocated location field', 'location field string', 'field string lookup', 'string lookup method', 'lookup method ha', 'method ha known', 'ha known yield', 'known yield better', 'yield better coverage', 'better coverage social', 'coverage social medium']"
https://www.jmir.org/2019/6/e14199/,0,The selection of the tweets and their users was based on the filtered real-time streaming support provided by the Twitter API. In the first step we selected the users who showed potential signs of depression on Twitter on the basis of the 20 most frequent words in Spanish expressed by patients suffering from depression in clinical settings. These words were jointly identified and selected by a psychologist and a family physician with clinical experience and were based on the definition and general features of depression according to the Diagnostic and Statistical Manual of Mental Disorders [42]. The list of words used and their English translations are shown in Textbox 1. During June 2018 1470000 tweets including 1 or more occurrences of the words listed in Textbox 1 were collected. From this collection of tweets and to select the users who publicly stated in the textual description associated to their profile that they suffered from depression all the profile descriptions including 1 or more occurrences of the word “depr” and all the possible derivations related to the word depression in Spanish such as “depre” “depresión” “depresivo” “depresiva” “deprimido” and “deprimida” were considered. From the 720 users who included 1 or more of these words in their description profile 90 users who stated they suffered from depression or were receiving treatment for depression were selected for the analysis. This selection was performed by a psychologist verifying that the statements were related to real expressions of depression excluding quotes jokes or fake ones. For each of these depressed Twitter users we collected all the most recent tweets from their timeline up to a maximum of about 3200 tweets. Thus a total of 189669 tweets were collected a figure that was reduced to 140946 after discarding the retweets. These 140946 tweets constituted the depressive users dataset. Examples of sentences appearing in the user profiles that were used for selecting the depressive users are: “Paciente psiquiátrico con depresión crónica” (Psychiatric patient with chronic depression; example of a profile sentence that indicates depression). “Colecciono errores traducidos a tweets depresivos y a uno que otro impulso de amor” (I gather errors translated into depressing tweets and into one or another love impulse; example of a profile sentence that does not indicate depression). Once the users with profile sentences indicating depression had been retrieved their Twitter timelines were collected. Only those users having in their timeline at least 10 tweets that suggested signs of depression were retained for further analyses. For each user the selection of these tweets was performed by manually inspecting the tweets of the user’s complete timeline in reverse temporal order starting from the most recent one to the oldest tweet of the timeline retrieved by means of the Twitter API . Finally a total number of 1000 tweets issued by the 90 depressive users suggesting signs of depression were detected and used for the analysis. This set of tweets provided us with the depressive tweets dataset which was used to analyze linguistic features of tweets showing signs of depression. It has to be mentioned that these 1000 tweets were not to be included in the depressive users dataset (see Figure 1). At the same time more than 97500000 tweets were also collected in June 2018: such tweets were gathered by listening to the public Twitter stream during this time span by only considering tweets with Spanish textual contents (as detected by Twitter language identification support). Given that Twitter requires more restrictive filters than just the language of the tweets we used a list of the most frequently used Spanish words (stopwords) to retrieve all tweets that included 1 or more of these words. The vast majority of Spanish tweets should match this criterion. A sample of 450 users who did not mention in their profile the word depression and its derivations were selected randomly from the 97500000 tweets. The complete timelines of these users were compiled (1141021 tweets) which were reduced to 712589 once retweets were removed. These 712589 tweets constituted the control dataset. To identify the language of a tweet we relied on the language automatically identified by Twitter for each tweet selecting tweets in Spanish. It has to be noted that these data can contain some tweets from unidentified depressive users.,the selection of the tweet and their user wa based on the filtered realtime streaming support provided by the twitter api in the first step we selected the user who showed potential sign of depression on twitter on the basis of the most frequent word in spanish expressed by patient suffering from depression in clinical setting these word were jointly identified and selected by a psychologist and a family physician with clinical experience and were based on the definition and general feature of depression according to the diagnostic and statistical manual of mental disorder the list of word used and their english translation are shown in textbox during june tweet including or more occurrence of the word listed in textbox were collected from this collection of tweet and to select the user who publicly stated in the textual description associated to their profile that they suffered from depression all the profile description including or more occurrence of the word depr and all the possible derivation related to the word depression in spanish such a depre depresin depresivo depresiva deprimido and deprimida were considered from the user who included or more of these word in their description profile user who stated they suffered from depression or were receiving treatment for depression were selected for the analysis this selection wa performed by a psychologist verifying that the statement were related to real expression of depression excluding quote joke or fake one for each of these depressed twitter user we collected all the most recent tweet from their timeline up to a maximum of about tweet thus a total of tweet were collected a figure that wa reduced to after discarding the retweets these tweet constituted the depressive user dataset example of sentence appearing in the user profile that were used for selecting the depressive user are paciente psiquitrico con depresin crnica psychiatric patient with chronic depression example of a profile sentence that indicates depression colecciono errores traducidos a tweet depresivos y a uno que otro impulso de amor i gather error translated into depressing tweet and into one or another love impulse example of a profile sentence that doe not indicate depression once the user with profile sentence indicating depression had been retrieved their twitter timeline were collected only those user having in their timeline at least tweet that suggested sign of depression were retained for further analysis for each user the selection of these tweet wa performed by manually inspecting the tweet of the user complete timeline in reverse temporal order starting from the most recent one to the oldest tweet of the timeline retrieved by mean of the twitter api finally a total number of tweet issued by the depressive user suggesting sign of depression were detected and used for the analysis this set of tweet provided u with the depressive tweet dataset which wa used to analyze linguistic feature of tweet showing sign of depression it ha to be mentioned that these tweet were not to be included in the depressive user dataset see figure at the same time more than tweet were also collected in june such tweet were gathered by listening to the public twitter stream during this time span by only considering tweet with spanish textual content a detected by twitter language identification support given that twitter requires more restrictive filter than just the language of the tweet we used a list of the most frequently used spanish word stopwords to retrieve all tweet that included or more of these word the vast majority of spanish tweet should match this criterion a sample of user who did not mention in their profile the word depression and it derivation were selected randomly from the tweet the complete timeline of these user were compiled tweet which were reduced to once retweets were removed these tweet constituted the control dataset to identify the language of a tweet we relied on the language automatically identified by twitter for each tweet selecting tweet in spanish it ha to be noted that these data can contain some tweet from unidentified depressive user,"['selection', 'based', 'filtered', 'realtime', 'streaming', 'support', 'provided', 'api', 'first', 'step', 'selected', 'showed', 'potential', 'sign', 'basis', 'frequent', 'spanish', 'expressed', 'patient', 'suffering', 'clinical', 'setting', 'jointly', 'identified', 'selected', 'psychologist', 'family', 'physician', 'clinical', 'experience', 'based', 'definition', 'general', 'feature', 'according', 'diagnostic', 'statistical', 'manual', 'disorder', 'list', 'english', 'translation', 'shown', 'textbox', 'june', 'including', 'occurrence', 'listed', 'textbox', 'collected', 'collection', 'select', 'publicly', 'stated', 'textual', 'description', 'associated', 'profile', 'suffered', 'profile', 'description', 'including', 'occurrence', 'depr', 'possible', 'derivation', 'related', 'spanish', 'depre', 'depresin', 'depresivo', 'depresiva', 'deprimido', 'deprimida', 'considered', 'included', 'description', 'profile', 'stated', 'suffered', 'receiving', 'treatment', 'selected', 'analysis', 'selection', 'performed', 'psychologist', 'verifying', 'statement', 'related', 'real', 'expression', 'excluding', 'quote', 'joke', 'fake', 'one', 'depressed', 'collected', 'recent', 'timeline', 'maximum', 'thus', 'total', 'collected', 'figure', 'reduced', 'discarding', 'retweets', 'constituted', 'depressive', 'dataset', 'example', 'sentence', 'appearing', 'profile', 'selecting', 'depressive', 'paciente', 'psiquitrico', 'con', 'depresin', 'crnica', 'psychiatric', 'patient', 'chronic', 'example', 'profile', 'sentence', 'indicates', 'colecciono', 'errores', 'traducidos', 'depresivos', 'uno', 'que', 'otro', 'impulso', 'de', 'amor', 'gather', 'error', 'translated', 'depressing', 'one', 'another', 'love', 'impulse', 'example', 'profile', 'sentence', 'doe', 'indicate', 'profile', 'sentence', 'indicating', 'retrieved', 'timeline', 'collected', 'timeline', 'least', 'suggested', 'sign', 'retained', 'analysis', 'selection', 'performed', 'manually', 'inspecting', 'complete', 'timeline', 'reverse', 'temporal', 'order', 'starting', 'recent', 'one', 'oldest', 'timeline', 'retrieved', 'mean', 'api', 'finally', 'total', 'number', 'issued', 'depressive', 'suggesting', 'sign', 'detected', 'analysis', 'set', 'provided', 'u', 'depressive', 'dataset', 'analyze', 'linguistic', 'feature', 'showing', 'sign', 'ha', 'mentioned', 'included', 'depressive', 'dataset', 'see', 'figure', 'time', 'also', 'collected', 'june', 'gathered', 'listening', 'public', 'stream', 'time', 'span', 'considering', 'spanish', 'textual', 'content', 'detected', 'language', 'identification', 'support', 'given', 'requires', 'restrictive', 'filter', 'language', 'list', 'frequently', 'spanish', 'stopwords', 'retrieve', 'included', 'vast', 'majority', 'spanish', 'match', 'criterion', 'sample', 'mention', 'profile', 'derivation', 'selected', 'randomly', 'complete', 'timeline', 'compiled', 'reduced', 'retweets', 'removed', 'constituted', 'dataset', 'identify', 'language', 'relied', 'language', 'automatically', 'identified', 'selecting', 'spanish', 'ha', 'noted', 'contain', 'unidentified', 'depressive']","['selection based', 'based filtered', 'filtered realtime', 'realtime streaming', 'streaming support', 'support provided', 'provided api', 'api first', 'first step', 'step selected', 'selected showed', 'showed potential', 'potential sign', 'sign basis', 'basis frequent', 'frequent spanish', 'spanish expressed', 'expressed patient', 'patient suffering', 'suffering clinical', 'clinical setting', 'setting jointly', 'jointly identified', 'identified selected', 'selected psychologist', 'psychologist family', 'family physician', 'physician clinical', 'clinical experience', 'experience based', 'based definition', 'definition general', 'general feature', 'feature according', 'according diagnostic', 'diagnostic statistical', 'statistical manual', 'manual disorder', 'disorder list', 'list english', 'english translation', 'translation shown', 'shown textbox', 'textbox june', 'june including', 'including occurrence', 'occurrence listed', 'listed textbox', 'textbox collected', 'collected collection', 'collection select', 'select publicly', 'publicly stated', 'stated textual', 'textual description', 'description associated', 'associated profile', 'profile suffered', 'suffered profile', 'profile description', 'description including', 'including occurrence', 'occurrence depr', 'depr possible', 'possible derivation', 'derivation related', 'related spanish', 'spanish depre', 'depre depresin', 'depresin depresivo', 'depresivo depresiva', 'depresiva deprimido', 'deprimido deprimida', 'deprimida considered', 'considered included', 'included description', 'description profile', 'profile stated', 'stated suffered', 'suffered receiving', 'receiving treatment', 'treatment selected', 'selected analysis', 'analysis selection', 'selection performed', 'performed psychologist', 'psychologist verifying', 'verifying statement', 'statement related', 'related real', 'real expression', 'expression excluding', 'excluding quote', 'quote joke', 'joke fake', 'fake one', 'one depressed', 'depressed collected', 'collected recent', 'recent timeline', 'timeline maximum', 'maximum thus', 'thus total', 'total collected', 'collected figure', 'figure reduced', 'reduced discarding', 'discarding retweets', 'retweets constituted', 'constituted depressive', 'depressive dataset', 'dataset example', 'example sentence', 'sentence appearing', 'appearing profile', 'profile selecting', 'selecting depressive', 'depressive paciente', 'paciente psiquitrico', 'psiquitrico con', 'con depresin', 'depresin crnica', 'crnica psychiatric', 'psychiatric patient', 'patient chronic', 'chronic example', 'example profile', 'profile sentence', 'sentence indicates', 'indicates colecciono', 'colecciono errores', 'errores traducidos', 'traducidos depresivos', 'depresivos uno', 'uno que', 'que otro', 'otro impulso', 'impulso de', 'de amor', 'amor gather', 'gather error', 'error translated', 'translated depressing', 'depressing one', 'one another', 'another love', 'love impulse', 'impulse example', 'example profile', 'profile sentence', 'sentence doe', 'doe indicate', 'indicate profile', 'profile sentence', 'sentence indicating', 'indicating retrieved', 'retrieved timeline', 'timeline collected', 'collected timeline', 'timeline least', 'least suggested', 'suggested sign', 'sign retained', 'retained analysis', 'analysis selection', 'selection performed', 'performed manually', 'manually inspecting', 'inspecting complete', 'complete timeline', 'timeline reverse', 'reverse temporal', 'temporal order', 'order starting', 'starting recent', 'recent one', 'one oldest', 'oldest timeline', 'timeline retrieved', 'retrieved mean', 'mean api', 'api finally', 'finally total', 'total number', 'number issued', 'issued depressive', 'depressive suggesting', 'suggesting sign', 'sign detected', 'detected analysis', 'analysis set', 'set provided', 'provided u', 'u depressive', 'depressive dataset', 'dataset analyze', 'analyze linguistic', 'linguistic feature', 'feature showing', 'showing sign', 'sign ha', 'ha mentioned', 'mentioned included', 'included depressive', 'depressive dataset', 'dataset see', 'see figure', 'figure time', 'time also', 'also collected', 'collected june', 'june gathered', 'gathered listening', 'listening public', 'public stream', 'stream time', 'time span', 'span considering', 'considering spanish', 'spanish textual', 'textual content', 'content detected', 'detected language', 'language identification', 'identification support', 'support given', 'given requires', 'requires restrictive', 'restrictive filter', 'filter language', 'language list', 'list frequently', 'frequently spanish', 'spanish stopwords', 'stopwords retrieve', 'retrieve included', 'included vast', 'vast majority', 'majority spanish', 'spanish match', 'match criterion', 'criterion sample', 'sample mention', 'mention profile', 'profile derivation', 'derivation selected', 'selected randomly', 'randomly complete', 'complete timeline', 'timeline compiled', 'compiled reduced', 'reduced retweets', 'retweets removed', 'removed constituted', 'constituted dataset', 'dataset identify', 'identify language', 'language relied', 'relied language', 'language automatically', 'automatically identified', 'identified selecting', 'selecting spanish', 'spanish ha', 'ha noted', 'noted contain', 'contain unidentified', 'unidentified depressive']","['selection based filtered', 'based filtered realtime', 'filtered realtime streaming', 'realtime streaming support', 'streaming support provided', 'support provided api', 'provided api first', 'api first step', 'first step selected', 'step selected showed', 'selected showed potential', 'showed potential sign', 'potential sign basis', 'sign basis frequent', 'basis frequent spanish', 'frequent spanish expressed', 'spanish expressed patient', 'expressed patient suffering', 'patient suffering clinical', 'suffering clinical setting', 'clinical setting jointly', 'setting jointly identified', 'jointly identified selected', 'identified selected psychologist', 'selected psychologist family', 'psychologist family physician', 'family physician clinical', 'physician clinical experience', 'clinical experience based', 'experience based definition', 'based definition general', 'definition general feature', 'general feature according', 'feature according diagnostic', 'according diagnostic statistical', 'diagnostic statistical manual', 'statistical manual disorder', 'manual disorder list', 'disorder list english', 'list english translation', 'english translation shown', 'translation shown textbox', 'shown textbox june', 'textbox june including', 'june including occurrence', 'including occurrence listed', 'occurrence listed textbox', 'listed textbox collected', 'textbox collected collection', 'collected collection select', 'collection select publicly', 'select publicly stated', 'publicly stated textual', 'stated textual description', 'textual description associated', 'description associated profile', 'associated profile suffered', 'profile suffered profile', 'suffered profile description', 'profile description including', 'description including occurrence', 'including occurrence depr', 'occurrence depr possible', 'depr possible derivation', 'possible derivation related', 'derivation related spanish', 'related spanish depre', 'spanish depre depresin', 'depre depresin depresivo', 'depresin depresivo depresiva', 'depresivo depresiva deprimido', 'depresiva deprimido deprimida', 'deprimido deprimida considered', 'deprimida considered included', 'considered included description', 'included description profile', 'description profile stated', 'profile stated suffered', 'stated suffered receiving', 'suffered receiving treatment', 'receiving treatment selected', 'treatment selected analysis', 'selected analysis selection', 'analysis selection performed', 'selection performed psychologist', 'performed psychologist verifying', 'psychologist verifying statement', 'verifying statement related', 'statement related real', 'related real expression', 'real expression excluding', 'expression excluding quote', 'excluding quote joke', 'quote joke fake', 'joke fake one', 'fake one depressed', 'one depressed collected', 'depressed collected recent', 'collected recent timeline', 'recent timeline maximum', 'timeline maximum thus', 'maximum thus total', 'thus total collected', 'total collected figure', 'collected figure reduced', 'figure reduced discarding', 'reduced discarding retweets', 'discarding retweets constituted', 'retweets constituted depressive', 'constituted depressive dataset', 'depressive dataset example', 'dataset example sentence', 'example sentence appearing', 'sentence appearing profile', 'appearing profile selecting', 'profile selecting depressive', 'selecting depressive paciente', 'depressive paciente psiquitrico', 'paciente psiquitrico con', 'psiquitrico con depresin', 'con depresin crnica', 'depresin crnica psychiatric', 'crnica psychiatric patient', 'psychiatric patient chronic', 'patient chronic example', 'chronic example profile', 'example profile sentence', 'profile sentence indicates', 'sentence indicates colecciono', 'indicates colecciono errores', 'colecciono errores traducidos', 'errores traducidos depresivos', 'traducidos depresivos uno', 'depresivos uno que', 'uno que otro', 'que otro impulso', 'otro impulso de', 'impulso de amor', 'de amor gather', 'amor gather error', 'gather error translated', 'error translated depressing', 'translated depressing one', 'depressing one another', 'one another love', 'another love impulse', 'love impulse example', 'impulse example profile', 'example profile sentence', 'profile sentence doe', 'sentence doe indicate', 'doe indicate profile', 'indicate profile sentence', 'profile sentence indicating', 'sentence indicating retrieved', 'indicating retrieved timeline', 'retrieved timeline collected', 'timeline collected timeline', 'collected timeline least', 'timeline least suggested', 'least suggested sign', 'suggested sign retained', 'sign retained analysis', 'retained analysis selection', 'analysis selection performed', 'selection performed manually', 'performed manually inspecting', 'manually inspecting complete', 'inspecting complete timeline', 'complete timeline reverse', 'timeline reverse temporal', 'reverse temporal order', 'temporal order starting', 'order starting recent', 'starting recent one', 'recent one oldest', 'one oldest timeline', 'oldest timeline retrieved', 'timeline retrieved mean', 'retrieved mean api', 'mean api finally', 'api finally total', 'finally total number', 'total number issued', 'number issued depressive', 'issued depressive suggesting', 'depressive suggesting sign', 'suggesting sign detected', 'sign detected analysis', 'detected analysis set', 'analysis set provided', 'set provided u', 'provided u depressive', 'u depressive dataset', 'depressive dataset analyze', 'dataset analyze linguistic', 'analyze linguistic feature', 'linguistic feature showing', 'feature showing sign', 'showing sign ha', 'sign ha mentioned', 'ha mentioned included', 'mentioned included depressive', 'included depressive dataset', 'depressive dataset see', 'dataset see figure', 'see figure time', 'figure time also', 'time also collected', 'also collected june', 'collected june gathered', 'june gathered listening', 'gathered listening public', 'listening public stream', 'public stream time', 'stream time span', 'time span considering', 'span considering spanish', 'considering spanish textual', 'spanish textual content', 'textual content detected', 'content detected language', 'detected language identification', 'language identification support', 'identification support given', 'support given requires', 'given requires restrictive', 'requires restrictive filter', 'restrictive filter language', 'filter language list', 'language list frequently', 'list frequently spanish', 'frequently spanish stopwords', 'spanish stopwords retrieve', 'stopwords retrieve included', 'retrieve included vast', 'included vast majority', 'vast majority spanish', 'majority spanish match', 'spanish match criterion', 'match criterion sample', 'criterion sample mention', 'sample mention profile', 'mention profile derivation', 'profile derivation selected', 'derivation selected randomly', 'selected randomly complete', 'randomly complete timeline', 'complete timeline compiled', 'timeline compiled reduced', 'compiled reduced retweets', 'reduced retweets removed', 'retweets removed constituted', 'removed constituted dataset', 'constituted dataset identify', 'dataset identify language', 'identify language relied', 'language relied language', 'relied language automatically', 'language automatically identified', 'automatically identified selecting', 'identified selecting spanish', 'selecting spanish ha', 'spanish ha noted', 'ha noted contain', 'noted contain unidentified', 'contain unidentified depressive']"
https://aclanthology.org/W18-0608.pdf,0,Data was collected from 7 Cups of Tea an anonymous online chat-based peer support community for emotional distress1 . Users agree at signup that their data may be used for the purposes of research. All the data used for the current study was anonymous and securely stored. This research was performed in line with the ethical and privacy protocols outlined in detail in (Benton et al. 2017). Data from 7 Cups takes the form of written dialogue between users of the service and volunteers who are trained as “active listeners”. A fragment of an exchange between the user of the service (U) and the volunteer (V) might go as follows: For the analyses reported in this paper we used only text generated by users of the service not the volunteers providing peer support. Users who reported depression as their primary concern at sign up were eligible for inclusion in analyses. Our original sample was comprised of 23048 conversations involving 1937 unique users. Users were excluded from the sample if they did not indicate their culture or if they selected ‘Other’. This resulted in the exclusion of 199 and 130 users respectively. The original sample also included users identifying as Native American or American Indian. This group was excluded from analyses since the majority of the data among these users was not English. This resulted in the removal of 15 users leaving a total sample size of 1593.,data wa collected from cup of tea an anonymous online chatbased peer support community for emotional distress user agree at signup that their data may be used for the purpose of research all the data used for the current study wa anonymous and securely stored this research wa performed in line with the ethical and privacy protocol outlined in detail in benton et al data from cup take the form of written dialogue between user of the service and volunteer who are trained a active listener a fragment of an exchange between the user of the service u and the volunteer v might go a follows for the analysis reported in this paper we used only text generated by user of the service not the volunteer providing peer support user who reported depression a their primary concern at sign up were eligible for inclusion in analysis our original sample wa comprised of conversation involving unique user user were excluded from the sample if they did not indicate their culture or if they selected other this resulted in the exclusion of and user respectively the original sample also included user identifying a native american or american indian this group wa excluded from analysis since the majority of the data among these user wa not english this resulted in the removal of user leaving a total sample size of,"['collected', 'cup', 'tea', 'anonymous', 'online', 'chatbased', 'peer', 'support', 'community', 'emotional', 'distress', 'agree', 'signup', 'may', 'purpose', 'research', 'current', 'study', 'anonymous', 'securely', 'stored', 'research', 'performed', 'line', 'ethical', 'privacy', 'protocol', 'outlined', 'detail', 'benton', 'et', 'al', 'cup', 'take', 'form', 'written', 'dialogue', 'service', 'volunteer', 'trained', 'active', 'listener', 'fragment', 'exchange', 'service', 'u', 'volunteer', 'v', 'might', 'go', 'follows', 'analysis', 'reported', 'paper', 'text', 'generated', 'service', 'volunteer', 'providing', 'peer', 'support', 'reported', 'primary', 'concern', 'sign', 'eligible', 'inclusion', 'analysis', 'original', 'sample', 'comprised', 'conversation', 'involving', 'unique', 'excluded', 'sample', 'indicate', 'culture', 'selected', 'resulted', 'exclusion', 'respectively', 'original', 'sample', 'also', 'included', 'identifying', 'native', 'american', 'american', 'indian', 'group', 'excluded', 'analysis', 'since', 'majority', 'among', 'english', 'resulted', 'removal', 'leaving', 'total', 'sample', 'size']","['collected cup', 'cup tea', 'tea anonymous', 'anonymous online', 'online chatbased', 'chatbased peer', 'peer support', 'support community', 'community emotional', 'emotional distress', 'distress agree', 'agree signup', 'signup may', 'may purpose', 'purpose research', 'research current', 'current study', 'study anonymous', 'anonymous securely', 'securely stored', 'stored research', 'research performed', 'performed line', 'line ethical', 'ethical privacy', 'privacy protocol', 'protocol outlined', 'outlined detail', 'detail benton', 'benton et', 'et al', 'al cup', 'cup take', 'take form', 'form written', 'written dialogue', 'dialogue service', 'service volunteer', 'volunteer trained', 'trained active', 'active listener', 'listener fragment', 'fragment exchange', 'exchange service', 'service u', 'u volunteer', 'volunteer v', 'v might', 'might go', 'go follows', 'follows analysis', 'analysis reported', 'reported paper', 'paper text', 'text generated', 'generated service', 'service volunteer', 'volunteer providing', 'providing peer', 'peer support', 'support reported', 'reported primary', 'primary concern', 'concern sign', 'sign eligible', 'eligible inclusion', 'inclusion analysis', 'analysis original', 'original sample', 'sample comprised', 'comprised conversation', 'conversation involving', 'involving unique', 'unique excluded', 'excluded sample', 'sample indicate', 'indicate culture', 'culture selected', 'selected resulted', 'resulted exclusion', 'exclusion respectively', 'respectively original', 'original sample', 'sample also', 'also included', 'included identifying', 'identifying native', 'native american', 'american american', 'american indian', 'indian group', 'group excluded', 'excluded analysis', 'analysis since', 'since majority', 'majority among', 'among english', 'english resulted', 'resulted removal', 'removal leaving', 'leaving total', 'total sample', 'sample size']","['collected cup tea', 'cup tea anonymous', 'tea anonymous online', 'anonymous online chatbased', 'online chatbased peer', 'chatbased peer support', 'peer support community', 'support community emotional', 'community emotional distress', 'emotional distress agree', 'distress agree signup', 'agree signup may', 'signup may purpose', 'may purpose research', 'purpose research current', 'research current study', 'current study anonymous', 'study anonymous securely', 'anonymous securely stored', 'securely stored research', 'stored research performed', 'research performed line', 'performed line ethical', 'line ethical privacy', 'ethical privacy protocol', 'privacy protocol outlined', 'protocol outlined detail', 'outlined detail benton', 'detail benton et', 'benton et al', 'et al cup', 'al cup take', 'cup take form', 'take form written', 'form written dialogue', 'written dialogue service', 'dialogue service volunteer', 'service volunteer trained', 'volunteer trained active', 'trained active listener', 'active listener fragment', 'listener fragment exchange', 'fragment exchange service', 'exchange service u', 'service u volunteer', 'u volunteer v', 'volunteer v might', 'v might go', 'might go follows', 'go follows analysis', 'follows analysis reported', 'analysis reported paper', 'reported paper text', 'paper text generated', 'text generated service', 'generated service volunteer', 'service volunteer providing', 'volunteer providing peer', 'providing peer support', 'peer support reported', 'support reported primary', 'reported primary concern', 'primary concern sign', 'concern sign eligible', 'sign eligible inclusion', 'eligible inclusion analysis', 'inclusion analysis original', 'analysis original sample', 'original sample comprised', 'sample comprised conversation', 'comprised conversation involving', 'conversation involving unique', 'involving unique excluded', 'unique excluded sample', 'excluded sample indicate', 'sample indicate culture', 'indicate culture selected', 'culture selected resulted', 'selected resulted exclusion', 'resulted exclusion respectively', 'exclusion respectively original', 'respectively original sample', 'original sample also', 'sample also included', 'also included identifying', 'included identifying native', 'identifying native american', 'native american american', 'american american indian', 'american indian group', 'indian group excluded', 'group excluded analysis', 'excluded analysis since', 'analysis since majority', 'since majority among', 'majority among english', 'among english resulted', 'english resulted removal', 'resulted removal leaving', 'removal leaving total', 'leaving total sample', 'total sample size']"
https://dl.acm.org/doi/pdf/10.1145/3359169,1,"4.2.1 General linguistic differences. To better understand the content-based differences between posts from individuals in the minority sample we compare the top unigrams bigrams and trigrams from each minority country to those from the majority sample. We filter out any words that were only one character as well as the Natural Language Toolkit’s built in stopwords [16]. The top 5 n-grams unigrams bigrams and trigrams can be seen in Table 3. N-grams that have been redacted are the usernames of Talklife users that were discussed in plain text by others on the website. We can make a few observations from the relative frequency of n-grams. First English is the most commonly used language within each community on Talklife as words from languages native to each country were not the majority of any of the n-grams. Second top-5 n-grams from the majority sample are not common to the top-5 n-grams in any of the minority sample suggesting a difference in how people express mental health. Even when people are talking about the same theme such as getting support they tend to use different phrases: the majority sample uses “need someone to talk to” while Indians prefer “like to talk to friends”. Third while much of the language most often expressed by users in the majority sample is support language expressing individual distress (such as “i’m” or “feel”) we find that people from the minority sample are more likely to talk about themselves in relation to other people as illustrated by the bolded n-grams in Table 3. For example individuals from the minority sample use the term “us” at a higher amount than those in the majority sample. Further individuals from India often talk about wanting or needing friends whereas individuals from Malaysia and Filipino refer to loneliness (using the word “alone”) more often than people in the majority sample. This use of terms related to interpersonal connections to express mental distress is seen in India [73] Malaysia [92] and the Philippines [35] when individuals experiencing distress are asked to describe how they are feeling. Our finding extends this work in the context of online mental health support communities. Finally references to religion (such as “god” or “pray find peace”) are more common in posts from Malaysia and the Philippines. This follows past research on the expression of mental health in both Malaysia [65] and the Philippines [55 84] showing that religion is often used as a foundation for how people from these cultural backgrounds express mental health concerns and for how people support one another. The higher presence of references to religion among users from Malaysia and the Philippines might also suggest that index posts that discuss religion but do not specifically discuss distress might actually be one culturally-sanctioned method of signposting a state of mental distress as seen in past research in offline contexts [24 55]. 4.2.2 Differences in Clinical Language Use. Based on past research showing that individuals from minority countries often express mental distress in non-clinical terms [52 53 64] we examine the use of clinical language on online mental health support communities. For this analysis of clinical language around mental distress from different countries we use data from the top 25% of users based on number of posts on Talklife to analyze a greater number of posts. As Table 4 shows the amount of clinical language consistently differs between the majority sample and minority samples. We find that clinical language is used less frequently in posts from the minority sample than in the majority sample. For Indians Malaysians and Filipinos 14.2-17.5% of index posts have clinical language respectively compared to 22.2% for the majority sample. We see a similar but smaller difference for support responses: 6.6-7.9% of support responses tend to have clinical language for the minority sample compared to 8.6% for the majority sample.These differences are significant with a p < .01 unless otherwise indicated.3 For all countries though we find consistently that support responses have less amounts of clinical language than index posts. However if we look at the frequency of clinical language within posts the difference between the minority sample and the majority sample is small. Combined with the evidence that the fraction of posts with at least one use of clinical language is substantially lower for the minority sample this implies that there must be higher use of clinical language terms per post for each post from the minority sample that does contain clinical language. This implication is verified by the last two rows of Table 4: when we restrict our analysis to only those posts that do contain clinical language posts from the minority sample use more clinical mental health language in comparison to the majority sample for both index posts and support responses with a statistically significant difference at p = .01. Thus while a smaller fraction of posts in the minority sample use clinical language overall there is a big variation in its use. Posts that do use clinical language tend to use many more terms per post than those from majority countries suggesting that people who do use clinical language from minority countries are participating in a standard and globalized language around describing clinical mental health. Table 5 confirms the above result when we look at the top unigrams bigrams and trigrams of clinical language used by people from different countries. The percentages for each term or n-gram correspond to the fraction of occurrences of the n-gram relative to all clinical n-grams. We find that there is not much variation between the clinical language terms that are used across the different countries. The disorders that are talked about most explicitly (such as “borderline personality disorder” “personality disorder” or “social anxiety”) are used at relatively consistent rates between both the minority sample and the majority sample. As posited above it is possible that this consistency in types of clinical language hints at some form of standardization in how those users who use clinical language use it a globalized online clinical language around mental health and a potential difference between past work on expressions of distress in offline settings. That said it should be noted that some of the most popular n-grams especially unigrams such as “sleep” and “night” can function as false positives as not every mention of sleep or night on Talklife is a reference to a mental health issue (such as “sleep paralysis” or “night terrors”). These terms may also be used in different (and culturally bound) ways. More specific terms (bigrams such as ""anxiety disorder"" and trigrams such as ""borderline personality disorder"" are used rarely. Finally we also report potential cross-cultural differences when individuals first begin to use clinical language on Talklife. On average Indians tend to use clinical language on the 6th post (mean = 6.3 σ = 12.12) whereas Malaysians Filipinos and the population from the majority sample tend to use clinical language on the 4th post (mean = 4.31 4.51 4.13 σ = 5.7 6.8 6.6). Interpretation of these results will require more work investigations the reasons why Indians use clinical language later in a thread. On balance these results shows that individuals from the minority sample use lesser amounts of clinical language overall but also show more variation in use than people from the majority sample and add nuanced perspective to past work [52 53 64] showing that individuals from these minority countries are often less likely to use clinical language when conceptualizing and describing their experience of mental distress.",general linguistic difference to better understand the contentbased difference between post from individual in the minority sample we compare the top unigrams bigram and trigram from each minority country to those from the majority sample we filter out any word that were only one character a well a the natural language toolkits built in stopwords the top ngrams unigrams bigram and trigram can be seen in table ngrams that have been redacted are the usernames of talklife user that were discussed in plain text by others on the website we can make a few observation from the relative frequency of ngrams first english is the most commonly used language within each community on talklife a word from language native to each country were not the majority of any of the ngrams second top ngrams from the majority sample are not common to the top ngrams in any of the minority sample suggesting a difference in how people express mental health even when people are talking about the same theme such a getting support they tend to use different phrase the majority sample us need someone to talk to while indian prefer like to talk to friend third while much of the language most often expressed by user in the majority sample is support language expressing individual distress such a im or feel we find that people from the minority sample are more likely to talk about themselves in relation to other people a illustrated by the bolded ngrams in table for example individual from the minority sample use the term u at a higher amount than those in the majority sample further individual from india often talk about wanting or needing friend whereas individual from malaysia and filipino refer to loneliness using the word alone more often than people in the majority sample this use of term related to interpersonal connection to express mental distress is seen in india malaysia and the philippine when individual experiencing distress are asked to describe how they are feeling our finding extends this work in the context of online mental health support community finally reference to religion such a god or pray find peace are more common in post from malaysia and the philippine this follows past research on the expression of mental health in both malaysia and the philippine showing that religion is often used a a foundation for how people from these cultural background express mental health concern and for how people support one another the higher presence of reference to religion among user from malaysia and the philippine might also suggest that index post that discus religion but do not specifically discus distress might actually be one culturallysanctioned method of signposting a state of mental distress a seen in past research in offline context difference in clinical language use based on past research showing that individual from minority country often express mental distress in nonclinical term we examine the use of clinical language on online mental health support community for this analysis of clinical language around mental distress from different country we use data from the top of user based on number of post on talklife to analyze a greater number of post a table show the amount of clinical language consistently differs between the majority sample and minority sample we find that clinical language is used le frequently in post from the minority sample than in the majority sample for indian malaysian and filipino of index post have clinical language respectively compared to for the majority sample we see a similar but smaller difference for support response of support response tend to have clinical language for the minority sample compared to for the majority samplethese difference are significant with a p unless otherwise indicated for all country though we find consistently that support response have le amount of clinical language than index post however if we look at the frequency of clinical language within post the difference between the minority sample and the majority sample is small combined with the evidence that the fraction of post with at least one use of clinical language is substantially lower for the minority sample this implies that there must be higher use of clinical language term per post for each post from the minority sample that doe contain clinical language this implication is verified by the last two row of table when we restrict our analysis to only those post that do contain clinical language post from the minority sample use more clinical mental health language in comparison to the majority sample for both index post and support response with a statistically significant difference at p thus while a smaller fraction of post in the minority sample use clinical language overall there is a big variation in it use post that do use clinical language tend to use many more term per post than those from majority country suggesting that people who do use clinical language from minority country are participating in a standard and globalized language around describing clinical mental health table confirms the above result when we look at the top unigrams bigram and trigram of clinical language used by people from different country the percentage for each term or ngram correspond to the fraction of occurrence of the ngram relative to all clinical ngrams we find that there is not much variation between the clinical language term that are used across the different country the disorder that are talked about most explicitly such a borderline personality disorder personality disorder or social anxiety are used at relatively consistent rate between both the minority sample and the majority sample a posited above it is possible that this consistency in type of clinical language hint at some form of standardization in how those user who use clinical language use it a globalized online clinical language around mental health and a potential difference between past work on expression of distress in offline setting that said it should be noted that some of the most popular ngrams especially unigrams such a sleep and night can function a false positive a not every mention of sleep or night on talklife is a reference to a mental health issue such a sleep paralysis or night terror these term may also be used in different and culturally bound way more specific term bigram such a anxiety disorder and trigram such a borderline personality disorder are used rarely finally we also report potential crosscultural difference when individual first begin to use clinical language on talklife on average indian tend to use clinical language on the th post mean whereas malaysian filipino and the population from the majority sample tend to use clinical language on the th post mean interpretation of these result will require more work investigation the reason why indian use clinical language later in a thread on balance these result show that individual from the minority sample use lesser amount of clinical language overall but also show more variation in use than people from the majority sample and add nuanced perspective to past work showing that individual from these minority country are often le likely to use clinical language when conceptualizing and describing their experience of mental distress,"['general', 'linguistic', 'difference', 'better', 'understand', 'contentbased', 'difference', 'individual', 'minority', 'sample', 'compare', 'top', 'unigrams', 'bigram', 'trigram', 'minority', 'country', 'majority', 'sample', 'filter', 'one', 'character', 'well', 'natural', 'language', 'toolkits', 'built', 'stopwords', 'top', 'ngrams', 'unigrams', 'bigram', 'trigram', 'seen', 'table', 'ngrams', 'redacted', 'usernames', 'talklife', 'discussed', 'plain', 'text', 'others', 'website', 'make', 'observation', 'relative', 'frequency', 'ngrams', 'first', 'english', 'commonly', 'language', 'within', 'community', 'talklife', 'language', 'native', 'country', 'majority', 'ngrams', 'second', 'top', 'ngrams', 'majority', 'sample', 'common', 'top', 'ngrams', 'minority', 'sample', 'suggesting', 'difference', 'people', 'express', 'even', 'people', 'talking', 'theme', 'getting', 'support', 'tend', 'use', 'different', 'phrase', 'majority', 'sample', 'us', 'need', 'someone', 'talk', 'indian', 'prefer', 'like', 'talk', 'friend', 'third', 'much', 'language', 'often', 'expressed', 'majority', 'sample', 'support', 'language', 'expressing', 'individual', 'distress', 'im', 'feel', 'find', 'people', 'minority', 'sample', 'likely', 'talk', 'relation', 'people', 'illustrated', 'bolded', 'ngrams', 'table', 'example', 'individual', 'minority', 'sample', 'use', 'term', 'u', 'higher', 'amount', 'majority', 'sample', 'individual', 'india', 'often', 'talk', 'wanting', 'needing', 'friend', 'whereas', 'individual', 'malaysia', 'filipino', 'refer', 'loneliness', 'using', 'alone', 'often', 'people', 'majority', 'sample', 'use', 'term', 'related', 'interpersonal', 'connection', 'express', 'distress', 'seen', 'india', 'malaysia', 'philippine', 'individual', 'experiencing', 'distress', 'asked', 'describe', 'feeling', 'finding', 'extends', 'work', 'context', 'online', 'support', 'community', 'finally', 'reference', 'religion', 'god', 'pray', 'find', 'peace', 'common', 'malaysia', 'philippine', 'follows', 'past', 'research', 'expression', 'malaysia', 'philippine', 'showing', 'religion', 'often', 'foundation', 'people', 'cultural', 'background', 'express', 'concern', 'people', 'support', 'one', 'another', 'higher', 'presence', 'reference', 'religion', 'among', 'malaysia', 'philippine', 'might', 'also', 'suggest', 'index', 'discus', 'religion', 'specifically', 'discus', 'distress', 'might', 'actually', 'one', 'culturallysanctioned', 'method', 'signposting', 'state', 'distress', 'seen', 'past', 'research', 'offline', 'context', 'difference', 'clinical', 'language', 'use', 'based', 'past', 'research', 'showing', 'individual', 'minority', 'country', 'often', 'express', 'distress', 'nonclinical', 'term', 'examine', 'use', 'clinical', 'language', 'online', 'support', 'community', 'analysis', 'clinical', 'language', 'around', 'distress', 'different', 'country', 'use', 'top', 'based', 'number', 'talklife', 'analyze', 'greater', 'number', 'table', 'show', 'amount', 'clinical', 'language', 'consistently', 'differs', 'majority', 'sample', 'minority', 'sample', 'find', 'clinical', 'language', 'le', 'frequently', 'minority', 'sample', 'majority', 'sample', 'indian', 'malaysian', 'filipino', 'index', 'clinical', 'language', 'respectively', 'compared', 'majority', 'sample', 'see', 'similar', 'smaller', 'difference', 'support', 'response', 'support', 'response', 'tend', 'clinical', 'language', 'minority', 'sample', 'compared', 'majority', 'samplethese', 'difference', 'significant', 'p', 'unless', 'otherwise', 'indicated', 'country', 'though', 'find', 'consistently', 'support', 'response', 'le', 'amount', 'clinical', 'language', 'index', 'however', 'look', 'frequency', 'clinical', 'language', 'within', 'difference', 'minority', 'sample', 'majority', 'sample', 'small', 'combined', 'evidence', 'fraction', 'least', 'one', 'use', 'clinical', 'language', 'substantially', 'lower', 'minority', 'sample', 'implies', 'must', 'higher', 'use', 'clinical', 'language', 'term', 'per', 'minority', 'sample', 'doe', 'contain', 'clinical', 'language', 'implication', 'verified', 'last', 'two', 'row', 'table', 'restrict', 'analysis', 'contain', 'clinical', 'language', 'minority', 'sample', 'use', 'clinical', 'language', 'comparison', 'majority', 'sample', 'index', 'support', 'response', 'statistically', 'significant', 'difference', 'p', 'thus', 'smaller', 'fraction', 'minority', 'sample', 'use', 'clinical', 'language', 'overall', 'big', 'variation', 'use', 'use', 'clinical', 'language', 'tend', 'use', 'many', 'term', 'per', 'majority', 'country', 'suggesting', 'people', 'use', 'clinical', 'language', 'minority', 'country', 'participating', 'standard', 'globalized', 'language', 'around', 'describing', 'clinical', 'table', 'confirms', 'result', 'look', 'top', 'unigrams', 'bigram', 'trigram', 'clinical', 'language', 'people', 'different', 'country', 'percentage', 'term', 'ngram', 'correspond', 'fraction', 'occurrence', 'ngram', 'relative', 'clinical', 'ngrams', 'find', 'much', 'variation', 'clinical', 'language', 'term', 'across', 'different', 'country', 'disorder', 'talked', 'explicitly', 'borderline', 'personality', 'disorder', 'personality', 'disorder', 'social', 'anxiety', 'relatively', 'consistent', 'rate', 'minority', 'sample', 'majority', 'sample', 'posited', 'possible', 'consistency', 'type', 'clinical', 'language', 'hint', 'form', 'standardization', 'use', 'clinical', 'language', 'use', 'globalized', 'online', 'clinical', 'language', 'around', 'potential', 'difference', 'past', 'work', 'expression', 'distress', 'offline', 'setting', 'said', 'noted', 'popular', 'ngrams', 'especially', 'unigrams', 'sleep', 'night', 'function', 'false', 'positive', 'every', 'mention', 'sleep', 'night', 'talklife', 'reference', 'issue', 'sleep', 'paralysis', 'night', 'terror', 'term', 'may', 'also', 'different', 'culturally', 'bound', 'way', 'specific', 'term', 'bigram', 'anxiety', 'disorder', 'trigram', 'borderline', 'personality', 'disorder', 'rarely', 'finally', 'also', 'report', 'potential', 'crosscultural', 'difference', 'individual', 'first', 'begin', 'use', 'clinical', 'language', 'talklife', 'average', 'indian', 'tend', 'use', 'clinical', 'language', 'th', 'mean', 'whereas', 'malaysian', 'filipino', 'population', 'majority', 'sample', 'tend', 'use', 'clinical', 'language', 'th', 'mean', 'interpretation', 'result', 'require', 'work', 'investigation', 'reason', 'indian', 'use', 'clinical', 'language', 'later', 'thread', 'balance', 'result', 'show', 'individual', 'minority', 'sample', 'use', 'lesser', 'amount', 'clinical', 'language', 'overall', 'also', 'show', 'variation', 'use', 'people', 'majority', 'sample', 'add', 'nuanced', 'perspective', 'past', 'work', 'showing', 'individual', 'minority', 'country', 'often', 'le', 'likely', 'use', 'clinical', 'language', 'conceptualizing', 'describing', 'experience', 'distress']","['general linguistic', 'linguistic difference', 'difference better', 'better understand', 'understand contentbased', 'contentbased difference', 'difference individual', 'individual minority', 'minority sample', 'sample compare', 'compare top', 'top unigrams', 'unigrams bigram', 'bigram trigram', 'trigram minority', 'minority country', 'country majority', 'majority sample', 'sample filter', 'filter one', 'one character', 'character well', 'well natural', 'natural language', 'language toolkits', 'toolkits built', 'built stopwords', 'stopwords top', 'top ngrams', 'ngrams unigrams', 'unigrams bigram', 'bigram trigram', 'trigram seen', 'seen table', 'table ngrams', 'ngrams redacted', 'redacted usernames', 'usernames talklife', 'talklife discussed', 'discussed plain', 'plain text', 'text others', 'others website', 'website make', 'make observation', 'observation relative', 'relative frequency', 'frequency ngrams', 'ngrams first', 'first english', 'english commonly', 'commonly language', 'language within', 'within community', 'community talklife', 'talklife language', 'language native', 'native country', 'country majority', 'majority ngrams', 'ngrams second', 'second top', 'top ngrams', 'ngrams majority', 'majority sample', 'sample common', 'common top', 'top ngrams', 'ngrams minority', 'minority sample', 'sample suggesting', 'suggesting difference', 'difference people', 'people express', 'express even', 'even people', 'people talking', 'talking theme', 'theme getting', 'getting support', 'support tend', 'tend use', 'use different', 'different phrase', 'phrase majority', 'majority sample', 'sample us', 'us need', 'need someone', 'someone talk', 'talk indian', 'indian prefer', 'prefer like', 'like talk', 'talk friend', 'friend third', 'third much', 'much language', 'language often', 'often expressed', 'expressed majority', 'majority sample', 'sample support', 'support language', 'language expressing', 'expressing individual', 'individual distress', 'distress im', 'im feel', 'feel find', 'find people', 'people minority', 'minority sample', 'sample likely', 'likely talk', 'talk relation', 'relation people', 'people illustrated', 'illustrated bolded', 'bolded ngrams', 'ngrams table', 'table example', 'example individual', 'individual minority', 'minority sample', 'sample use', 'use term', 'term u', 'u higher', 'higher amount', 'amount majority', 'majority sample', 'sample individual', 'individual india', 'india often', 'often talk', 'talk wanting', 'wanting needing', 'needing friend', 'friend whereas', 'whereas individual', 'individual malaysia', 'malaysia filipino', 'filipino refer', 'refer loneliness', 'loneliness using', 'using alone', 'alone often', 'often people', 'people majority', 'majority sample', 'sample use', 'use term', 'term related', 'related interpersonal', 'interpersonal connection', 'connection express', 'express distress', 'distress seen', 'seen india', 'india malaysia', 'malaysia philippine', 'philippine individual', 'individual experiencing', 'experiencing distress', 'distress asked', 'asked describe', 'describe feeling', 'feeling finding', 'finding extends', 'extends work', 'work context', 'context online', 'online support', 'support community', 'community finally', 'finally reference', 'reference religion', 'religion god', 'god pray', 'pray find', 'find peace', 'peace common', 'common malaysia', 'malaysia philippine', 'philippine follows', 'follows past', 'past research', 'research expression', 'expression malaysia', 'malaysia philippine', 'philippine showing', 'showing religion', 'religion often', 'often foundation', 'foundation people', 'people cultural', 'cultural background', 'background express', 'express concern', 'concern people', 'people support', 'support one', 'one another', 'another higher', 'higher presence', 'presence reference', 'reference religion', 'religion among', 'among malaysia', 'malaysia philippine', 'philippine might', 'might also', 'also suggest', 'suggest index', 'index discus', 'discus religion', 'religion specifically', 'specifically discus', 'discus distress', 'distress might', 'might actually', 'actually one', 'one culturallysanctioned', 'culturallysanctioned method', 'method signposting', 'signposting state', 'state distress', 'distress seen', 'seen past', 'past research', 'research offline', 'offline context', 'context difference', 'difference clinical', 'clinical language', 'language use', 'use based', 'based past', 'past research', 'research showing', 'showing individual', 'individual minority', 'minority country', 'country often', 'often express', 'express distress', 'distress nonclinical', 'nonclinical term', 'term examine', 'examine use', 'use clinical', 'clinical language', 'language online', 'online support', 'support community', 'community analysis', 'analysis clinical', 'clinical language', 'language around', 'around distress', 'distress different', 'different country', 'country use', 'use top', 'top based', 'based number', 'number talklife', 'talklife analyze', 'analyze greater', 'greater number', 'number table', 'table show', 'show amount', 'amount clinical', 'clinical language', 'language consistently', 'consistently differs', 'differs majority', 'majority sample', 'sample minority', 'minority sample', 'sample find', 'find clinical', 'clinical language', 'language le', 'le frequently', 'frequently minority', 'minority sample', 'sample majority', 'majority sample', 'sample indian', 'indian malaysian', 'malaysian filipino', 'filipino index', 'index clinical', 'clinical language', 'language respectively', 'respectively compared', 'compared majority', 'majority sample', 'sample see', 'see similar', 'similar smaller', 'smaller difference', 'difference support', 'support response', 'response support', 'support response', 'response tend', 'tend clinical', 'clinical language', 'language minority', 'minority sample', 'sample compared', 'compared majority', 'majority samplethese', 'samplethese difference', 'difference significant', 'significant p', 'p unless', 'unless otherwise', 'otherwise indicated', 'indicated country', 'country though', 'though find', 'find consistently', 'consistently support', 'support response', 'response le', 'le amount', 'amount clinical', 'clinical language', 'language index', 'index however', 'however look', 'look frequency', 'frequency clinical', 'clinical language', 'language within', 'within difference', 'difference minority', 'minority sample', 'sample majority', 'majority sample', 'sample small', 'small combined', 'combined evidence', 'evidence fraction', 'fraction least', 'least one', 'one use', 'use clinical', 'clinical language', 'language substantially', 'substantially lower', 'lower minority', 'minority sample', 'sample implies', 'implies must', 'must higher', 'higher use', 'use clinical', 'clinical language', 'language term', 'term per', 'per minority', 'minority sample', 'sample doe', 'doe contain', 'contain clinical', 'clinical language', 'language implication', 'implication verified', 'verified last', 'last two', 'two row', 'row table', 'table restrict', 'restrict analysis', 'analysis contain', 'contain clinical', 'clinical language', 'language minority', 'minority sample', 'sample use', 'use clinical', 'clinical language', 'language comparison', 'comparison majority', 'majority sample', 'sample index', 'index support', 'support response', 'response statistically', 'statistically significant', 'significant difference', 'difference p', 'p thus', 'thus smaller', 'smaller fraction', 'fraction minority', 'minority sample', 'sample use', 'use clinical', 'clinical language', 'language overall', 'overall big', 'big variation', 'variation use', 'use use', 'use clinical', 'clinical language', 'language tend', 'tend use', 'use many', 'many term', 'term per', 'per majority', 'majority country', 'country suggesting', 'suggesting people', 'people use', 'use clinical', 'clinical language', 'language minority', 'minority country', 'country participating', 'participating standard', 'standard globalized', 'globalized language', 'language around', 'around describing', 'describing clinical', 'clinical table', 'table confirms', 'confirms result', 'result look', 'look top', 'top unigrams', 'unigrams bigram', 'bigram trigram', 'trigram clinical', 'clinical language', 'language people', 'people different', 'different country', 'country percentage', 'percentage term', 'term ngram', 'ngram correspond', 'correspond fraction', 'fraction occurrence', 'occurrence ngram', 'ngram relative', 'relative clinical', 'clinical ngrams', 'ngrams find', 'find much', 'much variation', 'variation clinical', 'clinical language', 'language term', 'term across', 'across different', 'different country', 'country disorder', 'disorder talked', 'talked explicitly', 'explicitly borderline', 'borderline personality', 'personality disorder', 'disorder personality', 'personality disorder', 'disorder social', 'social anxiety', 'anxiety relatively', 'relatively consistent', 'consistent rate', 'rate minority', 'minority sample', 'sample majority', 'majority sample', 'sample posited', 'posited possible', 'possible consistency', 'consistency type', 'type clinical', 'clinical language', 'language hint', 'hint form', 'form standardization', 'standardization use', 'use clinical', 'clinical language', 'language use', 'use globalized', 'globalized online', 'online clinical', 'clinical language', 'language around', 'around potential', 'potential difference', 'difference past', 'past work', 'work expression', 'expression distress', 'distress offline', 'offline setting', 'setting said', 'said noted', 'noted popular', 'popular ngrams', 'ngrams especially', 'especially unigrams', 'unigrams sleep', 'sleep night', 'night function', 'function false', 'false positive', 'positive every', 'every mention', 'mention sleep', 'sleep night', 'night talklife', 'talklife reference', 'reference issue', 'issue sleep', 'sleep paralysis', 'paralysis night', 'night terror', 'terror term', 'term may', 'may also', 'also different', 'different culturally', 'culturally bound', 'bound way', 'way specific', 'specific term', 'term bigram', 'bigram anxiety', 'anxiety disorder', 'disorder trigram', 'trigram borderline', 'borderline personality', 'personality disorder', 'disorder rarely', 'rarely finally', 'finally also', 'also report', 'report potential', 'potential crosscultural', 'crosscultural difference', 'difference individual', 'individual first', 'first begin', 'begin use', 'use clinical', 'clinical language', 'language talklife', 'talklife average', 'average indian', 'indian tend', 'tend use', 'use clinical', 'clinical language', 'language th', 'th mean', 'mean whereas', 'whereas malaysian', 'malaysian filipino', 'filipino population', 'population majority', 'majority sample', 'sample tend', 'tend use', 'use clinical', 'clinical language', 'language th', 'th mean', 'mean interpretation', 'interpretation result', 'result require', 'require work', 'work investigation', 'investigation reason', 'reason indian', 'indian use', 'use clinical', 'clinical language', 'language later', 'later thread', 'thread balance', 'balance result', 'result show', 'show individual', 'individual minority', 'minority sample', 'sample use', 'use lesser', 'lesser amount', 'amount clinical', 'clinical language', 'language overall', 'overall also', 'also show', 'show variation', 'variation use', 'use people', 'people majority', 'majority sample', 'sample add', 'add nuanced', 'nuanced perspective', 'perspective past', 'past work', 'work showing', 'showing individual', 'individual minority', 'minority country', 'country often', 'often le', 'le likely', 'likely use', 'use clinical', 'clinical language', 'language conceptualizing', 'conceptualizing describing', 'describing experience', 'experience distress']","['general linguistic difference', 'linguistic difference better', 'difference better understand', 'better understand contentbased', 'understand contentbased difference', 'contentbased difference individual', 'difference individual minority', 'individual minority sample', 'minority sample compare', 'sample compare top', 'compare top unigrams', 'top unigrams bigram', 'unigrams bigram trigram', 'bigram trigram minority', 'trigram minority country', 'minority country majority', 'country majority sample', 'majority sample filter', 'sample filter one', 'filter one character', 'one character well', 'character well natural', 'well natural language', 'natural language toolkits', 'language toolkits built', 'toolkits built stopwords', 'built stopwords top', 'stopwords top ngrams', 'top ngrams unigrams', 'ngrams unigrams bigram', 'unigrams bigram trigram', 'bigram trigram seen', 'trigram seen table', 'seen table ngrams', 'table ngrams redacted', 'ngrams redacted usernames', 'redacted usernames talklife', 'usernames talklife discussed', 'talklife discussed plain', 'discussed plain text', 'plain text others', 'text others website', 'others website make', 'website make observation', 'make observation relative', 'observation relative frequency', 'relative frequency ngrams', 'frequency ngrams first', 'ngrams first english', 'first english commonly', 'english commonly language', 'commonly language within', 'language within community', 'within community talklife', 'community talklife language', 'talklife language native', 'language native country', 'native country majority', 'country majority ngrams', 'majority ngrams second', 'ngrams second top', 'second top ngrams', 'top ngrams majority', 'ngrams majority sample', 'majority sample common', 'sample common top', 'common top ngrams', 'top ngrams minority', 'ngrams minority sample', 'minority sample suggesting', 'sample suggesting difference', 'suggesting difference people', 'difference people express', 'people express even', 'express even people', 'even people talking', 'people talking theme', 'talking theme getting', 'theme getting support', 'getting support tend', 'support tend use', 'tend use different', 'use different phrase', 'different phrase majority', 'phrase majority sample', 'majority sample us', 'sample us need', 'us need someone', 'need someone talk', 'someone talk indian', 'talk indian prefer', 'indian prefer like', 'prefer like talk', 'like talk friend', 'talk friend third', 'friend third much', 'third much language', 'much language often', 'language often expressed', 'often expressed majority', 'expressed majority sample', 'majority sample support', 'sample support language', 'support language expressing', 'language expressing individual', 'expressing individual distress', 'individual distress im', 'distress im feel', 'im feel find', 'feel find people', 'find people minority', 'people minority sample', 'minority sample likely', 'sample likely talk', 'likely talk relation', 'talk relation people', 'relation people illustrated', 'people illustrated bolded', 'illustrated bolded ngrams', 'bolded ngrams table', 'ngrams table example', 'table example individual', 'example individual minority', 'individual minority sample', 'minority sample use', 'sample use term', 'use term u', 'term u higher', 'u higher amount', 'higher amount majority', 'amount majority sample', 'majority sample individual', 'sample individual india', 'individual india often', 'india often talk', 'often talk wanting', 'talk wanting needing', 'wanting needing friend', 'needing friend whereas', 'friend whereas individual', 'whereas individual malaysia', 'individual malaysia filipino', 'malaysia filipino refer', 'filipino refer loneliness', 'refer loneliness using', 'loneliness using alone', 'using alone often', 'alone often people', 'often people majority', 'people majority sample', 'majority sample use', 'sample use term', 'use term related', 'term related interpersonal', 'related interpersonal connection', 'interpersonal connection express', 'connection express distress', 'express distress seen', 'distress seen india', 'seen india malaysia', 'india malaysia philippine', 'malaysia philippine individual', 'philippine individual experiencing', 'individual experiencing distress', 'experiencing distress asked', 'distress asked describe', 'asked describe feeling', 'describe feeling finding', 'feeling finding extends', 'finding extends work', 'extends work context', 'work context online', 'context online support', 'online support community', 'support community finally', 'community finally reference', 'finally reference religion', 'reference religion god', 'religion god pray', 'god pray find', 'pray find peace', 'find peace common', 'peace common malaysia', 'common malaysia philippine', 'malaysia philippine follows', 'philippine follows past', 'follows past research', 'past research expression', 'research expression malaysia', 'expression malaysia philippine', 'malaysia philippine showing', 'philippine showing religion', 'showing religion often', 'religion often foundation', 'often foundation people', 'foundation people cultural', 'people cultural background', 'cultural background express', 'background express concern', 'express concern people', 'concern people support', 'people support one', 'support one another', 'one another higher', 'another higher presence', 'higher presence reference', 'presence reference religion', 'reference religion among', 'religion among malaysia', 'among malaysia philippine', 'malaysia philippine might', 'philippine might also', 'might also suggest', 'also suggest index', 'suggest index discus', 'index discus religion', 'discus religion specifically', 'religion specifically discus', 'specifically discus distress', 'discus distress might', 'distress might actually', 'might actually one', 'actually one culturallysanctioned', 'one culturallysanctioned method', 'culturallysanctioned method signposting', 'method signposting state', 'signposting state distress', 'state distress seen', 'distress seen past', 'seen past research', 'past research offline', 'research offline context', 'offline context difference', 'context difference clinical', 'difference clinical language', 'clinical language use', 'language use based', 'use based past', 'based past research', 'past research showing', 'research showing individual', 'showing individual minority', 'individual minority country', 'minority country often', 'country often express', 'often express distress', 'express distress nonclinical', 'distress nonclinical term', 'nonclinical term examine', 'term examine use', 'examine use clinical', 'use clinical language', 'clinical language online', 'language online support', 'online support community', 'support community analysis', 'community analysis clinical', 'analysis clinical language', 'clinical language around', 'language around distress', 'around distress different', 'distress different country', 'different country use', 'country use top', 'use top based', 'top based number', 'based number talklife', 'number talklife analyze', 'talklife analyze greater', 'analyze greater number', 'greater number table', 'number table show', 'table show amount', 'show amount clinical', 'amount clinical language', 'clinical language consistently', 'language consistently differs', 'consistently differs majority', 'differs majority sample', 'majority sample minority', 'sample minority sample', 'minority sample find', 'sample find clinical', 'find clinical language', 'clinical language le', 'language le frequently', 'le frequently minority', 'frequently minority sample', 'minority sample majority', 'sample majority sample', 'majority sample indian', 'sample indian malaysian', 'indian malaysian filipino', 'malaysian filipino index', 'filipino index clinical', 'index clinical language', 'clinical language respectively', 'language respectively compared', 'respectively compared majority', 'compared majority sample', 'majority sample see', 'sample see similar', 'see similar smaller', 'similar smaller difference', 'smaller difference support', 'difference support response', 'support response support', 'response support response', 'support response tend', 'response tend clinical', 'tend clinical language', 'clinical language minority', 'language minority sample', 'minority sample compared', 'sample compared majority', 'compared majority samplethese', 'majority samplethese difference', 'samplethese difference significant', 'difference significant p', 'significant p unless', 'p unless otherwise', 'unless otherwise indicated', 'otherwise indicated country', 'indicated country though', 'country though find', 'though find consistently', 'find consistently support', 'consistently support response', 'support response le', 'response le amount', 'le amount clinical', 'amount clinical language', 'clinical language index', 'language index however', 'index however look', 'however look frequency', 'look frequency clinical', 'frequency clinical language', 'clinical language within', 'language within difference', 'within difference minority', 'difference minority sample', 'minority sample majority', 'sample majority sample', 'majority sample small', 'sample small combined', 'small combined evidence', 'combined evidence fraction', 'evidence fraction least', 'fraction least one', 'least one use', 'one use clinical', 'use clinical language', 'clinical language substantially', 'language substantially lower', 'substantially lower minority', 'lower minority sample', 'minority sample implies', 'sample implies must', 'implies must higher', 'must higher use', 'higher use clinical', 'use clinical language', 'clinical language term', 'language term per', 'term per minority', 'per minority sample', 'minority sample doe', 'sample doe contain', 'doe contain clinical', 'contain clinical language', 'clinical language implication', 'language implication verified', 'implication verified last', 'verified last two', 'last two row', 'two row table', 'row table restrict', 'table restrict analysis', 'restrict analysis contain', 'analysis contain clinical', 'contain clinical language', 'clinical language minority', 'language minority sample', 'minority sample use', 'sample use clinical', 'use clinical language', 'clinical language comparison', 'language comparison majority', 'comparison majority sample', 'majority sample index', 'sample index support', 'index support response', 'support response statistically', 'response statistically significant', 'statistically significant difference', 'significant difference p', 'difference p thus', 'p thus smaller', 'thus smaller fraction', 'smaller fraction minority', 'fraction minority sample', 'minority sample use', 'sample use clinical', 'use clinical language', 'clinical language overall', 'language overall big', 'overall big variation', 'big variation use', 'variation use use', 'use use clinical', 'use clinical language', 'clinical language tend', 'language tend use', 'tend use many', 'use many term', 'many term per', 'term per majority', 'per majority country', 'majority country suggesting', 'country suggesting people', 'suggesting people use', 'people use clinical', 'use clinical language', 'clinical language minority', 'language minority country', 'minority country participating', 'country participating standard', 'participating standard globalized', 'standard globalized language', 'globalized language around', 'language around describing', 'around describing clinical', 'describing clinical table', 'clinical table confirms', 'table confirms result', 'confirms result look', 'result look top', 'look top unigrams', 'top unigrams bigram', 'unigrams bigram trigram', 'bigram trigram clinical', 'trigram clinical language', 'clinical language people', 'language people different', 'people different country', 'different country percentage', 'country percentage term', 'percentage term ngram', 'term ngram correspond', 'ngram correspond fraction', 'correspond fraction occurrence', 'fraction occurrence ngram', 'occurrence ngram relative', 'ngram relative clinical', 'relative clinical ngrams', 'clinical ngrams find', 'ngrams find much', 'find much variation', 'much variation clinical', 'variation clinical language', 'clinical language term', 'language term across', 'term across different', 'across different country', 'different country disorder', 'country disorder talked', 'disorder talked explicitly', 'talked explicitly borderline', 'explicitly borderline personality', 'borderline personality disorder', 'personality disorder personality', 'disorder personality disorder', 'personality disorder social', 'disorder social anxiety', 'social anxiety relatively', 'anxiety relatively consistent', 'relatively consistent rate', 'consistent rate minority', 'rate minority sample', 'minority sample majority', 'sample majority sample', 'majority sample posited', 'sample posited possible', 'posited possible consistency', 'possible consistency type', 'consistency type clinical', 'type clinical language', 'clinical language hint', 'language hint form', 'hint form standardization', 'form standardization use', 'standardization use clinical', 'use clinical language', 'clinical language use', 'language use globalized', 'use globalized online', 'globalized online clinical', 'online clinical language', 'clinical language around', 'language around potential', 'around potential difference', 'potential difference past', 'difference past work', 'past work expression', 'work expression distress', 'expression distress offline', 'distress offline setting', 'offline setting said', 'setting said noted', 'said noted popular', 'noted popular ngrams', 'popular ngrams especially', 'ngrams especially unigrams', 'especially unigrams sleep', 'unigrams sleep night', 'sleep night function', 'night function false', 'function false positive', 'false positive every', 'positive every mention', 'every mention sleep', 'mention sleep night', 'sleep night talklife', 'night talklife reference', 'talklife reference issue', 'reference issue sleep', 'issue sleep paralysis', 'sleep paralysis night', 'paralysis night terror', 'night terror term', 'terror term may', 'term may also', 'may also different', 'also different culturally', 'different culturally bound', 'culturally bound way', 'bound way specific', 'way specific term', 'specific term bigram', 'term bigram anxiety', 'bigram anxiety disorder', 'anxiety disorder trigram', 'disorder trigram borderline', 'trigram borderline personality', 'borderline personality disorder', 'personality disorder rarely', 'disorder rarely finally', 'rarely finally also', 'finally also report', 'also report potential', 'report potential crosscultural', 'potential crosscultural difference', 'crosscultural difference individual', 'difference individual first', 'individual first begin', 'first begin use', 'begin use clinical', 'use clinical language', 'clinical language talklife', 'language talklife average', 'talklife average indian', 'average indian tend', 'indian tend use', 'tend use clinical', 'use clinical language', 'clinical language th', 'language th mean', 'th mean whereas', 'mean whereas malaysian', 'whereas malaysian filipino', 'malaysian filipino population', 'filipino population majority', 'population majority sample', 'majority sample tend', 'sample tend use', 'tend use clinical', 'use clinical language', 'clinical language th', 'language th mean', 'th mean interpretation', 'mean interpretation result', 'interpretation result require', 'result require work', 'require work investigation', 'work investigation reason', 'investigation reason indian', 'reason indian use', 'indian use clinical', 'use clinical language', 'clinical language later', 'language later thread', 'later thread balance', 'thread balance result', 'balance result show', 'result show individual', 'show individual minority', 'individual minority sample', 'minority sample use', 'sample use lesser', 'use lesser amount', 'lesser amount clinical', 'amount clinical language', 'clinical language overall', 'language overall also', 'overall also show', 'also show variation', 'show variation use', 'variation use people', 'use people majority', 'people majority sample', 'majority sample add', 'sample add nuanced', 'add nuanced perspective', 'nuanced perspective past', 'perspective past work', 'past work showing', 'work showing individual', 'showing individual minority', 'individual minority country', 'minority country often', 'country often le', 'often le likely', 'le likely use', 'likely use clinical', 'use clinical language', 'clinical language conceptualizing', 'language conceptualizing describing', 'conceptualizing describing experience', 'describing experience distress']"
https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-018-2197-z,1,In this section we first present the data gathered and used in our analysis. Researchers interested in the code and the data are invited to contact the authors. Reddit is a website which enables users to aggregate rate and discuss news entertainment politics and many other topics. According to Alexa it is the 8th most popular website in the world. It was estimated by the Pew research center that 6% of online adults use Reddit [26]. The site is organized into a collection of “subreddits” each focused on a particular topic and administered by a collection of moderators. The subreddit r/SuicideWatch is a forum in which online users are encouraged to post their thoughts regarding suicide. At the time of our data collection it had over 58000 subscribers. Sometimes users express a preoccupation with the thought of suicide. Other times users discuss immediate plans to take their own life. These posts often contain a description of their mental state including depression reaction to stress their feelings of being alone and having a low self-esteem. While most online sources of data are notoriously noisy this particular subreddit is remarkably clean. Given the serious nature of the subreddit individuals are less likely to post harassing comments or off-topic remarks. When users post such comments the moderators of the subreddit quickly remove them. We collected all posts from its inception in 2008 to 2016. Each post is often commented on by other individuals. In this work we focused on the original post as it most often represents the suicidal ideation of a user and comments often represent emotional support from other users. We cleaned this data. First we removed empty posts in which the content had been deleted. Second we removed links and replaced them with the word “link”. Third we concatenated the text of the post to the title as many users begin their post in the title and continue in the body of the post. Finally we removed punctuation and other special characters. After cleaning this data we had 131728 posts with 27978246 words of which 84607 words were unique posted by 63252 unique users.,in this section we first present the data gathered and used in our analysis researcher interested in the code and the data are invited to contact the author reddit is a website which enables user to aggregate rate and discus news entertainment politics and many other topic according to alexa it is the th most popular website in the world it wa estimated by the pew research center that of online adult use reddit the site is organized into a collection of subreddits each focused on a particular topic and administered by a collection of moderator the subreddit rsuicidewatch is a forum in which online user are encouraged to post their thought regarding suicide at the time of our data collection it had over subscriber sometimes user express a preoccupation with the thought of suicide other time user discus immediate plan to take their own life these post often contain a description of their mental state including depression reaction to stress their feeling of being alone and having a low selfesteem while most online source of data are notoriously noisy this particular subreddit is remarkably clean given the serious nature of the subreddit individual are le likely to post harassing comment or offtopic remark when user post such comment the moderator of the subreddit quickly remove them we collected all post from it inception in to each post is often commented on by other individual in this work we focused on the original post a it most often represents the suicidal ideation of a user and comment often represent emotional support from other user we cleaned this data first we removed empty post in which the content had been deleted second we removed link and replaced them with the word link third we concatenated the text of the post to the title a many user begin their post in the title and continue in the body of the post finally we removed punctuation and other special character after cleaning this data we had post with word of which word were unique posted by unique user,"['section', 'first', 'present', 'gathered', 'analysis', 'researcher', 'interested', 'code', 'invited', 'contact', 'author', 'reddit', 'website', 'enables', 'aggregate', 'rate', 'discus', 'news', 'entertainment', 'politics', 'many', 'topic', 'according', 'alexa', 'th', 'popular', 'website', 'world', 'estimated', 'pew', 'research', 'center', 'online', 'adult', 'use', 'reddit', 'site', 'organized', 'collection', 'subreddits', 'focused', 'particular', 'topic', 'administered', 'collection', 'moderator', 'subreddit', 'rsuicidewatch', 'forum', 'online', 'encouraged', 'thought', 'regarding', 'suicide', 'time', 'collection', 'subscriber', 'sometimes', 'express', 'preoccupation', 'thought', 'suicide', 'time', 'discus', 'immediate', 'plan', 'take', 'life', 'often', 'contain', 'description', 'state', 'including', 'reaction', 'stress', 'feeling', 'alone', 'low', 'selfesteem', 'online', 'source', 'notoriously', 'noisy', 'particular', 'subreddit', 'remarkably', 'clean', 'given', 'serious', 'nature', 'subreddit', 'individual', 'le', 'likely', 'harassing', 'comment', 'offtopic', 'remark', 'comment', 'moderator', 'subreddit', 'quickly', 'remove', 'collected', 'inception', 'often', 'commented', 'individual', 'work', 'focused', 'original', 'often', 'represents', 'suicidal', 'ideation', 'comment', 'often', 'represent', 'emotional', 'support', 'cleaned', 'first', 'removed', 'empty', 'content', 'deleted', 'second', 'removed', 'link', 'replaced', 'link', 'third', 'concatenated', 'text', 'title', 'many', 'begin', 'title', 'continue', 'body', 'finally', 'removed', 'punctuation', 'special', 'character', 'cleaning', 'unique', 'posted', 'unique']","['section first', 'first present', 'present gathered', 'gathered analysis', 'analysis researcher', 'researcher interested', 'interested code', 'code invited', 'invited contact', 'contact author', 'author reddit', 'reddit website', 'website enables', 'enables aggregate', 'aggregate rate', 'rate discus', 'discus news', 'news entertainment', 'entertainment politics', 'politics many', 'many topic', 'topic according', 'according alexa', 'alexa th', 'th popular', 'popular website', 'website world', 'world estimated', 'estimated pew', 'pew research', 'research center', 'center online', 'online adult', 'adult use', 'use reddit', 'reddit site', 'site organized', 'organized collection', 'collection subreddits', 'subreddits focused', 'focused particular', 'particular topic', 'topic administered', 'administered collection', 'collection moderator', 'moderator subreddit', 'subreddit rsuicidewatch', 'rsuicidewatch forum', 'forum online', 'online encouraged', 'encouraged thought', 'thought regarding', 'regarding suicide', 'suicide time', 'time collection', 'collection subscriber', 'subscriber sometimes', 'sometimes express', 'express preoccupation', 'preoccupation thought', 'thought suicide', 'suicide time', 'time discus', 'discus immediate', 'immediate plan', 'plan take', 'take life', 'life often', 'often contain', 'contain description', 'description state', 'state including', 'including reaction', 'reaction stress', 'stress feeling', 'feeling alone', 'alone low', 'low selfesteem', 'selfesteem online', 'online source', 'source notoriously', 'notoriously noisy', 'noisy particular', 'particular subreddit', 'subreddit remarkably', 'remarkably clean', 'clean given', 'given serious', 'serious nature', 'nature subreddit', 'subreddit individual', 'individual le', 'le likely', 'likely harassing', 'harassing comment', 'comment offtopic', 'offtopic remark', 'remark comment', 'comment moderator', 'moderator subreddit', 'subreddit quickly', 'quickly remove', 'remove collected', 'collected inception', 'inception often', 'often commented', 'commented individual', 'individual work', 'work focused', 'focused original', 'original often', 'often represents', 'represents suicidal', 'suicidal ideation', 'ideation comment', 'comment often', 'often represent', 'represent emotional', 'emotional support', 'support cleaned', 'cleaned first', 'first removed', 'removed empty', 'empty content', 'content deleted', 'deleted second', 'second removed', 'removed link', 'link replaced', 'replaced link', 'link third', 'third concatenated', 'concatenated text', 'text title', 'title many', 'many begin', 'begin title', 'title continue', 'continue body', 'body finally', 'finally removed', 'removed punctuation', 'punctuation special', 'special character', 'character cleaning', 'cleaning unique', 'unique posted', 'posted unique']","['section first present', 'first present gathered', 'present gathered analysis', 'gathered analysis researcher', 'analysis researcher interested', 'researcher interested code', 'interested code invited', 'code invited contact', 'invited contact author', 'contact author reddit', 'author reddit website', 'reddit website enables', 'website enables aggregate', 'enables aggregate rate', 'aggregate rate discus', 'rate discus news', 'discus news entertainment', 'news entertainment politics', 'entertainment politics many', 'politics many topic', 'many topic according', 'topic according alexa', 'according alexa th', 'alexa th popular', 'th popular website', 'popular website world', 'website world estimated', 'world estimated pew', 'estimated pew research', 'pew research center', 'research center online', 'center online adult', 'online adult use', 'adult use reddit', 'use reddit site', 'reddit site organized', 'site organized collection', 'organized collection subreddits', 'collection subreddits focused', 'subreddits focused particular', 'focused particular topic', 'particular topic administered', 'topic administered collection', 'administered collection moderator', 'collection moderator subreddit', 'moderator subreddit rsuicidewatch', 'subreddit rsuicidewatch forum', 'rsuicidewatch forum online', 'forum online encouraged', 'online encouraged thought', 'encouraged thought regarding', 'thought regarding suicide', 'regarding suicide time', 'suicide time collection', 'time collection subscriber', 'collection subscriber sometimes', 'subscriber sometimes express', 'sometimes express preoccupation', 'express preoccupation thought', 'preoccupation thought suicide', 'thought suicide time', 'suicide time discus', 'time discus immediate', 'discus immediate plan', 'immediate plan take', 'plan take life', 'take life often', 'life often contain', 'often contain description', 'contain description state', 'description state including', 'state including reaction', 'including reaction stress', 'reaction stress feeling', 'stress feeling alone', 'feeling alone low', 'alone low selfesteem', 'low selfesteem online', 'selfesteem online source', 'online source notoriously', 'source notoriously noisy', 'notoriously noisy particular', 'noisy particular subreddit', 'particular subreddit remarkably', 'subreddit remarkably clean', 'remarkably clean given', 'clean given serious', 'given serious nature', 'serious nature subreddit', 'nature subreddit individual', 'subreddit individual le', 'individual le likely', 'le likely harassing', 'likely harassing comment', 'harassing comment offtopic', 'comment offtopic remark', 'offtopic remark comment', 'remark comment moderator', 'comment moderator subreddit', 'moderator subreddit quickly', 'subreddit quickly remove', 'quickly remove collected', 'remove collected inception', 'collected inception often', 'inception often commented', 'often commented individual', 'commented individual work', 'individual work focused', 'work focused original', 'focused original often', 'original often represents', 'often represents suicidal', 'represents suicidal ideation', 'suicidal ideation comment', 'ideation comment often', 'comment often represent', 'often represent emotional', 'represent emotional support', 'emotional support cleaned', 'support cleaned first', 'cleaned first removed', 'first removed empty', 'removed empty content', 'empty content deleted', 'content deleted second', 'deleted second removed', 'second removed link', 'removed link replaced', 'link replaced link', 'replaced link third', 'link third concatenated', 'third concatenated text', 'concatenated text title', 'text title many', 'title many begin', 'many begin title', 'begin title continue', 'title continue body', 'continue body finally', 'body finally removed', 'finally removed punctuation', 'removed punctuation special', 'punctuation special character', 'special character cleaning', 'character cleaning unique', 'cleaning unique posted', 'unique posted unique']"
https://ieeexplore.ieee.org/abstract/document/8609647,0,Reddit is a multilingual Online Social Network founded in 2005 and organized in subcommunities by areas of interest called subreddits. We obtained data from the Reddit's data repository4 focusing on four subreddits where people discuss issues related to mental heath disorders: Depression (/r/depression) Suicide Watch (/r/Suicide Watch) Anxiety (/r/anxiety) and Bipolar (/r/bipolar). Our dataset is comprised of user activities (posts and comments) that took place between 2011 and 201 7. Here we focus on data from January 2017 to December 2017. In total we obtained 261511 posts and 1256669 comments from 184708 unique users. Table I shows the total number of users posts and comments per subreddit. The total number of comments in each community is at least 4.2 times larger than the number of posts which suggests a supportive behavior among users.,reddit is a multilingual online social network founded in and organized in subcommunities by area of interest called subreddits we obtained data from the reddits data repository focusing on four subreddits where people discus issue related to mental heath disorder depression rdepression suicide watch rsuicide watch anxiety ranxiety and bipolar rbipolar our dataset is comprised of user activity post and comment that took place between and here we focus on data from january to december in total we obtained post and comment from unique user table i show the total number of user post and comment per subreddit the total number of comment in each community is at least time larger than the number of post which suggests a supportive behavior among user,"['reddit', 'multilingual', 'online', 'social', 'network', 'founded', 'organized', 'subcommunities', 'area', 'interest', 'called', 'subreddits', 'obtained', 'reddits', 'repository', 'focusing', 'four', 'subreddits', 'people', 'discus', 'issue', 'related', 'heath', 'disorder', 'rdepression', 'suicide', 'watch', 'rsuicide', 'watch', 'anxiety', 'ranxiety', 'bipolar', 'rbipolar', 'dataset', 'comprised', 'activity', 'comment', 'took', 'place', 'focus', 'january', 'december', 'total', 'obtained', 'comment', 'unique', 'table', 'show', 'total', 'number', 'comment', 'per', 'subreddit', 'total', 'number', 'comment', 'community', 'least', 'time', 'larger', 'number', 'suggests', 'supportive', 'behavior', 'among']","['reddit multilingual', 'multilingual online', 'online social', 'social network', 'network founded', 'founded organized', 'organized subcommunities', 'subcommunities area', 'area interest', 'interest called', 'called subreddits', 'subreddits obtained', 'obtained reddits', 'reddits repository', 'repository focusing', 'focusing four', 'four subreddits', 'subreddits people', 'people discus', 'discus issue', 'issue related', 'related heath', 'heath disorder', 'disorder rdepression', 'rdepression suicide', 'suicide watch', 'watch rsuicide', 'rsuicide watch', 'watch anxiety', 'anxiety ranxiety', 'ranxiety bipolar', 'bipolar rbipolar', 'rbipolar dataset', 'dataset comprised', 'comprised activity', 'activity comment', 'comment took', 'took place', 'place focus', 'focus january', 'january december', 'december total', 'total obtained', 'obtained comment', 'comment unique', 'unique table', 'table show', 'show total', 'total number', 'number comment', 'comment per', 'per subreddit', 'subreddit total', 'total number', 'number comment', 'comment community', 'community least', 'least time', 'time larger', 'larger number', 'number suggests', 'suggests supportive', 'supportive behavior', 'behavior among']","['reddit multilingual online', 'multilingual online social', 'online social network', 'social network founded', 'network founded organized', 'founded organized subcommunities', 'organized subcommunities area', 'subcommunities area interest', 'area interest called', 'interest called subreddits', 'called subreddits obtained', 'subreddits obtained reddits', 'obtained reddits repository', 'reddits repository focusing', 'repository focusing four', 'focusing four subreddits', 'four subreddits people', 'subreddits people discus', 'people discus issue', 'discus issue related', 'issue related heath', 'related heath disorder', 'heath disorder rdepression', 'disorder rdepression suicide', 'rdepression suicide watch', 'suicide watch rsuicide', 'watch rsuicide watch', 'rsuicide watch anxiety', 'watch anxiety ranxiety', 'anxiety ranxiety bipolar', 'ranxiety bipolar rbipolar', 'bipolar rbipolar dataset', 'rbipolar dataset comprised', 'dataset comprised activity', 'comprised activity comment', 'activity comment took', 'comment took place', 'took place focus', 'place focus january', 'focus january december', 'january december total', 'december total obtained', 'total obtained comment', 'obtained comment unique', 'comment unique table', 'unique table show', 'table show total', 'show total number', 'total number comment', 'number comment per', 'comment per subreddit', 'per subreddit total', 'subreddit total number', 'total number comment', 'number comment community', 'comment community least', 'community least time', 'least time larger', 'time larger number', 'larger number suggests', 'number suggests supportive', 'suggests supportive behavior', 'supportive behavior among']"
