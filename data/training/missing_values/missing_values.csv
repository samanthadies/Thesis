Link to paper,Title,Section,Score,Text,Text 2,Text 3
https://link.springer.com/content/pdf/10.1140/epjds/s13688-017-0110-z.pdf,Instagram photos reveal predictive markers of depression,Improving Data Quality,0,We employed several quality assurance measures in our data collection process to reduce noisy and unreliable data. Our surveys were only visible to MTurk crowdworkers who had completed at least  previous tasks with a minimum % approval rating; MTurk workers with this level of experience and approval rating have been found to provide reliable valid survey responses []. We also restricted access to only American IP addresses as MTurk data collected from outside the United States are generally of poorer quality []. All participants were only permitted to take the survey once. We excluded participants who had successfully completed our survey but who had a lifetime total of fewer than five Instagram posts. We also excluded participants with CESD scores of  or higher. Studies have indicated that a CES-D score of  represents an optimal cutoff for identifying clinically relevant depression across a range of age groups and circumstances [ ].,,
https://dl.acm.org/doi/abs/10.1145/2858036.2858207 ,Discovering Shifts to Suicidal Ideation from Mental Health Content in Social Media,Constructing User Classes Data Collection MHs and SW Content Verification,0,We split our data into two sequential time periods (t1 from Feb 11 2014 to Aug 11 2014 and t2 from Aug 12 2014 to November 11 2014). Using these two time periods we created two sets of users. Note that since Reddit does not enforce the real name rule of having exactly one account per person our reference to “users” in this paper is equivalent to “user accounts”. First we identified those users that posted on MHs during t1 but did not post on SW during t1 or t2 (i.e. users that discuss mental health topics but not on SW; hereafter “MH”). The second class included those who posted on MHs during t1 and posted in SW during t2 (i.e. users that discuss mental health topics originally not related to suicide but eventually transition to talk about suicide; hereafter “MH → SW”). Figure 1 shows a schematic description of our user class construction. Note that by focusing on users that initiate at least one post on SW or the MHs as opposed to only commenting we can focus on those frequenting the communities for support disregarding those primarily providing help through commentary. This split yielded 440 MH → SW users; which is 1.52% of the total number of 28831 accounts who posted in MHs but never on SW during either of the periods. To construct a MH cohort of equal size who did not post on SW in either period we randomly sampled a set of 440 users from the 28831 users. Note although MH users did not post on SW during our timeframe of analysis they may have done so outside the bounds of our analysis. To support our goal of characterizing differences between the MH → SW and MH users we obtained via Reddit’s API the timeline of posts and comments authored by the 880 users (the API only provides the last 1000 public posts and comments for a user). For each post we obtained their associated metadata (e.g. vote difference or score) and comments. Our final dataset contained 4731 posts and 46949 comments from the 440 MH → SW users and 8318 posts and 54086 comments from the 440 MH users. We note an important concern: individuals may post suicidal thoughts on MHs never engaging on SW and thus “corrupting” the MHs data with discussions of suicidal ideation. We argue against this possibility. (1) SW is a prominent suicide support forum and the role of this community in suicide prevention and in acting as an inoculator of vulnerable thoughts is well-recognized [31]. (2) Most MHs (e.g. r/depression) clearly specify in their guidelines that suicidal thoughts should go to SW: “It’s usually better to post anything that specifically involves suicidal thoughts or intent in /r/SuicideWatch rather than here. If you’re concerned about someone else who may be at risk for suicide please check out their talking tips and risk assessment guide.” (3) Finally discussions with the moderators of SW confirmed that active steps are taken to move all suicidal ideation related content to SW. Given these considerations we expect that few suicidal ideation posts appear on subreddits outside of SW.,We obtained post and comment data from a number of mental health subreddits (henceforth MHs) and a suicide support subreddit “r/SuicideWatch” (henceforth SW). We focused on a set of 14 MHs that have been examined in prior work on mental health discourse [51 39]. These subreddits included r/depression r/mentalhealth r/traumatoolbox r/bipolarreddit r/BPD r/ptsd r/psychoticreddit r/EatingDisorders r/StopSelfHarm r/survivorsofabuse r/rapecounseling r/hardshipmates r/panicparty r/socialanxiety. While SW solely focuses on helping those contemplating suicide the other MHs cover a variety of mental health concerns but not specifically suicidal ideation [31]. All of these subreddits host public content. We used Reddit’s official API to collect posts comments and associated metadata from the SW and MHs subreddits (http://www.reddit.com/dev/api). Our analysis in this paper is based on all content shared on MHs between February 11 and November 11 2014 (63485 posts 209766 comments and 35038 users). We refer to the data obtained from SW during the same time period (16348 posts 9224 users) to identify those individuals in MHs who go on to post on SW over time.,Following our data collection we focused on verifying whether MHs and SW subreddit content actually relate to discussion of mental health concerns and suicidal ideation. The MHs have been previously examined for understanding mental health discourse on Reddit [51 39]. For SW we consulted (1) a licensed clinical psychologist/suicide prevention expert and (2) two active moderators of SW to obtain qualitative grounding that the content in SW indeed related to expressions of suicidal ideation. Example (paraphrased) titles of posts from one of the MHs and SW are given in Table 1.
https://dl.acm.org/doi/abs/10.1145/2702613.2732733,Detecting and Characterizing Mental Health Related Self-Disclosure in Social Media,Self Disclosure Inference,0,Based on the training data thus created we pursued the use of supervised learning to develop a classifier which would indicate whether a post is of high low or no self-disclosure. We tested a variety of different classification techniques (decision trees k Nearest Neighbor naive Bayes). The best performing classifier was found to be a perceptron classifier with adaptive boosting used to amplify performance [17] whose results will be used in the remainder of this paper. We used the following feature generation rules: First we eliminated stopwords from each post based on standard list provided by Python’s NLTK library. Next we performed stemming using Porter Stemmer. We extracted uni- bi- and tri-grams from each post and considered those with five or more occurrences. We also computed two additional features – length of each post and whether the author of the post is an exclusive poster on mental health forums or is observed in our dataset to post on other forums as well. Thus each post was characterized by 1070 features. We used standard 10-fold cross validation (CV) to evaluate the classifier and ran our model over 100 random 10-fold CV assignments for generalizability of the results. We report the average accuracy precision recall F1 specificity as metrics of performance. We find that our classifier based on the perception model yields an average accuracy of 78.4% in detecting high or low self-disclosure with .74 precision and .86 recall (see Table 4 for details). Other methods like k-NN (k=5) give higher precision but at the expense of very low recall. Figure 1 gives the ROC (receiver operating characteristic) curves for all the models. Per the ROC curve corresponding to the perceptron model we find it to yield the maximum area under curve (.81) hence best performance. We further identify in Table 5 the n-grams (or features) with the highest weights given by the perceptron – it implies these features were the most significant in the classification task. We provide some brief qualitative examinations of these n-grams in the light of prior psychology literature on selfdisclosure and mental health [10 11]. We find that the ngrams primarily are associated with vulnerable and selfloathing thoughts (e.g. thoughts of suicide) bear a negative tone or depict confessional experiences. Based on prior research [10 11 12] and our own work on mental health discourse on Reddit [4] we find that these are the topical dimensions along which high self-disclosure and low/no selfdisclosure posts vary. In essence high self-disclosure posts share extensively their personal beliefs and fear for instance their vital constructs and private sensitive informational attributes. The post excerpts below have been classified to be of high self-disclosure and through them we demonstrate the use of some of the n-grams in Table 7:,,
https://dl.acm.org/doi/abs/10.1145/2702123.2702280,Recognizing Depression from Twitter Activity,Data Gathering,0,In this study we gathered information on depression levels of Twitter users and their activity histories. To do this we published a website to administer a questionnaire and disseminated information about the website over Twitter1. In contrast to De Choudhury et al. [14] who collected data from Englishspeaking users through crowdsourcing this study collected data from Japanese-speaking volunteers. This approach was used to investigate the extent to which depression risk can be estimated for a population different from the population considered by the prior research [14]. Figure 1 shows a screenshot of our website. The website collected the responses to a questionnaire to evaluate the degree of depression of the Twitter users who participated (hereinafter the participants) and to collect the histories of participants activities on Twitter. The activity histories of participants were collected through the Twitter application programming interface (API)2 and the questionnaires to determine degree of depression were completed by participants through their web browsers. Before data collection visitors to the website were presented with a written explanation of the aims of the experiment the information that would be collected and how that information would be handled. Those who consented to become participants after receiving the explanation logged into their individual Twitter accounts through the OAuth authorization process. Next participants were surveyed on gender age occupation and history of depression following which they answered a questionnaire designed to evaluate degree of depression. A message called the “kokoro score” (“kokoro” is a Japanese word meaning “heart”) determined on the basis of answers to the questionnaire and information in the collected tweets was displayed to participants after completion of the questionnaire (Fig. 2). Experiment participants were able to tweet the message displayed which made it possible to promote the website over Twitter by word-of-mouth in a type of snowball sampling. The CES-D questionnaire was used to evaluate the degree of depression [30]. In the CES-D test participants answered 20 questions on a Likert-type 4-point scale. Each answer was assigned a score of 0-3 points with the sum of the points from all answers used as the score to estimate likelihood of depression. Several standards exist by which to determine the appropriate cutoff score for identifying depression. In this research we regarded a score of 22 points or higher as indicating active depression and a score of 21 points or lower as indicating no active depression; these are the same values as used in [14] and give a cutoff score of 22. In addition answers to BDI [2] a depression scale used with characteristics similar to CESD were collected to ensure the reliability of data. For each participant scores were calculated on both scales with poor correlation regarded as indicating unreliable answers. The time taken to answer the questionnaires was also recorded and those completed in too brief a time were excluded. After each participant answered the questionnaire the activity history of that participant on Twitter was collected from Twitter by using the API. At most 3200 tweets were collected for each participant and the number of users following the participant and being followed by the participant were recorded. Tweets published after the questionnaire was taken were discarded. The website was opened to the public on 4 December 2013 at which time the authors publicized it on their Twitter accounts. Between 4 December 2013 and 8 February 2014 219 people participated in the experiment. After eliminating participants who did not tweet and participants who answered the questionnaire in fewer than 30 seconds (as previously mentioned to ensure the reliability of the questionnaire answers) 214 sets of answers remained. Only the first set of answers was used for participants who completed the questionnaire more than once. As a result data about 209 experiment participants (male: 121; female: 88) aged 16 to 55 (mean: 28.8 years; standard deviation: 8.2 years) were analyzed. The correlations between CES-D score and BDI score for these participants were high 0.87 and there were no participants with uncorrelated scores so the data for all 209 participants were used; excluded datasets are not discussed any further. Figure 3 shows the histogram of CES-D scores of 209 participants. Among the participants 81 (resp. 128) were estimated to have (resp. not have) active depression for an incidence of approximately 39%. This incidence is similar to that found by De Choudhury et al. [14] who identified depression in approximately 36% of participants. Table 1 gives statistics on the activity histories of participants.,,
https://www.nature.com/articles/s41598-017-12961-9,Forecasting the onset and course of mental illness with Twitter data,Improving Data Quality,0,In an effort to minimize noisy and unreliable data we applied several quality assurance measures in our data collection process. MTurk workers who have completed at least 100 tasks with a minimum 95% approval rating have been found to provide reliable valid survey responses33. We restricted survey visibility only to workers with these qualifications. Survey access was also restricted to U.S. IP addresses as MTurk data collected from outside the United States are generally of poorer quality34. All participants were only permitted to take the survey once. We excluded participants with a total of fewer than five Twitter posts. We also excluded participants with CES-D scores of 21 or lower (depression) or TSQ scores of 5 or lower (PTSD). Studies have indicated that a CES-D score of 22 represents an optimal cutoff for identifying clinically relevant depression3536; an equivalent TSQ cutoff of 6 has been found to be optimal in the case of PTSD32. We note here that in the study that inspired the present work De Choudhury et al.8 used two depression scales (CES-D and BDI) and filtered individuals whose depression score did not correlate across the both scales. This additional criteria is a methodological strength of De Choudhury et al.8 with respect to the present work.,,
https://aclanthology.org/W14-3214.pdf,Towards Assessing Changes in Degree of Depression through Facebook,Dataset,0,We used a dataset of 28749 nonclinical users who opted into a Facebook application (“MyPersonality”; Kosinski and Stillwell 2012) between June 2009 and March 2011 completed a 100-item personality questionnaire (an International Personality Item Pool (IPIP) proxy to the NEO-PI-R (Goldberg 1999) and shared access to their status updates containing at least 500 words. Users wrote on average of 4236 words (69917624 total word instances) and a subset of 16507 users provided gender and age in which 57.0% were female and the mean age was 24.8. The dataset was divided into training and testing samples. In particular the testing sample consisted of a random set of 1000 users who wrote at least 1000 words and completed the personality measure while the training set contained the 27749 remaining users.,,
https://ieeexplore.ieee.org/abstract/document/6784326,Affective and Content Analysis of Online Depression Communities,Feature Extraction,0,To characterize the difference between CLINICAL and CONTROL communities a variety of features are extracted: Affective features: We use the lexicon—Affective Norms for English Words (ANEW) [5]—to extract the sentiment conveyed in the content. This lexicon consists of 1034 words rated in terms of valence and arousal and is thus suitable for a quantitative estimation. The valence of ANEW words is on a scale of 1 (very unpleasant) to 9 (very pleasant). The arousal is measured on the same scale—1 (least active) to 9 (most active). A cloud visualization of ANEW words used in the blog posts made by CLINICAL and CONTROL groups is illustrated in Fig. 1. Mood tags: LiveJournal provides a mechanism for users to tag their posts from a list of 132 pre-defined mood labels.4 Thus in addition to the emotion expressed in the text of posts the mood tag produced allows us direct access to the user sentiment. A cloud visualization of moods tagged on blog posts made by CLINICAL and CONTROL communities is llustrated in Fig. 2. LIWC features: We examine the proportions of words in psycholinguistic categories as defined in the LIWC package [27]: linguistic social affective cognitive perceptual biological relativity personal concerns and spoken.5 Table 3 presents the mean of these LIWC psycholinguistic processes for the CLINICAL and CONTROL communities. Whilst similar in the use words with positive emotion people in the CLINICAL communities tend to use words with more negative emotion—as examples anxiety anger and sadness. Further they discuss more issues about health and death in comparison with the CONTROL group. On the other hand the users in the CONTROL group discuss more neutral life related topics—ingestion home and leisure words. Topics: For extracting topics latent Dirichlet allocation (LDA) [4] is used as a Bayesian probabilistic modelling framework. LDA extracts the probabilities —that is words in a topic and then assigns a topic to each word in a document. For the inference part we implemented Gibbs inference detailed in [10]. We set the number of topics to 50 run the Gibbs for 5000 samples and use the last Gibbs sample to interpret the results.,,
https://www.jmir.org/2017/7/e243/,Assessing Suicide Risk and Emotional Distress in Chinese Social Media: A Text Mining and Machine Learning Study,Dta Collections,0,A Web-based survey of Weibo users was conducted to assess the respondents’ suicide risk and emotional distress (ie depression anxiety and stress). The invitation letter to participate in this survey was widely sent out to general Weibo users by various promotion activities. For a Weibo user to be eligible for the study she or he had to be 18 years or older (by self-report). A 30 Renminbi incentive for each complete survey was provided to boost the respond rate. With the respondents’ consent their Weibo posts that were posted in the public domain during the 12 months before the survey were downloaded by calling Weibo API. The survey fulfilled the Checklist for Reporting Results of Internet E-Surveys (CHERRIES) checklist and details of the procedure have been reported in previous publications [2232]. In addition when multiple survey feedback were submitted from the same Internet protocol addresses only the first submission was used to avoid duplicate participation. In contrast to a previous study [32] this study excluded those who posted nothing throughout the 12 months but not those who posted fewer than 100 posts. Eventually data provided by 974 respondents remained for further analyses. The study has obtained ethical approvals from the Human Research Ethical Review Committee at the University of Hong Kong and the Institute Review Board of the Institute of Psychology at the Chinese Academy of Sciences. The survey measured respondents’ suicide probability score depression anxiety stress and Weibo suicide communication (WSC) as the outcome variables. In addition the respondents’ Weibo posts language features were extracted as independent variables or features for machine learning. The details of how those data were obtained are elaborated in the following subsections.,,
https://dl.acm.org/doi/abs/10.1145/3025453.3025909,A Social Media Based Index of Mental Well-Being in College Campuses,Data,2,We first obtained a list of 150 ranked major universities in the United States by crawling the US News and World Report website [48]. This list is constructed based on the Carnegie classification employed extensively by higher education researchers and using a set of 16 indicators of academic excellence defined by US News. The list includes a variety of universities spread across the US in different settings (e.g. urban rural) and with a wide range of student enrollment sizes. Figure 1(a) shows their geographic distribution. As a part of this crawl we also obtained university metadata: gender distribution of students average tuition and fees and academic calendar (semester/quarter). To obtain further information about the nature of the student body we crawled the Wikipedia pages of all of the 150 universities. From these pages we extracted the size of student enrollment type (public/private) and setting (rural/suburban/urban/city) at every institution. These definitions come from a formal categorization scheme used by the US Department of Education. The student body enrollment sizes ranged from 2255 to 97494 with 98 public and 52 private universities. 50 universities were reported to be urban 47 city 39 suburban and 13 rural. Finally we obtained information on racial diversity of the universities from a website known as Priceonomics [58]. The website calculates the Herfindahl-Hirschman Index (HHI) by combining the race/ethnicity distribution of student bodies at different universities with data given from the Department of Education. HHI ranges from 1 (the least diverse: a population of all one type) to 1/N (the most diverse) where N is the number of different racial categories being analyzed. Social Media Data of Universities Next we obtained social media data of the above universities. Specifically we focused on the social media Reddit. Why Reddit? Reddit is known to be a widely used online forum and social media site among the college student demographic [23]. Due to its forum structure it is extensively used for both content sharing as well as for obtaining feedback and information from communities of interest. Reddit harbors a variety of communities known as “subreddits” including many dedicated to specific university campuses. This allows a large sample of posts shared by students of a university to be collected in one place. Our preliminary manual inspection of university subreddits (e.g. r/gatech or r/KState) revealed that these subreddits are appropriated by students to discuss college topics (Table 1). Focusing on these public Reddit communities also does not require explicit data collection efforts to be coordinated at each of the 150 university sites. Although more students are likely to use Facebook due to its largely privately shared content it is challenging to obtain access to a large dataset of a university’s students. Next while Twitter is also widely adopted without explicit self-reported information it is challenging to identify college student accounts. Finally prior work [2 18] notes that semianonymity of Reddit enables candid self disclosure around stigmatized topics like mental health. Initial Data Acquisition. We leveraged the archive of all of Reddit data made available on Google’s BigQuery [11]. BigQuery is a cloud based managed data warehouse that allows third parties to access large publicly available dataset through simple SQL-type queries. Our queries grabbed all posts ranging between June 2011 and February 2016 available in the Reddit data archive. This included 424984 posts from 153378 unique users across all of the 146 universities with a mean of 2910.8 posts ( = 4329.6) and 1050 unique users ( = 1407) per subreddit. Filling the Gaps in Subreddit Data. The second step of our data collection process focused on identifying subreddits with insufficient data and supplementing them through additional alternative data collection. Through Reddit’s official API (https://www.reddit.com/dev/api/) we obtained the most recent number of subscribers in the 146 university subreddits (as of July 2016). Then to investigate if and to what extent some subreddits may have had unusually low data as given in step 1 we determined the median unique user to subscriber ratio in each subreddit. This allows us to capture the subreddits where the subscriber count is high however the data obtained is not sufficiently representative. For subreddits with unique user to subscriber ratio under median (.42) (73 in all) we performed a one-time data collection using the Reddit API. This gave us a set of (at most) 1000 most recent posts for each subreddit with a total of 39824 posts added to the data obtained in step 1 following de-duplication. We note that this procedure did not skew the yearly distributions of data across the subreddits: The skew (yearly rate of change) before and after data filling were 4.86 and 5.05 respectively which were found to be statistically equivalent based on a two-sample equivalence test (p = .013 p = .025) a test that uses two one-sided t-tests on the before-after yearly rates of change from both sides of a chosen difference interval [1 1].,,
https://ieeexplore.ieee.org/abstract/document/7752434,MIDAS: Mental illness detection and analysis via social media,Feature Extraction,2,In this work we are focused on two main type of features (linguistic and behavioral). TF-IDF is adopted to model the linguist features of patients and Pattern of Life Features (PLF) adopted from the work of Coppersmith et al. [1] is used to model the behavioral style of patients. TF-IDF Features To capture the frequent and representative words used by the patients TF-IDF is applied on the unigram and bigrams collected from all the patients' tweets. Pattern of Life Features (PLF) These features reveal the emotional patterns and behavioral tendency of users by measuring polarity emotion and social interactions. In order to fully compose the PLF we combined the following list of features: Age and Gender: Twitter does not publicly provide information about the age and gender of its users mainly due to privacy concerns so we adopted the work of Sap et al. [5] to fill in this information. Polarity Features: The Sentiment140 API 3 was used to label each tweet as either positive negative or neutral. The polarity is furthermore transformed into five different values to capture the affective traits of each user: 1) Positive Ratio: the percentage of positive tweets 2) Negative Ratio: the percentage of negative tweets 3) Positive Combo: captures the mania and hypomania traits of patients which is determined by the number of continuous positive posts appearing more than x amount of times within a period of time in minutes T. 4) Negative Combos: captures the depression traits of patients and is determined by the number of continuous negative posts appearing more than x amount of times within a period of time in minutes T. 5) Flips Ratio: quantifies the emotional unstableness and is determined by counting how frequently two continuous tweets with different polarity (either positive to negative or negative to positive) appear together within a period of time in minutes T. In our work x is set to 2 and T is set to 30 minutes. Social Features: These features can demonstrate how users are behaving with respect to their environment. The following are the social features designed for each user: 1) Tweeting Frequency; the frequency of daily posts; 2) Mention Ratio: the percentage of posts which contain at least one mention of another user; 3) Frequent Mentions: the number of Twitter users mentioned more than three times which is a measurement of how many close friends a particular user may have; 4) Unique Mentions: the number of unique users mentioned which is a measure of the width of a user's social network.,,
https://www.nature.com/articles/s41598-020-68764-y,A deep learning model for detecting mental illness from user content on social media,The Data Preprocessing Procedure,0,The data pre-processing procedure for the collected post data is presented in Fig. 1. After collecting the data each title was combined with its corresponding post. We removed unnecessary punctuation marks and white spaces for each post. Then we used the natural language toolkit (NLTK) implemented in Python to tokenize users’ posts and filter frequently employed words (stop words). Porter Stemmer a tool used to define a series of guidelines for exploring word meaning and source was employed on the tokenized words to convert a word to its root meaning and to decrease the number of word corpus. After this procedure data from 228060 users with 488472 posts in total were employed for the analysis.,,
https://aclanthology.org/W19-3013.pdf,Mental Health Surveillance over Social Media with Digital Cohorts,Building Digital Cohorts,2,Our cohort construction process entails two key steps: first randomly selecting a large sample of Twitter users; and second annotating those users with key demographic attributes. While such attributes are not provided by the API automated methods can be used to infer such traits from data (Cesare et al. 2017). Following this approach we develop a demographic inference pipeline to automatically infer age gender race/ethnicity and location for each cohort candidate. Age Identifying age based on the content of a user can be challenging and exact age often cannot be determined based on language use alone. Therefore we use discrete categories that provide a more accurate estimate of age: Teenager (below 19) 20s 30s 40s 50s (50 years or older). Gender The gender was inferred using Demographer a supervised model that predicts the (binary) gender of Twitter users with features based on the name field on the user profile (Knowles et al. 2016). Race/Ethnicity The standard formulation of race and ethnicity is not well understood by the general public so categorizing social media users along these two axes may not be reasonable. Therefore we use a single measure of multicultural expression that includes five categories: White (W) Asian (A) Black (B) Hispanic (H) and Other. Location The location was inferred using Carmen an open-source library for geolocating tweets that uses a series of rules to lookup location strings in a location knowledge-base (Dredze et al. 2013). We use the inferred location to select users that live in the United States. The age and race/ethnicity attributes were inferred with custom supervised classifiers based on Amir et al. (2017)’s user-level model. The classifiers were trained and evaluated on a dataset of 5K annotated users attaining performances of 0.28 and 0.41 Average F1 respectively. See the supplemental notes for additional details on these experiments1 .,,
https://link.springer.com/chapter/10.1007/978-3-319-67186-4_6,Social Media Mining to Understand Public Mental Health,Topic Modeling,1,First we removed journals with no text and those with fewer than 20 characters1 leaving 1.1 million journals for topic modelling. Next we pre-processed the text using the Stanford Tweet Tokenizer which is a “Twitter-aware” tokenizer designed to handle short informal text [1]. We used the option that truncates characters repeating 3 or more times converting phrases such as “I’m sooooo happyy” to “I’m soo happyy”. On average the number of tokens per journal was 27.7. Since we are interested in topics we removed stopwords and tokens with fewer than two letters and we only retained nouns which appear in the WordNet corpus [10]. After this filtering the average number of nouns per journal was 7. Examples of frequently appearing nouns in alphabetical order include “anxiety” “class” “dinner” “family” “god” “job” “lunch” “miss” “school” “sick” “sleep” and “work”. We then iteratively clustered the journals into topics (details below) and removed nouns that do not refer to topics such as numbers timings (e.g. “today” “yesterday”) general feelings (e.g. “feel” “like”) proper nouns and nouns that have ambiguous meanings (e.g. “overall” “true”). Lastly we only retained nouns that appeared more than ten times in the dataset. This process resulted in a vocabulary of 8386 words for topic modelling. Each journal is represented as a 8386-dimensional term frequency vector with each component denoting the term-frequency/ inverse-document-frequency (TF-IDF) of the corresponding term. Algorithm 1 summarizes our topic modelling methodology. Given a TF-IDF term frequency vector for each journal we run non-negative matrix factorization (NMF) [8] implemented in Python’s scikit-learn package [12]. The objective of NMF is to find two matrices whose product approximates the original matrix. In our case one matrix is the weighted set of topics in each journal and the other is the weighted set of words that belong to each topic. Hence each journal is represented as a combination of topics which are themselves composed of a weighted combination of words. We chose NMF because its non-negativity constraint aids with interpretability. In the context of analyzing word frequencies negative presence of a word would not be interpretable. This is because we only track word occurrences and not semantics or syntax. Unlike other matrix factorization methods NMF reconstructs each document from a sum of positive parts which enables us to easily manually label the discovered topics. Iterating from 4 to 40 topics we derived 37 different topic matrices (steps 1 and 2 of Algorithm 1). Each matrix consists of one topic per row. Each topic has a positive weight for each word in the vocabulary. Stronger weights indicate higher relevance to the topic. The final topic matrix we used has 14 topics and is shown in Table 1. We show the first six words in this table for simplicity where we sorted the words associated with each topic from highest relevance to lowest. When judging the topic matrices we considered the top twenty most important words per topic. Using this information we manually labeled each row in the matrix with a corresponding topic. Furthermore we manually evaluated each matrix based on the distinctness between topics consistency within topics and interpretability. During this process we compiled a custom list of removed words that we mentioned earlier in this section. The groups of words we removed appeared as stand-alone topics that did not offer information about what the journal was about. For example proper nouns appeared as a stand-alone topic. Other words which we deemed too general or ambiguous appeared across several topics and hence did not provide discriminative information. We tested different levels of regularization to enforce sparseness in our models (see [8] for a discussion) but did not find significant differences. However one important modification we made to regularize each topic was to make their first words only as strong as their second ones (by default first words are stronger than second words which are stronger than third words and so on). This is since the most relevant word for each topic tended to be too strong of a signal regardless of how we changed the number of topics pre-processing procedure or regularization in the objective function. For example the word “love” in a journal about sports would be so strong that the journal would be labeled as relating to romantic love. Lowering the importance of first words was sufficient to eliminate the false positives we identified. Given the final topic matrix (summarized in Table 1) the next step is to use it to assign labels to journals (steps 3 and 4 of Algorithm 1). We plotted the distribution of how important each topic was to all journals in the dataset with importance ranging from zero to one. Each distribution had a similar shape with a clear inflection point between 0.05 to 0.15 importance. Figure 4 shows an example importance distribution for the topic “Work” where the inflection point occurs at 0.1 importance.,,
https://dl.acm.org/doi/abs/10.1145/2556288.2557214,Seeking and sharing health information online: comparing search engines and social media,Data on Health Conditions,0,A key challenge of this research was to identify the different health conditions on which people seek and share information via search engines and Twitter respectively. We identified four broad categories of conditions based on their severity and types: (1) symptoms of major diseases (2) benign explanations (non-life-threatening illnesses) (3) serious illnesses and (4) disabilities. We also characterized each condition by the degree of perceived social stigma provided by third-party judges. Our final list contained 165 conditions.,,
https://www.aaai.org/ocs/index.php/ICWSM/ICWSM14/paper/viewPaper/8075,Mental Health Discourse on reddit: Self-Disclosure Social Support and Anonymity,Data and Methods,0,reddit is a social news website where registered users submit content in the form of links or text posts. Users also known as “redditors” can then vote each submission “up” or “down” to rank the post and determine its position or prominence on the site’s pages. These two attributes associated with a post are referred to as “upvotes” and “downvotes”. Redditors can also comment on posts and respond back in a conversation tree of comments. Content entries that is the posts are organized by areas of interest or sub-communities called “subreddits” such as politics programming or science. As of 2013 reddit’s official statistics included 56 billion page views 731 million unique visitors 40855032 posts and 404603286 comments (http://blog.reddit.com/2013/ 12/top-posts-of-2013-stats-and-snoo-years.html). We used of reddit’s official API (http://www.reddit.com/ dev /api) to collect posts comments and associated metadata from several mental health subreddits: specifically using a Python wrapper PRAW (https://praw.readthedocs.org/en/ latest/index.html). The subreddits we crawled were: alcoholism anxiety bipolarreddit depression mentalhealth MMFB (Make Me Feel Better) socialanxiety SuicideWatch. All of these subreddits host public content. In order to arrive at a comprehensive list of subreddits to focus on we utilized reddit’s native subreddit search feature (http://www.reddit.com/reddits) and searched for subreddits on “mental health”. Two researchers familiar with reddit employed an initial filtering step on the search results returned so that we focus on high precision subreddits discussing mental health concerns and issues. Thereafter we focused on a snowball approach in which starting with a few seed subreddits (mentalhealth depression) we compiled a second list of “related” or “similar” subreddits that are listed in the profile pages of the seed subreddits. Following a second filtering step we arrived at the list of subreddits listed above. For each of these subreddits we obtained daily crawls of their posts in the New category. Corresponding to each post we collected information on the title of the post the body or textual content id timestamp when the post was made author id and the number of upvotes and downvotes it obtained. Since posts gather comments over a period of time following the time of sharing we crawled all of the comments per post that were shared over a three day period after the post was made. Qualitative examinations of the subreddits of interest revealed that 90% or more of the comments to any post were typically made in a three day window following the time the post is made—hence the choice. The crawl of the subreddits used in this paper We present some descriptive statistics of our crawled data. Our dataset contained 20411 posts with at least one comment and 97661 comments in all with 27102 unique users who made posts comments or both. A set of 7823 users (28.79%) were found to write both at least one post and comment. CDF of the user distribution over posts and comments is given in Figure 1. The figure shows the expected heavy tail trend observed in several social phenomena. Also see Figure 2 for the distribution of comments over time following post share. It illustrates the quick responsivity culture in the communities we study (peak at 3 hours). Some of the additional statistics of our dataset are given in Table 1. Further example titles of a few,,
https://dl.acm.org/doi/abs/10.1145/3025453.3025932,Modeling and Understanding Visual Attributes of Mental Health Disclosures in Social Media,Data Reliability,0,Next we assessed the suitability and reliability of our collected corpus of Instagram posts and users for our later analyses. For the purpose we extracted n-grams (n=3) from the profile biographies of users. The top 10 uni- bi- and trigrams are shown in Table 3. They show that users are appropriating Instagram to seek and provide social and emotional support around different mental health concerns (“need someone talk” “feel free dm”). There are also explicit mentions of specific psychological challenges around mental health (“depression anxiety” “telling suicidal kids”) including warnings for profile visitors (“trigger warning”) and personal experiences of the condition (“alone alone alone”). We corroborated these observations with a licensed psychiatrist and concluded that the users in our dataset are engaging in genuine mental health disclosures tend to demonstrate disinhibition towards sharing their mental health experiences and are appropriating the platform specifically for this purpose via the chosen account.,,
https://www.sciencedirect.com/science/article/pii/S0747563215300996,A content analysis of depression-related tweets,Tweets Related to Depression,0,Tweets about depression were collected by Simply Measured a company that specializes in social media measurement and analytics (Simply Measured 2014). Simply Measured has access to the Twitter “firehose” (or full volume of tweets) via Gnip a licensed company that can retrieve the full Twitter data stream. All tweets in the English language that contained at least either “depressed” “#depressed” “depression” or “#depression” were collected between April 11 and May 4 2014. We scanned a random sample of the tweets to identify common phrases that included our keywords of interest but were not about mental health. In SAS version 9.3 (SAS Institute Inc. Cary NC) we used the index function which searches a character expression (in this case the text of the tweet) for a specific string of characters to locate and remove such tweets from our sample. We removed tweets that included the following terms regardless of capitalization: “Great Depression” “economic depression” “during the depression” “depression era” “tropical depression” and “depressed real estate”. The popularity and influence of the Tweeters was described using the distribution of followers and Klout Scores. While number of followers is a measure of popularity Klout Score is a measure of influence. Klout Scores range from 0 to 100 with a higher score indicating higher influence. Klout Score is calculated based on an algorithm that considers over 400 signals from eight different online networks. Examples of signals include the amount of retweets a person generates in relation to the amount of tweets shared and the amount of engagement a user drives from unique individuals (e.g. lots of retweets from different individuals as opposed to lots of retweets from one person) (Klout Inc. 2014).,,
https://aclanthology.org/W15-1202.pdf,Quantifying the Language of Schizophrenia in Social Media,Age- and Gender- Matched Controls,1,Since mental health conditions including schizophrenia have different prevalence rates depending on age and gender (among other demographic variables) controlling for these will be important when examining systematic differences between schizophrenic users and community controls. In particular we would like to be able to attribute any quantifiable signals we observe to the presence or absence of schizophrenia rather than to a confounding age or gender divergence between the populations (Dos Reis and Culotta 2015). To that end we estimated the age and gender of all our users (from their language usage) via the tools graciously made available by the World Well-Being Project (Sap et al. 2014). For each user we applied a hard threshold to the gender prediction to obtain a binary ‘Female’ or ‘Male’ label. Then in order to select the best match for each schizophrenia user we selected the community control that had the same gender label and was closest in age (without replacement).,,
https://dl.acm.org/doi/abs/10.1145/2700171.2791026,Detecting Changes in Suicide Content Manifested in Social Media Following Celebrity Suicides,Wikipedia Data Social Media Data,0,Next we compiled a list of reported celebrity suicides which fell within the time range of our Reddit data. Defining who is a “celebrity” is nontrivial so we refer to the Wikipedia page listing celebrity suicides7 as a way to measure who has sufficient celebrity status for inclusion. We obtained 10 reported celebrity suicides in the same period as our Reddit data; their names and reported suicides are shown in Table 2. We measure the prominence of a celebrity’s death by measuring the change in Wikipedia page views for the celebrity’s Wikipedia page. Wikipedia provides daily page view statistics for each page.8 We compare the number of page-views in the two weeks prior to their death with the two weeks following their death in terms of z-score (Figure 1). Here z-scores are computed by converting the page views to standard normal variable with 0-mean and standard deviation of 1. For 9/10 of the cases we see a notable spike in number of views showing that the suicides of these individuals were well-known enough to be viewable on such a macro scale and for examining the presence of Werther Effect in social media. We note two aspects related to the above analysis and which will be used through the rest of this paper. First since we are focusing on different types of data sources—Wikipedia and Reddit we use z-score conversion as a normalization technique for the Wikipedia page views and Reddit’s SW posting activity volume. Further the above observation in Wikipedia data and the analyses that ensue focus on observing changes over a two week window preceding and succeeding a celebrity suicide; this choice is motivated by our initial analyses and from the literature on Werther Effect [17].,As mentioned earlier we examine data from Reddit. We provide a description of the features of this social media platform which are important to understand the context of our research problem. Reddit allows users to submit content in the form of links or text posts organized by areas of interest or sub-communities called “subreddits” (e.g. politics programming science). Users can voice their opinion on the post via a voting mechanism which allows more popular submissions to be featured more prominently according to their score: the difference between the “upvotes” and “downvotes” cast on each post (also known as “score”). Users can also engage with each other via a comment thread attached to each post. In 2014 Reddit had 71 billion page views over 8 000 active communities 55 million posts and 535 million comments5 . In this paper we focus specifically on the subreddit called “SuicideWatch” a forum for users contemplating suicide and who seek help advice and support. It is a strong support community with (currently) about 35K subscribers. The community is highly moderated with many of its moderators and active subscribers adopting prominent roles in providing support to individuals showing vulnerability. In this subreddit votes on posts are used as a proxy for support and to increase or decrease a post’s prominence rather than as a statement of agreement/disagreement. We used Reddit’s official API6 to collect posts comments and associated metadata from r/SuicideWatch (hereafter SW). Our analysis in this paper is based on all posts made to SW between October 16 2013 and December 19 2014 – 66059 posts from 19159 unique users. Example (paraphrased) titles of posts from r/SuicideWatch are given in Table 1. We collected the title of the post the body or textual content ID timestamp author ID the number of upvotes and downvotes the post obtained including the difference between upvotes and downvotes on the post (i.e. score.),
https://aclanthology.org/W17-3110.pdf,Small but Mighty: Affective Micropatterns for Quantifying Mental Health from Social Media Language,Data,0,We briefly explain the data collection method here but we refer the interested reader with further questions on the methodology to Coppersmith et al. (2016) for the suicide attempt data and Coppersmith et al. (2014a) for all other conditions. The data for these analyses are Twitter posts collected via two methods. Most of the data come from users who have publicly discussed their mental health conditions. These users are frequently referred to as “self-stated diagnosis” users as they state publicly something like “I was diagnosed with schizophrenia” or “I’m so thankful to have survived my suicide attempt last year”. The data for users with a suicide attempt was supplemented by data from OurDataHelps.org a data donation site where people provide access to their public posts and fill out a short questionnaire about their mental health history. Data are then deidentified and made available to researchers addressing questions of interest to the mental health community. Donors provide consent for their data to be used in mental health research upon signup. Of the users who attempted suicide 146 came from OurDataHelps.org. Specifically we examine generalized anxiety disorder eating disorders panic attacks schizophrenia and attempted suicides. These conditions were selected based on the theory that there are important timing aspects to their symptoms – ebbing and flowing of symptoms as treatment is effective (especially schizophrenia) onset and exacerbation of symptoms by external events and stress and punctuated events in time of psychological symptoms (suicide attempts panic attacks and binging/purging behavior with eating disorders). We use the Twitter streaming API to collect a sample of users who used a series of mental health words or phrases in their tweet text (e.g. ‘schizophrenia‘ or ‘suicide attempt‘). Each tweet that uses one of these phrases is examined via regular expression to indicate that the user is talking about themselves. Finally those tweets that pass the regular expression are examined by a human to confirm (to the best of our ability) that their selfstatement of diagnosis appears to be genuine. This results in a dataset with users that have a self-stated diagnosis of generalized anxiety disorder (n = 2408) an eating disorder (749) panic attacks (263) schizophrenia (350) or someone who would go on to attempt suicide (423). Some of these users do not exhibit the sort of posting behavior required to create micropatterns (i.e. they rarely post multiple times within a 3 hour time window). We exclude these users from our analysis which is 5-9% of users for most conditions with the exception of those with a suicide attempt where a little over half the users do not exhibit this posting behavior. The resultant dataset used for analyses is: generalized anxiety disorder (n = 2271) eating disorders (687) panic attacks (247) schizophrenia (318) suicide attempts (157). In order to allow comparisons of each condition to control users we gather a random sample of 10000 Twitter users for whom at least 75% of their posts are identified by Twitter as English. All the users with a self-stated diagnoses and all members of this control population have their age and gender estimated according to Sap et al. (2014). For each user with a self-stated diagnosis we find a matched control through the following procedure: create a pool of users where the estimated gender matches and the estimated age is within the same 10-year bracket (the suggested accuracy of the age estimator). From that pool of age- and gender- matched users we select the user whose tweets start and end over the most similar timeframe. We will refer to these age- gender- and time-matched controls simply as “matched controls” throughout the rest of this paper. All tweets were publicly posted by their author (i.e. no users marked at “protected” or “private” were included). On average users had 2949 tweets. The distribution of estimated age and genders for users with each self-stated condition can be seen in Figure 1. For most conditions the population skews female though for schizophrenia the genders are roughly balanced. The average age tends to be in the early-to-mid 20s.,,
https://dl.acm.org/doi/abs/10.1145/2998181.2998220,Gender and Cross-Cultural Differences in Social Media Disclosures of Mental Illness,Gender Inference,2,For inferring gender of a user in our two datasets we compared account names to existing name databases. First of all we started cleaning the account names and subsequently identifying the first and last name on the basis of a 1-gram lookup. Thereafter we compared the names to country-specific name databases. For this the country origin of a Twitter user was firstly retrieved using the Country Name Inference giving the user’s specified location. If the gender could not be retrieved using the previous method it would look up the name over all the country-specific lookup tables. For analytical simplicity we only consider binary gender (female/male) in this paper.,,
https://www.jmir.org/2019/6/e14199/,Detecting Signs of Depression in Tweets in Spanish: Behavioral and Linguistic Analysis,Data Collection and User Selectio,0,The selection of the tweets and their users was based on the filtered real-time streaming support provided by the Twitter API. In the first step we selected the users who showed potential signs of depression on Twitter on the basis of the 20 most frequent words in Spanish expressed by patients suffering from depression in clinical settings. These words were jointly identified and selected by a psychologist and a family physician with clinical experience and were based on the definition and general features of depression according to the Diagnostic and Statistical Manual of Mental Disorders [42]. The list of words used and their English translations are shown in Textbox 1. During June 2018 1470000 tweets including 1 or more occurrences of the words listed in Textbox 1 were collected. From this collection of tweets and to select the users who publicly stated in the textual description associated to their profile that they suffered from depression all the profile descriptions including 1 or more occurrences of the word “depr” and all the possible derivations related to the word depression in Spanish such as “depre” “depresión” “depresivo” “depresiva” “deprimido” and “deprimida” were considered. From the 720 users who included 1 or more of these words in their description profile 90 users who stated they suffered from depression or were receiving treatment for depression were selected for the analysis. This selection was performed by a psychologist verifying that the statements were related to real expressions of depression excluding quotes jokes or fake ones. For each of these depressed Twitter users we collected all the most recent tweets from their timeline up to a maximum of about 3200 tweets. Thus a total of 189669 tweets were collected a figure that was reduced to 140946 after discarding the retweets. These 140946 tweets constituted the depressive users dataset. Examples of sentences appearing in the user profiles that were used for selecting the depressive users are: “Paciente psiquiátrico con depresión crónica” (Psychiatric patient with chronic depression; example of a profile sentence that indicates depression). “Colecciono errores traducidos a tweets depresivos y a uno que otro impulso de amor” (I gather errors translated into depressing tweets and into one or another love impulse; example of a profile sentence that does not indicate depression). Once the users with profile sentences indicating depression had been retrieved their Twitter timelines were collected. Only those users having in their timeline at least 10 tweets that suggested signs of depression were retained for further analyses. For each user the selection of these tweets was performed by manually inspecting the tweets of the user’s complete timeline in reverse temporal order starting from the most recent one to the oldest tweet of the timeline retrieved by means of the Twitter API . Finally a total number of 1000 tweets issued by the 90 depressive users suggesting signs of depression were detected and used for the analysis. This set of tweets provided us with the depressive tweets dataset which was used to analyze linguistic features of tweets showing signs of depression. It has to be mentioned that these 1000 tweets were not to be included in the depressive users dataset (see Figure 1). At the same time more than 97500000 tweets were also collected in June 2018: such tweets were gathered by listening to the public Twitter stream during this time span by only considering tweets with Spanish textual contents (as detected by Twitter language identification support). Given that Twitter requires more restrictive filters than just the language of the tweets we used a list of the most frequently used Spanish words (stopwords) to retrieve all tweets that included 1 or more of these words. The vast majority of Spanish tweets should match this criterion. A sample of 450 users who did not mention in their profile the word depression and its derivations were selected randomly from the 97500000 tweets. The complete timelines of these users were compiled (1141021 tweets) which were reduced to 712589 once retweets were removed. These 712589 tweets constituted the control dataset. To identify the language of a tweet we relied on the language automatically identified by Twitter for each tweet selecting tweets in Spanish. It has to be noted that these data can contain some tweets from unidentified depressive users.,,
https://aclanthology.org/W18-0608.pdf,Cross-cultural differences in language markers of depression online,Data Collection,0,Data was collected from 7 Cups of Tea an anonymous online chat-based peer support community for emotional distress1 . Users agree at signup that their data may be used for the purposes of research. All the data used for the current study was anonymous and securely stored. This research was performed in line with the ethical and privacy protocols outlined in detail in (Benton et al. 2017). Data from 7 Cups takes the form of written dialogue between users of the service and volunteers who are trained as “active listeners”. A fragment of an exchange between the user of the service (U) and the volunteer (V) might go as follows: For the analyses reported in this paper we used only text generated by users of the service not the volunteers providing peer support. Users who reported depression as their primary concern at sign up were eligible for inclusion in analyses. Our original sample was comprised of 23048 conversations involving 1937 unique users. Users were excluded from the sample if they did not indicate their culture or if they selected ‘Other’. This resulted in the exclusion of 199 and 130 users respectively. The original sample also included users identifying as Native American or American Indian. This group was excluded from analyses since the majority of the data among these users was not English. This resulted in the removal of 15 users leaving a total sample size of 1593.,,
https://dl.acm.org/doi/pdf/10.1145/3359169,Cross-Cultural Differences in the Use of Online Mental Health Support Forums,Method of Analysis,0,"Selection Criteria and Data Scope. To understand the impact of cultural differences on how individuals use online mental health platforms we begin our analysis by creating a dataset of users from different national communities on Talklife a support platform with over half a million users [91]. For this analysis due to the fact that most research in CSCW on mental health online has been done either agnostic of cultural context [12 34] or in a Western context [60 67 88] we choose to focus on users from non-Western countries following Zhang et al. [103]. As researchers located in the Global South and with lived experience interacting with the health system and diverse explanatory models [52] of mental illness we believe that moving the focus of CSCW and CSCWadjacent mental health research away from the West is crucial to better meet the needs of people often underserved by the medical system [70]. To create these subgroups of users we choose the three non-Western countries with the highest user populations on Talklife or India Malaysia and the Philippines. Guided by the rich amount of literature on the unique nuances to mental health expression for each country [35 62 77 80] we examine the national identity linguistic and behavior-based differences of use between each user subgroup. In particular this research notes that as a result of cultural norms around the sharing of distress and alternative conceptualizations of mental illness in India Malaysia and the Philippines symptoms are often expressed in somatic and religious terms as opposed to traditionally clinical or psychiatric terms. We choose to analyze each subgroup at the national level for both theoretical and practical reasons. On a theoretical level in past work in the medical anthropology of mental health national identity has commonly been used for a approximate level of analysis for cultural identity [31 33 52]. Additionally on a more practical level each user’s country was determined using their IP address by Talklife and shared with us in an user-anonymized dataset. Inferring a more precise location could potentially compromise user anonymity as discussed in past work [47] and did not seem to have any more significant value for our analysis of cultural differences than analysis at the national level. We analyze data from 10532 Indian users 3370 Malaysian users and 3370 Filipino users as shown in Table 2. Collectively we refer to these countries as the minority sample. As a comparison set we construct a random sample of all threads on Talklife and refer to it as the majority sample. Due to the relative prevalence of users from Western English-speaking countries in Talklife most of the threads in the majority sample include posts from countries such as the USA UK and Canada. Indians are the largest non-Western minority subgroup on Talklife. Data was sampled from May 2012 to June 2018. Following this cross-national analysis to see if our broader results on Talklife generalize to a differently structured online mental health community we picked the largest Western country (the United States) and the largest non-Western country (India) represented on 7Cups a similar support platform with more than 15000 users actively using the platform each week [7]. Using 7Cups data we repeat our analysis testing for the same cultural differences we found in our Talklife sample. For this analysis we were provided a sample of data on activity from 6055 Indian users and 18581 American users as shown in Table 2. Unlike our sample of Talklife users this dataset is not a random sample. There is an upsampling of Indian users to ensure that we have data from a sufficient number of Indians in the dataset. Like on Talklife Indians are the largest non-Western minority subgroup on 7Cups. We focus on Indian users due to a lack of sufficient data on users from Malaysia or the Philippines. Data was sampled from March 2014 - August 2018. 3.1.2 Defining Cultural Identity and Use of Clinical Language. In this work we examine the relationship between cultural identity and use of online mental health support forums. To do so we leverage Tomlinson’s definition of cultural identity as “self and communal definitions based around specific usually politically inflected differentiations: gender sexuality class religion race and ethnicity nationality"" [94] particularly looking at the aspect that of modern cultural identity that runs along national lines as delineated by Hall et al. [41]. As a diverse and amorphous form of identity cultural identity can often intersect and interact with other forms of identity including religious or ethnic identity. However in the absence of direct information about religious or ethnic identity based on the data available we use national identity as a proxy for cultural identity. Additionally following Schlesinger et al’s [83] call for more intersectional analyses and methods within HCI we also include analyses of adjacent and intersecting identities when relevant including religious identity. To analyze clinical language we use a broader definition of clinical language than just specific medical diagnoses. Following methods used in past work to analyze antidepressant related language [30] we create a dataset of clinical mental health language including unigrams bigrams and trigrams from a list of mental disorders as defined by the International Classification of Diseases (ICD-10) and Diagnostic and Statistical Manual of Mental Disorders (DSM-5) [100]. We also included all unigrams from the MacMillan Dictionary list of words used to describe illnesses and diseases both specifically for mental illness and general illness [1–3]. As a result we include unigrams like “night"" (from night terrors) or sleep (from “sleep disorder"") as these are often correlated with specific symptoms of mental illness or distress such as sleep issues or being awake at night [30]. This included any clinically common abbreviations for mental disorders such as OCD for “obsessive compulsive disorder"" or BPD for “borderline personality disorder."" Shorthand for disorders commonly used by online communities such as “pro-ana” (as used in pro-eating disorder communities) [22] were not included due to the difficulty in finding an exhaustive list of these terms across disorders. We choose to use terms from and associated with DSM and ICD categorized disorders as a result of the common usage of these frameworks globally [99]. Throughout our analysis of these varied factors we use µ to represent means and σ to represent standard deviations. 3.1.3 Constraints Limitations and Tradeoffs. Cultural identity can exist at many different and intersecting levels including subcultures and subcommunities within the larger umbrella of a cultural identity. As a result for the purpose of this analysis we had to adopt some constraints in order to do a meaningful and specific analysis. One large limiting constraint that we chose for this study is to use national identity at the state level as a proxy for cultural identity. Though a major and formative part of modern cultural identity as argued by both Hall [41] and Tomlinson [94] each country we analyze is incredibly diverse with many individual cultural identities that both intersect and diverge from a greater national identity [54 64 89]. A more rich analysis of these other forms of cultural identity is beyond the scope of this work but could lead to richer conclusions about the nature of cultural identity in online mental health support communities particularly with regard to cultural differences between users with the same national identity. Additionally to stay consistent between analyses as a result of a lack of data on users from Malaysia and the Philippines we only analyze users in India on 7Cups and extend these findings to the experience of being part of a minority group on an online mental health forum. We draw validity for these exploratory findings from similar consistent patterns we observe between Indian Malaysian and Filipino users but a deeper analysis with a larger dataset is likely necessary to determine when and for which minority communities these conclusions do not hold true. Additionally while we construct clinical language through use of the commonly used DSM and ICD both frameworks of illness categorization have significant limitations particularly in the countries we have selected. For example there are both mental health disorders that are culturebound [74] as well as mental health language that is used in different ways within the specific countries we analyze such as depression often being an umbrella term for all mental illnesses [53]. Additionally it is clear that online support communities often develop their own cultural norms and language around mental health [21 72] and a deeper understanding of how this plays out on Talklife and 7Cups is neither the focus nor within the scope of this work. In this work we intentionally use standard clinical and medical terms for mental health disorders in our analysis of clinical language. As detailed in past anthropological research [52] it is theorized that the use of medical and clinical language is representative of a medicalized explanatory model of illness and we frame use of this language across cultures as a approximate signifier of a greater awareness of the presence of a mental disorder as opposed to conceptualizing distress as “stress"" “tension"" or “depression"" [25 53 98]. For our analysis we strictly analyzed posts that were in the Latin alphabet with almost all posts on both Talklife and 7Cups being in English. However as both Malay [8] and Tagalog [82] are most commonly written in the Latin script and since it is common for users from India speakers to use romanized versions of Indian languages online [79] it is possible that a small minority of posts in our analysis were text in a different language. However as confirmed by only seeing English words used in our analysis of the top n-grams among each user subgroup it is clear that English is the predominant language on both platforms. Though beyond the immediate scope of this work a greater analysis of non-English code-switching on these platforms could lead to a deeper understanding of the impact of interactions on expression between users with the same national identity but different language preferences.",,
https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-018-2197-z,Automatic extraction of informal topics from online suicidal ideation,Suicide Watch,0,In this section we first present the data gathered and used in our analysis. Researchers interested in the code and the data are invited to contact the authors. Reddit is a website which enables users to aggregate rate and discuss news entertainment politics and many other topics. According to Alexa it is the 8th most popular website in the world. It was estimated by the Pew research center that 6% of online adults use Reddit [26]. The site is organized into a collection of “subreddits” each focused on a particular topic and administered by a collection of moderators. The subreddit r/SuicideWatch is a forum in which online users are encouraged to post their thoughts regarding suicide. At the time of our data collection it had over 58000 subscribers. Sometimes users express a preoccupation with the thought of suicide. Other times users discuss immediate plans to take their own life. These posts often contain a description of their mental state including depression reaction to stress their feelings of being alone and having a low self-esteem. While most online sources of data are notoriously noisy this particular subreddit is remarkably clean. Given the serious nature of the subreddit individuals are less likely to post harassing comments or off-topic remarks. When users post such comments the moderators of the subreddit quickly remove them. We collected all posts from its inception in 2008 to 2016. Each post is often commented on by other individuals. In this work we focused on the original post as it most often represents the suicidal ideation of a user and comments often represent emotional support from other users. We cleaned this data. First we removed empty posts in which the content had been deleted. Second we removed links and replaced them with the word “link”. Third we concatenated the text of the post to the title as many users begin their post in the title and continue in the body of the post. Finally we removed punctuation and other special characters. After cleaning this data we had 131728 posts with 27978246 words of which 84607 words were unique posted by 63252 unique users.,,
https://ieeexplore.ieee.org/abstract/document/8609647,Online Social Networks in Health Care: A Study of Mental Disorders on Reddit,Dataset,0,Reddit is a multilingual Online Social Network founded in 2005 and organized in subcommunities by areas of interest called subreddits. We obtained data from the Reddit's data repository4 focusing on four subreddits where people discuss issues related to mental heath disorders: Depression (/r/depression) Suicide Watch (/r/Suicide Watch) Anxiety (/r/anxiety) and Bipolar (/r/bipolar). Our dataset is comprised of user activities (posts and comments) that took place between 2011 and 201 7. Here we focus on data from January 2017 to December 2017. In total we obtained 261511 posts and 1256669 comments from 184708 unique users. Table I shows the total number of users posts and comments per subreddit. The total number of comments in each community is at least 4.2 times larger than the number of posts which suggests a supportive behavior among users.,,