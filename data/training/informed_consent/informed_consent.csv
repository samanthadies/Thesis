Link to paper,Title,Section,Score,Text,Text 2
https://link.springer.com/content/pdf/10.1140/epjds/s13688-017-0110-z.pdf,Instagram photos reveal predictive markers of depression,Data Collection,2,Data collection was crowdsourced using Amazon’s Mechanical Turk (MTurk) crowdwork platform. Separate surveys were created for depressed and healthy individuals. In the depressed survey participants were invited to complete a survey that involved passing a series of inclusion criteria responding to a standardized clinical depression survey answering questions related to demographics and history of depression and sharing social media history. We used the CES-D (Center for Epidemiologic Studies Depression Scale) questionnaire to screen participant depression levels []. CES-D assessment quality has been demonstrated as on-par with other depression inventories including the Beck Depression Inventory and the Kellner Symptom Questionnaire [ ]. Healthy participants were screened to ensure no history of depression and active Instagram use. See Additional file  for actual survey text. Qualified participants were asked to share their Instagram usernames and history. An app embedded in the survey allowed participants to securely log into their Instagram accounts and agree to share their data.b Upon securing consent we made a one-time collection of participants’ entire Instagram posting history. In total we collected  photographs from  Instagram users  of whom had a history of depression. We asked a different set of MTurk crowdworkers to rate the Instagram photographs collected. This new task asked participants to rate a random selection of  photos from the data we collected. Raters were asked to judge how interesting likable happy and sad each photo seemed on a continuous - scale. Each photo was rated by at least three different raters and ratings were averaged across raters. Raters were not informed that photos were from Instagram nor were they given any information about the study participants who provided the photos including mental health status. Each ratings category showed good inter-rater agreement. Only a subset of participant Instagram photos were rated (N = ). We limited ratings data to a subset because this task was time-consuming for crowdworkers and so proved a costly form of data collection. For the depressed sample ratings were only made for photos posted within a year in either direction of the date of first depression diagnosis. Within this subset for each user the nearest  posts prior to the diagnosis date were rated. For the control population the most recent  photos from each user’s date of participation in this study were rated.,Data privacy was a concern for this study. Strict anonymity was nearly impossible to guarantee to participants given that usernames and personal photographs posted to Instagram often contain identifiable features. We made sure participants were informed of the risks of being personally identified and assured them that no data with personal identifiers including usernames would be made public or published in any format.
https://dl.acm.org/doi/abs/10.1145/2858036.2858207 ,Discovering Shifts to Suicidal Ideation from Mental Health Content in Social Media,Privacy Ethics and Disclosure,0,Privacy Ethics and Disclosure. We use public data from Reddit. Personally identifiable information was removed and content was de-identified and paraphrased before being reported in the paper for exemplary purposes. This work has been approved by the appropriate Institutional Review Board (IRB). Our work does not make any diagnostic claims related to mental illness or suicide.,
https://dl.acm.org/doi/abs/10.1145/2702613.2732733,Detecting and Characterizing Mental Health Related Self-Disclosure in Social Media,Self-Disclosure Data,0,Based on the training data thus created we pursued the use of supervised learning to develop a classifier which would indicate whether a post is of high low or no self-disclosure. We tested a variety of different classification techniques (decision trees k Nearest Neighbor naive Bayes). The best performing classifier was found to be a perceptron classifier with adaptive boosting used to amplify performance [17] whose results will be used in the remainder of this paper. We used the following feature generation rules: First we eliminated stopwords from each post based on standard list provided by Python’s NLTK library. Next we performed stemming using Porter Stemmer. We extracted uni- bi- and tri-grams from each post and considered those with five or more occurrences. We also computed two additional features – length of each post and whether the author of the post is an exclusive poster on mental health forums or is observed in our dataset to post on other forums as well. Thus each post was characterized by 1070 features. We used standard 10-fold cross validation (CV) to evaluate the classifier and ran our model over 100 random 10-fold CV assignments for generalizability of the results. We report the average accuracy precision recall F1 specificity as metrics of performance. We find that our classifier based on the perception model yields an average accuracy of 78.4% in detecting high or low self-disclosure with .74 precision and .86 recall (see Table 4 for details). Other methods like k-NN (k=5) give higher precision but at the expense of very low recall. Figure 1 gives the ROC (receiver operating characteristic) curves for all the models. Per the ROC curve corresponding to the perceptron model we find it to yield the maximum area under curve (.81) hence best performance. We further identify in Table 5 the n-grams (or features) with the highest weights given by the perceptron – it implies these features were the most significant in the classification task. We provide some brief qualitative examinations of these n-grams in the light of prior psychology literature on selfdisclosure and mental health [10 11]. We find that the ngrams primarily are associated with vulnerable and selfloathing thoughts (e.g. thoughts of suicide) bear a negative tone or depict confessional experiences. Based on prior research [10 11 12] and our own work on mental health discourse on Reddit [4] we find that these are the topical dimensions along which high self-disclosure and low/no selfdisclosure posts vary. In essence high self-disclosure posts share extensively their personal beliefs and fear for instance their vital constructs and private sensitive informational attributes. The post excerpts below have been classified to be of high self-disclosure and through them we demonstrate the use of some of the n-grams in Table 7: \“I don’t want to kill myself I haven’t felt suicidal in a long time but I just want to stop life for a while you know?” “My dad would beat the living shit out of me […]. I’ve been to the hospital so many times I’ve lost track” “I hate this. I hate myself. I don't want to f****** be this person anymore. I'm unmotivated unfocused immature.”,
https://dl.acm.org/doi/abs/10.1145/2702123.2702280,Recognizing Depression from Twitter Activity,Data Gathering,2,In this study we gathered information on depression levels of Twitter users and their activity histories. To do this we published a website to administer a questionnaire and disseminated information about the website over Twitter1. In contrast to De Choudhury et al. [14] who collected data from Englishspeaking users through crowdsourcing this study collected data from Japanese-speaking volunteers. This approach was used to investigate the extent to which depression risk can be estimated for a population different from the population considered by the prior research [14]. Figure 1 shows a screenshot of our website. The website collected the responses to a questionnaire to evaluate the degree of depression of the Twitter users who participated (hereinafter the participants) and to collect the histories of participants activities on Twitter. The activity histories of participants were collected through the Twitter application programming interface (API)2 and the questionnaires to determine degree of depression were completed by participants through their web browsers. Before data collection visitors to the website were presented with a written explanation of the aims of the experiment the information that would be collected and how that information would be handled. Those who consented to become participants after receiving the explanation logged into their individual Twitter accounts through the OAuth authorization process. Next participants were surveyed on gender age occupation and history of depression following which they answered a questionnaire designed to evaluate degree of depression. A message called the “kokoro score” (“kokoro” is a Japanese word meaning “heart”) determined on the basis of answers to the questionnaire and information in the collected tweets was displayed to participants after completion of the questionnaire (Fig. 2). Experiment participants were able to tweet the message displayed which made it possible to promote the website over Twitter by word-of-mouth in a type of snowball sampling. The CES-D questionnaire was used to evaluate the degree of depression [30]. In the CES-D test participants answered 20 questions on a Likert-type 4-point scale. Each answer was assigned a score of 0-3 points with the sum of the points from all answers used as the score to estimate likelihood of depression. Several standards exist by which to determine the appropriate cutoff score for identifying depression. In this research we regarded a score of 22 points or higher as indicating active depression and a score of 21 points or lower as indicating no active depression; these are the same values as used in [14] and give a cutoff score of 22. In addition answers to BDI [2] a depression scale used with characteristics similar to CESD were collected to ensure the reliability of data. For each participant scores were calculated on both scales with poor correlation regarded as indicating unreliable answers. The time taken to answer the questionnaires was also recorded and those completed in too brief a time were excluded. After each participant answered the questionnaire the activity history of that participant on Twitter was collected from Twitter by using the API. At most 3200 tweets were collected for each participant and the number of users following the participant and being followed by the participant were recorded. Tweets published after the questionnaire was taken were discarded. The website was opened to the public on 4 December 2013 at which time the authors publicized it on their Twitter accounts. Between 4 December 2013 and 8 February 2014 219 people participated in the experiment. After eliminating participants who did not tweet and participants who answered the questionnaire in fewer than 30 seconds (as previously mentioned to ensure the reliability of the questionnaire answers) 214 sets of answers remained. Only the first set of answers was used for participants who completed the questionnaire more than once. As a result data about 209 experiment participants (male: 121; female: 88) aged 16 to 55 (mean: 28.8 years; standard deviation: 8.2 years) were analyzed. The correlations between CES-D score and BDI score for these participants were high 0.87 and there were no participants with uncorrelated scores so the data for all 209 participants were used; excluded datasets are not discussed any further. Figure 3 shows the histogram of CES-D scores of 209 participants. Among the participants 81 (resp. 128) were estimated to have (resp. not have) active depression for an incidence of approximately 39%. This incidence is similar to that found by De Choudhury et al. [14] who identified depression in approximately 36% of participants. Table 1 gives statistics on the activity histories of participants.,
https://www.nature.com/articles/s41598-017-12961-9,Forecasting the onset and course of mental illness with Twitter data,Method,2,The methods used in recruitment data collection and analysis are adopted from Reece and Danforth12. The present study was reviewed and approved by the Harvard University Institutional Review Board approval #15-2529 as well as the University of Vermont Institutional Review Board approval #CHRMS-16-135. All experimental procedures were performed in accordance with Institutional Review Board guidelines. All study participants provided informed consent and acknowledged all of the study goals expectations and procedures including data privacy prior to any data collection. Surveys were built using the Qualtrics survey platform and analyses were performed using Python and R. Twitter data collection apps were written in Python using the Twitter developer’s Application Programming Interface (API).,
https://aclanthology.org/W14-3214.pdf,Towards Assessing Changes in Degree of Depression through Facebook,Dataset,1,We used a dataset of 28749 nonclinical users who opted into a Facebook application (“MyPersonality”; Kosinski and Stillwell 2012) between June 2009 and March 2011 completed a 100-item personality questionnaire (an International Personality Item Pool (IPIP) proxy to the NEO-PI-R (Goldberg 1999) and shared access to their status updates containing at least 500 words. Users wrote on average of 4236 words (69917624 total word instances) and a subset of 16507 users provided gender and age in which 57.0% were female and the mean age was 24.8. The dataset was divided into training and testing samples. In particular the testing sample consisted of a random set of 1000 users who wrote at least 1000 words and completed the personality measure while the training set contained the 27749 remaining users.,
https://ieeexplore.ieee.org/abstract/document/6784326,Affective and Content Analysis of Online Depression Communities,Data Sets,0,a) CLINICAL Communities: Communities who are interested in ‘depression’ and with at least 200 posts are extracted from LiveJournal. This is identified through the ‘Search communities by interest’2 provided by LiveJournal and results in 24 communities with 38401 posts. The CLINICAL communities are grouped based on name and description of the individual community: depression bipolar self-harm attachment/separation and suicide (See Table 1 for statistics). The earliest community creation date was in 2001 thus our data set spans over 10 years. b) CONTROL communities: We constructed a CONTROL data set using five popular categories of communities in the LiveJournal Directory.3 We select communities who have at least 200 posts resulting in 23 communities with 229563 posts. This set is called CONTROL and the statistics of these 23 communities and their description are shown in Table 2.,
https://www.jmir.org/2017/7/e243/,Assessing Suicide Risk and Emotional Distress in Chinese Social Media: A Text Mining and Machine Learning Study,Data Collection,2,A Web-based survey of Weibo users was conducted to assess the respondents’ suicide risk and emotional distress (ie depression anxiety and stress). The invitation letter to participate in this survey was widely sent out to general Weibo users by various promotion activities. For a Weibo user to be eligible for the study she or he had to be 18 years or older (by self-report). A 30 Renminbi incentive for each complete survey was provided to boost the respond rate. With the respondents’ consent their Weibo posts that were posted in the public domain during the 12 months before the survey were downloaded by calling Weibo API. The survey fulfilled the Checklist for Reporting Results of Internet E-Surveys (CHERRIES) checklist and details of the procedure have been reported in previous publications [2232]. In addition when multiple survey feedback were submitted from the same Internet protocol addresses only the first submission was used to avoid duplicate participation. In contrast to a previous study [32] this study excluded those who posted nothing throughout the 12 months but not those who posted fewer than 100 posts. Eventually data provided by 974 respondents remained for further analyses. The study has obtained ethical approvals from the Human Research Ethical Review Committee at the University of Hong Kong and the Institute Review Board of the Institute of Psychology at the Chinese Academy of Sciences. The survey measured respondents’ suicide probability score depression anxiety stress and Weibo suicide communication (WSC) as the outcome variables. In addition the respondents’ Weibo posts language features were extracted as independent variables or features for machine learning. The details of how those data were obtained are elaborated in the following subsections.,
https://dl.acm.org/doi/abs/10.1145/3025453.3025909,A Social Media Based Index of Mental Well-Being in College Campuses,Mental Health and Control Dataset,0,We gained access to a sample of 63485 public posts from 35038 unique users shared between 2014 and 2016 in a variety of mental health subreddits—this repository of posts has been used in prior work to study mental health selfdisclosure and support seeking manifested in social media [18 50 39 20]. This dataset includes posts and associated metadata spanning 14 mental health related subreddits such as r/depression r/mentalhealth and r/traumatoolbox r/bipolarreddit. From this corpus we excluded posts that contained only a title without a post body. This gave us 21734 posts. We refer to these posts as MH posts. Our control data also relied on a dataset compiled and utilized in prior work [50]; it contains posts from subreddits such as r/WorldNews r/food and r/AskReddit. We randomly sampled an equal number of posts (21734) as the MH posts above for our control dataset. We refer to these posts as CL posts.,
https://ieeexplore.ieee.org/abstract/document/7752434,MIDAS: Mental illness detection and analysis via social media,Data Collection,0,To train our models we require information from two different types of users: patients and non-patients. Therefore we employed a combined - manual effort and keyword matching - data collection approach to efficiently collect data for these users. For the collection of patients we manually collect the community portals relevant to both mental disorders. 2 From these portals' followers list we select the self-reported users who explicitly state in their profile description that they suffer from a mental illness; i.e. for a given user we are checking if his/her profile contains any keyword related to a target disorder (e.g. “borderline” “bpd” “bipolar”). Non-patients are referred to as random active Twitter users who are not explicitly stating that they are suffering from Bipolar disorder (hereinafter referred to as “BD”) or Borderline Personality Disorder (hereinafter referred to as “BPD”). To obtain these users we randomly sampled Twitter IDs. Thereafter we proceeded to download the tweets from the selected IDs. After the users have been identified we manually label them into one of two categories: 1) Patient: a person who is suffering from a mental disorder 2) Not-related: any user who we don't consider to be a patient. Lastly after having obtained the final list of patients we retrieve their tweets. These steps are applied for the collection of both BPD and BD patient datasets.,
https://www.nature.com/articles/s41598-020-68764-y,A deep learning model for detecting mental illness from user content on social media,Ethics Declarations Data Collection,0,This study was approved by the Ethical Committee and Institutional Review Board of the Department of Applied Artificial Intelligence Sungkyunkwan University (#H1AAI2020).,We collected post data from the following six mental-health-related subreddits each of which is reported to be associated with a specific disorder10: r/depression r/Anxiety r/bipolar r/BPD r/schizophrenia and r/autism. In addition we further collected post data from the most popular health-related subreddit17 r/mentalhealth to analyze posts with general health information. From each subreddit we collected all the user IDs who had at least one post related to the mental health. Along with user IDs we also collected titles and posts using the PushshiftAPI18. Note that all the user information is anonymized hence no personally identifiable information was not included; we followed all the anonymization process guided by the Sungkyunkwan University Institutional Review Board (IRB). Overall the current study collected information from 248537 users who wrote 633385 posts in the seven subreddits from January 2017 to December 2018. Table 1 summarizes the information of collected data.
https://aclanthology.org/W19-3013.pdf,Mental Health Surveillance over Social Media with Digital Cohorts,Privacy and Ethical Considerations,2,The majority of social media analysis approaches try to extract signals from individual posts and thus do not need to record any personal information. However as we start moving towards userlevel analyses we are collecting and storing complete records of social media users communications. Even though this information is publicly available people might not be consciously aware of the implications of sharing all their data and certainly have not given explicit consent for their data to be analyzed in aggregate. This is even more pertinent for analyses involving sensitive information (e.g. health related issues). As it has been demonstrated by the recent incidents involving companies inadvertently sharing or failing to protect users personal data there is a serious danger of abuse and exploitation for systems that collect and store large amounts of personal data. Even though this is in large part an ethical question there are technical solutions that can be used to partially address this issue. One is to use anonymization techniques to obfuscate any details that allow third parties (even analysts) to identify the individuals that are involved in the study. Another is to store only abstract representations — which can still be updated and consumed by predictive models —  and discard the actual content. In regards to consent there are initiatives to support voluntary data donation for research purposes e.g. the Our Data Helps program6 .,
https://link.springer.com/chapter/10.1007/978-3-319-67186-4_6,Social Media Mining to Understand Public Mental Health,Data,0,We analyze two datasets: (1) user communities on Reddit and (2) journals from a mental health journalling mobile app. We omit the name of the app for privacy and we refer to it as the “journalling app”. Reddit is a social media platform that was originally used for sharing and rating content such as news documentaries and music. Users post in and subscribe to self-organized communities known as subreddits; subscribing to a subreddit allows a user to view all posts from that subreddit. An advantage of analyzing Reddit data is that the subreddits are labelled according to their topics. Utilizing curated lists from volunteer Reddit users we crawled all subreddits related to mental health as well as all subreddits linked by these communities. The second dataset consists of anonymized journal posts from a mobile app designed to help people track their moods and share them anonymously if they desire. For each journal post the app requires the user to label the journal post with at least one mood selected from a pre-populated list including “happy” “sad” etc. We obtained all journals and the associated moods written between January 2016 and January 2017. This amounts to over 1.2 million journals written by approximately 75000 users. Figure 1 plots the number of journals posted over time. Most of the journals were written in the first half of 2016 although we inspected topic distributions per month and did not find seasonal effects. Towards the beginning of 2016 many new users registered on the app and eventually stopped using it. Like weight-loss and productivity apps we believe this influx is tied to users looking to improve their habits as a New Year’s resolution. Each journal can be set to be private or public (visible to all other users of the app). Roughly one third of all journals are public. Figure 2 plots the number of users on the y-axis versus the percentage of journals they posted publicly. Most users are either mostly private or mostly public. Most journals are relatively short just like Twitter posts that are at most 140 characters. The average length of a journal with text in it is 128 characters; there are roughly 100000 journal that have no text only a mood label. We observed that private users tend to write journals that are slightly but statistically significantly longer than those written by public users by approximately 10 characters. Figure 3 shows the distribution of journal lengths where the spikes correspond to 0 length (mood only) 200 characters (the default limit set by the app) and 300 characters (set as the maximum for visualization purposes). Users of the app can optionally enter their location age and gender. While most users did not enter this information we found that those who revealed their location are mostly from North America those who revealed their gender are predominantly female and those who revealed their age have an average age of 25.,
https://dl.acm.org/doi/abs/10.1145/2556288.2557214,Seeking and sharing health information online: comparing search engines and social media,Health information Seeking and Sharing Survey,0,To gain qualitative insight into people’s health information seeking and sharing practices we conducted an online survey during June 2013 using a recruiting service (Cint) that offers “Census representative sampling” in terms of gender and age throughout regions of the U.S. Respondents were paid approximately 4 USD to complete the survey and were required to have a Twitter account to qualify to participate. The survey comprised 37 questions and took approximately 10 minutes to complete. Participants were asked if they had ever sought health information on a search engine such as Google or Bing or on Twitter and whether they had ever shared health information on Twitter. If they answered affirmatively they were asked to describe the most recent occasion on which they did so including the health condition that motivated them to search or share and their objective in performing the activity. The survey included questions about how often participants used search engines and Twitter for various types of information seeking or sharing views about risks associated with each platform and basic demographics. In total 237 respondents completed the survey. After discarding low quality responses (as determined by nonsensical or sarcastic responses to open questions) 210 valid survey responses were analyzed. Of these respondents 53% were female they resided in 38 U.S. states and the District of Columbia and 43% had a college degree or higher. Ages ranged from 18 to 70 years (median = 35 years). Respondents reported using search engines frequently (76% at least once per day with only 7% using them less than once per week). 71% of respondents had public Twitter accounts while the remaining 29% had “protected” accounts (i.e. only approved followers could view their postings).,
https://www.aaai.org/ocs/index.php/ICWSM/ICWSM14/paper/viewPaper/8075,Mental Health Discourse on reddit: Self-Disclosure Social Support and Anonymity,Data and Methods,0,reddit is a social news website where registered users submit content in the form of links or text posts. Users also known as “redditors” can then vote each submission “up” or “down” to rank the post and determine its position or prominence on the site’s pages. These two attributes associated with a post are referred to as “upvotes” and “downvotes”. Redditors can also comment on posts and respond back in a conversation tree of comments. Content entries that is the posts are organized by areas of interest or sub-communities called “subreddits” such as politics programming or science. As of 2013 reddit’s official statistics included 56 billion page views 731 million unique visitors 40855032 posts and 404603286 comments (http://blog.reddit.com/2013/ 12/top-posts-of-2013-stats-and-snoo-years.html). We used of reddit’s official API (http://www.reddit.com/ dev /api) to collect posts comments and associated metadata from several mental health subreddits: specifically using a Python wrapper PRAW (https://praw.readthedocs.org/en/ latest/index.html). The subreddits we crawled were: alcoholism anxiety bipolarreddit depression mentalhealth MMFB (Make Me Feel Better) socialanxiety SuicideWatch. All of these subreddits host public content. In order to arrive at a comprehensive list of subreddits to focus on we utilized reddit’s native subreddit search feature (http://www.reddit.com/reddits) and searched for subreddits on “mental health”. Two researchers familiar with reddit employed an initial filtering step on the search results returned so that we focus on high precision subreddits discussing mental health concerns and issues. Thereafter we focused on a snowball approach in which starting with a few seed subreddits (mentalhealth depression) we compiled a second list of “related” or “similar” subreddits that are listed in the profile pages of the seed subreddits. Following a second filtering step we arrived at the list of subreddits listed above. For each of these subreddits we obtained daily crawls of their posts in the New category. Corresponding to each post we collected information on the title of the post the body or textual content id timestamp when the post was made author id and the number of upvotes and downvotes it obtained. Since posts gather comments over a period of time following the time of sharing we crawled all of the comments per post that were shared over a three day period after the post was made. Qualitative examinations of the subreddits of interest revealed that 90% or more of the comments to any post were typically made in a three day window following the time the post is made—hence the choice. The crawl of the subreddits used in this paper We present some descriptive statistics of our crawled data. Our dataset contained 20411 posts with at least one comment and 97661 comments in all with 27102 unique users who made posts comments or both. A set of 7823 users (28.79%) were found to write both at least one post and comment. CDF of the user distribution over posts and comments is given in Figure 1. The figure shows the expected heavy tail trend observed in several social phenomena. Also see Figure 2 for the distribution of comments over time following post share. It illustrates the quick responsivity culture in the communities we study (peak at 3 hours). Some of the additional statistics of our dataset are given in Table 1. Further example titles of a few,
https://dl.acm.org/doi/abs/10.1145/3025453.3025932,Modeling and Understanding Visual Attributes of Mental Health Disclosures in Social Media,Data Collection,0,We utilized Instagram’s official API1 to obtain the dataset used in this paper. Each post in this dataset is public and contains post-related information such as the image caption likes comments hashtags filter and geolocation if tagged. Referring to prior literature [10] we adopted an iterative approach to first identify a set of appropriate distinguishing hashtags around different prominent mental illnesses prevalent in social media. With the seed tags we performed an initial data collection of 1.5 million posts shared on Instagram between Dec 2010 and Nov 2015. Then by leveraging an association rule mining approach we compiled the top k (k = 39 frequency ≥ 5000) co-occurring tags in the 1.5M posts and then appended them to the original seed tag list for further data collection. Table 1 lists a sample set of tags used to crawl the dataset. This final list of 45 tags was thereafter passed on to a psychiatry researcher to be categorized into different disorder types. For tags that described experiences or symptoms crosscutting across different conditions (e.g. “anxiety”) they were counted toward each disorder type. Table 2 gives a list of the ten different disorders identified in our data. We additionally consulted the Diagnostic and Statistical Manual of Mental Health Disorders (DSM-V [5]) that indicates these disorders to be prominent mental health challenges in populations. This categorization of the mental health challenges was conducted to ensure that our data used in the ensuing analysis focused on well-validated and clinically recognized conditions. At the same time it allowed us to focus on a diverse range of disorders expressed on social media rather than specific ones studied in prior work [12 13 27]; thus enabling us to discover generalized patterns in visual disclosures of mental health challenges in social media. Our final crawl included 2757044 posts from 151638 users spanning these disorders.,
https://www.sciencedirect.com/science/article/pii/S0747563215300996,A content analysis of depression-related tweets,Source of Tweets,0,Most of the tweets (n = 1806 92%) were from ordinary people or other Twitter accounts that did not fall into the above categories. Only 6% (n = 112) of the tweets were from health focused handles such as health or government organizations or handles promoting healthy lifestyles (e.g. Doctor's Nutrition @drsnutritionetx Natural Health @naturalhealth92). In addition only 3% (n = 60) were from clinicians or therapists.,
https://aclanthology.org/W15-1202.pdf,Quantifying the Language of Schizophrenia in Social Media,Data,0,We follow the data acquisition and curation process of Coppersmith et al. (2014a) summarizing the major points here: Social media such as Twitter contains frequent public statements by users reporting diagnoses for various medical conditions. Many talk about physical health conditions (e.g. cancer flu) but some also discuss mental illness including schizophrenia. There are a variety of motivations for users to share this information on social media: to offer or seek support to fight the stigma of mental illness or perhaps to offer an explanation for certain behaviors.4 We obtain messages with these self-reported diagnoses using the Twitter API and filtered via (caseinsensitive) regular expression to require “schizo” or a close phonetic approximation to be present; our expression matched “schizophrenia” its subtypes and various approximations: “schizo” “skitzo” “skitso” “schizotypal” “schizoid” etc. All data we collect are public posts made between 2008 and 2015 and exclude any message marked as ‘private’ by the author. All use of the data reported in this paper has been approved by the appropriate Institutional Review Board (IRB). Each self-stated diagnosis included in this study was examined by a human annotator (one of the authors) to verify that it appeared to be a genuine statement of a schizophrenia diagnosis excluding jokes quotes or disingenuous statements. We obtained 174 users with an apparently genuine selfstated diagnosis of a schizophrenia-related condition. Note that we cannot be certain that the Twitter user was actually diagnosed with schizophrenia only that their statement of being diagnosed appears to be genuine. Previous work indicates that interannotator agreement for this task is good: κ = 0.77 (Coppersmith et al. 2014a). For each user we obtained a set of their public Twitter posts via the Twitter API collecting up to 3200 tweets.5 As we wish to focus on user-authored content we exclude from analysis all retweets and any tweets that contain a URL (which often contain text that the user did not author). We lowercase all words and convert any non-standard characters (including emoji) to a systematic ASCII representation via Unidecode.6 For our community controls we used randomlyselected Twitter users who primarily tweet in English. Specifically during a two week period in early 2014 each Twitter user who was included in Twitter’s 1% “spritzer” sample had an equal chance for inclusion in our pool of community controls. We then collected some of their historic tweets and assessed the language(s) they tweeted in according to the Chromium Compact Language Detector.7 Users were excluded from our community controls if their tweets were less than 75% English.8,
https://dl.acm.org/doi/abs/10.1145/2700171.2791026,Detecting Changes in Suicide Content Manifested in Social Media Following Celebrity Suicides,Privacy Note,0,Privacy Note: All social media data used in this paper are publicly available. At no time did we contact or interact with a user. Given the sensitive nature of this research we took care to anonymize the data in analysis and presentation so as to minimize any inadvertent disclosure of personal information or information that may reveal cues about an individual’s online identity. Approval was obtained from the relevant institutional review boards.,
https://aclanthology.org/W17-3110.pdf,Small but Mighty: Affective Micropatterns for Quantifying Mental Health from Social Media Language,Data,2,We briefly explain the data collection method here but we refer the interested reader with further questions on the methodology to Coppersmith et al. (2016) for the suicide attempt data and Coppersmith et al. (2014a) for all other conditions. The data for these analyses are Twitter posts collected via two methods. Most of the data come from users who have publicly discussed their mental health conditions. These users are frequently referred to as “self-stated diagnosis” users as they state publicly something like “I was diagnosed with schizophrenia” or “I’m so thankful to have survived my suicide attempt last year”. The data for users with a suicide attempt was supplemented by data from OurDataHelps.org a data donation site where people provide access to their public posts and fill out a short questionnaire about their mental health history. Data are then deidentified and made available to researchers addressing questions of interest to the mental health community. Donors provide consent for their data to be used in mental health research upon signup. Of the users who attempted suicide 146 came from OurDataHelps.org. Specifically we examine generalized anxiety disorder eating disorders panic attacks schizophrenia and attempted suicides. These conditions were selected based on the theory that there are important timing aspects to their symptoms – ebbing and flowing of symptoms as treatment is effective (especially schizophrenia) onset and exacerbation of symptoms by external events and stress and punctuated events in time of psychological symptoms (suicide attempts panic attacks and binging/purging behavior with eating disorders). We use the Twitter streaming API to collect a sample of users who used a series of mental health words or phrases in their tweet text (e.g. ‘schizophrenia‘ or ‘suicide attempt‘). Each tweet that uses one of these phrases is examined via regular expression to indicate that the user is talking about themselves. Finally those tweets that pass the regular expression are examined by a human to confirm (to the best of our ability) that their selfstatement of diagnosis appears to be genuine. This results in a dataset with users that have a self-stated diagnosis of generalized anxiety disorder (n = 2408) an eating disorder (749) panic attacks (263) schizophrenia (350) or someone who would go on to attempt suicide (423). Some of these users do not exhibit the sort of posting behavior required to create micropatterns (i.e. they rarely post multiple times within a 3 hour time window). We exclude these users from our analysis which is 5-9% of users for most conditions with the exception of those with a suicide attempt where a little over half the users do not exhibit this posting behavior. The resultant dataset used for analyses is: generalized anxiety disorder (n = 2271) eating disorders (687) panic attacks (247) schizophrenia (318) suicide attempts (157). In order to allow comparisons of each condition to control users we gather a random sample of 10000 Twitter users for whom at least 75% of their posts are identified by Twitter as English. All the users with a self-stated diagnoses and all members of this control population have their age and gender estimated according to Sap et al. (2014). For each user with a self-stated diagnosis we find a matched control through the following procedure: create a pool of users where the estimated gender matches and the estimated age is within the same 10-year bracket (the suggested accuracy of the age estimator). From that pool of age- and gender- matched users we select the user whose tweets start and end over the most similar timeframe. We will refer to these age- gender- and time-matched controls simply as “matched controls” throughout the rest of this paper. All tweets were publicly posted by their author (i.e. no users marked at “protected” or “private” were included). On average users had 2949 tweets. The distribution of estimated age and genders for users with each self-stated condition can be seen in Figure 1. For most conditions the population skews female though for schizophrenia the genders are roughly balanced. The average age tends to be in the early-to-mid 20s.,
https://dl.acm.org/doi/abs/10.1145/2998181.2998220,Gender and Cross-Cultural Differences in Social Media Disclosures of Mental Illness,Privacy and Ethics,0,Privacy and Ethics. We leverage public data from Twitter and Reddit for our work; hence our work did not qualify for approval from our Institutional Review Board. Nevertheless we took greater care in de-identifying and paraphrasing any content we present as examples to support our investigation. Importantly our work does not make any diagnostic claims about mental illness experiences of the population we study.,
https://www.jmir.org/2019/6/e14199/,Detecting Signs of Depression in Tweets in Spanish: Behavioral and Linguistic Analysis,Data Collection and User Selection,0,The selection of the tweets and their users was based on the filtered real-time streaming support provided by the Twitter API. In the first step we selected the users who showed potential signs of depression on Twitter on the basis of the 20 most frequent words in Spanish expressed by patients suffering from depression in clinical settings. These words were jointly identified and selected by a psychologist and a family physician with clinical experience and were based on the definition and general features of depression according to the Diagnostic and Statistical Manual of Mental Disorders [42]. The list of words used and their English translations are shown in Textbox 1. During June 2018 1470000 tweets including 1 or more occurrences of the words listed in Textbox 1 were collected. From this collection of tweets and to select the users who publicly stated in the textual description associated to their profile that they suffered from depression all the profile descriptions including 1 or more occurrences of the word “depr” and all the possible derivations related to the word depression in Spanish such as “depre” “depresión” “depresivo” “depresiva” “deprimido” and “deprimida” were considered. From the 720 users who included 1 or more of these words in their description profile 90 users who stated they suffered from depression or were receiving treatment for depression were selected for the analysis. This selection was performed by a psychologist verifying that the statements were related to real expressions of depression excluding quotes jokes or fake ones. For each of these depressed Twitter users we collected all the most recent tweets from their timeline up to a maximum of about 3200 tweets. Thus a total of 189669 tweets were collected a figure that was reduced to 140946 after discarding the retweets. These 140946 tweets constituted the depressive users dataset. Examples of sentences appearing in the user profiles that were used for selecting the depressive users are: “Paciente psiquiátrico con depresión crónica” (Psychiatric patient with chronic depression; example of a profile sentence that indicates depression). “Colecciono errores traducidos a tweets depresivos y a uno que otro impulso de amor” (I gather errors translated into depressing tweets and into one or another love impulse; example of a profile sentence that does not indicate depression). Once the users with profile sentences indicating depression had been retrieved their Twitter timelines were collected. Only those users having in their timeline at least 10 tweets that suggested signs of depression were retained for further analyses. For each user the selection of these tweets was performed by manually inspecting the tweets of the user’s complete timeline in reverse temporal order starting from the most recent one to the oldest tweet of the timeline retrieved by means of the Twitter API . Finally a total number of 1000 tweets issued by the 90 depressive users suggesting signs of depression were detected and used for the analysis. This set of tweets provided us with the depressive tweets dataset which was used to analyze linguistic features of tweets showing signs of depression. It has to be mentioned that these 1000 tweets were not to be included in the depressive users dataset (see Figure 1). At the same time more than 97500000 tweets were also collected in June 2018: such tweets were gathered by listening to the public Twitter stream during this time span by only considering tweets with Spanish textual contents (as detected by Twitter language identification support). Given that Twitter requires more restrictive filters than just the language of the tweets we used a list of the most frequently used Spanish words (stopwords) to retrieve all tweets that included 1 or more of these words. The vast majority of Spanish tweets should match this criterion. A sample of 450 users who did not mention in their profile the word depression and its derivations were selected randomly from the 97500000 tweets. The complete timelines of these users were compiled (1141021 tweets) which were reduced to 712589 once retweets were removed. These 712589 tweets constituted the control dataset. To identify the language of a tweet we relied on the language automatically identified by Twitter for each tweet selecting tweets in Spanish. It has to be noted that these data can contain some tweets from unidentified depressive users.,
https://aclanthology.org/W18-0608.pdf,Cross-cultural differences in language markers of depression online,Data Collection,2,Data was collected from 7 Cups of Tea an anonymous online chat-based peer support community for emotional distress1 . Users agree at signup that their data may be used for the purposes of research. All the data used for the current study was anonymous and securely stored. This research was performed in line with the ethical and privacy protocols outlined in detail in (Benton et al. 2017). Data from 7 Cups takes the form of written dialogue between users of the service and volunteers who are trained as “active listeners”. A fragment of an exchange between the user of the service (U) and the volunteer (V) might go as follows: For the analyses reported in this paper we used only text generated by users of the service not the volunteers providing peer support. Users who reported depression as their primary concern at sign up were eligible for inclusion in analyses. Our original sample was comprised of 23048 conversations involving 1937 unique users. Users were excluded from the sample if they did not indicate their culture or if they selected ‘Other’. This resulted in the exclusion of 199 and 130 users respectively. The original sample also included users identifying as Native American or American Indian. This group was excluded from analyses since the majority of the data among these users was not English. This resulted in the removal of 15 users leaving a total sample size of 1593.,
https://dl.acm.org/doi/pdf/10.1145/3359169,Cross-Cultural Differences in the Use of Online Mental Health Support Forums,Privacy Ethics and Disclosure,2,All data analyzed in this study was sourced (with license and consent) from the Talklife and 7Cups platforms. Additionally to maintain user anonymity all personally identifiable information was removed from the dataset before any findings were reported. Additionally all work was approved by our institution’s Institutional Review Board.,
https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-018-2197-z,Automatic extraction of informal topics from online suicidal ideation,Ethics Declarations,1,Ethics approval and consent to participate Not applicable.,
https://ieeexplore.ieee.org/abstract/document/8609647,Online Social Networks in Health Care: A Study of Mental Disorders on Reddit,Dataset,0,Reddit is a multilingual Online Social Network founded in 2005 and organized in subcommunities by areas of interest called subreddits. We obtained data from the Reddit's data repository4 focusing on four subreddits where people discuss issues related to mental heath disorders: Depression (/r/depression) Suicide Watch (/r/Suicide Watch) Anxiety (/r/anxiety) and Bipolar (/r/bipolar). Our dataset is comprised of user activities (posts and comments) that took place between 2011 and 201 7. Here we focus on data from January 2017 to December 2017. In total we obtained 261511 posts and 1256669 comments from 184708 unique users. Table I shows the total number of users posts and comments per subreddit. The total number of comments in each community is at least 4.2 times larger than the number of posts which suggests a supportive behavior among users.,