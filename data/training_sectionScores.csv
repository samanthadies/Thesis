Link to paper,doc_num,section_num,section_name,researchers,num_researchers,choudhury,drezde,coppersmith,year,text,data_source_i,class_collection_i,ground_truth_size_i,ground_truth_discussion_i,random_sample_i,replication_i,dem_dist_i,informed_consent_i,data_public_i,irb_i,human_subject_protection_i,limitations_i,preprocess_anonymity_i,preprocess_data_quality_i,preprocess_missing_values_i,preprocess_text_i,preprocess_feature_reconstruction_i,preprocess_i,ethics_section_i
https://link.springer.com/content/pdf/10.1140/epjds/s13688-017-0110-z.pdf,1,1,introduction,"Reece, Danforth",2,0,0,0,2017,The advent of social media presents a promising new opportunity for early detection and intervention in psychiatric disorders. Predictive screening methods have successfully analyzed online media to detect a number of harmful health conditions [–]. All of these studies relied on text analysis  however  and none have yet harnessed the wealth of psychological data encoded in visual social media  such as photographs posted to Instagram. In this report  we introduce a methodology for analyzing photographic data from Instagram to predictively screen for depression. There is good reason to prioritize research into Instagram analysis for health screening. Instagram members currently contribute almost million new posts per day []  and Instagram’s rate of new users joining has recently outpaced Twitter  YouTube  LinkedIn  and even Facebook []. A nascent literature on depression and Instagram use has so far either yielded results that are too general or too labor-intensive to be of practical significance for predictive analytics [  ]. In particular  Lup et al. [] only attempted to correlate Instagram usership with depressive symptoms  and Andalibi et al. [] employed a time-consuming qualitative coding method which the authors acknowledged made it ‘impossible to qualitatively analyze’ Instagram data at scale (p.). In our research  we incorporated an ensemble of computational methods from machine learning  image processing  and other data-scientific disciplines to extract useful psychological indicators from photographic data. Our goal was to successfully identify and predict markers of depression in Instagram users’ posted photographs. Hypothesis Instagram posts made by individuals diagnosed with depression can be reliably distinguished from posts made by healthy controls  using only measures extracted computationally from posted photos and associated metadata.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://link.springer.com/content/pdf/10.1140/epjds/s13688-017-0110-z.pdf,1,2,photographic markers of depression,"Reece, Danforth",2,0,0,0,2017,Photographs posted to Instagram offer a vast array of features that might be analyzed for psychological insight. The content of photographs can be coded for any number of characteristics: Are there people present? Is the setting in nature or indoors? Is it night or day? Image statistical properties can also be evaluated at a per-pixel level  including values for average color and brightness. Instagram metadata offers additional information: Did the photo receive any comments? How many ‘Likes’ did it get? Finally  platform activity measures  such as usage and posting frequency  may also yield clues as to an Instagram user’s mental state. We incorporated only a narrow subset of possible features into our predictive models  motivated in part by prior research into the relationship between mood and visual preferences. In studies associating mood  color  and mental health  healthy individuals identified darker  grayer colors with negative mood  and generally preferred brighter  more vivid colors [–]. By contrast  depressed individuals were found to prefer darker  grayer colors []. In addition  Barrick  Taylor  & Correa [] found a positive correlation between self-identification with depression and a tendency to perceive one’s surroundings as gray or lacking in color. These findings motivated us to include measures of hue  saturation  and brightness in our analysis. We also tracked the use of Instagram filters  which allow users to modify the color and tint of a photograph. Depression is strongly associated with reduced social activity [  ]. As Instagram is used to share personal experiences  it is reasonable to infer that posted photos with people in them may capture aspects of a user’s social life. On this premise  we used a face detection algorithm to analyze Instagram posts for the presence and number of human faces in each photograph. We also counted the number of comments and likes each post received as measures of community engagement  and used posting frequency as a metric for user engagement.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://link.springer.com/content/pdf/10.1140/epjds/s13688-017-0110-z.pdf,1,3,early screening applications,"Reece, Danforth",2,0,0,0,2017,Hypothesis is a necessary first step  as it addresses an unanswered basic question: Is depression detectable in Instagram posts? On finding support for Hypothesis   a natural question arises: Is depression detectable in Instagram posts  before the date of first diagnosis? After receiving a depression diagnosis  individuals may come to identify with their diagnosis [  ]. Individuals’ self-portrayal on social media may then be influenced by this identification. It is possible that a successful predictive model  trained on the entirety of depressed Instagram users’ posting histories  might not actually detect depressive signals  per se  but rather purposeful content choices intended to convey a depressive condition. Training a model using only posts made by depressed participants prior to the date of first diagnosis addresses this potential confounding factor. Hypothesis Instagram posts made by depressed individuals prior to the date of first clinical diagnosis can be reliably distinguished from posts made by healthy controls. If support is found for Hypothesis   this would not only demonstrate a methodological advance for researchers  but also serve as a proof-of-concept for future healthcare applications. As such  we benchmarked the accuracy of our model against the ability of general practitioners to correctly diagnose depression as shown in a meta-analysis by Mitchell  Vaze  and Rao []. The authors analyzed studies that evaluated general practitioners’ abilities to correctly diagnose depression in their patients  without assistance from scales  questionnaires  or other measurement instruments. Out of   patient outcomes included across the pooled studies  .% were actually depressed  as evaluated separately by psychiatrists or validated interview-based measures conducted by researchers. General practitioners were able to correctly rule out depression in non-depressed patients % of the time  but only diagnosed depressed patients correctly % of the time. We refer to these meta-analysis findings [] as a comparison point to evaluate the usefulness of our models. A major strength of our proposed models is that their features are generated using entirely computational means - pixel analysis  face detection  and metadata parsing - which can be done at scale  without additional human input. It seems natural to wonder whether these machine-extracted features pick up on similar signals that humans might use to identify mood and psychological condition  or whether they attend to wholly different information. A computer may be able to analyze the average saturation value of a million pixels  but can it pick out a happy selfie from a sad one? Understanding whether machine learning and human opinion are sensitive to the same indicators of depression may be valuable information for future research and applications. Furthermore  insight into these issues may help to frame our results in the larger discussion around human versus machine learning  which occupies a central role in the contemporary academic landscape. To address these questions  we solicited human assessments of the Instagram photographs we collected. We asked new participants to evaluate photos on four simple metrics: happiness  sadness  interestingness  and likability. These ratings categories were intended to capture human impressions that were both intuitive and quantifiable  and which had some relationship to established depression indicators. DSM-IV [] criteria for Major Depressive Disorder includes feeling sad as a primary criterion  so sadness (and its anti-correlate  happiness) seemed obvious candidates as ratings categories. Epstein et al. [] found depressed individuals ‘had difficulty reconciling a self-image as an ‘outgoing likeable person’’  which prompted likability as an informative metric. We hypothesized that human raters should find photographs posted by depressed individuals to be sadder  less happy  and less likable  on average. Finally  we considered interestingness as a novel factor  without a clear directional hypothesis. Hypothesis a Human ratings of Instagram posts on common semantic categories can distinguish between posts made by depressed and healthy individuals. Hypothesis b Human ratings are positively correlated with computationally-extracted features. If human and machine predictors show positive correlation  we can infer that each set of features tracks similar signals of depression. In this case  the strength of the human model simply suggests whether it is better or worse than the machine model. On the other hand  if machine and human features show little or no correlation  then regardless of human model performance  we would know that the machine features are capable of screening for depression  but use different information signals than what are captured by the affective ratings categories.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://link.springer.com/content/pdf/10.1140/epjds/s13688-017-0110-z.pdf,1,4,data collection,"Reece, Danforth",2,0,0,0,2017,Data collection was crowdsourced using Amazon’s Mechanical Turk (MTurk) crowdwork platform. Separate surveys were created for depressed and healthy individuals. In the depressed survey  participants were invited to complete a survey that involved passing a series of inclusion criteria  responding to a standardized clinical depression survey  answering questions related to demographics and history of depression  and sharing social media history. We used the CES-D (Center for Epidemiologic Studies Depression Scale) questionnaire to screen participant depression levels []. CES-D assessment quality has been demonstrated as on-par with other depression inventories  including the Beck Depression Inventory and the Kellner Symptom Questionnaire [  ]. Healthy participants were screened to ensure no history of depression and active Instagram use. See Additional file for actual survey text. Qualified participants were asked to share their Instagram usernames and history. An app embedded in the survey allowed participants to securely log into their Instagram accounts and agree to share their data.b Upon securing consent  we made a one-time collection of participants’ entire Instagram posting history. In total we collected   photographs from Instagram users  of whom had a history of depression. We asked a different set of MTurk crowdworkers to rate the Instagram photographs collected. This new task asked participants to rate a random selection of photos from the data we collected. Raters were asked to judge how interesting  likable  happy  and sad each photo seemed  on a continuous - scale. Each photo was rated by at least three different raters  and ratings were averaged across raters. Raters were not informed that photos were from Instagram  nor were they given any information about the study participants who provided the photos  including mental health status. Each ratings category showed good inter-rater agreement. Only a subset of participant Instagram photos were rated (N =  ). We limited ratings data to a subset because this task was time-consuming for crowdworkers  and so proved a costly form of data collection. For the depressed sample  ratings were only made for photos posted within a year in either direction of the date of first depression diagnosis. Within this subset  for each user the nearest posts prior to the diagnosis date were rated. For the control population  the most recent photos from each user’s date of participation in this study were rated.,2,2,2,2,2,0,0,2,0,0,,0,0,2,0,0,0,,0
https://link.springer.com/content/pdf/10.1140/epjds/s13688-017-0110-z.pdf,1,5,participant safety and privacy,"Reece, Danforth",2,0,0,0,2017,Data privacy was a concern for this study. Strict anonymity was nearly impossible to guarantee to participants  given that usernames and personal photographs posted to Instagram often contain identifiable features. We made sure participants were informed of the risks of being personally identified  and assured them that no data with personal identifiers  including usernames  would be made public or published in any format.,0,0,0,0,0,0,0,0,0,0,,0,2,0,0,0,0,,1
https://link.springer.com/content/pdf/10.1140/epjds/s13688-017-0110-z.pdf,1,6,improving data quality,"Reece, Danforth",2,0,0,0,2017,We employed several quality assurance measures in our data collection process to reduce noisy and unreliable data. Our surveys were only visible to MTurk crowdworkers who had completed at least previous tasks with a minimum % approval rating; MTurk workers with this level of experience and approval rating have been found to provide reliable  valid survey responses []. We also restricted access to only American IP addresses  as MTurk data collected from outside the United States are generally of poorer quality []. All participants were only permitted to take the survey once. We excluded participants who had successfully completed our survey  but who had a lifetime total of fewer than five Instagram posts. We also excluded participants with CESD scores of or higher. Studies have indicated that a CES-D score of represents an optimal cutoff for identifying clinically relevant depression across a range of age groups and circumstances [  ].,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://link.springer.com/content/pdf/10.1140/epjds/s13688-017-0110-z.pdf,1,7,feature extraction,"Reece, Danforth",2,0,0,0,2017,Several different types of information were extracted from the collected Instagram data. We used total posts per user  per day  as a measure of user activity. We gauged community reaction by counting the number of comments and ‘likes’ each posted photograph received. Face detection software was used to determine whether or not a photograph contained a human face  as well as count the total number of faces in each photo  as a proxy measure for participants’ social activity levels. Pixel-level averages were computed for Hue  Saturation  and Value (HSV)  three color properties commonly used in image analysis. Hue describes an image’s coloring on the light spectrum (ranging from red to blue/purple). Lower hue values indicate more red  and higher hue values indicate more blue. Saturation refers to the vividness of an image. Low saturation makes an image appear grey and faded. Value refers to image brightness. Lower brightness scores indicate a darker image. See Figure for a comparison of high and low HSV values. We also checked metadata to assess whether an Instagram-provided filter was applied to alter the appearance of a photograph. Collectively  these measures served as the feature set in our primary model. For the separate model fit on ratings data  we used only the four ratings categories (happy  sad  likable  interesting) as predictors.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,2,,0
https://link.springer.com/content/pdf/10.1140/epjds/s13688-017-0110-z.pdf,1,8,units of observation,"Reece, Danforth",2,0,0,0,2017,In determining the best time span for this analysis  we encountered a difficult question: When and for how long does depression occur? A diagnosis of depression does not indicate the persistence of a depressive state for every moment of every day  and to conduct analysis using an individual’s entire posting history as a single unit of observation is therefore rather specious. At the other extreme  to take each individual photograph as units of observation runs the risk of being too granular. De Choudhury et al. [] looked at all of a given user’s posts in a single day  and aggregated those data into per-person  per-day units of observation. We adopted this precedent of ‘user-days’ as a unit of analysis.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://link.springer.com/content/pdf/10.1140/epjds/s13688-017-0110-z.pdf,1,9,statistical framework,"Reece, Danforth",2,0,0,0,2017,We used Bayesian logistic regression with uninformative priors to determine the strength of individual predictors. Two separate models were trained. The All-data model used all collected data to address Hypothesis . The Pre-diagnosis model used all data collected from healthy participants  but only pre-diagnosis data from depressed participants  to address Hypothesis . We also fit an ‘intercept-only’ model  in which all predictors are zero-weighted to simulate a model under a null hypothesis. Bayes factors were used to assess model fit. Details on Bayesian estimation  model optimization and selection  and diagnostic checks are available in Additional file . We also employed a suite of supervised machine learning algorithms to estimate the predictive capacity of our models.We report prediction results only from the best-performing algorithm  a -tree Random Forests classifier. As an informal benchmark for comparison  we present general practitioners’ unassisted diagnostic accuracy as reported in Mitchell  Vaze  and Rao [].d In evaluating binary classification accuracy  a simple proportion of correct classifications is often inappropriate. In cases where data exhibit a class imbalance  i.e. more healthy than depressed observations (or vice-versa)  reporting naive accuracy can be misleading. (A classification accuracy of % seems excellent until it is revealed that % of the data modeled belong to a single class.) Additionally  naive accuracy scores are opaque to the specific strengths and weaknesses of a binary classifier. Instead  we report precision  recall  specificity  negative predictive value  and F scores for fuller context. Definitions for these terms are as follows:,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://link.springer.com/content/pdf/10.1140/epjds/s13688-017-0110-z.pdf,1,10,results,"Reece, Danforth",2,0,0,0,2017,Both All-data and Pre-diagnosis models were decisively superior to a null model (Kall = .; Kpre = .)  see page of the Additional file for a description of K. All-data predictors were significant with % probability. Pre-diagnosis and All-data confidence levels were largely identical  with two exceptions: Pre-diagnosis Brightness decreased to % confidence  and Pre-diagnosis posting frequency dropped to % confidence  suggesting a null predictive value in the latter case. Increased hue  along with decreased brightness and saturation  predicted target class observations. This means that photos posted by depressed individuals tended to be bluer  darker  and grayer (see Figure ). The more comments Instagram posts received  the more likely they were posted by depressed participants  but the opposite was true for likes received. In the All-data model  higher posting frequency was also associated with depression. Depressed participants were more likely to post photos with faces  but had a lower average face count per photograph than healthy participants. Finally  depressed participants were less likely to apply Instagram filters to their posted photos. Figure shows the magnitude and direction of regression coefficients for both models. A closer look at filter usage in depressed versus healthy participants provided additional texture. Instagram filters were used differently by target and control groups (χ all = .  p = . × –; χ pre = .  p = . × –). In particular  depressed participants were less likely than healthy controls to use any filters at all. When depressed participants did employ filters  they most disproportionately favored the ‘Inkwell’ filter  which converts color photographs to black-and-white images (see Figure ). Conversely  healthy participants most disproportionately favored the Valencia filter  which lightens the tint of photos. Examples of filtered photographs are provided in Additional file . Our best All-data machine learning classifier  averaged over five randomized iterations  improved over Mitchell et al. [] general practitioner accuracy on most metrics (see Table ). Compared with Mitchell et al. [] results  the All-data model was less conservative (lower specificity) but better able to positively identify target class observations (higher recall). Given observations  our model correctly identified % of all target class cases (n = )  with a relatively low number of false alarms (n = ) and misses (n = ). Pre-diagnosis predictions showed improvement over the Mitchell et al. [] benchmark on precision and specificity. The Pre-diagnosis model found only about a third of actual target class observations  but it was correct most of the time when it did predict a target class label. By comparison  although Mitchell et al. [] general practitioners discovered more true cases of depression  they were more likely than not to misdiagnose healthy subjects as depressed. Out of the four predictors used in the human ratings model (happiness  sadness  likability  interestingness)  only the sadness and happiness ratings were significant predictors of depression. Depressed participants’ photos were more likely to be sadder and less happy than those of healthy participants. Ratings assessments generally showed strong patterns of correlation with one another  but exhibited extremely low correlation with computational features. The modest positive correlation of human-rated happiness with the presence and number of faces in a photograph was the only exception to this trend. Correlation matrices for all models are available in Additional file .,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://link.springer.com/content/pdf/10.1140/epjds/s13688-017-0110-z.pdf,1,11,discussion,"Reece, Danforth",2,0,0,0,2017,The present study employed computational machine learning techniques to screen for depression using photographs posted to Instagram. Our results supported Hypothesis   that markers of depression are observable in Instagram user behavior  and Hypothesis   that these depressive signals are detectable in posts made even before the date of first diagnosis. Human ratings proved capable of distinguishing between Instagram posts made by depressed and healthy individuals (Hypothesis a)  but showed little or no correlation with most computational features (Hypothesis b). Our findings establish that visual social media data are amenable to analysis of affect using scalable  computational methods. One avenue for future research might integrate textual analysis of Instagram posts’ comments  captions  and tags. Considering the early success of textual analysis in detecting various health and psychological signals on social media [    ]  the modeling of textual and visual features together could well prove superior to either medium on its own. Our model showed considerable improvement over the ability of unassisted general practitioners to correctly diagnose depression. On average  more than half of general practitioners’ depression diagnoses were false positives []. By comparison  the majority of both All-data and Pre-diagnosis depression classifications were correct. As false diagnoses are costly for both healthcare programs and individuals  this improvement is noteworthy. Health care providers may be able to improve quality of care and better identify individuals in need of treatment based on the simple  low-cost methods outlined in this report. Given that mental health services are unavailable or underfunded in many countries []  this computational approach  requiring only patients’ digital consent to share their social media histories  may open avenues to care which are currently difficult or impossible to provide. On the other hand  our Pre-diagnosis prediction engine was rather conservative  and tended to classify most observations as healthy. There is good reason to believe  however  that the Pre-diagnosis prediction accuracy observed represents a lower bound on performance. Ideally  we would have used the All-data classifier to evaluate the Pre-diagnosis data  as that model was trained on a much larger dataset. The fact that the Pre-diagnosis data was a subset of the full dataset meant that applying the All-data model to Prediagnosis observations would have artificially inflated accuracy  due to information leakage between training and test data. Instead  we trained a new classifier for Pre-diagnosis  using training and test partitions contained within the Pre-diagnosis data  which left the Pre-diagnosis model with considerably fewer data points to train on. As a result  it is likely that Pre-diagnosis accuracy scores understate the technique’s true capacity. Regarding the strength of specific predictive features  some results match common perceptions regarding the effects of depression on behavior. Photos posted to Instagram by depressed individuals were more likely to be bluer  grayer  and darker  and receive fewer likes. Depressed Instagram users in our sample had an outsized preference for filtering out all color from posted photos  and showed an aversion to artificially lightening photos  compared to non-depressed controls. These results matched well with the literature linking depression and a preference for darker  bluer  and monochromatic colors [–]. Depressed users were more likely to post photos with faces  but they tended to post fewer faces per photo. This finding may be an oblique indicator that depressed users interact in smaller social settings  or at least choose only to share experiences of this sort on social media. This would be in accordance with previous findings that reduced social interactivity is an indicator of depression [    ]. Other  seemingly obvious  relationships failed to emerge. For example  when people rated a photograph as sad  that impression was unrelated to how blue  dark  or gray that photo was. Both ‘sad’ and ‘blue  dark  and gray’ were strong predictors of depression  however  and semantically these descriptions seem like they should match well with one another  as well as link to depression. These divergences may serve as the basis for a number of future research inquiries into the relationship between depressive behavior and common perceptions of depression. A general limitation to these findings concerns the non-specific use of the term ‘depression’ in the data collection process. We acknowledge that depression describes a general clinical status  and is frequently comorbid with other conditions. It is possible that a specific diagnostic class is responsible for driving the observed results  and future research should fine-tune questionnaires to acquire specific diagnostic information. Additionally  it is possible that our results are in some way specific to individuals who received clinical diagnoses. Current perspectives on depression treatment indicate that people who are ‘well-informed and psychologically minded  experience typical symptoms of depression and little stigma  and have confidence in the effectiveness of treatment  few concerns about side effects  adequate social support  and high self-efficacy’ seek out mental health services []. The intersection of these qualities with typical Instagram user demographics suggests caution in making broad inferences  based on our findings. As these methods provide a tool for inferring personal information about individuals  two points of caution should be considered. First  data privacy and ethical research practices are of particular concern  given recent admissions that individuals’ social media data were experimentally manipulated or exposed without permission [  ]. It is perhaps reflective of a current general skepticism towards social media research that  of the individuals who began our survey  (%) refused to share their Instagram data  even after we provided numerous privacy guarantees. Future research should prioritize establishing confidence among experimental participants that their data will remain secure and private. Second  data trends often change over time  leading socio-technical models of this sort to degrade without frequent calibration []. The findings reported here should not be taken as enduring facts  but rather as promising leads upon which to build and refine subsequent models. Paired with a commensurate focus on upholding data privacy and ethical analytics  the present work may serve as a blueprint for effective mental health screening in an increasingly digitalized society. More generally  these findings support the notion that major changes in individual psychology are transmitted in social media use  and can be identified via computational methods.,0,0,0,0,0,0,0,2,0,0,,1,0,0,0,0,0,,0
https://link.springer.com/content/pdf/10.1140/epjds/s13688-017-0110-z.pdf,1,12,additional material,"Reece, Danforth",2,0,0,0,2017,The authors thank K Lix for conversations and manuscript review. CMD acknowledges funding from the National Science Foundation under Grant No. IIS-1447634. AGR acknowledges support from the Sackler Scholar Programme in Psychobiology Code associated with this study is available publicly on the github page of AGR: https://github.com/andrewreece/predicthealth. This study was reviewed and approved by the Harvard University Institutional Review Board  approval #15-2529 and by the University of Vermont Institutional Review Board  approval #CHRMS-16-135. The authors declare that they have no competing interests. N/A AGR and CMD designed the study. AGR performed the study and analyzed results. AGR and CMD authored the manuscript.,0,0,0,0,0,2,0,0,0,2,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/2858036.2858207 ,2,1,abstract,"De Choudhury, Kiciman, Drezde, Coppersmith, Kumar",5,1,1,1,2016,History of mental illness is a major factor behind suicide risk and ideation. However research efforts toward characterizing and forecasting this risk is limited due to the paucity of information regarding suicide ideation  exacerbated by the stigma of mental illness. This paper fills gaps in the literature by developing a statistical methodology to infer which individuals could undergo transitions from mental health discourse to suicidal ideation. We utilize semi-anonymous support communities on Reddit as unobtrusive data sources to infer the likelihood of these shifts. We develop language and interactional measures for this purpose  as well as a propensity score matching based statistical approach. Our approach allows us to derive distinct markers of shifts to suicidal ideation. These markers can be modeled in a prediction framework to identify individuals likely to engage in suicidal ideation in the future. We discuss societal and ethical implications of this research.,0,0,0,0,0,0,0,0,0,0,,0,2,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/2858036.2858207 ,2,2,introduction,"De Choudhury, Kiciman, Drezde, Coppersmith, Kumar",5,1,1,1,2016,A central challenge in public health revolves around how to identify individuals who are at risk for taking their own lives [3  8  66]. One of the ten leading causes of death in the United States  suicide represents 1.4% of the total number of adult deaths1 . Yet suicide prevention remains difficult. Suicidal acts are multifactorial events [65]  and different categories of suicidal behavior have different pathogenesis  expression  and often an underlying mental illness [66]. Extending appropriate clinical and psychiatric care to suicidal patients relies heavily on identifying those at risk [29]. Suicidal ideation is defined as tendencies and cognitions related to ending one’s life  ranging from the thought that life is not worth living  through concrete plans for killing oneself  to an intense delusional preoccupation with self-destruction [5]. Therefore  immense scientific and practical value lies in being able to understand the intensity  pervasiveness  and characteristics of the ideation  since this may predict later suicide risk or attempt [6]. Mental illness is a major risk factor of suicide — 80% of those who attempt or die by suicide are known to have had some form of mental illness [67]. However  the majority of those challenged by mental illness do not engage in suicidal ideation [3]. Hence  prior literature in cognitive and clinical psychology [26] has underscored the understanding specific “suicidogenic” elements in manifestations of mental illness. Existing efforts toward discovering and recognizing suicidogenic elements have primarily been through the examination of psychological  psychiatric  and demographic variables of individuals [41  4]. However these assessments face two significant methodological challenges: (1) In many studies  data is collected after the suicide attempt or completed suicide  providing “postdictors” rather than predictors of suicidal behavior and are therefore prone to include hindsight bias; and (2) the relatively rare occurrence of completed suicides and the stigma associated with suicide reporting in the general population has made studies challenging and expensive to conduct  additionally requiring extremely long follow-up intervals. Consequently  there is limited research on examining factors associated with the development of future suicidal thoughts among mental illness prone populations [5].,0,0,0,0,0,0,0,0,0,0,,2,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/2858036.2858207 ,2,3,contributions,"De Choudhury, Kiciman, Drezde, Coppersmith, Kumar",5,1,1,1,2016,This paper proposes social media as a way to characterize and predict shifts from discussion of mental health content to expression of suicidal ideation. We focus on a popular discussion-oriented social media site  Reddit  specifically several mental health and suicide support communities. Due to the semi-anonymous nature of these communities [51]  the content shared by individuals allows us to obtain high quality  self-reported data around mental health concerns and suicidal ideation [31]. The central research question investigated in this paper involves: Can we forecast whether an individual engaged in mental health discussions would  in the future  discuss suicidal ideation? Towards this goal  we make the following two contributions: (1) We characterize participants in Reddit’s mental health communities who go on to post on the platform’s suicide support forum using a number of linguistic and social interaction based measures that have been known to characterize an individual’s behavioral and psychological state [65]. (2) We propose the novel application of propensity score matching to explore how users may share suicidal ideation content in the future  while controlling for the historical use of linguistic constructs of mental health. The challenges of interpreting correlational statistics from observational studies like ours is well-recognized [1  72]. Through statistical analysis methods developed for causal inference  we isolate the effects of linguistic constructs from observed confounding factors  and are able to derive valuable insights into factors related to future suicide ideation.,0,0,0,0,0,0,0,0,0,0,,0,2,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/2858036.2858207 ,2,4,findings,"De Choudhury, Kiciman, Drezde, Coppersmith, Kumar",5,1,1,1,2016,From a population of individuals who post about mental health concerns on Reddit  we examine differences between those who proceed to discuss suicidal ideation in the future  from those who do not. We identify changes in linguistic structures  interpersonal awareness  social interaction and content between these two groups  some of which align with findings in the suicide literature [5]. Specifically  we observe transition to suicidal ideation to be associated with psychological states like heightened self-attentional focus  poor linguistic coherence and linguistic coordination with the community  reduced social engagement  and manifestation of hopelessness  anxiety  impulsiveness and loneliness. Finally  we examine whether we can automatically predict the tendency of individuals discussing mental health concerns to engage in these characteristic behaviors. For this purpose  we develop a logistic regression classifier that yields high accuracy. We situate our findings in the cognitive psychological integrative model of suicide [25] in order to derive qualitative interpretations  and discuss the implications of our work for HCI research  design and ethics as well as for developing timely interventions.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/2858036.2858207 ,2,5,privacy ethics and disclosure,"De Choudhury, Kiciman, Drezde, Coppersmith, Kumar",5,1,1,1,2016,We use public data from Reddit. Personally identifiable information was removed and content was de-identified and paraphrased before being reported in the paper for exemplary purposes. This work has been approved by the appropriate Institutional Review Board (IRB). Our work does not make any diagnostic claims related to mental illness or suicide.,0,0,0,0,0,0,0,0,2,2,,0,0,0,0,0,0,,2
https://dl.acm.org/doi/abs/10.1145/2858036.2858207 ,2,6,mental illness and suicide,"De Choudhury, Kiciman, Drezde, Coppersmith, Kumar",5,1,1,1,2016,A number of mental health disorders  such as depression  tend to be closely related to suicide [65]. Reportedly  one in six patients who fall under the category of major depression as set forth in the Diagnostic and Statistical Manual of Mental Disorders (DSM) [2] dies as a result of suicide [11]. Literature in psychology has suggested the need to identify specific attributes of mental illnesses that relate to increased likelihood of suicidal thoughts [65  52]. Nock and Kazdin [48] found that cognitive factors associated with depression are of greater importance than the affective dimension of depression in predicting suicide-related outcomes. Another strong correlation exists between affective disorders  attempted suicide  and borderline personality disorder [14]. Kashden et al. [37]  who compared non-suicidal and suicidal psychiatric inpatients to community high school students  found suicidal inpatients to be characterized by impulsivity  hopelessness  and depression. Further  in a study by Lewinsohn et al. [42]  the diagnoses with the strongest association with suicide attempts among young adults were combinations of depressive disorder with substance use  disruptive behavior  or anxiety (also see [63]). Broadly  researchers have identified three stages leading to suicidal ideation among individuals with some form of mental illness [4  66]: a) thinking  b) ambivalence  and c) decision making. Together  these stages define the cognitive psychological integrative model of suicide [55  29]  wherein the thinking stage may include thoughts of hopelessness  selfhatred  distress and anxiety; ambivalence relates to lowered self-esteem  regulation and reduced social cohesion; and decision making involves aggression and explicit plans of taking one’s life [59]. Individuals may seek help  advice and support on mental health related social media forums during any of these stages  and thus these forums provide a non-reactive and non-intrusive way to measure risk factors of suicidal ideation among individuals vulnerable to different mental illnesses.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/2858036.2858207 ,2,7,mental health and suicide studies on social media,"De Choudhury, Kiciman, Drezde, Coppersmith, Kumar",5,1,1,1,2016,In recent years  social media has been recognized to be a powerful “lens” that can provide insights into psychological states  health and well-being of individuals and populations [50  19]. Linguistic attributes of shared content and social interactional patterns have been utilized to understand and infer risk to major depressive disorder [24  49  32  16  60  70]  postpartum depression [21  22]  addiction [47  44]  and other mental health concerns [35  18  17  46]. Since social media is recorded in the present and preserved  it minimizes the hindsight bias sometimes induced by retrospective analyses. The rich repository of social media data also allows for the discovery  tracking  and perhaps forecasting of risk attributes longitudinally. Beyond observation and insight  social media may also provide mechanisms through which timely support may be extended to vulnerable communities. There exists some research examining suicide in social media [58  43  15  9  28  38]. Authors in [71] focused on South Korean blogs to predict nationwide suicide rate data (also see [36] for a similar study in the US context). Studying linguistic features of suicidal ideation  authors in [68] surveyed a sample of Twitter users to examine the association between suicide-related tweets and suicidal behavior. However bulk of this prior work focused at macro-level trends (e.g.  national suicide rates) or examined differences between suicide-related content and general content shared on social media. To the best of our knowledge  there has not been prior work forecasting likelihood of suicidal ideation in an individual based on mental health discussions on social media. It is important to identify and differentiate how and which social media markers of mental health concerns may relate to future suicidal ideation  given the important temporal link between history of mental illness and future suicide risk. Further  mental illness is associated with vulnerability  hence identifying markers that may indicate increased suicidal thoughts in the future may help the deployment of appropriate interventions. Our paper builds on this emergent body of research by analyzing data shared on mental health communities in Reddit  to probe attributes of individuals contemplating suicide in the future. We additionally note that a major hurdle in studying suicide-related issues in the computing field has been the lack of appropriate ground truth data on individuals actually suffering from suicidal thoughts. The social stigma associated with such sensitive disclosure may further prevent individuals from self-reporting their condition on social media. In our work  we partially address this challenge by studying semi-anonymous communities on Reddit where vulnerable individuals voluntarily participate for seeking help and support. Finally  while existing work has often relied on lexicon matching or phrase identification techniques to identify correlational attributes of interest such as suicide intent or mental illness risk  in this paper we infer more robust insights by extending current methodology with causal inference techniques based on propensity score matching,0,0,0,0,0,0,0,0,0,0,,2,2,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/2858036.2858207 ,2,8,data,"De Choudhury, Kiciman, Drezde, Coppersmith, Kumar",5,1,1,1,2016,We begin with a brief description of the features of Reddit  which are important to understand the context of our research problem. Reddit has many of the characteristics of an online forum; users or “redditors” can submit content in the form of link posts or text posts. Posts are organized by areas of interest or sub-communities called “subreddits”. Besides posting  redditors can also engage via “upvoting” or “downvoting” a post  or responding on a post through comments.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,1,,0
https://dl.acm.org/doi/abs/10.1145/2858036.2858207 ,2,9,data collection,"De Choudhury, Kiciman, Drezde, Coppersmith, Kumar",5,1,1,1,2016,We obtained post and comment data from a number of mental health subreddits (henceforth MHs) and a suicide support subreddit “r/SuicideWatch” (henceforth SW). We focused on a set of 14 MHs that have been examined in prior work on mental health discourse [51  39]. These subreddits included r/depression  r/mentalhealth  r/traumatoolbox  r/bipolarreddit  r/BPD  r/ptsd  r/psychoticreddit  r/EatingDisorders  r/StopSelfHarm  r/survivorsofabuse  r/rapecounseling  r/hardshipmates  r/panicparty  r/socialanxiety. While SW solely focuses on helping those contemplating suicide  the other MHs cover a variety of mental health concerns but not specifically suicidal ideation [31]. All of these subreddits host public content. We used Reddit’s official API to collect posts  comments  and associated metadata from the SW and MHs subreddits (http://www.reddit.com/dev/api). Our analysis in this paper is based on all content shared on MHs between February 11 and November 11 2014 (63 485 posts  209 766 comments and 35 038 users). We refer to the data obtained from SW during the same time period (16 348 posts  9 224 users) to identify those individuals in MHs who go on to post on SW over time.,2,2,2,2,0,1,0,0,2,0,,0,0,1,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/2858036.2858207 ,2,10,mhs and sw content verification,"De Choudhury, Kiciman, Drezde, Coppersmith, Kumar",5,1,1,1,2016,Following our data collection  we focused on verifying whether MHs and SW subreddit content actually relate to discussion of mental health concerns and suicidal ideation. The MHs have been previously examined for understanding mental health discourse on Reddit [51  39]. For SW  we consulted (1) a licensed clinical psychologist/suicide prevention expert and (2) two active moderators of SW to obtain qualitative grounding that the content in SW indeed related to expressions of suicidal ideation. Example (paraphrased) titles of posts from one of the MHs and SW are given in Table 1.,2,2,2,2,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/2858036.2858207 ,2,11,constructing user classes,"De Choudhury, Kiciman, Drezde, Coppersmith, Kumar",5,1,1,1,2016,We split our data into two sequential time periods (t1 from Feb 11 2014 to Aug 11 2014  and t2 from Aug 12 2014 to November 11 2014). Using these two time periods  we created two sets of users. Note that since Reddit does not enforce the real name rule of having exactly one account per person  our reference to “users” in this paper is equivalent to “user accounts”. First  we identified those users that posted on MHs during t1  but did not post on SW during t1 or t2 (i.e.  users that discuss mental health topics but not on SW; hereafter “MH”). The second class included those who posted on MHs during t1 and posted in SW during t2 (i.e.  users that discuss mental health topics  originally not related to suicide  but eventually transition to talk about suicide; hereafter “MH → SW”). Figure 1 shows a schematic description of our user class construction. Note that by focusing on users that initiate at least one post on SW or the MHs  as opposed to only commenting  we can focus on those frequenting the communities for support  disregarding those primarily providing help through commentary. This split yielded 440 MH → SW users; which is 1.52% of the total number of 28 831 accounts who posted in MHs but never on SW during either of the periods. To construct a MH cohort of equal size who did not post on SW in either period  we randomly sampled a set of 440 users from the 28 831 users. Note  although MH users did not post on SW during our timeframe of analysis  they may have done so outside the bounds of our analysis. To support our goal of characterizing differences between the MH → SW and MH users  we obtained via Reddit’s API the timeline of posts and comments authored by the 880 users (the API only provides the last 1000 public posts and comments for a user). For each post  we obtained their associated metadata (e.g.  vote difference or score) and comments. Our final dataset contained 4 731 posts and 46 949 comments from the 440 MH → SW users  and 8 318 posts and 54 086 comments from the 440 MH users. We note an important concern: individuals may post suicidal thoughts on MHs  never engaging on SW  and thus “corrupting” the MHs data with discussions of suicidal ideation. We argue against this possibility. (1) SW is a prominent suicide support forum  and the role of this community in suicide prevention and in acting as an inoculator of vulnerable thoughts is well-recognized [31]. (2) Most MHs (e.g.  r/depression) clearly specify in their guidelines that suicidal thoughts should go to SW: “It’s usually better to post anything that specifically involves suicidal thoughts or intent in /r/SuicideWatch rather than here. If you’re concerned about someone else who may be at risk for suicide  please check out their talking tips and risk assessment guide.” (3) Finally  discussions with the moderators of SW confirmed that active steps are taken to move all suicidal ideation related content to SW. Given these considerations  we expect that few suicidal ideation posts appear on subreddits outside of SW.,2,2,2,2,2,0,0,0,2,0,,0,0,0,0,0,1,,0
https://dl.acm.org/doi/abs/10.1145/2858036.2858207 ,2,12,methods,"De Choudhury, Kiciman, Drezde, Coppersmith, Kumar",5,1,1,1,2016,We present our various measures and methods by which we identify differences between MH → SW and MH users. This will include both characterizations of differences as well as automated methods for differentiating between the groups.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,1,,0
https://dl.acm.org/doi/abs/10.1145/2858036.2858207 ,2,13,linguistic interpersonal interaction measures,"De Choudhury, Kiciman, Drezde, Coppersmith, Kumar",5,1,1,1,2016,Our first set of methods include developing three sets of measures spanning: linguistic structure  interpersonal awareness and interaction. The choice of these measures is motivated by literature that examines associations between the behavioral expression of individuals and their responses to crises  including vulnerability due to mental illness [13  23]. Each of these measure categories consists of the following variables: Linguistic Structure. For this measure  we compute the fraction of nouns  verbs2   and adverbs in posts and comments; automated readability index  a measure to gauge the understandability of text [62]; and linguistic accommodation  a process by which individuals in a conversation adjust their language style according to that of others [20]3 . Together  these variables characterize the text shared by the user classes beyond their informational content. Per literature in psycholinguistics  such structure is known to relate to an individual’s underlying psychological and cognitive state and can reveal cues about their social coordination [54]. Interpersonal Awareness. This measure category includes: proportion of first person singular (indicating pre-occupation with self)  first person plural (indicating collective attention)  second person and third person pronouns (indicating social interactivity and reference to people or objects in the environment). Literature has indicated that pronoun use can quantify an individual’s self and social awareness and can reveal mental well-being  including that manifested in social media [21]. Interaction. Variables corresponding to this measure category include: volume of posts and comments authored  post length  length of comments authored  volume of comments received on shared posts  length of comments received  mean vote difference (difference between upvotes and downvotes on posts authored)  and response velocity (in minutes)  given by the time elapsed between the first comment and the time the corresponding post was shared.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/2858036.2858207 ,2,14,prediciton framework,"De Choudhury, Kiciman, Drezde, Coppersmith, Kumar",5,1,1,1,2016,We frame our prediction problem of identifying which MH user will go on to post on SW in the future as a supervised learning task. We first build a probability distribution over all unigrams and bigrams (referred to as tokens) in the posts and comments of both user classes. Thereafter  we construct several regularized logistic regression based binary classifiers  where the response variable is whether a user belongs to MH → SW or to MH. We consider different sets of predictor variables for the classifiers based on our three measure categories and the tokens obtained from above. We consider five models: (1) Linguistic Structure; (2) Interpersonal Awareness; (3) Interaction where the predictor variables of each correspond to their respective variables; (4) Content consisting of the unigram and bigram tokens along with their relative frequencies given from above; and (5) Full comprising all variables from all measure categories and the tokens from the Content model.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,2,0,,0
https://dl.acm.org/doi/abs/10.1145/2858036.2858207 ,2,15,propensity score matching,"De Choudhury, Kiciman, Drezde, Coppersmith, Kumar",5,1,1,1,2016,We borrow methods from the causal analysis literature [1  72]  although we recognize that our data does not meet the strong assumptions that are required to infer true causality. First  all confounding variables are not included among the observed covariates. Secondly  the stable unit treatment value (SUTVA) assumption (where one individual’s outcome must be independent of whether another individual takes a treatment) likely does not hold in these communities we study. Though this prevents us from making any causal claims  in practice we find that the results of our stratified analysis provides significant insight beyond simpler correlational techniques in this observational study. To go beyond prediction and to better understand the possible causal factors involved in users’ transitions from posting in MH to posting in SW  we wish to isolate the “effects” of individual tokens (unigrams and bigrams) in posts and reduce bias due to confounding variables. Essentially  we wish to estimate the effect of a specific treatment (the use of a target token in an MH post) on a measured outcome (the likelihood of transitioning to post in SW) conditioned on confounding variables (all previously written tokens in MH posts). To do so  we work within the potential outcomes framework [57  34] for causal modeling  applying stratified propensity score matching to estimate causal effects [56]. Stratified propensity score matching achieves this by subdividing the treatment group (individuals who used the target token) and the control group (individuals who did not use the target token) into comparable groups based on the individuals’ estimated propensity to use the token. We learn a propensity estimating function based on covariates (past tokens from the histories of both the control and treatment group members). This balances the distributions of confounding factors within each strata  creating comparable treatment and control groups within each strata. The observed difference in the likelihood of SW transitions between the two groups then provides an estimate of the effect of the treatment  as the groups are otherwise comparable. In our implementation  for a given target token  we estimate the propensity score using the averaged perceptron learning algorithm [27] and stratify the users into 10 strata. Estimation is conducted based on a binary vector representation of user posting history  H = h1  ...  hn  where hi is 1 if the user posted the token i prior to posting the target token  and 0 otherwise. Per [10]  we use trimming to limit our comparisons to strata with sufficient common support  and report the population average treatment effect over them  as well as the z-score and χ 2 tests of statistical significance. We perform this analysis for all target unigrams and bigrams (tokens) used by more than 10 individuals in MH (11278 tokens).,0,0,0,0,0,0,0,0,0,0,,2,0,0,0,2,0,,0
https://dl.acm.org/doi/abs/10.1145/2858036.2858207 ,2,16,results,"De Choudhury, Kiciman, Drezde, Coppersmith, Kumar",5,1,1,1,2016,We present our results in three phases. We begin by describing differences characterizing the MH → SW and MH users through the linguistic structure  interpersonal awareness and interaction based measure categories we defined above. Next we discuss differences in content of posts and comments shared by the two user classes. Finally  we present the results of supervised classification of the users.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/2858036.2858207 ,2,17,linguistic interpersonal interaction differences,"De Choudhury, Kiciman, Drezde, Coppersmith, Kumar",5,1,1,1,2016,Table 2 shows differences between the MH → SW and MH users along the linguistic structure  interpersonal awareness and interaction based measure categories. We show each variable’s mean value per measure type for both groups and the zscore of the difference  based on Wilcoxon signed rank tests. Observation 1. MH → SW users show poorer linguistic structure and accommodation including lowered readability. Per Table 2  MH → SW use more verbs (z = 2.1) and adverbs (z = 4.8) (which indicate discourse around actions)  but less entities  e.g.  nouns (z = 6.5). Together this reveals poor linguistic structure [69] and indicates lowered interest in objects and things [12]. Expressing more about actions is also known to be correlated with sensitive disclosure [33]. Further  we observe lower readability index of the posts shared by MH → SW users (z = 5.5); such language framing limitations are linked to decreased cognitive functioning and coherence [53]. Finally  we observe that MH → SW users exhibit lowered sense of linguistic accommodation to the general content on MHs (z = 5.4)  compared to the MH users. This may indicate decreased involvement of the MH → SW users with the communities  as well as decreased ability or intent to adjust to their norms and conventions. Observation 2. MH → SW users show higher self-attentional focus and greater detachment from the social realm. MH → SW users also use greater number of first person singular pronouns (z = −10.6). This generally indicates that  MH → SW users convey more personal stories and may be high in self-preoccupation [7]. Lower use of second person pronouns (z = 8)  first person plural pronouns (z = 4.5) and third person pronouns (z = 6.3) in posts from MH → SW users might imply less interactive users who are less socially bothered regarding the larger Reddit audience. Observation 3. MH → SW users show lowered social engagement and access to support and increased selfdisclosure. Finally  MH → SW users tend to have longer (z = −15.4) but fewer (z = 2.5) posts. More verbosity in shared content has previously been shown to be a sign of increased selfdisclosure and cognitive complexity; however less activity in community settings has also previously been shown to be indicative of social isolation [30]. MH → SW users receive fewer comments on their posts (z = 5.4) and had smaller differences in their voting scores (z = 7.1)  which may be an indicator of lower engagement and lower access to social support from the community as compared to posts from the MH users  as also observed in [51].,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/2858036.2858207 ,2,18,content differences,"De Choudhury, Kiciman, Drezde, Coppersmith, Kumar",5,1,1,1,2016,Our next analysis focuses on the content (posts and comments) shared by the MH → SW and MH users. First we establish differences between the two cohorts based on our propensity score matching methodology  and then present a qualitative interpretation of our quantitative observations. Propensity Score Analysis In Table 3 we report 70 tokens with the highest z scores that distinguished between MH → SW and MH users based on propensity score matching; specifically increased the likelihood of a MH account’s posting in SW in the future based on a particular token used in the past. The tokens reported were all found to be significant at the p = .001 level. Corresponding to each token  we also report the absolute number of users in our dataset (out of a total of 880) who used the token (treatment count)  the proportion of users in our data who fell into an unclipped strata of the token (population coverage)  the percent increase in likelihood of posting in SW in the future based on use of the token in the past (average treatment effect)  the z score of the token’s likelihood of use between the two user classes  and associated χ 2 statistic of this difference. We find that controlling for historical use of different tokens in MH content  use of tokens such as “depression” (z = 8.04)  “useless” (z = 7.05)  “suicide” (z = 6.66)  “anxiety” (z = 6.56)  “no friends” (z = 6)  “have nothing” (z = 5.98)  “kills” (z = 5.9) and “to cry” (z = 5.5) significantly increases a user’s likelihood to post in SW in the future. For “depression” this increase is 30%  for “suicide” it is 32%  for “no friends” it is 51%  for “to cry” it is 51%  while for “kills” it is 53%. In addition to the tokens in Table 3 we look at effects of pronoun usage (“I”  “you”  “he”  “she”  “we”  “they”) and corresponding possessive pronouns  and find that the use of “I” and “my” have a statistically significant large effect (I: effect=+37%  z = 2.8; my: effect=+28%  z = 4.11); the use of 3rd person female pronouns have some statistically significant effect (her: effect=+10%; z = 3.01; she: effect=+8%  z = 2.09); and all other pronouns have effect < 7% with lower or no statistical significance). Broadly use of these tokens indicate a negative attitude  experience of emotional distress and self-focus [7]; an observation that aligns with our observations made above related to the measure of interpersonal awareness. Which are the tokens that decrease the likelihood of posting in SW in the future? We show tokens with the most negative treatment effects and high z-score values in Table 4. Use of tokens like “counseling” (z = −4.09)  “relationship that” (z = −3.89)  “intimate” (z = −3.73)  “hope it” (z = −4.28)  “i agree” (z = −4.54) and “and enjoy” (z = −4.44) result in reducing the likelihood of posting in SW in the future by 50- 57%. This shows a tendency of the users of these tokens to maintain a positive outlook towards life  remain hopeful (perhaps of recovery)  agreeableness and focus on valuing social ties  including discussion of treatment or therapy. Note that the effects of using a token may not be homogeneous. Certain people may see no effect of using a token  while others see a large effect. Figure 2 explores this for the tokens depression  suicide  anxiety  suicidal  and can t: the most significant target words with at least 100 people using the token. Within each plot  we show how the future likelihood to post in SW varies across strata for people using and not using the target token. In the case of “depression”  for example  we see that for people with a very low estimated propensity to use the word “depression”  use the word  it has a large effect on their likelihood to post on SW. While people who have the highest estimated propensity to use the word “depression” see no additional change from using the word. We see similar variances for “suicide”  while the effect of using the word “can t” is approximately constant across strata. To investigate deeper into heterogeneous effects  we search across all treatment tokens to find those that  at different strata  both increase and decrease the likelihood of posting in SW. We find 161 such treatment tokens (62 unigrams and 99 bigrams) where there is at least one strata with a positive effect and one with a negative effect (p < 0.05). While their effects are significant within the strata  because they have contradictory effects at different strata  these tokens do not necessarily have large or significant average treatment effects. Table 5 shows a selection of these treatment tokens  along with the effect and the top 5 distinguishing tokens for those strata with the most significant positive and negative effects. Distinguishing tokens are ranked as the ratio of the frequency of occurrence within the strata to the ratio in the set as a whole. For example  the treatment token “stressed” increases SW posting likelihood in the 9th strata by 44%  but decreases SW posting likelihood by 33% in the 3rd strata. The former strata is distinguished by tokens as “i do” and “i hate”  while the latter is distinguished by “there and” and “deal with”. These results highlight the importance of context in interpreting the likely outcomes of many words used in MH posts. We discuss the context of use of the different treatment tokens (e.g.  “depression  “suicide”) that are linked to increased likelihood of posting in SW. In Table 6 we present 20 tokens that were most predictive of the use of four treatment tokens. We observe that the predictive tokens are considerably different across the MH → SW and MH classes (based on Mann Whitney U tests; also ref. Kendall’s τ for rank correlation)  indicating that the context in which the treatment tokens are used are distinct across those who go on to post on SW in the future  versus individuals who do not. Qualitative Interpretation Are there meaningful themes that characterize the different treatment tokens (Table 3) linked to heightened likelihood of posting in SW in the future? Specifically  in what ways do these distinguishing tokens relate to known psychological attributes of suicide ideation examined in the literature?,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,2,0,,0
https://dl.acm.org/doi/abs/10.1145/2858036.2858207 ,2,19,spectral clustering of treatment tokens,"De Choudhury, Kiciman, Drezde, Coppersmith, Kumar",5,1,1,1,2016,To address these questions  we extract thematic clusters in an unsupervised manner from co-occurrence relationships between the tokens. That is  for each unique token pair  we compute their normalized frequency of appearing together in a post or comment of the 880 users in our dataset; we consider the top 100 000 most frequent co-occurring token pairs. Specifically  we use the normalized spectral clustering algorithm [64]. This algorithm accomplishes the partitioning by mapping the original space of pairwise co-occurrence relationships to an eigen space. We find that the clusters of tokens obtained through this method show significant differences among each other  based on the Kruskal-Wallis one way analysis of variance test (p < .001).,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,2,1,,0
https://dl.acm.org/doi/abs/10.1145/2858036.2858207 ,2,20,extracting themes from clusters,"De Choudhury, Kiciman, Drezde, Coppersmith, Kumar",5,1,1,1,2016,Next  to examine the most dominant themes in the set of clusters obtained from spectral clustering  we analyze the clusters corresponding to the first six eigenvalues of the Laplacian matrix given by spectral clustering. Two researchers familiar with mental health content on social media inspected the set of tokens in these clusters for validation purposes. They used a semi-open coding approach to develop a codebook and extracted descriptive topical themes for the clusters (Cohen’s κ=.74). During the codebook development  the two annotators referred to prior literature on the cognitive psychological integrative model of suicide [37  29  25]. We now present a qualitative analysis on the context in which the different tokens in each of the six clusters are used in posts in our dataset. We frame our discussion using the cognitive psychological integrative model of suicide [37  29  25]. Hopelessness: Tokens in the first theme cluster (“have nothing”  “no real”  “kill myself”  “abandoned”  “die”) were found to relate to signals of hopelessness among individuals. We note that the cognitive psychological integrative model of suicide [25] has identified hopelessness as an important mediating variable between mental illness and suicidal ideation and there is ample evidence of the decisive role of hopelessness as an indicator both of current suicide intent and as a predictor of future suicidal behavior [37  29]: But I want to die. I feel so abandoned. I must be an idiot. I hope for some random event to kill me so that nobody has to be guilty. My loved ones would mourn me but they would move on. At least easier than if I actively killed myself. Anxiety: The second theme cluster with the highest eigenvalue related to signs of anxiousness (“anxiety”  “panic”  “to cry”). The cognitive psychological model of suicide has also attached great importance to individuals’ feelings of anxiety despair as a predictor of future suicide [52]: There are times when my brain seems to shut off and I calm down. But then I panic... about anything. I think I have anxiety. I feel like nothing is mine. I don’t  I genuinely don’t  remember a time where I felt normal. Impulsiveness: We observed manifestation of impulsive tones in tokens of the third theme cluster. The cognitive suicide model also suggests that impulsivity resulting from cognitive deficits (e.g.  cognitive rigidity  dichotomous thinking  and inability to generate or act on alternative solutions) are prominent markers of suicide ideation [4  37]: Theres a terrible feeling through my whole body every waking moment I have and theres only 2 ways to ending it. It hasnt been getting better only worse  I am freaking out. The only thing stopping me is I dont know about/have access to anything that would make it quick and clean Self-Esteem: The cognitive suicide model has further found lowered self-esteem and self-efficacy to be important attributes among those who are prone to suicide ideation [61]. Feelings of social isolation and loneliness  conceptualized as a part of the cognitive vulnerability  have consistently been shown to be related to suicidal ideation  attempts  and completions [8]. We find that tokens of the fourth cluster appear in posts bearing a tone of decreased self-esteem  including that of guilt  self-loathing and regret: I am too ugly to even make friends. I hate it. People do not want to be associated with me because of my image. I have tried talking to girls and they’ve all told me to go away and to just give up. So here I am  giving up and ending everything. Loneliness: The suicide model also situates loneliness as a risk that exacerbates the frequency of thoughts of suicide [4]. Our fifth theme cluster includes tokens that indicate expressions of social isolation and detachment from the social realm  consisting of friend and family ties: I honestly just don’t think my friends I’ve been helping out so much and supporting so much really even care about me. I’ve been living with my parents since i graduated and they aren’t happy about it. today i had to deal with being yelled at by each of them. i have no one and i never felt such pain  i am hurt and i am alone. Severe or Stigmatized Illness: Per the cognitive suicide model  experience of stigmatized and/or terminal illness (e.g.  cancer) is linked to bereavement  marginalization and perceived lack of social support [54]. Tokens like “depression”  “disorder”  “psychosis” indicate expression of such distress: Depression and psychosis suck. I’ve been battling these for many desperate years  turns out  I can’t win  no matter how hard I try. I’m tired of pretending. I’m tired of making others happy to forget about my own issues.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,2,0,,0
https://dl.acm.org/doi/abs/10.1145/2858036.2858207 ,2,21,classification results,"De Choudhury, Kiciman, Drezde, Coppersmith, Kumar",5,1,1,1,2016,In this final subsection  we examine to what extent the linguistic structural  interpersonal awareness  interaction and content variables may be able to predict and classify MH → SW users from MH users. For this supervised learning task  we set aside 20% of our user set (total 880 users) as our held out validation set. We performed k-fold cross validation on the rest 80% users (k = 10) for tuning parameters of all of the five regularized logistic regression models discussed in section 4. To evaluate the goodness of fits of the regularized logistic regression models we use deviance. Due to the randomness introduced by cross-validation  we ran our models k = 10 times and here we report the results corresponding to the lowest deviances that we obtained in any of the runs. Compared to the Null model  we observe that all of our models provide considerable explanatory power with significant reduction in deviances (Table 7). Particularly  the Full model  that uses all variables yields the best fit. We find that the difference between the deviance of the Null model and the deviance of the Full model approximately follows a χ 2 distribution: χ 2 (15017  N = 4769) = 9190.6 − 1864.4 = 7326.2  p < 10−9 . Finally  we summarize the performance of the Full regularized logistic regression model on the heldout dataset of 176 users (88 MH → SW and 88 MH users). From Table 8 we find that our model gives high accuracy in classifying the two classes  with a precision  recall and F-1 score of .8 each. Table 8 also gives the confusion matrix corresponding to the binary classification  from where we observe that MH users are marginally better classified (higher accuracy of 83.5%) compared to the MH → SW users (accuracy: 77.5%). We report the receiver operating characteristic (ROC) curve in Figure 3; the area under curve (AUC) is found to be .87.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/2858036.2858207 ,2,22,clinical and societal relevence,"De Choudhury, Kiciman, Drezde, Coppersmith, Kumar",5,1,1,1,2016,Through this paper  we have provided a methodology to help identify individuals engaging in mental health discussions who are at a greater likelihood of transitioning to suicidal ideation discussion. An important contribution of our propensity score matching approach  in particular  has been the ability to identify linguistic constructs prior to any overt posts linked to suicidal ideation  which indicate ripe areas for further study involving causal inference. Thus we believe our methods can pave the way for longitudinal analysis of mental health content. This can help create provisions for early diagnosis of predisposition to suicidal thoughts  including therapeutic arrangements for suicide prevention. Furthermore  our work indicates linguistic constructs that should be further investigated for their ability to forecast risk  contrary to existing post-hoc approaches identifying the behavioral and cognitive markers of suicidal ideation. Broadly  our work opens up some promising opportunities of employing an unobtrusive data source like social media to understand and infer macroscale rates of suicidal ideation among those challenged by mental health concerns. However our approach does not act as a standalone mechanism for estimating risk to suicidal ideation among those involved in mental health discussions. We caution against employing the social media predictor variables and the linguistic tokens our methods extracted as blanket filtering approaches to judge possible suicidal ideation. Such decisions are not only a controversial territory4   but also can have drastic implications for one’s health  well-being and self-esteem. Our methods and findings can be best leveraged as a complementary screening tool and used in conjunction with clinical  validated and conventional forms of well-being assessment.,0,0,0,0,0,0,0,0,0,0,,2,0,0,0,2,0,,0
https://dl.acm.org/doi/abs/10.1145/2858036.2858207 ,2,23,implications for hci research and design,"De Choudhury, Kiciman, Drezde, Coppersmith, Kumar",5,1,1,1,2016,Provisions for Support and Interventions Social media platforms  although do not have any legal obligation  have recently been stepping up to the cause of extending help to those who are perceived to be vulnerable. For instance  recently  Facebook  in partnership with the National Suicide Prevention Lifeline  added a new feature of suicide prevention  through which an individual whose post is perceived to be distressful by a Facebook contact  could receive a support related intervention5 . An important consideration of most of these efforts is that they either rely on people to report concerning posts or users  or apply rudimentary blanket policies around specific keywords/phrases. Both of these approaches are prone to missing vulnerable posts (those not reported)  or misjudge flippant references as relating to dangerous behaviors. Our methods and findings may be utilized to expand these efforts  for instance  towards designing (semi)-automated personalized and adaptive interventions toward curbing suicidal tendencies  at the same time to improve access to appropriate peer and expert social and emotional support. We outline the following two design directions: Individuals whose content contain phrases and other linguistic constructs relating to suicidal ideation  as revealed by our methods  may be flagged in the interfaces of moderators and other clinical experts for help and support. Community moderators may also be allowed to maintain a “risk list” in their interfaces that would include individuals forecasted by our methods to exhibit signs of suicidal ideation in the future. This would allow improved preparedness to bring timely and appropriate help to those in need. Further  on being informed that an individual in the community could be prone to suicidal thoughts in the future  moderators and experts may make provisions to connect them with appropriate mental health resources (e.g.. a hotline or a community like 7CupsofTea)  encouraging peers or trusted friends and family  or field private messages with relevant information on help seeking or therapy. Interventions may also be designed that promote self-reflection of one’s activity and behavior on these mental health support-seeking social media platforms. Our methods may be employed for automated (self)-assessment of behavior  cognition and affect  including serving as an early warning mechanism to individuals struggling with mental health concerns. Building off our methods  reflective interventions could also be designed to reveal longitudinal trends relating to specific markers of suicide ideation; for instance  to identify time periods of anomalous patterns  which are known to be otherwise difficult for individuals to keep track of [45]. Logging of these longitudinal trends can also serve as a diary-style data source to aid care-givers or other trained professionals and clinicians gain a deeper understanding of an individuals risk to dangerous behaviors in the future.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/2858036.2858207 ,2,24,ethical considerations,"De Choudhury, Kiciman, Drezde, Coppersmith, Kumar",5,1,1,1,2016,Attempts to extend support to vulnerable populations  like those examined here  need a careful consideration of the risks and ethical challenges. Most importantly  at the time of design of the above suggested interventions  acceptability to social media users needs to be thoroughly investigated. In general  any intervention built out of automated algorithms like the one we proposed here  needs to honor the privacy of the individuals and those who volunteer to provide help and support. Further  beyond the design suggestions outlined above  actual modes of intervening and offering support (when  where  how) to individuals forecasted to express suicidal ideation in the future is a research and ethical question of its own. For instance  one point of intervention design is how to lead to positive behavior change  instead of counter-helpful outcomes. An unhelpful outcome could include chilling effects in participation in the community  or suicide ideation moving on to fringe or peripheral platforms where such populations might be difficult to extend help to. Finally  caution also needs to be adopted to ensure that alongside the interventions and analysis of the behavior of vulnerable communities to allow extending help and advice  ecosystems like Reddit continue to be perceived as a safe place for seeking support  and for therapeutic self-disclosure.,0,0,0,0,0,0,0,0,0,0,,2,0,0,0,0,0,,2
https://dl.acm.org/doi/abs/10.1145/2858036.2858207 ,2,25,limitations and future directions,"De Choudhury, Kiciman, Drezde, Coppersmith, Kumar",5,1,1,1,2016,There are some limitations to our work. We presume selfselection biases in who posts on MH and SW. Reddit allows the use of throwaway accounts or semi-anonymous identities [40]  including having multiple accounts. Hence  given the sensitive and stigmatized perception of suicide  shift to SW from MH communities may happen via such an account. Such shifts would not be captured by our data collection process. Users with history of mental illness may also directly post on SW without ever posting on any mental health subreddits. Despite these limitations  we believe our work allows us to focus on a high precision dataset where do see these transitory thoughts and cognitions and our statistical method allows us characterize and predict these shifts. We also acknowledge that although our dataset is relatively larger than what has been studied in the psychology literature [11]  the Reddit communities are help-seeking forums. We cannot guarantee generalizability with respect to the larger population. Additionally  our findings do not provide insights into why an individual posting on a mental health community might decide to make the transition to SW. We also acknowledge that our algorithm for predicting which MH user will go on to post on SW in the future  is not 100% accurate  hence caution is suggested in interpreting cases of false positives or false negatives. Importantly  our inferences do not directly imply the individuals are at risk of suicide  or acted on their thoughts. Further  we note thar there are latent factors  beyond what can be observed on Reddit  that may be driving the patterns of suicidal ideation we observed.,0,0,0,0,0,0,0,0,0,0,,2,2,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/2858036.2858207 ,2,26,conclusion,"De Choudhury, Kiciman, Drezde, Coppersmith, Kumar",5,1,1,1,2016,In this paper  we presented a statistical methodology to identify whether an individual engaged in mental health discourse on social media is likely to transition to that around suicidal ideation in the future. We leveraged a large dataset from a number of mental health and suicide support communities on Reddit to address our research problem. We discovered a number of distinct markers characterizing these shifts: heightened self-attentional focus  poor linguistic coherence and coordination with the community  reduced social engagement and manifestation of hopelessness  anxiety  impulsiveness and loneliness in shared content. Through a logistic regression framework  we were also able to distinguish between individuals likely to undergo these shifts versus others who do not with high accuracy. Our findings indicate the potential of developing new kinds of technological provisions for social support and interventions catering to vulnerable populations.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/2702613.2732733,3,1,abstract,"Balani, De Choudhury",2,1,0,0,2015,Self-disclosure is an important element facilitating improved psychological wellbeing in individuals with mental illness. As social media is increasingly adopted in health related discourse  we examine how these new platforms might be allowing honest and candid expression of thoughts  experiences and beliefs. Specifically  we seek to detect levels of self-disclosure manifested in posts shared on different mental health forums on Reddit. We develop a classifier for the purpose based on content features. The classifier is able to characterize a Reddit post to be of high  low  or no selfdisclosure with 78% accuracy. Applying this classifier to general mental health discourse on Reddit  we find that bulk of such discourse is characterized by high self-disclosure  and that the community responds distinctively to posts that disclose less or more. We conclude with the potential of harnessing our proposed self-disclosure detection algorithm in psychological therapy via social media. We also discuss design considerations for improved community moderation and support in these vulnerable self-disclosing communities.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/2702613.2732733,3,2,introduction,"Balani, De Choudhury",2,1,0,0,2015,Over the past several years  online communities catering to health needs and challenges have shown considerable uptake [6]. These tools are known to act as a constantly available and conducive source of information  advice  and support. An important motivation behind the use of these online resources for health concerns is that they support open and honest discourse [16]. Such discourse is often referred to as “self-disclosure” – self-disclosure is the telling of the previously unknown so that it becomes shared knowledge  the “process of making the self known to others” [12]. Literature indicates that self-disclosure can be an important therapeutic ingredient and is linked to improved physical and psychological well-being [5]. In the context of health conditions that are typically considered socially stigmatic  such as mental illness  self-disclosure has been noted to be a basic element in the attainment of improved health [14]. This is because self-disclosure results in disinhibition [16]  which is known to play a positive role in psychological counseling. Our motivation is rooted in this rich body of work examining the important role of self-disclosure in mental health. We present preliminary findings on automatic detection and characterization of self-disclosure in content shared on mental health online communities in Reddit. Automatic detection of self-disclosure levels in social media posts may help community moderators direct appropriate help and advice in a timely fashion to individuals with mental health challenges. It can also help build tailored recommender tools that  based on self-disclosure levels  match potential parties in the community to a post’s author. We focus on a large variety of mental health focused forums on Reddit and build a classifier to identify levels of selfdisclosure in the content posted on these forums. Next we characterize attributes of mental health discourse in the forums via the lens of the levels of self-disclosure in them. We find noted differences in ways individuals socially engage on these forums  depending on their chosen level of selfdisclosure. We discuss implications of our work in social system design  in the potential of harnessing social media for psychological therapy and counseling  and the ethical challenges in this line of research.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/2702613.2732733,3,3,related work,"Balani, De Choudhury",2,1,0,0,2015,Self-disclosure has been widely investigated both in the psychology and the computer mediated communication (CMC) literature. A rich body of this work has argued selfdisclosure to be beneficial: having been linked to trust and group identity [12]  as well as playing an important role in social interactions by reducing uncertainty [3]. Selfdisclosures are usually made for a purpose [2]  including expression  relationship development and social control [2]. In the context of mental health  Ellis [5] reported that discourse on emotionally laden traumatic experiences can be a safe way of confronting mental illness. On similar lines seminal work by Pennebaker et al. [14] found that participants assigned to a trauma-writing condition (where they wrote about a traumatic and upsetting experience) showed immune system benefits. Disclosure has also been associated with reduced visits to medical centers and psychological benefits in the form of improved affect [15]. Additionally  prior research in CMC found that medical patients tend to report more symptoms and undesirable behaviors when interviewed by computer rather than face-toface [8]. Ferriter [7] found that pre-clinical psychiatric interviews conducted using CMC yielded more honest  candid answers. In the UK  the Samaritans report that although only 20% of telephone callers report suicidal feelings  this number increases to around 50% of email contacts [11]. We situate our work within the findings from this body of literature. We build our observation from recent findings on the widespread adoption of online social platforms for health related discourse [6]. However note that literature examining levels of self-disclosure around online mental health discourse is limited. While there have been recent work on modeling self-disclosure manifested in social media content [10  18]  however they (a) focus primarily on Twitter  and (b) do not focus on health. Therefore the impact of online self-disclosure in individuals challenged with behavioral health concerns is under-examined; also little is understood about how self-disclosure manifests in other social platforms. In our prior research [4]  we examined how use of the social media Reddit for mental health purposes is sometimes characterized by disinhibiting behavior  along with high levels of social support from the larger community. Through this paper  we expand this research by examining how to detect and characterize levels of self-disclosure in health discourse on Reddit  focusing on mental health communities.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/2702613.2732733,3,4,data collection,"Balani, De Choudhury",2,1,0,0,2015,We used Reddit's official API to collect posts  comments on posts  and associated metadata from several mental health focused subreddits. We build on the data collection methodology we used in [4]. In order to arrive at a comprehensive list of subreddits to focus on  we utilized Reddit's native subreddit search feature (http://www.reddit.com/reddits). We searched for subreddits on “mental health”. Two researchers familiar with Reddit employed an initial filtering step on the search results returned  so that we “seed” on high precision subreddits discussing mental health concerns. Thereafter  we focused on a snowball approach to compile a second list of “related” or “similar” subreddits that are mentioned in the profile pages of the seed subreddits. Sample of subreddits (31 in all) we crawled are given in Table 1. Note all of these subreddits host public content. For the purposes of self-disclosure detection  we also identified subreddits (sample listed in Table 2) as our control group (total of 12 subreddits) – meaning they are unrelated to mental health topics. For sanity check  we randomly sampled a set of 200 posts from the control subreddits  and two researchers familiar with Reddit manually checked their content for presence of any mental health content. We found that 97% of subreddit content in our sample were not about any mental health concern (Cohen’s Kappa for inter-rater agreement was .84). In all  our dataset had 32 509 posts from 23 807 users in the mental health subreddits  and 15 383 posts from 13 216 users in the control forums. For each of the unique users in the mental health forums  we further collected all of their Reddit post/comment histories (last 1000 posts/comments per Reddit API limits) if their number of posts and comments in our dataset was five or more – this gave us 7 248 users and 4.1M posts/comments.,2,2,2,2,2,1,0,0,2,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/2702613.2732733,3,5,generating ground truth data on self disclosure,"Balani, De Choudhury",2,1,0,0,2015,Automatic detection of self-disclosure levels in posts necessitates obtaining gold standard labels on self-disclosure  in essence  “ground truth”. For the purpose  two raters familiar with Reddit and its mental health communities in particular  independently rated a small random sample (50 posts) with equal proportions from mental health and control subreddits  for three levels of self-disclosure – no selfdisclosure  low  and high self-disclosure. These three classes of self-disclosure were chosen based on categorization by Bak et al in [10]. The raters mutually discussed their labels thereafter and thus came up with a set of rules for rating. The rules were further aligned with observations in prior work [9  10  11]. Per the rules: a. Posts that either reveal personal information (e.g.  age  location  gender etc.) or divulge sensitive or vulnerable thoughts  beliefs  or embarrassing/confessional experiences were to be considered to be indicative of high self-disclosure. Joinson [10] characterized sensitive disclosure in terms of the extent of “revealed vulnerability”. b. Posts about self but not disclosing any personal or emotionally vulnerable content was to be considered of low self-disclosure. No self-disclosure posts were those which were about people or things other than the posting author  and which divulged information unrelated to the self. Following these mutually agreed upon rules  the previous two raters and an additional rater familiar with Reddit independently coded a larger sample of 800 posts to create a training set for the purposes of classification. The raters had good agreement in their ratings: Fleiss’ Kappa was found to be .73. However  given the subjective nature of characterization of self-disclosure  we considered only those posts in the training set for which we had agreement across all three raters – this gave us 627 posts. Across the three categories  the coded set consisted of 38% posts of high selfdisclosure  35% posts of low self-disclosure  and 27% with no self-disclosure. Table 3 gives examples of mental health post excerpts with high self-disclosure  while Table 4 and 5 provide examples of low and no self-disclosure posts. Note that including non-mental health posts in the training set was essential so as to let the detector learn on posts of low and no self-disclosure and on those not mental health related.,0,0,0,0,2,0,0,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/2702613.2732733,3,6,self disclosure inference,"Balani, De Choudhury",2,1,0,0,2015,Based on the training data thus created  we pursued the use of supervised learning to develop a classifier  which would indicate whether a post is of high  low or no self-disclosure. We tested a variety of different classification techniques (decision  trees  k Nearest Neighbor  naive Bayes). The best performing classifier was found to be a perceptron classifier  with adaptive boosting used to amplify performance [17]  whose results will be used in the remainder of this paper. We used the following feature generation rules: First we eliminated stopwords from each post based on standard list provided by Python’s NLTK library. Next we performed stemming using Porter Stemmer. We extracted uni-  bi-  and tri-grams from each post  and considered those with five or more occurrences. We also computed two additional features – length of each post  and whether the author of the post is an exclusive poster on mental health forums  or is observed in our dataset  to post on other forums as well. Thus each post was characterized by 1070 features. We used standard 10-fold cross validation (CV) to evaluate the classifier  and ran our model over 100 random 10-fold CV assignments for generalizability of the results. We report the average accuracy  precision  recall  F1  specificity as metrics of performance. We find that our classifier based on the perception model yields an average accuracy of 78.4% in detecting high or low self-disclosure  with .74 precision and .86 recall (see Table 4 for details). Other methods like k-NN (k=5) give higher precision  but at the expense of very low recall. Figure 1 gives the ROC (receiver operating characteristic) curves for all the models. Per the ROC curve corresponding to the perceptron model  we find it to yield the maximum area under curve (.81)  hence best performance. We further identify  in Table 5  the n-grams (or features) with the highest weights given by the perceptron – it implies these features were the most significant in the classification task. We provide some brief qualitative examinations of these n-grams  in the light of prior psychology literature on selfdisclosure and mental health [10  11]. We find that the ngrams primarily are associated with vulnerable and selfloathing thoughts (e.g.  thoughts of suicide)  bear a negative tone  or depict confessional experiences. Based on prior research [10  11  12] and our own work on mental health discourse on Reddit [4]  we find that these are the topical dimensions along which high self-disclosure and low/no selfdisclosure posts vary. In essence  high self-disclosure posts share extensively their personal beliefs and fear  for instance  their vital constructs and private  sensitive informational attributes. The post excerpts below have been classified to be of high self-disclosure and through them  we demonstrate the use of some of the n-grams in Table 7: “I don’t want to kill myself  I haven’t felt suicidal in a long time  but I just want to stop life for a while  you know?” “My dad would beat the living shit out of me […]. I’ve been to the hospital so many times I’ve lost track” “I hate this. I hate myself. I don't want to f****** be this person anymore. I'm unmotivated  unfocused  immature.”,0,0,0,0,2,0,0,0,0,0,,0,0,0,0,2,0,,0
https://dl.acm.org/doi/abs/10.1145/2702613.2732733,3,7,characterizing discourse vis self disclosure,"Balani, De Choudhury",2,1,0,0,2015,Following the development of the self-disclosure classifier  we wanted to apply it on an unlabeled set of mental health posts in order to study how levels of self-disclosure characterize the nature of discourse and community response on these Reddit forums. For the purpose we used the unlabeled 4.1M mental health forum posts obtained by crawling the histories of those users in mental health forums who posted five or more times in our observed data. On applying our trained self-disclosure classifier on these 4.1M posts  we find that 1.9M posts were classified as high self-disclosure posts  1.3M posts were categorized to be of low self-disclosure  while .9M were assigned to be having no self-disclosure. This indicates that in general  mental health communities on Reddit are characterized by high selfdisclosure in the shared content. Presumably  individuals find these forums to be an open platform that allows expressing views and thoughts about a topic often considered to be sensitive or unacceptable to the mainstream. The ability to be fairly anonymous on Reddit perhaps facilitates such high self-disclosure – unlike Facebook  a user does not need to specify any personal information in their profiles (e.g.  location  age  name  gender etc.). We further observe from Figure 2 that majority of the users who self-disclose more  tend to also have longer tenure on Reddit. By tenure  here we mean the total amount of time (in months) the user has been observed to be active on Reddit based on our dataset. Most low self-disclosure users have shorter tenure relatively  the same is true for the bulk of the no self-disclosure users (F= 6.2; p<.01 based on KruskalWallis test). This shows that users find the ability to selfdisclose more to be helpful  resulting in continued involvement and engagement on the social media site. Finally  from Table 6 we observe that high self-disclosure posts receive fewer upvotes  greater downvotes  but higher number of comments as well as high response rate. Here response rate is calculated as the normalized time elapsed between post time and timestamp of first comment on the post. This shows that high self-disclosure on mental health subreddits  while may not be highly endorsed by the greater Reddit community due to their caustic and disinhibiting tone  however do receive social support through increased levels of engagement – commentary and rapidity of responses shows that the community is keen to help and advise these individuals with honest and disinhibiting self-disclosures. The greater conversational length as indicated by the high number of comments also aligns with prior literature  where it has been found that candid discourse results in longer verbal exchange [4  10  14].,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,1,,0
https://dl.acm.org/doi/abs/10.1145/2702613.2732733,3,8,design implications and future directions,"Balani, De Choudhury",2,1,0,0,2015,Our findings  though preliminary  bear several implications in design. Psychology literature indicates mental illness challenged people’s apprehensions about experiencing and disclosing negative emotions is the greatest barrier to seeking counseling [14  15]. Hence the fact that these mental health subreddits are allowing individuals to be more expressive or engage in greater self-disclosure shows promise for web based psychotherapy. High self-disclosing redditors could be provided with services that can draw their attention to relevant recent conversations that have happened in the past and the community’s response to it  so that they perceive a sense of support when they visit these mental health communities. These users could also be flagged in the interfaces of the community moderators so that they can pay special attention to their requests for support  advice  or help  especially because some of them seem to be sharing emotionally vulnerable content. Nevertheless  future research in online health intervention design needs to consider ways in which the identities of these vulnerable communities may be protected against revelation to unintended audiences. Conversations on the ethical dimensions of this line of research has been on the rise  and we hope to engage with the ethics and clinician communities to ensure that the algorithms of self-disclosure detection proposed here are utilized in the interest of the affected communities in ethically appropriate ways. In subsequent research  we intend to investigate how individuals tailor their self-disclosure on these forums over time  based on the feedback from the community. Further it will be interesting to study how the extent of social stigma typical of mental illness or the online identity of an individual relates to levels of self-disclosure on social media.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/2702613.2732733,3,9,conclusion,"Balani, De Choudhury",2,1,0,0,2015,We presented preliminary investigation of data-driven methods to detect levels of mental health related selfdisclosure in social media content  particularly focusing on Reddit. Since greater self-disclosure facilitates improved mental health  we hope our research suggests ways to leverage shared online content to measure self-disclosure. Thus it may enable us design better health interventions and support systems that can cater to the needs of these emotionally vulnerable communities better.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/2702123.2702280,4,1,abstract,"Tsugawa, Kikuchi, Kishino, Nakajima, Itoh, Ohsaki",6,0,0,0,2015,In this paper  we extensively evaluate the effectiveness of using a user’s social media activities for estimating degree of depression. As ground truth data  we use the results of a web-based questionnaire for measuring degree of depression of Twitter users. We extract several features from the activity histories of Twitter users. By leveraging these features  we construct models for estimating the presence of active depression. Through experiments  we show that (1) features obtained from user activities can be used to predict depression of users with an accuracy of 69%  (2) topics of tweets estimated with a topic model are useful features  (3) approximately two months of observation data are necessary for recognizing depression  and longer observation periods do not contribute to improving the accuracy of estimation for current depression; sometimes  longer periods worsen the accuracy.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/2702123.2702280,4,2,introduction,"Tsugawa, Kikuchi, Kishino, Nakajima, Itoh, Ohsaki",6,0,0,0,2015,Depression has become recognized as a major public health problem around the world [21]. A 2012 survey conducted by the World Health Organization (WHO) suggests that depression affects 350 million globally [21]. Among other associated problems  depression can lead to suicide [32]  and so the high number of people with depression is considered to be a serious problem. Also in Japan  depression and suicide are recognized as serious problems. An OECD (Organisation for Economic Co-operation and Development) report shows that the standardized suicide rate per hundred thousand people in Japan is 20.9 in 2011  which is much higher than OECD average of 12.4 [24]. Suicide of young people is particularly serious since for 15–39 year old people in Japan  the top cause of death is suicide [15]. To cope with such a serious problem  the government of Japan is making strong efforts on the care of depression [15] since depression is considered to be a major cause of suicide [32]. Before depression can be effectively treated  it must be recognized in individuals [18  34  36]. The WHO reports that the majority of people who need treatment for depression do not receive it [21]. Even during visits with primary care physicians  depression is often unrecognized and undiagnosed; as a result  many people with depression do not receive adequate treatment [36]. Hence  for the effective treatment of depression in the population  it is important that people know how to recognize symptoms of depression in themselves and those around them. Several measures for estimating signs of depression in individuals have been proposed in the form of questionnaires [2  17  30  40]. Among such questionnaires  there are many scales for estimating severity of depression: the Center for Epidemiologic Studies Depression Scale (CES-D) [30]  Beck’s Depression Scale (BDI) [2]  Zung’s Self-rating Depression Scale (SDS) [40]  and Hamilton’s Depression Rating Scale (HDRS) [17] are the more popular. Results on these scales are obtained from answers to the questionnaires  either through self-reporting or third-party observation. These subjective measures are widely used and are expected to achieve high estimation accuracy. Nevertheless  developing techniques for recognizing depression in individuals by using objective  rather than subjective  information is important. One reason for this is that questionnaires are often costly  and sometimes significantly so. In contrast  a method for recognizing depression in individuals by examining objective information  such as records of daily activities  could be applied for a very low cost in many cases. A technique to identify symptoms of depression in individuals from objective information would hasten recognition of depression by individuals and thereby allow earlier access to effective treatment. As objective information for identifying depression in individuals  we focus on large-scale records of users’ activities in social media. In recent years  social media has become increasingly popular  and therefore both large-scale and finegrained records of users’ activities are available [26]. Many researchers are interested in analyzing such large-scale data  and prior analyses have used those for purposes such as estimating the future movement of stock market indices and predicting the results of elections [6  23  38  39  33]. Since social media are often used for expressing emotions  the records of users’ activities are a promising source for information that could be used to recognize symptoms of depression in the users. The use of data from social media to estimate severity of depression in social media users has been studied before. For example  Tsugawa et al. [37] found that frequency of word usage in messages (hereinafter  tweets) on the popular microblogging service Twitter could be used for recognizing depression among users. Pioneering work by De Choudhury et al. [14] describes how to construct support vector machine (SVM) classifiers that can be used to identify users with depression using features obtained from the records of individual users’ activities on Twitter. That research shows the potential for recognizing symptoms of depression in a user by examining the user’s social media activities. The results of prior research suggest that the use of social media for recognizing depression is a promising approach. However  research into these techniques is just beginning  and so extensive evaluation is necessary. As an example  the sensitivity of results to the period over which a user’s social media activities are analyzed is not yet known. Additionally  the generality of the results of [14] should be verified by analyzing other datasets. It should be important to share and accumulate the results from several datasets among researchers particularly in such an emerging research field. In this paper  we extensively evaluate the effectiveness of using a user’s social media activities for estimating degree of depression. As ground truth data  we use the results of a webbased questionnaire for measuring degree of depression; this questionnaire was completed by the Twitter users who agreed to participate. From these data and user activity histories  we investigate the relation between features of each user’s activity and that user’s score on the depression questionnaire. We construct several models to predict users with depression from the features of the user’s activity. For this purpose  we use SVM and investigate the estimation accuracy of the models. Our main contributions are summarized as follows. • We show that features obtained from user activities can be used to predict depression of users with an accuracy of 69%  which is similar to the rate in [14]. While De Choudhury et al. [14] use data about English-speaking users  we use data about Japanese-speaking users. Together  these results suggest that the use of data from social media is an effective approach for users with different backgrounds (here  English-speaking and Japanese-speaking users). • We show the effectiveness of using topics  as identified in tweets by a topic model [5]  as features for the SVM. Our results show that applying a simple bag-of-words model (i.e.  word frequencies) to tweets results in overfitting  but using topics inferred from word frequencies prevents overfitting and improves estimation accuracy. • We investigate the relation between the number of tweets used for estimation and the estimation accuracy. We suggest an appropriate amount of tweets for recognizing depression. Additionally  we find that about two months of observation data are necessary for recognizing depression  and longer observation periods do not contribute to improving the accuracy of estimation for current depression; sometimes  longer periods worsen the accuracy,0,0,0,0,0,0,2,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/2702123.2702280,4,3,related work,"Tsugawa, Kikuchi, Kishino, Nakajima, Itoh, Ohsaki",6,0,0,0,2015,In the areas of medicine and psychology  several questionnaire-based measures for rating depression in individuals have been proposed [2  17  30  40]. CES-D [30]  BDI [2]  and SDS [40] estimate the severity of depression in individuals from the self-reported answers to 20 questions. HDRS uses the answers given by a third party to 17 questions for the same purpose [17]. Approaches that use objective information  such as log data about an individual’s activities to estimate depression severity have been studied recently. Resnik et al. [31] propose a method for identifying depression in individuals by analyzing textual data from essays written by the individuals. Resnik et al. obtain topics for the essays by applying a popular topicextraction model  latent Dirichlet allocation (LDA) [5]. By using the discovered topics as features in machine learning  depression severity is estimated. That research is an important work that shows the potential for using texts written by an individual to estimate severity of depression in that person. However  the essays are typically difficult to obtain. Due to the widespread adoption of social media and the availability of large-scale data from social media  approaches that use such data for depression screening are receiving increased attention from researchers. Moreno et al. [22] shows that college students display symptoms consistent with depression on Facebook  a popular social networking service. Park et al. [27] analyze differences between Twitter users with and without depression by analyzing their activities. In Park et al. [28]  a similar analysis is performed by analyzing data from Facebook. Using multiple regression analysis  Tsugawa et al. [37] show that frequencies of word usage on Twitter are useful features for recognizing depression among users. The main objective of such research is to clarify which features that can obtained from user activity are useful for estimating the severity of depression. De Choudhury et al. [14] are pioneering in demonstrating the estimation accuracy that could be achieved by using activities on Twitter to predict depression among users. In their study  De Choudhury et al. obtained training data for machine learning by appealing to large numbers of people for help (popularly known as crowdsourcing). Then  models that could be used to predict risk of depression were identified from several features obtained from the records of user activity on Twitter by using SVM. Experimental results show that depression can be recognized among users with an accuracy of approximately 70%. Such approaches are applied to prediction of postpartum depression from Facebook and Twitter data [11  13]. De Choudhury et al. have also proposed a method for estimating the depressive tendencies of populations by a similar approach [12]. Twitter data are also suggested to be effective for estimating health-related statistics [10  29]. Our study builds on the mentioned prior work and contributes to enhancing methods for predicting depression risk from objective information  particularly large-scale data from social media. Research into using social media data for recognizing depression in individuals is just beginning  and so whenever possible the effectiveness of such approaches should be validated for several datasets. In this study  we adopt an approach similar to that in De Choudhury et al. [14] and apply it to Japanese-speaking Twitter users. Note that this is not the first study on depression of non-English social media users since differences between Korean social media users with and without depression have been analyzed in [27  28]. This study extends the prediction framework of [14] to Japanese-speaking users. We also examine the effectiveness of using topics of tweet messages as a feature for estimating depression risk. Additionally  we investigate the effects on accuracy of observation period and number of data items used for estimation; this has not yet been fully investigated.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/2702123.2702280,4,4,data gathering,"Tsugawa, Kikuchi, Kishino, Nakajima, Itoh, Ohsaki",6,0,0,0,2015,In this study  we gathered information on depression levels of Twitter users and their activity histories. To do this  we published a website to administer a questionnaire and disseminated information about the website over Twitter1. In contrast to De Choudhury et al. [14]  who collected data from Englishspeaking users through crowdsourcing  this study collected data from Japanese-speaking volunteers. This approach was used to investigate the extent to which depression risk can be estimated for a population different from the population considered by the prior research [14]. Figure 1 shows a screenshot of our website. The website collected the responses to a questionnaire to evaluate the degree of depression of the Twitter users who participated (hereinafter  the participants) and to collect the histories of participants activities on Twitter. The activity histories of participants were collected through the Twitter application programming interface (API)2  and the questionnaires to determine degree of depression were completed by participants through their web browsers. Before data collection  visitors to the website were presented with a written explanation of the aims of the experiment  the information that would be collected  and how that information would be handled. Those who consented to become participants after receiving the explanation logged into their individual Twitter accounts through the OAuth authorization process. Next  participants were surveyed on gender  age  occupation  and history of depression  following which they answered a questionnaire designed to evaluate degree of depression. A message called the “kokoro score ” (“kokoro” is a Japanese word meaning “heart”) determined on the basis of answers to the questionnaire and information in the collected tweets  was displayed to participants after completion of the questionnaire (Fig. 2). Experiment participants were able to tweet the message displayed  which made it possible to promote the website over Twitter by word-of-mouth in a type of snowball sampling. The CES-D questionnaire was used to evaluate the degree of depression [30]. In the CES-D test  participants answered 20 questions on a Likert-type 4-point scale. Each answer was assigned a score of 0-3 points  with the sum of the points from all answers used as the score to estimate likelihood of depression. Several standards exist by which to determine the appropriate cutoff score for identifying depression. In this research  we regarded a score of 22 points or higher as indicating active depression and a score of 21 points or lower as indicating no active depression; these are the same values as used in [14] and give a cutoff score of 22. In addition  answers to BDI [2]  a depression scale used with characteristics similar to CESD  were collected to ensure the reliability of data. For each participant  scores were calculated on both scales  with poor correlation regarded as indicating unreliable answers. The time taken to answer the questionnaires was also recorded  and those completed in too brief a time were excluded. After each participant answered the questionnaire  the activity history of that participant on Twitter was collected from Twitter by using the API. At most 3 200 tweets were collected for each participant  and the number of users following the participant and being followed by the participant were recorded. Tweets published after the questionnaire was taken were discarded. The website was opened to the public on 4 December 2013  at which time the authors publicized it on their Twitter accounts. Between 4 December 2013 and 8 February 2014  219 people participated in the experiment. After eliminating participants who did not tweet and participants who answered the questionnaire in fewer than 30 seconds (as previously mentioned  to ensure the reliability of the questionnaire answers)  214 sets of answers remained. Only the first set of answers was used for participants who completed the questionnaire more than once. As a result  data about 209 experiment participants (male: 121; female: 88) aged 16 to 55 (mean: 28.8 years; standard deviation: 8.2 years) were analyzed. The correlations between CES-D score and BDI score for these participants were high  0.87  and there were no participants with uncorrelated scores  so the data for all 209 participants were used; excluded datasets are not discussed any further. Figure 3 shows the histogram of CES-D scores of 209 participants. Among the participants  81 (resp. 128) were estimated to have (resp. not have) active depression  for an incidence of approximately 39%. This incidence is similar to that found by De Choudhury et al. [14]  who identified depression in approximately 36% of participants. Table 1 gives statistics on the activity histories of participants.,2,2,2,2,0,0,0,2,0,2,,0,0,1,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/2702123.2702280,4,5,features for recognizing depression,"Tsugawa, Kikuchi, Kishino, Nakajima, Itoh, Ohsaki",6,0,0,0,2015,It is possible to extract various features from the activity histories of Twitter users. This section explains what kinds of features are used to estimate degree of depression and the way in which these quantities are extracted. Table 2 shows the features used in this study. A detailed explanation of each feature follows. The frequencies of words in a tweet (i.e.  its bag of words) are used as a basic feature relating to the content of the tweet. Tsugawa et al. showed that the word frequencies are useful for identifying depression [37]. MeCab [20] was used to for morphological stemming and categorization of the Japanese tweet text to obtain accurate word frequencies. Particles  auxiliary verbs  adnominal adjectives  and visual symbols were excluded for extracting content words. Words used by only one participant were also excluded  resulting in a total of 84 255 distinct words. However  most of these words were rarely used  and the distribution of word frequencies is extremely biased (see Fig. 4). Because words with a low rate of use were regarded as unlikely to be associated with depression for most users  the frequencies of only the 20 000 words with the highest rate of use (corresponding to 25 or more uses across all participants) were used as a feature in this study. Furthermore  because the number and length of tweets differed by participant  the word frequencies were normalized by the total number of words in the tweets. The topics of the tweets of each user  as estimated by using a representative topic model LDA [5]  were used as a second feature relating to the content of the tweets. With LDA  the distribution of topics in each document is estimated from the word frequencies in each text through unsupervised learning on the assumption that the text and the words in it are generated according to a particular topic [5]. In LDA  the number of topics to identify and a set of documents (as bags of words) are used as input  and a topic distribution is output for each document. As mentioned in Related Work Section  the topics of essays written by university students were estimated by using LDA and found to be useful in evaluating degree of depression [31]. From that study  topics are expected to be a useful feature. A set of all tweets of each user was used as the user document for input in LDA  and the 20 000 words selected as described above were used as the words. We used LDA with collapsed Gibbs sampling [16]. As the parameters of LDA  we used α = 50/K and β = 0.1  where K is the number of topics [16]. All extracted topics were used as the features. The ratio of positive words and the ratio of negative words used in the tweet text are used as the final features relating to tweet content. Users with depression are intuitively expected to use negative words more frequently than users without depression do. To categorize words  a dictionary of affective words [19]  which is compiled by manual evaluation of a dictionary of positive and negative words extracted according to a technique proposed in the literature [35]  is used. The dictionary contains 760 positive words and 862 negative words. The user’s timing of tweets  frequency of tweets  average number of words  retweet rate (rate of republishing other users’ tweets)  mention rate (rate of directly referencing at least one other user)  ratio of tweets containing a uniform resource locator (URL)  number of users being followed  and number of users following are used as features independent of the content of the tweet. The relative ratios of tweets posted during each hour of the day were used to characterize the timing of tweets; the number of posts per day was used as the posting frequency; and the ratio of qualifying tweets to all tweets were used for the retweet ratio  mention ratio  and ratio of tweets containing a URL. These features are used in prior research [14].,0,0,0,0,0,0,0,0,0,0,,2,0,0,0,0,1,,0
https://dl.acm.org/doi/abs/10.1145/2702123.2702280,4,6,comparison of features between users with and without depression,"Tsugawa, Kikuchi, Kishino, Nakajima, Itoh, Ohsaki",6,0,0,0,2015,Differences in the features described in the previous section according to the presence or absence of depression were investigated. A complete discussion of individual-level differences in word frequencies and topics according to the presence of depression would be lengthy and could obfuscate the underlying results. Accordingly  in this section we focus on features other than the bags of words and topics. In the next section  we use the bags of words and topics to predict depression among participants and check those results. First  a comparison was made of the posting frequency by time for all participants (Fig. 5). Figure 5 shows that the posting frequency was the highest at 11 p.m. and the lowest at 4 a.m. through 5 a.m. (all times are Japan Standard Time) among all participants  independent of the presence of depression. Almost no difference was found in posting frequency of the two groups. This differs from the results of [14]  in which a large difference between participants with and without depression was observed in relative frequencies by time. The difference in observed trends suggests that the feature relating to posting time is not robust. Next  the averages of each feature  partitioned by depression status  are examined (see Fig. 6). A significant difference between the two groups was found for the relative ratio of negative words  posting frequency  retweet rate  and ratio of tweets containing a URL. Individual t-tests confirmed significant differences in these features according to the presence of depression  with a significance level of 5%. Furthermore  although not statistically significant in our study  a numerical difference can be seen between groups in the number of followees and followers. These features were found to be significant in [14]  and these are expected to be useful features in the general case. Similarly  although a significant difference was observed in mention frequency in [14]  no such difference was observed in this study. This suggests that mention frequency is not a robust feature  in the same way as posting time. The differences between this study and [14] might be caused by the cultural differences between Japanese-speaking and English-speaking users. However  further investigations are necessary to clarify the reason of the differences; i.e.  whether these are really due to the cultural differences  or just due to the differences of the participants. The above analysis identifies several features that may be helpful in recognizing depression in Twitter users. In the next section  we investigate the accuracy of estimating the presence of active depression from these features.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/2702123.2702280,4,7,prediction accuracy,"Tsugawa, Kikuchi, Kishino, Nakajima, Itoh, Ohsaki",6,0,0,0,2015,In this section  we investigate the degree of accuracy with which the presence of active depression can be ascertained from the features extracted from a user’s activity history. Classifiers were constructed by machine learning with SVM for estimating the presence of active depression  and their classification accuracy was evaluated by 10-fold crossvalidation. Several classifiers were constructed from the extracted features  and the classification accuracy of each was investigated. The precision  recall  F-measure  and accuracy of the estimations (where the CES-D scores are taken as ground truth with a cutoff score of 22) are used as indices to evaluate classification accuracy. The precision is the proportion of participants with depression among those classified as having depression; the recall is the proportion of participants with depression who were classified as having depression; the F-measure is the harmonic mean of the precision and the recall; and the accuracy is the rate of correct classification across all participants. We note here that classifying all participants as not having depression yields precision  recall  F-measure and accuracy of 0  0  0  and 0.61  respectively. In the complementary case  in which all participants are classified as having depression  the corresponding values are 0.39  1  0.56  and 0.39  in the same order. Table 3 shows the classification accuracy of the constructed model. The classification accuracies are the average values given by 10-fold cross-validation carried out 100 times. There are a large number of combinations of all features  so only the results of the most accurate models and those models useful for discussion are shown. A radial basis function kernel was used for the SVM kernel. Table 3 shows that the presence of active depression can be estimated by the most accurate model with 0.61 precision  0.37 recall  0.46 F-measure and 66% accuracy. Because the research of De Choudhury et al. and that of this study use different data  a simple comparison cannot be carried out  but these accuracies can be considered to be comparable level to those reported by De Choudhury et al. [14]. Since similar results were obtained from different datasets  estimating the presence of depression by examining users’ activity histories on Twitter can be considered to be a useful technique. Looking at the features used  it is found that the estimation accuracy of models using the bag of words as a feature is low. The cause of this may be that this approach results in overfitting. When using the bag of words feature  the feature dimensions are too high  which is known to degrade performance. In Tab. 3  limiting the number of words to 2 000 (rather than 20 000) improves accuracy with the bag of words feature  which confirms that overfitting is occurring. However  even after this adjustment  accuracy with that feature is low in comparison to the accuracy of models without it. In contrast  models using topics as a feature achieved a high accuracy. This means it may be possible to improve estimation accuracy by reducing the dimensions to the broad feature of topics  rather than using the narrow feature of word frequencies. Not only topics but also other features (positive  negative  tweet frequency  RT  URL  followee  and follower from Tab. 2) were found to result in more accurate models than using the bag of words  and these additional features may be useful when predicting the presence of depression. Together with topics  models using the number of followees and the posting frequency achieved the highest accuracy of the models investigated here. Looking at the number of topics in the topic model  it was found that accuracy was the highest in this experiment when the number of topics was set at 10. This is probably because using a very small number of topics lowered the expressiveness of the model using a large number of topics made it difficult to capture the predominant topics.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/2702123.2702280,4,8,effects of the amount of data on prediction accuracy,"Tsugawa, Kikuchi, Kishino, Nakajima, Itoh, Ohsaki",6,0,0,0,2015,Next  we varied the quantity of tweets used for learning and estimation  and investigated the resulting estimation accuracy in order to find the quantity of data needed to make accurate predictions. Features were extracted using the N most-recent tweets (prior to study participation) and applied these features to model learning and prediction. For users with fewer than N tweets  all available tweets were used. Here  we discuss the results for the following models: the 10-topic model (Model 1); a model using the features positive  negative  tweet frequency  RT  URL  followee  and follower (Model 2); and a 10-topic model including the features positive  negative  tweet frequency  RT  URL  followee  and follower with the highest estimation accuracy from the previous section (Model 3). Figure 7 shows the relation between N (the number of tweets used) and the F-measure (used to representing estimation accuracy). From these results  it is found that the F-measure reaches approximately 0.45 when the number of tweets used is approximately 500–1000  and it remains almost unchanged even when the number of tweets used is further increased. This implies that observing around 500–1000 tweets is sufficient for recognizing depression  and observation of a higher number of tweets contributes negligibly to estimation accuracy. The time span over which tweets were observed was also controlled  and the F-measure was calculated to assess the resulting estimation accuracy. The tweets used in learning and prediction were limited to those made in the W weeks prior to the day on which the participant answered the questionnaire  and the previously mentioned models were used for prediction. Figure 8 shows the relation between the time span W and the F-measure. From this result  it is found that the F-measure reaches 0.4– 0.5 when the tweets of the prior 6–16 weeks (1.5–4 months) are used  and lengthening the time span does not improve this; in some cases  longer time spans result in worse results. This indicates that using tweets from the most recent 6–16 weeks is sufficient for recognizing depression  and observations spanning a longer period may be less accurate. This could be because out-of-date information may not reflect the current state of depression; in such cases  the additional data do not contain information useful for prediction. The highest accuracy is achieved when using tweets from the recent 8 weeks  and the presence of active depression can be estimated by Model 3 with 0.64 precision  0.43 recall  0.52 F-measure and 69% accuracy.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/2702123.2702280,4,9,discussion,"Tsugawa, Kikuchi, Kishino, Nakajima, Itoh, Ohsaki",6,0,0,0,2015,This study investigated the degree to which the activity history of Twitter users is useful in recognizing depression. The results showed that the activity history of users can be used to recognize the presence of depression with an accuracy of approximately 69%. The fact that similar results were obtained between user groups with different languages and backgrounds  as here and in [14]  strongly suggests that using social media activity is a valid approach to recognizing depression. Sociological studies show that the rate of self-disclosure of Japanese is lower than that of American (e.g.  [1]); i.e.  Japanese people talk less about themselves than American people. In contrast  Japanese people post tweets very frequently. The ratio of Japanese tweets around the world is 14% in 20113 although the population of Japanese is only 130 million. It should be noted that using social media data is an effective approach for people with such different cultural backgrounds. Note that we do not claim that our models achieved higher performance than those in [14]  but just claim that our models achieved comparable performance with [14]. While it is difficult to directly compare the results in [14] and ours since the data are different from each other  the values of precision and recall reported in [14] are higher than ours. There were several challenges to apply the framework of [14] to Japanese-speaking twitter users  and these challenges might affect the performance of our models. Our models didn’t include several features used in [14]. Choudhury et al. [14] show that usage of 1st person pronoun and usage of 3rd person pronoun are useful features. However  as linguistic researches discuss (e.g.  [25])  Japanese personal pronouns are significantly different at least from those in the Western languages. Japanese have a large number of forms for each person pronoun. For instance  in [25]  21 types of Japanese 1st person pronoun are listed  and we can observe more variety of forms of pronoun in social media texts. Moreover  the subject words are often missing in Japanese texts  and using pronouns for subject words is sometimes unnatural particularly in twitter-like short and casual texts. We therefore decided not to include such features in our models. Moreover  to the best of our knowledge  there are no available standard Japanese dictionaries like ANEW lexicon [7]  and we therefore used a simple list of positive and negative words. These limitations might affect our model performance. While linguistic-based features and some dictionary-based features were not used in our study  we adopted a topic modeling approach and demonstrated the usefulness of it. Our experimental results showed that the model including topics as the features of SVM achieves 2% higher accuracy and 0.05 point higher F-measure than the model without topics (see Tab. 3). Resnik et al. [31] show the usefulness of a topic model when recognizing depression by analyzing essays written by university students. This study extends that result to Twitter. A single tweet is short  but because a large number of tweets (around 1 000 here) can be used  it is possible to extract topics successfully  which should make Twitter data useful for recognizing depression. Using topic model is expected to be a promising approach also for recognizing depression of non-Japanese twitter users since Resnik et al. [31] have already shown the usefulness of a topic model to English documents  and topic modeling approaches are successful in other tasks such as predicting stock market prices [33] and public health statistics [29] from social media text data. We therefore expect that the accuracy of the model found in [14] could be increased by including topics as a feature. The experimental results of this paper suggest that the length of the selected activity history is important. De Choudhury et al. [14] set the observation period at one year  but our experimental results indicate that accuracy is likely to be improved by adjusting this period appropriately. However  the period of tweets that is informative may differ by user. A more detailed analysis is necessary to isolate per-user differences. This study has some limitations. First  there is still room for further improvement of the features used. De Choudhury et al. [14] reduced the dimensions of high-dimensional features by using principal component analysis and treated the results as SVM features. They also applied data abstraction techniques such as using the average  variance  and entropy of the features for each day. It is important to verify how effective these techniques would be on the data of this study. It may also be important to demonstrate what kind of machine learning method is most effective for the purpose of recognizing depression. Rather than using only SVM  the use of the newest machine learning frameworks  such as deep learning [3  4] and ensemble learning [8  9]  is expected to have the potential to further increase estimation accuracy. In particular  deep learning is known to be able to extract useful features from within a large quantity of features. The bag of words and other features that could not be applied effectively in this study may be more effective if deep learning is used in place of SVM. Finally  this experiment was carried out using information from only 209 users; it is important to determine the degree to which the accuracy increases (if at all) by increasing the number of users to a larger number  such as 1 000 or 10 000 people. Collecting a large quantity of learning data is difficult to do because of the data characteristics  but the collection of larger-scale learning data may be important to obtain the best possible results from the approach described here.,0,0,0,0,0,0,0,0,0,0,,1,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/2702123.2702280,4,10,conclusion,"Tsugawa, Kikuchi, Kishino, Nakajima, Itoh, Ohsaki",6,0,0,0,2015,This study investigated how useful the various features extracted from Twitter user history are for recognizing depression  and the degree of accuracy with which the presence of active depression could be detected by using these features. Our aim was to establish a method by which to recognize depression by analyzing the large-scale records of users’ activities in social media. The following specific results were obtained: depression can be recognized in users with an accuracy of approximately 69%; topics extracted by a topic model are useful features; around two-months observation is sufficient for the tweets used in learning and prediction; and long observation periods may decrease accuracy. The estimation of user depression over time is suggested for future research. The ability to estimate daily variations in depression may be a useful tool for self-diagnosis as well as for diagnosis by a medical professional. Determining techniques that could be used in a medical context to identify depression from social media users’ activities is an important future task.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://www.nature.com/articles/s41598-017-12961-9,5,1,abstract,"Reece, Reagan, Lix, Dodds, Danforth, Langer",6,0,0,0,2017,We developed computational models to predict the emergence of depression and Post-Traumatic Stress Disorder in Twitter users. Twitter data and details of depression history were collected from 204 individuals (105 depressed  99 healthy). We extracted predictive features measuring affect  linguistic style  and context from participant tweets (N = 279 951) and built models using these features with supervised learning algorithms. Resulting models successfully discriminated between depressed and healthy content  and compared favorably to general practitioners’ average success rates in diagnosing depression  albeit in a separate population. Results held even when the analysis was restricted to content posted before first depression diagnosis. State-space temporal analysis suggests that onset of depression may be detectable from Twitter data several months prior to diagnosis. Predictive results were replicated with a separate sample of individuals diagnosed with PTSD (Nusers = 174  Ntweets = 243 775). A state-space time series model revealed indicators of PTSD almost immediately post-trauma  often many months prior to clinical diagnosis. These methods suggest a data-driven  predictive approach for early screening and detection of mental illness.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://www.nature.com/articles/s41598-017-12961-9,5,2,introduction,"Reece, Reagan, Lix, Dodds, Danforth, Langer",6,0,0,0,2017,Social media data provide valuable clues about physical and mental health conditions. This holds true even in cases where social media users are not yet aware that their health has changed. For example  searching for information on certain health symptoms has been shown to provide accurate early-warning indicators for hard-to-detect cancers1. Social media networks have been used to plot the trajectory of disease outbreaks2 3 4  and to track regional dietary health5. In addition to physical ailments  predictive screening methods have successfully identified markers in social media data for a number of mental health issues  including addiction6  depression7 8 9 10 11 12  Post-Traumatic Stress Disorder (PTSD)13 14  and suicidal ideation15. The field of predictive health screening with social media data is still in its infancy  however  and considerable refinements are needed to develop methodologies that can effectively augment health care. In this report  we present a set of improved methods and novel contributions for predicting and tracking depression and PTSD on Twitter. Depression has emerged as the leading mental health condition of interest among computational social scientists7 8 9 10 11 12  as it is a relatively common mental disorder16 and influences a range of behaviors and patterns of communication17. Underdiagnosis of depression remains a persistent problem; a recent survey of a major metropolitan area found nearly half (45%) of all cases of major depression were undiagnosed18. PTSD  while less common19  is frequently comorbid with major depression20. Studies have found that PTSD is underdiagnosed or under-treated by a majority of primary-care physicians21 22. The costs of underdiagnosis of these conditions  both to human quality of life and health care systems  are considerable. Computational methods for early screening and diagnosis of depression and PTSD have the potential to make a positive impact on a major public health issue  with minimal associated costs and labor intensity.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://www.nature.com/articles/s41598-017-12961-9,5,3,improvements and novel contributions,"Reece, Reagan, Lix, Dodds, Danforth, Langer",6,0,0,0,2017,Early efforts to detect depression and PTSD signals in Twitter data have been promising. Park et al.11 established that Twitter users suffering from depression tended to post tweets containing more negative emotional sentiment compared to healthy users. De Choudhury et al.7 successfully identified new mothers suffering from postpartum depression  based on changes in Twitter usage and tweet content. In a separate analysis  De Choudhury et al.8 found that depressive signals were observable in tweets made by individuals with Major Depressive Disorder. In addition  De Choudhury et al.23 found that increased social isolation  measured via Facebook data  was predictive of postpartum depression in mothers. A small number of studies have attempted to identify PTSD markers in Twitter data13 14. This growing literature has employed progressively more sophisticated methods for making intelligent inferences about Twitter users’ mental health based on their online activity. Despite these advances  we identified a number of methodological issues in recent reports which we have improved upon in the present work. A brief review of these modifications provide motivation for the results that follow. This growing literature has employed progressively more sophisticated methods for making intelligent inferences about Twitter users’ mental health based on their online activity. Despite these advances  we identified a number of methodological issues in recent reports which we have improved upon in the present work. A brief review of these modifications provide motivation for the results that follow. We chose to use only tweets posted prior to the date of subjects’ first depression diagnosis  rather than focus on recent depressive episodes  for three reasons. First  self-reported information about depressive symptoms is often inaccurate24. By contrast  a clinical diagnosis is an explicit event that does not rely on subjective impressions  as may be the case with self-reported onset dates. Second  individuals diagnosed with depression often come to identify with their diagnosis25 26  and subsequent choices  including how to portray oneself on social media  may be influenced by this identification. It is possible that the predictive signals indicated in De Choudhury et al.8 were not tracking depressive symptoms  per se  but rather identified purposeful communication choices on the part of depressed Twitter users. Third  and most important  if we are able to accurately discriminate between depressed and healthy participants using only tweets posted prior to first diagnosis  this would support a stronger claim than has been made previously - namely  that Twitter data are capable not only of detecting depression  but can do so before the first diagnosis has been made. While date of first diagnosis provides a more reliable temporal marker than self-reported onset of symptoms  onset timing is also valuable to researchers and health care professionals looking to better understand depression. This is especially true regarding the onset of an individual’s first depressive episode. Winokur27 found that over 50% of depression patients experienced first onset at least 6 months prior to diagnosis. The months during which individuals suffering from depression are undiagnosed and untreated pose a significant health risk. Given that the changes that occur with the first onset of depression may be reflected in social media data  we hypothesized that a computational approach could model the progression of depression without any explicit estimates of onset. Using only the content of participants’ tweets  we generated a time series model which charts the course of illness in depressed individuals  and compared this with healthy participants’ data. To our knowledge  De Choudhury et al.8 represent the state-of-the-art in depression screening on Twitter  and our own work was informed by their innovative methods. Accordingly  we report model accuracy scores from De Choudhury et al.8 along with our results as a point of comparison. All of the above methodological improvements were also applied to our PTSD analysis  which used a separate cohort of study participants. Extant literature on PTSD detection in Twitter data13 14 differ from our analysis in important ways. Previous research used bulk collections of public tweets  and assigned PTSD labels to users based on tweets which mentioned a PTSD diagnosis. By comparison  we communicated directly with participants  and excluded any who could not report a specific date on which they received a professional clinical diagnosis. Our analytical approach incorporated a wide array of metadata features and semantic measures  which were limited14 or missing entirely13 from earlier research. Most importantly  previous research focused only on differentiating PTSD users from healthy users  without any consideration of timing with respect to the dates of traumatic events or diagnoses. Our models focused specifically on identifying predictive markers of PTSD prior to diagnosis date  as well as tracking the course of this disorder over time.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://www.nature.com/articles/s41598-017-12961-9,5,4,comparison with trained heathcare professionals,"Reece, Reagan, Lix, Dodds, Danforth, Langer",6,0,0,0,2017,Mitchell  Vaze  and Rao28 evaluated general practitioners’ abilities to correctly diagnose depression in their patients  without assistance from scales  questionnaires  or other measurement instruments. Out of 50 371 patient outcomes culled from 118 studies  21.9% of patients were actually depressed. General practitioners were able to correctly rule out depression in 81% of non-depressed patients  but only correctly diagnosed depressed patients 42% of the time. Taubman-Ben-Ari et al.22 tested primary-care physicians’ abilities to detect PTSD. PTSD prevalence was 7.5% for men and 10.5% for women in the observed sample (N = 683). Physicians correctly identified 2.5% of PTSD cases  and out of all PTSD diagnoses made  only 43% were accurate. We refer to general practitioner accuracy rates22 28 as an informal benchmark for the quality of our computational models.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://www.nature.com/articles/s41598-017-12961-9,5,5,method,"Reece, Reagan, Lix, Dodds, Danforth, Langer",6,0,0,0,2017,The methods used in recruitment  data collection  and analysis are adopted from Reece and Danforth12. The present study was reviewed and approved by the Harvard University Institutional Review Board  approval #15-2529  as well as the University of Vermont Institutional Review Board  approval #CHRMS-16-135. All experimental procedures were performed in accordance with Institutional Review Board guidelines. All study participants provided informed consent and acknowledged all of the study goals  expectations  and procedures  including data privacy  prior to any data collection. Surveys were built using the Qualtrics survey platform  and analyses were performed using Python and R. Twitter data collection apps were written in Python  using the Twitter developer’s Application Programming Interface (API).,0,0,0,0,0,0,0,2,0,2,,0,0,0,0,0,0,,0
https://www.nature.com/articles/s41598-017-12961-9,5,6,data collection,"Reece, Reagan, Lix, Dodds, Danforth, Langer",6,0,0,0,2017,Participants were recruited using Amazon’s Mechanical Turk (MTurk) crowdwork platform  and we collected user data from both the survey on MTurk and participants’ Twitter history. Recruitment and data collection procedures were identical for depression and PTSD samples  with the exception of the condition-specific questionnaire used for screening. Separate surveys were created for affected and healthy samples. The term “affected” here refers to participants affected by either depression or PTSD  respectively. In the affected sample surveys  participants were invited to complete a questionnaire that involved passing a series of inclusion criteria  responding to a standardized clinical assessment survey  answering questions related to demographics and mental health history  and sharing social media history. We used the CES-D (Center for Epidemiologic Studies Depression Scale) questionnaire to screen participant depression levels29. CES-D assessment quality has been demonstrated as on-par with other depression inventories  including the Beck Depression Inventory and the Kellner Symptom Questionnaire30 31. The Trauma Screening Questionnaire (TSQ) was used to screen for PTSD32. A comparison cohort of healthy participants were screened to ensure no history of depression or PTSD  respectively  and for active Twitter use. Qualified participants were asked to share their Twitter usernames and history. An app embedded in the survey allowed participants to securely log into their Twitter accounts and agree to share their data. Upon securing consent  we made a one-time collection of participants’ Twitter posting history  up to the most recent 3 200 tweets (this limit is imposed by the Twitter API). In total we collected 279 951 tweets from 204 Twitter users for the depression analysis  and 243 775 tweets from 174 Twitter users for the PTSD analysis. Details on participant data protection measures are outlined below.,2,1,2,2,0,0,2,2,0,0,,0,0,2,0,0,0,,0
https://www.nature.com/articles/s41598-017-12961-9,5,7,inclusion criteria,"Reece, Reagan, Lix, Dodds, Danforth, Langer",6,0,0,0,2017,The surveys for affected samples collected age data from participants  and asked qualified participants questions related to their first clinical diagnosis of either depression or PTSD  as well as questions about social media usage at the time of diagnosis. These questions were given in addition to the CES-D or TSQ scales. The purpose of these questions was to determine: The date of first clinical diagnosis of the condition  Whether or not the individual suspected having the condition before diagnosis  and  If so  the number of days prior to diagnosis that this suspicion began In the case that participants could not recall exact dates  they were instructed to approximate the actual date. The survey for healthy participants collected age and gender data from participants. It also asked four questions regarding personal health history  which were used as inclusion criteria for this and three other studies. These questions were as follows:Have you ever been pregnant? Have you ever been clinically diagnosed with depression? Have you ever been clinically diagnosed with Post-Traumatic Stress Disorder? Have you ever been diagnosed with cancer? Participants’ responses to these questions were not used in analysis  and only served to include qualified respondents in each of the various studies  including the depression- and PTSD-related studies reported here.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://www.nature.com/articles/s41598-017-12961-9,5,8,participant safety and privacy,"Reece, Reagan, Lix, Dodds, Danforth, Langer",6,0,0,0,2017,This study design raised two important issues regarding ethical research practices  as it concerned both individuals with mental illness and potentially personally identifiable information. We were unable to guarantee strict anonymity to participants  given that usernames and personal information posted to Twitter are often inherently specific to participants’ identities (such as usernames and tweets containing real names). As we potentially had the capacity to link study participants’ identities to sensitive health information  study participants were informed of the risks of being personally identified from their social media data. Participants were assured that no personal identifiers  including usernames  would ever be made public or published in any format. We used Turk Prime  an interface for conducting MTurk studies  to mask participants’ MTurk worker IDs from our records. We made it clear that any links between social media data and private personal health data would be available only to our team of researchers  and participants were able to request to have their data removed at any time.,0,0,0,0,0,0,0,0,0,0,,0,2,0,0,0,0,,1
https://www.nature.com/articles/s41598-017-12961-9,5,9,improving data quality,"Reece, Reagan, Lix, Dodds, Danforth, Langer",6,0,0,0,2017,In an effort to minimize noisy and unreliable data  we applied several quality assurance measures in our data collection process. MTurk workers who have completed at least 100 tasks  with a minimum 95% approval rating  have been found to provide reliable  valid survey responses33. We restricted survey visibility only to workers with these qualifications. Survey access was also restricted to U.S. IP addresses  as MTurk data collected from outside the United States are generally of poorer quality34. All participants were only permitted to take the survey once. We excluded participants with a total of fewer than five Twitter posts. We also excluded participants with CES-D scores of 21 or lower (depression)  or TSQ scores of 5 or lower (PTSD). Studies have indicated that a CES-D score of 22 represents an optimal cutoff for identifying clinically relevant depression35 36; an equivalent TSQ cutoff of 6 has been found to be optimal in the case of PTSD32. We note here that in the study that inspired the present work  De Choudhury et al.8 used two depression scales (CES-D and BDI)  and filtered individuals whose depression score did not correlate across the both scales. This additional criteria is a methodological strength of De Choudhury et al.8 with respect to the present work.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://www.nature.com/articles/s41598-017-12961-9,5,10,summary statistics,"Reece, Reagan, Lix, Dodds, Danforth, Langer",6,0,0,0,2017,All data collection took place between February 1  2016 and June 10  2016. Across both depressed and healthy groups  we collected data from 204 Twitter users  totaling 279 951 tweets. This number includes up to 3 200 tweets from each participant’s Twitter history; the analyses in this report focus only on tweets from depressed users created before the date of first depression diagnosis. The mean number of posts per user was 1372.71 (SD = 1281.74). This distribution was skewed by a smaller number of frequent posters  as evidenced by a median value of just 861 posts per user. See Table 1 for summary statistics. In the depressed group  147 crowdworkers successfully completed participation and provided access to their Twitter data. Imposing the CES-D cutoff reduced the number of viable participants to 105. The mean age for viable participants was 30.3 years (SD = 8.34)  with a range of 18 to 64 years. Dates of participants’ first depression diagnoses ranged from March 2010 to February 2016  with nearly all diagnosis dates (92%) occurring in the period 2013–2015. In the healthy group  99 participants completed participation and provided access to their Twitter data. The mean age for this group was 33.9 years  with a range of 19 to 63 years  and 42% of respondents were female. (Gender data were not collected for affected sample surveys). For the PTSD analysis  we collected data from 174 Twitter users  totaling 243 775. The mean number of posts per user was 1372.71 (SD = 1281.74). This distribution was skewed by a smaller number of frequent posters  as evidenced by a median value of just 862 posts per user. In the PTSD sample  73 crowdworkers successfully completed participation and provided access to their Twitter data. Imposing the TSQ cutoff reduced the number of viable participants to 63. The mean age for viable participants was 30.64 years (SD = 7.57)  with a range of 21 to 54 years. Dates of participants’ first PTSD diagnoses ranged from April 2010 to December 2015  with nearly all diagnosis dates (94%) occurring in the period 2013–2015. In the healthy group  111 participants completed participation and provided access to their Twitter data. The mean age for this group was 33.25 years  with a range of 19 to 63 years  and 51% of respondents were female.,0,0,0,0,1,0,0,0,0,0,,0,0,0,0,0,0,,0
https://www.nature.com/articles/s41598-017-12961-9,5,11,feature extraction,"Reece, Reagan, Lix, Dodds, Danforth, Langer",6,0,0,0,2017,We extracted several categories of predictors from the Twitter posts collected. Predictor selection for both depression and PTSD was based on prior machine learning models of depression in Twitter data7 8  as the two conditions’ high comorbidity rates suggest their predictive signals may exhibit considerable overlap20. Depressed Twitter users have been observed to tweet less frequently than non-depressed users8  so we used total tweets per user  per day  as a measure of user activity. Tweet metadata was analyzed to assess average word count per tweet (here  a word is defined as a set of characters surrounded by whitespace)  whether or not the tweet was a retweet  and whether or not the tweet was a reply to someone else’s tweet. The labMT  LIWC 2007  and ANEW unigram sentiment instruments were used to quantify the happiness of tweet language37 38 39 40. The use of labMT  which has shown strong prior performance in analyzing happiness on Twitter41 42  is novel with respect to depression screening; ANEW and LIWC have been successfully applied in previous studies on depression and Twitter7 8 14. LIWC was also used to compile frequency counts of various parts of speech (e.g.  pronouns  verbs  adjectives) and semantic categories (e.g.  food words  familial terms  profanity) as additional predictors37.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://www.nature.com/articles/s41598-017-12961-9,5,12,units of observation,"Reece, Reagan, Lix, Dodds, Danforth, Langer",6,0,0,0,2017,Determining the best time span for analysis raises a difficult question: When and for how long does mental illness occur? Receiving a clinical diagnosis of depression or PTSD does not imply that an individual remains in a persistent state of illness  and so to conduct analysis with an individual’s entire posting history as a single unit of observation is a dubious proposition. At the other extreme  to take one tweet as a unit of observation runs the risk of being too granular. De Choudhury et al.8 looked at all of a given user’s tweets in a single day  and aggregated those data into per-person  per-day units of observation. In this report we have followed the convention of aggregated “user-days” as a primary unit of analysis  rather than try to categorize a person’s entire history  or analyze each individual tweet. In our own previous research  however  we have found that many Twitter users do not generate enough daily content to make for robust unigram sentiment analysis43. For completeness  we conducted analyses using both daily and weekly units of observation. Both analyses yielded predictive models of similar strengths  with the weekly model showing a slight  but consistent  edge in performance. We report accuracy metrics from both analyses  but restrict other results to the daily-unit analysis to allow for more direct comparison with previous research. Details of weekly-unit analytical results are available in Supplementary Information  section II. When reporting results we occasionally refer to observations or tweets as “depressed”  e.g.  “depressed tweets received fewer likes”. It would be more correct to use the phrase “tweet data from depressed participants  aggregated by user-days” instead of “depressed tweets”  but we chose to sacrifice a degree of technical correctness for the sake of clarity.,0,0,0,0,0,1,0,0,0,0,,0,0,0,0,0,0,,0
https://www.nature.com/articles/s41598-017-12961-9,5,13,machine learning models,"Reece, Reagan, Lix, Dodds, Danforth, Langer",6,0,0,0,2017,We trained supervised machine learning classifiers to discriminate between affected and healthy sample members’ observations. Classifiers were trained on a randomly-selected 70% of total observations  and tested on the remaining 30%. Out of several candidate algorithms  a 1200-tree Random Forests classifier demonstrated best performance. Stratified five-fold cross-validation was used to optimize Random Forests hyperparameters  and final accuracy scores were averaged over five separate randomized runs. See Supplementary Information  section III for optimization details. Precision  recall  specificity  negative predictive value  and F1 accuracy scores are reported  and general practitioners’ unassisted diagnostic accuracy rates as reported in Mitchell  Vaze  and Rao28 (MVR) and Taubman-Ben-Ari et al.22 (TBA) are used as informal benchmarks for depression and PTSD  respectively. In addition to the fact that our results are drawn from different samples  using different observational units  than our chosen comparisons  comparing point estimates of accuracy metrics is not a statistically formal means of model comparison. We felt it was more meaningful to frame our findings in a realistic context  however informal  rather than to benchmark against a naive statistical model that simply predicted the majority class for all observations.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://www.nature.com/articles/s41598-017-12961-9,5,14,time series analysis,"Reece, Reagan, Lix, Dodds, Danforth, Langer",6,0,0,0,2017,Predictive screening methods use indirect indicators  such as language use on social media  to infer health status. We are not actually interested in the average word count of depressed individuals’ tweets  for example  but rather we hope that this measure will allow us some access to the underlying variable we truly care about: depression. Accordingly  state-space models  which use observable data to estimate the status of a latent  or hidden  variable over time  may provide useful insights. We trained a two-state Hidden Markov Model (HMM) to detect differential changes between affected and healthy groups over time. We used the hmmlearn Python module44 to fit emission and transition matrices (using expectation-maximization) and hidden state sequence (using the Viterbi path algorithm); this module also provided mean parameter estimates for all predictors  for each latent state. The use of HMM presents an interpretability challenge: how to know whether resulting latent states have any relationship to the clinical condition of interest? Consider the case of depression: Finding evidence that HMM had  in fact  recovered two states from our data that closely resembled the depressed and healthy classes was prerequisite to making any inferences based on HMM output. We addressed this issue by comparing HMM output with mean differences between depressed and healthy observations in the raw data. If the directions of the differences between HMM mean parameter estimates generally agree with the true differences in the data  this provides evidence that the two sample groups in our data (depressed and healthy) are well-characterized by HMM latent states. For example  if depressed observations contained more sad words on average than healthy observations (variable name: “LIWC_sad”)  then the HMM state with the higher LIWC_sad estimate is more likely to be the depressed one  given that HMM does track depression (i.e.  the latent states generated by HMM map onto “depressed” and “healthy”). If  on the other hand  HMM-generated states are weakly or not at all related to depression  there should be no clear alignment between HMM means and means in the raw data. The same procedure was applied when fitting an HMM to the PTSD data.,0,0,0,0,0,0,0,0,0,0,,1,0,0,0,0,0,,0
https://www.nature.com/articles/s41598-017-12961-9,5,15,word shift graphs,"Reece, Reagan, Lix, Dodds, Danforth, Langer",6,0,0,0,2017,Machine learning algorithms provide powerful predictive capability  but most algorithms offer little in the way of context and interpretation. Word shift graphs use the labMT happiness scores  the most important predictor for both depression and PTSD analyses  to show qualitatively how inter-group differences may be driven by the usage of specific words in tweets39. We present word shift graphs comparing the way tweet language adjusted happiness scores in affected and healthy samples. The visualization ranks words by their contribution to the happiness difference between the two groups (for more explanation of word shift rankings  see 45 46). Word shifts are generated from a different statistical method than the machine learning algorithm we used to make predictions  and so should be treated as exploratory analyses independent of our main findings.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://www.nature.com/articles/s41598-017-12961-9,5,16,results,"Reece, Reagan, Lix, Dodds, Danforth, Langer",6,0,0,0,2017,For the depression study  we analyzed 74 990 daily observations (23 541 depressed) from 204 individuals (105 depressed). For the PTSD study  we analyzed 54 197 daily observations (13 008 PTSD) from 174 individuals (63 PTSD). Observations from affected sample members accounted for 31.4% and 24% of the entire datasets for depression and PTSD  respectively.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://www.nature.com/articles/s41598-017-12961-9,5,17,machine learning classifier,"Reece, Reagan, Lix, Dodds, Danforth, Langer",6,0,0,0,2017,Results are reported for both daily and weekly units of observation (see Table 2 and Fig. 1). Our best depression classifier  averaged over cross-validation iterations  improved over both Mitchell et al.28 and De Choudhury et al.8 on several metrics. Our depression model’s precision rate was considerably higher  with just over 1 false positive for every 10 depression diagnoses. By comparison  general practitioners from Mitchell et al.28 incorrectly diagnosed patients as having depression in more than half of all diagnoses.Our best PTSD classifier improved considerably over the primary-care physicians from Taubman-Ben-Ari et al.22 (TBA). Whereas more than half of all PTSD diagnoses made by TBA physicians were incorrect  our model was correct in roughly 9 out of every 10 (88.2%) of its PTSD predictions. Model recall rate was strong  with 68.3% discovery of actual PTSD sample observations.The labMT happiness score was the strongest predictor of both depression and PTSD. Notably  average labMT average happiness over user days showed only modest correlation with ANEW (rdepr = 0.37  rptsd = 0.36) and LIWC (rdepr = 0.36  rptsd = 0.37)  suggesting that labMT identifies relevant prediction signals not fully captured by other sentiment instruments. The additional benefit offered by labMT in this context may be a reflection of its inclusion of the 5000 most frequently used words on Twitter  including slang39. The second most important variable was word count  which represented the average number of words per tweet. Sentiment-related variables from ANEW and LIWC accounted for most of the remaining top predictors (see Fig. 1). As with most decision-tree classifiers  the Random Forests algorithm provides information on the relevance  but not the directionality  of predictors. In other words  we can know how important a variable was to the algorithm  but not if it was positively or negatively associated with the response variable. Word shift graphs  reported below  offer some indication of directionality but are computed differently than Random Forests and should not be used to directly interpret Random Forests output. However  as a means of informing early detection methods  predictor directionality becomes less important  as the goal is identification  rather than explanation  of possible mental health issues.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://www.nature.com/articles/s41598-017-12961-9,5,18,time course description,"Reece, Reagan, Lix, Dodds, Danforth, Langer",6,0,0,0,2017,A Hidden Markov Model simulated affected and healthy states. HMM states were determined to accurately track with affected and healthy groups by comparing differences in mean parameter estimates between the model fit and original data. Across all 40 predictors  HMM means were in agreement with true means for the depression sample in 38 cases (95% agreement)  and were in 100% agreement with true means for the PTSD sample. This evidence strongly suggested that the two states identified by HMM were closely aligned with the affected and healthy classes in our data  and we have reported HMM results based on this assumption. See Figs 2 and 3. Depressed individuals showed a slightly higher probability of depression even in the period nine months prior to diagnosis  and gradually diverged from healthy data points. Healthy individuals showed a steady  lower probability of depression  which did not change noticeably over an 18-month period. By three months prior to diagnosis  depressed subjects showed a marked rise in probability of being in a depressed state  whereas healthy individuals showed little or no change over the same time period. Post-diagnosis  probability of depression began to decrease after 3–4 months (90–120 days). This trajectory matches closely with average improvement time frames observed in therapeutic programs47. Given that HMM constructed latent states from unlabeled data  it is striking that HMM not only reconstructed the division between depressed and healthy groups  but also generated a plausible timeline for depression onset and recovery. Similarly  tweets from individuals with PTSD deviated from healthy tweets within months after the date of the traumatic event that caused PTSD (indicated by the orange line in Fig. 3)  and well over a year before the average time to diagnosis (the mean time period from trauma to diagnosis was 586 days). A decrease in PTSD probability can be observed shortly after diagnosis  indicating possible improvement due to treatment. While these results suggest promise  further analysis of the nature of aggregate time courses  including investigation of more sophisticated methods of time-series analysis44 48 49 will be a subject of future work. In particular  the health trajectories associated with individuals (not in aggregate)  other forms of text-based communication  and applications to other mental illnesses should be explored in greater detail.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://www.nature.com/articles/s41598-017-12961-9,5,19,word shift graphs,"Reece, Reagan, Lix, Dodds, Danforth, Langer",6,0,0,0,2017,We averaged labMT happiness scores across observations in each class  after the removal of common neutral words and re-tweeted promotional material39. Neutral words were words with labMT happiness scores between 4 and 6  on a 1–9 scale. This included many common parts of speech  including articles and pronouns  which contributed little to understanding inter-group differences in valenced language. Some of the positive language observed more frequently among healthy individuals came from re-tweets of promotional or other advertising material (e.g.  “win”  “free”  “gift”). We removed obvious promotional retweets when generating word shift graphs  as their removal did not significantly change mean tweet-happiness differences between groups  and the resulting graphs gave better impressions of what participants personally tweeted about. We observed that tweets authored by the depressed class were sadder (havg = 6.01) than the healthy class (havg = 6.15). In Fig. 4  we rank order individual words with respect to their contribution to this observed difference  and display the top contributing words. PTSD word shift graphs are included in Appendix III. The dominant contributor to the difference between depressed and healthy classes was an increase in usage of negative words by the depressed class  including “no”  “never”  “prison”  “murder”  and “death”. The second largest contributor was a decrease in positive language by the depressed class  relative to the healthy class  including fewer appearances of “happy”  “beach”  and “photo”. The increased usage of negatively valenced language by depressed individuals is congruent with previous research50.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://www.nature.com/articles/s41598-017-12961-9,5,20,discussion,"Reece, Reagan, Lix, Dodds, Danforth, Langer",6,0,0,0,2017,The aim of the present study was to identify predictive markers of depression and PTSD based on users’ Twitter data using computational methods. Our findings strongly support the claim that computational methods can effectively screen Twitter data for indicators of depression and PTSD. Our method identified these mental health conditions earlier and more accurately than the performance of trained health professionals  and was more precise than previous computational approaches. Our state-space models portrayed a timeline for depression which is impressively realistic  given that it was generated analyzing only the text of 140-character messages. In addition  HMM identified a rise in probability of PTSD within six months  post-trauma  compared to the average 19 month delay between trauma event and diagnosis experienced by the individuals in our sample. Word shifts provided context for the specific differences in language that shifted happiness scores between samples. These advances make improvements on existing predictive screening technology  as well as contribute novel methods for the identification and tracking of mental illness. The HMM depression timeline is an intriguing finding  and should be treated with both optimism and caution. HMM assigned each data point a probability of belonging to two latent state-spaces while “blind” to our actual states of interest  as affected/healthy labels were removed from HMM training data. Considering that the model could have used criteria completely unrelated to mental health to delineate between the two latent classes  it is noteworthy that the resulting states’ mean estimates for each variable in both PTSD and depression analyses closely resembled the mean estimates for affected and healthy participants’ data  respectively. This adds support for the claim that affected-condition and healthy Twitter users’ data are objectively different  in addition to providing justification for the use of HMM assignments as indicators of depression/PTSD signals at a given point in time. Despite this evidence in favor of applying HMM to analyze mental health trajectories  HMM is an unsupervised learning procedure and so conclusive HMM-based inferences should be approached cautiously and with close attention on validation procedures. The diverging trajectories observed in our HMM time series suggest that  with careful attention to model validity  state-space modeling may be used to identify and track the onset of certain mental illnesses over time  using only Twitter data. The labMT happiness measure proved to be the most important predictor in our model  and was considerably stronger than ANEW or LIWC happiness indicators. This is in line with a series of previous findings  which have found labMT measures to be a superior for tracking happiness in Twitter data40  and suggests that future research in this field should incorporate this instrument for more accurate measurements. That average tweet word count was the second most important predictor is intriguing  especially as increases in word count were positively associated with depression and PTSD. If anything  depression is often characterized by reduced communication17  although word count is distinct from posting frequency  which was not a significant predictor in our models. The current depression and PTSD literatures are largely devoid of studies relating verbosity to these conditions  and so this finding may motivate new inquiries into behavioral traits of mental health disorders as observed on social media. From a practical standpoint  our model showed considerable improvement over the ability of unassisted general practitioners to correctly diagnose depression and PTSD. Despite the imprecise nature of this comparison  given the paucity of data currently available to serve as benchmarks for the type of analysis performed in the present study  our model’s relative success seems encouraging. Health care providers may be able to improve quality of care and better identify individuals in need of treatment based on the simple  low-cost methods outlined in this report. Especially given that mental health services are unavailable or underfunded in many countries51  this computational approach  which only requires patients’ digital consent to share their social media history  may open avenues to care which are currently difficult or impossible to provide. Future investigations would use the same participant pool to collect both health professionals’ assessments  as well as computational models of participant social media behavior  to allow for more precise comparison. The present findings may be limited by the non-specific use of the term “depression” in participant surveys. While earlier research identified depression predictors in Twitter data for Major Depressive Disorder and postpartum depression7 8  we used a more general category in our recruitment and data collection to build a predictive model capable of screening for common depressive signals. We acknowledge that depression diagnoses exist across a clinical spectrum. It is possible that participants with a specific type of depression were responsible for the observed results. Future research might examine other specific depression classes  including manic depression and dysthymia  to determine whether predictive screening models should be segmented per diagnosis type. It is also possible that inferences from our results are limited specifically to Twitter users who have been diagnosed with depression or PTSD  and who are willing to share their social media history with researchers. Current literature on depression treatment suggests that people who seek out mental health services are usually “well-informed and psychologically minded  experience typical symptoms of depression and little stigma  and have confidence in the effectiveness of treatment  few concerns about side effects  adequate social support  and high self-efficacy”52. Since it is possible only a subset of Twitter users will fit this description  we recommend making conservative inferences about depression  as well as PTSD  based on our findings. Considering the frequent comorbidity of depression and PTSD20  together with the similarity in predictor importance observed across our analyses of these two conditions  the signals driving our predictive models may share considerable overlap. While our results do not offer strict segregation between these two conditions  this gives little cause for concern when considered from a mental health screening perspective. If the desired outcome is to identify individuals who may be in need of mental health services  whether an individual is flagged for evaluation for depression with possible associated PTSD  or vice-versa  becomes an academic distinction. If anything  the issue of comorbidity may serve as a useful reminder that this computational method should not be regarded as a standalone diagnostic tool  but rather as a technology for early identification of potential mental health issues. As the methods employed in the present study aim to infer health related information about individuals  some additional cautionary considerations are in order. Data privacy and ethical research practices are of particular concern  given recent admissions that individuals’ Facebook and dating profile data were experimentally manipulated or exposed without permission53 54. Indeed  we observed a response rate reflecting a seemingly reluctant population. Of the 2 261 individuals who began our survey  790 (35%) refused to share their Twitter username and history  even after we identified ourselves as an “academic  not-for-profit research team” and provided the above-mentioned guarantees about data privacy. Future research should prioritize establishing confidence among experimental participants that their data will remain secure and private. Complicating efforts to build socio-technical tools such as the models presented in this study  data trends often change over time  degrading model performance without frequent calibration55. As such  our results should be considered a methodological proof-of-concept upon which to build and refine subsequent models. This report provides an outline for an accessible  accurate  and inexpensive means of improving depression and PTSD screening  especially in contexts where in-person assessments are difficult or costly. In concert with robust data privacy and ethical analytics practices  future models based on our work may serve to augment traditional mental health care procedures. More generally  our results support the idea that computational analysis of social media can be used to identify major changes in individual psychology.,0,0,0,0,0,0,0,2,0,0,,0,0,0,0,0,0,,0
https://aclanthology.org/W14-3214.pdf,6,1,abstract,"Schwartz, Eichstaedt, Kern, Park, Sap, Stillwell, Kosinski, Ungar",8,0,0,0,2014,Depression is typically diagnosed as being present or absent. However  depression severity is believed to be continuously distributed rather than dichotomous. Severity may vary for a given patient daily and seasonally as a function of many variables ranging from life events to environmental factors. Repeated population-scale assessment of depression through questionnaires is expensive. In this paper we use survey responses and status updates from 28 749 Facebook users to develop a regression model that predicts users’ degree of depression based on their Facebook status updates. Our user-level predictive accuracy is modest  significantly outperforming a baseline of average user sentiment. We use our model to estimate user changes in depression across seasons  and find  consistent with literature  users’ degree of depression most often increases from summer to winter. We then show the potential to study factors driving individuals’ level of depression by looking at its most highly correlated language features.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://aclanthology.org/W14-3214.pdf,6,2,introduction,"Schwartz, Eichstaedt, Kern, Park, Sap, Stillwell, Kosinski, Ungar",8,0,0,0,2014,Depression  a common mental disorder  greatly contributes to the economic  social  and physical burden of people worldwide. Along with other mental disorders it has been related to early termination of education  unstable marriages  teenage pregnancy  financial problems  role impairment  heart disease  and other negative outcomes (Kessler and Bromet  2013; Lichtman et al.  2014) Currently  depression is primarily assessed through surveys. Diagnoses require a medical or psychological evaluation  and are typically classified into discrete categories (absent  mild  moderate  severe). Clinicians rely on retrospective reports by patients to monitor symptoms and treatment. Unobtrusive assessments based on language use in Facebook and social media usage could amend both the self-help resources available to patients as well as repertoire of clinicians with richer information. Such a tool could allow for more frequent and fine grained (i.e.  continuously scored) assessment and could provide contextualized information (e.g. specific words and online activities that are contributing to the user’s depression score). Here  we predict and characterize one’s degree of depression (DDep) based on their language use in Facebook. Datasets connecting surveyed depression with language in Facebook are rare at best. To operationalize DDep  we use the depression facet scores of the “Big 5” item pool (Goldberg  1999) from the MyPersonality dataset. This provides a continuous value outcome  for which we fit a regression model based on ngrams  LDA topics  and lexica usage. By predicting continuous values  rather than classes  one can track changes in DDep of varying size across time; we find significantly more users’ DDep increases from summer to winter than vice-versa. Our primary contribution is the exploration of predicting continuous-valued depression scores from individuals’ social media messages. To the best of our knowledge this has not previously been studied  with other social media and depression work focused on discrete classes: present or absent. We compare our predictive model of DDep to one derived from a state-of-the-art sentiment lexicon and look at changes across seasons. Finally  we characterize DDep by looking at its top ngram and topic correlates.,0,0,0,0,0,0,0,1,0,0,,0,0,0,0,0,0,,0
https://aclanthology.org/W14-3214.pdf,6,3,depression,"Schwartz, Eichstaedt, Kern, Park, Sap, Stillwell, Kosinski, Ungar",8,0,0,0,2014,Depression is generally characterized by persistent low mood  poor concentration  fatigue  and little interest in normally enjoyable activities. Depression can range from mild to severe  and can occur as an acute episode (major depressive episode)  extend chronically over time (major depressive disorder  persistent depressive disorder)  reoccur after a period of remission (recurrent depression)  or occur at specific periods (seasonal affective disorder  postpartum depression  premenstrual dysphoric disorder). Prevalence rates vary; the World Health Organization estimates that over 350 million people worldwide have a depressive disorder  with many more reporting at least some symptoms (Organization  2012). In the U.S.  in the World Health Mental Survey  over half of the respondents (62%) endorsed at least one diagnostic stem questions for depression  with 19.2% meeting criteria for at least one major depressive episode (Kessler et al.  2010). Although depression has long been defined as a single disease with a set of diagnostic criteria  it often occurs comorbidly with other psychological and physical disorders. Anxiety  anger  and other psychological disorders often co-occur with depression  and some have suggested that anxiety and depression are different manifestations of the same underlying pathology (Mineka et al.  1998). An expert panel convened by the American Heart Association recently recommended that depression be considered a formal risk factor for heart disease (Lichtman et al.  2014). Depression has been related to a range of physical conditions  including asthma  cancer  cardiovascular disease  diabetes  and chronic pain (Kessler and Bromet  2013)  although the causal direction is confounded; it may be that other factors cause both depression and physical illness (Friedman and Kern  2014). As noted previously  assessing degree of depression as a continuous value allows us to look at changes in depression across time. There has been longstanding interest and discussion of seasonal patterns of depression  with observations of seasonal depressive patterns apparent in ancient times  and the first systematic description occurring in 1984 (Westrin and Lam  2007). Commonly called Seasonal Affective Disorder (SAD)  the DSM-V now refers to this pattern as recurrent major depressive disorder with a seasonal pattern. A clinical diagnosis of seasonal depression requires that two major depressive episodes have occurred in the past two years  with the onset and remission showing a regular temporal pattern (predominantly with onset occurring in the fall/winter and full remission in spring/summer). Patients with depression often have common symptoms of low energy  reduced or intensified psychomotor movements  low concentration  indecisiveness  and thoughts of death  as well as related symptoms such as fatigue  insomnia  and weight gain. A challenge in diagnosis is that it relies on a patient’s historical report  and other possible causes such as physical illness must be ruled out. Further  with stigmas against mental illness and feats about seeking treatment  many cases go unrecognized  causing considerable burden on the individual and society as a whole. Prevalence rates vary  but rigorous reviews suggest a prevalence of .4% in the U.S.  although estimates have been reported as high as 10% (Blazer et al.  1998; Magnusson and Partonen  2005). There are a number of different hypotheses about the pathophysiology of S A D  including circadian  neurotransmitter  and genetic causes (Lam and Levitan  2000). Reviews suggest that light therapy is an effective and well-tolerated treatment  with effects equal to or larger than antidepressants (Golden et al.  2005; Lam and Levitan  2000; Thompson  2001; Westrin and Lam  2007). Attempts to explain why light therapy is so effective have included shifting photoperiods (lightdark cycles  with less light in the winter)  changes in melotonin secretion  and circadian phase shifts (Lam and Levitan  2000). One related explanation for the photoperiod effect is latitude  with the prevalence of seasonal depression increasing with growing distance from the equator. Although there has been some support for this hypothesis in the U.S. (Rosen et al.  1990)  findings in other countries have been mixed (Mersch et al.  1999). Although latitude may play some role  other factors such as climate  genetic vulnerability  and the sociocultural context may have a stronger impact. Altogether  inconsistent results suggest that there is considerable variation in the magnitude  causes and manifestations of seasonal depression  much of which is not fully understood  in part due to diagnostic issues (Lam and Levitan  2000). A weekly or even daily depression assessment tool would allow us to more fully understand the seasonal and other temporal changes in depression. We use the “depression facet” scores derived from a subset of the “big-5” personality items. Specifically  depression is one of several facets (e.g. anger  depression  anxiety  selfconsciousness  impulsiveness  vulnerability) of the neuroticism personality factor. Neuroticism refers to individual differences in the tendency to experience negative  distressing emotions  and behavioral and cognitive styles that result from this (McCrae and John  1992). It includes traits such as tension  depression  frustration  guilt  and selfconsciousness  and is associated with low selfesteem  irrational thoughts and behaviors  ineffective coping styles  and somatic complaints. Various scales have been developed to measure neuroticism  such as the Eysenck Personality Questionnaire (Eysenck and Eysenck  1975) and the NEO-PI-R (Costa and McCrae  1992). Some items on these scales overlap with self-reported items that screen for depression (e.g.  personality item: “I am often down in the dumps”; depression screening item: “how often have you been feeling down  depressed  or hopeless?”; see Table 1.)  such that the personality items effectively provide a proxy measure of depressive tendencies.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://aclanthology.org/W14-3214.pdf,6,4,related work,"Schwartz, Eichstaedt, Kern, Park, Sap, Stillwell, Kosinski, Ungar",8,0,0,0,2014,Depression has been linked with many online behaviors. In fact  even Internet usage itself seems to vary as a function of being depressed(Katikalapudi et al.  2012). Other behaviors include social networking (Moreno et al.  2011) and differences in location sharing on Facebook (Park et al.  2013). Most related to our work  are those using linguistic features to assess various measures of depression. For example  De Choudhury et al. (2013) used online posting behavior  network characteristics  and linguistic features when trying to predict depression rather than find its correlates. They used crowdsourcing to screen Twitter users with the CES-D test (Beekman et al.  1997)  while others analyzed one year of Facebook status updates for DSM diagnostic critera of a Major Depressive Episode (Moreno et al.  2011). In addition  Park et al. (2013) predicted results of the Beck Depression Inventory (Beck et al.  1961). While previous works have made major headway toward automatic depression assessment tools from social media  to the best of our knowledge  none have tried to predict depression as a continuum rather than a discrete  present or absent  attribute. For instance  Neuman et al. (2012) classified blog posts based on whether they contained signs of depression  and De Choudhury et al. (2013) classified which newfound mothers would suffer from postpartum depression.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://aclanthology.org/W14-3214.pdf,6,5,method,"Schwartz, Eichstaedt, Kern, Park, Sap, Stillwell, Kosinski, Ungar",8,0,0,0,2014,We used a dataset of 28 749 nonclinical users who opted into a Facebook application (“MyPersonality”; Kosinski and Stillwell  2012) between June 2009 and March 2011  completed a 100-item personality questionnaire (an International Personality Item Pool (IPIP) proxy to the NEO-PI-R (Goldberg  1999)  and shared access to their status updates containing at least 500 words. Users wrote on average of 4 236 words (69 917 624 total word instances)  and a subset of 16 507 users provided gender and age  in which 57.0% were female and the mean age was 24.8. The dataset was divided into training and testing samples. In particular  the testing sample consisted of a random set of 1000 users who wrote at least 1000 words and completed the personality measure  while the training set contained the 27 749 remaining users. We estimated user-level degree of depression (DDep) as the average response to seven depression facet items  which are nested within the larger Neuroticism item pool. For each item  users indicated how accurately short phrases described themselves (e.g.  “often feel blue”  “dislike myself”; responses ranged from 1 = very inaccurate to 5 = very accurate). Figure 1a shows the distribution of surveyassessed DDep (standardized). The items can be seen in Table 1. Figure 2 shows the daily averages of surveyassessed DDep  collapsed across years. A LOESS smoother over the daily averages illustrates a seasonal trend  with depression rising over the winter months and dropping during the summer. In order to get a continuous value output from our model  we explored regression techniques over our training data. Since this first work exploring regression was concerned primarily with language content  our features for predicting depression were based entirely on language use (other social media activity and friend networks may be considered in future work). These features can be broken into four categories: Ngrams of order to 1 to 3  found via HappierFunTokenizer  and restricted to those used by at least 5% of users (resulting in 10 450 ngrams). The features were encoded as relative frequency of mentioning each ngram (ng): 2000 LDA derived Facebook topics.1 Usage was calculate 64 LIWC categories (Pennebaker et al.  2007) as well as the sentiment lexicon from NRC Canada (Mohammad et al.  2013).2 Usage of a lexicon (lex) was calculated similar to the LDA topics  where w is the weight of the word in the lexicon in the case of sentiment and always 1 in the case of LIWC which has no weights: Encoded simply as the integer value for that user. We used penalized linear regression to fit our features to DDep. We experimented with a few penalization types over the training set and settled on L2 (“ridge regression”)  using Principal Components Analysis to first reduce the ngram and topic features to 10 % of their original size. In order to ensure users tested provided an adequate amount of features  we only tested over those with at least 1 000 words. However  we found that including more users in our training set at the expense of words per user increased model accuracy. Thus  we only required our training data users to mention 500 words  essentially allowing more noise in order to increase the number of training examples. We also experimented with training models on two sets of messages: all messages and the subset of messages written in the same three-month season as the survey administration (season only messages). Because the degree of depression may vary over time  we reasoned that messages written closer to survey administration might better reflect the degree of depression assessed by the survey. When generating predictions on users in the test set  we applied both the all messages model and the season only messages model to features from all messages and then to just the features from the same season as the survey administration.,2,2,1,2,2,0,0,0,0,0,,0,0,0,0,2,2,,0
https://aclanthology.org/W14-3214.pdf,6,6,evaluation and results,"Schwartz, Eichstaedt, Kern, Park, Sap, Stillwell, Kosinski, Ungar",8,0,0,0,2014,We evaluated accuracy using the Pearson correlation coefficient r between our predictions and survey-assessed DDep. As a baseline  we built a regression model simply using the NRC sentiment (Mohammad et al.  2013) feature. Accuracies are shown in Table 2. Accuracy was highest (r = .386) when we trained a model over all messages from users in the training set and then applied this model to all messages by users in the test set. Though our model allows for seasonal change in depression  we suspect the test across all messages was more accurate than that of only using the season in which the users depression was assessed due to the larger amount messages and language features provided to the model. Both models (season-only messages  and all messages) gave significant (p < 0.05) improvement over the baseline (r = .149) and though these accuracies may look small  it’s worth noting that a correlation above r = 0.3 is often regarded as a strong link between a behavior and a psychological outcome (Meyer et al.  2001). Still  we fit many behavior variables (i.e.  language use features) to an outcome and so we might hope for higher variance explained. We suspect having more users to train on and taking more features into account could improve results. For example  people who nearly stopped writing for a season would be thrown out of our analyses since it is completely based on language content  even though they are more likely to be depressed (social isolation is a common symptom in depression). Similarly  we do not use demographics in our models  even though women are more likely to become depressed than men. To assess individual seasonal changes in degree of depression  we predicted summer and winter DDep values for each user with at least 1000 words across both summer-only and winter-only messages  respectively. We then compared the differences across the seasonal predictions; Figure 3 shows the distribution of user-level seasonal differences across 676 users with sufficient language for both seasonal predictions. In line with the trends seen in survey data  average user-level DDep values  as predicted by language  were significantly higher in the winter months (t = 4.63  p < .001).,0,0,0,0,0,0,2,0,0,0,,0,0,0,0,0,0,,0
https://aclanthology.org/W14-3214.pdf,6,7,differential language analysis,"Schwartz, Eichstaedt, Kern, Park, Sap, Stillwell, Kosinski, Ungar",8,0,0,0,2014,Figure 4 shows the 100 ngrams most highly correlated with depression score across the 21 913 Facebook users in our dataset writing at least 1 000 words. Unlike typical word clouds  the clouds represent language that differentiates users scoring high on depression. The size of a word represents its correlation with depression (larger = stronger)  the color its relative frequency (grey = rarely used  blue = moderately used  red = frequently used). The f-word emerges as both the most correlated feature (as indicated by the size of the word) and is highly frequent (indicated by the red color). Together with words such as ‘pissed’ and ‘bloody’  these curse words suggest hostility or aggression. Similarly  words such has ‘hate’ and ‘lonely’ suggest negative social relationships. Perhaps surprisingly  the words ‘depression’ and ‘depressed’ emerge as highly correlated features. These face valid features occur infrequently (as indicated by their grey color)  yet are strongly associated with depressive tendencies  demonstrating the high statistical power of our approach applied to this large dataset in identifying significant but rarely used language features. The both frequent and highly correlated word ‘why’ hints at signs of hopelessness and meaninglessness  a core feature of depressive disorders. As illustrated in Figure 5  extending the words and phrase results  automatically derived topics demonstrate substantial overlap with the major clinical symptoms of major depressive disorder (American Psychiatric Association et al.  2013). Hopelessness and meaninglessness are seemingly expressed by ‘hopeless’ and ‘helpless’. Perhaps the most noticable symptom of depression  depressed mood  is expressed in topics mentioning ‘feel’  ‘crap’  ‘sad’  and ‘miserable’. Depression often affects psychomotor function  either in terms of fatigue and low energy or inversely as insomnia and hyperactivity. Such symptoms are reflected in words such as ‘tired’  and ‘sleep’. Depression is often expressed somatically through bodily symptoms  captured through ‘hurt’  ‘my head’ and ‘pain’. One of the most predictive questions on depressive screening questionnaires asks about suicidal thought  which appears with topics related to thoughts of death  with words such as ‘kill’  ‘die’  and ‘dying’. Topics also reflected hostility  aggression  and negative relationships with other people. Loneliness has emerged as one of the strongest predictors of physical morbidity and mortality (Hawkley and Cacioppo  2010)  and both ‘lonely’ and ‘alone’ appear as some of the most correlated single words. Given such striking descriptive results  future work might try to detect depression associated conditions as well such as insomnia  loneliness  and aggression.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://aclanthology.org/W14-3214.pdf,6,8,conclusion,"Schwartz, Eichstaedt, Kern, Park, Sap, Stillwell, Kosinski, Ungar",8,0,0,0,2014,Depression can be viewed as a continuous construct that changes over time  rather than simply as being a disease that one has or does not have. We showed that regression models based on Facebook language can be used to predict an individual’s degree of depression  as measured by a depression facet survey. In line with survey seasonal trends and the broader literature  we found that languagebased predictions of depression were higher in the winter than the summer  suggesting that our continuous predictions are capturing small  yet meaningful within-person changes. With further development of regression models  many users write enough on Facebook that we could estimate changes in their level of depression on a monthly or even weekly basis. Such estimates  correlated with word use over time offers potential both for research at the group-level (“What are the social and environmental determinants of depression?”  “How well are talk or medication-based interventions working?”) as well as  eventually  for medical and therapeutic application at the individual level (“How well am I doing and what depression relevant thoughts or behaviors have I disclosed in the past week?”).,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://ieeexplore.ieee.org/abstract/document/6784326,7,1,abstract,"Nguyen, Phung, Dao, Venkatesh, Berk",5,0,0,0,2014,"A large number of people use online communities to discuss mental health issues, thus offering opportunities for new understanding of these communities. This paper aims to study the characteristics of online depression communities (CLINICAL) in comparison with those joining other online communities (CONTROL). We use machine learning and statistical methods to discriminate online messages between depression and control communities using mood, psycholinguistic processes and content topics extracted from the posts generated by members of these communities. All aspects including mood, the written content and writing style are found to be significantly different between two types of communities. Sentiment analysis shows the clinical group have lower valence than people in the control group. For language styles and topics, statistical tests reject the hypothesis of equality on psycholinguistic processes and topics between two groups. We show good predictive validity in depression classification using topics and psycholinguistic clues as features. Clear discrimination between writing styles and contents, with good predictive power is an important step in understanding social media and its use in mental health.",0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://ieeexplore.ieee.org/abstract/document/6784326,7,2,introduction,"Nguyen, Phung, Dao, Venkatesh, Berk",5,0,0,0,2014,THE web has changed our lives irrevocably  and we are at the forefront of this shifted pattern of life with increasing social communication via the Internet. Networking and interaction play a central part in any social media system and ‘community’ is an example of this aspect of social media. For healthcare research  theoretical machine learning techniques applied to virtual communities enable us to answer the question ‘Is there difference between clinical and control communities?’ In this paper  the aspect given is not expressed in terms of friendship  exchange of information  social support or recreation  but rather with regard to the sentiment information  topics and linguistic styles that people express in their online writing. Online communities are diverse—in the demography  the reason for existence and facilities afforded by the hosting application. For the isolated or mentally ill  the web provides an alternative support  overcoming barriers of declining physical contact and social cohesion. But often the effectiveness of this support is blunted by the volume of information  its uncertain quality and the complexity of discovering and connecting with appropriate support communities. This problem is brought into sharp relief by the prevalence and the cost of just one mental disorder— depression. One in four people suffer from depression during their lives [15] and it is associated with million suicide incidents annually. Depression is a mental disorder requiring a continuing network of support from diverse sources—parents  relatives  friends  neighbors. Under these conditions  social media has unique value as online communities for people who share struggles with psychiatric disorders. Social media has been considered a new venue for depression investigations [7]  [19]  [25]  however  the community context in this venue has not been taken into account. For analyzing the textual content generated in social media  two representations have been widely used. First  linguistic styles have been found to be indicative of depression in several studies in psychiatry [29]  [30]  [33]. In these works  the Linguistic Inquiry and Word Count (LIWC) package [27]—a software developed in psychology supporting automatic text analysis—is used to extract the linguistic styles. It is then shown that such linguistic styles can be considered an indicator of depression. Second  topics have been used to characterize text documents [4]  [10]. Nevertheless  latent topics  extracted through probabilistic models  have not been constructed on the text corpus generated by mental health communities. These two representations enable understanding of what (topics) people are interested in and how (language styles) they discuss their interests  potentially providing insights into the differences between mental health communities and others. However  the affective aspect  in particular the role of sentiment and mood  has not been studied. Mood is an integral to a text  particularly for social media forums: two communities may discuss the same topics in entirely different atmospheres. For example  one forum might host conversations about politics in a cerebral and friendly fashion; another might discuss the same issue adversarially and use profanities. Thus  in addition to the depressive markers exhibited through topics and linguistic styles  in our analysis  mood and other affective information expressed by online depression bloggers is also taken into account. This paper uses a large scale cohort of data from nearly 10 000 people in 24 mental health communities (called the CLINICAL group) that broadly covers bipolar  self-harm  depression  suicide and separation communities. We present an analysis of this clinical group in contrast with control group of standard communities  focusing on mood  topics and psycholinguistic processes expressed in the content of posts. Both conventional statistical tests as well as machine learning algorithms are utilized in the analysis. We note that such data can be obtained unobtrusively and that the algorithms we construct are privacy preserving. A key contribution of this work is to introduce a comprehensive view of bloggers in such mental health communities using sentiment information  topics of interest and language styles. Of these  to our knowledge  the sentiment information of these communities is explored for the first time. Another contribution of this work is to provide a set of predictors of depression  consisting of the aspects that differentiate mental health communities from others. The significance of this study is the demonstration of the potential of social media as a new channel for depression screening and monitoring  providing a foundation for online early warning systems. Our results offer potential to apply machine learning to psychiatric practice and research.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://ieeexplore.ieee.org/abstract/document/6784326,7,3,background,"Nguyen, Phung, Dao, Venkatesh, Berk",5,0,0,0,2014,There has been an explosion in the scope and use of social media  whereby people divulge large amounts of data in real-time. In parallel  rapid advances in machine learning and data mining have been made [16]. This allows us to discover patterns in data on a theoretical basis and make predictions based on patterns detected from this data. As an example  using Twitter posts  pandemic (H1N1) 2009 influenza and validated disease activity were tracked. Machine learning was able to construct a predictive model utilizing a million influenza-related tweets and this was compared to the CDC’s reported data. The model could predict disease activity in real-time  producing accurate data 1-2 weeks before the CDC reports [31]. Using the Internet as a form of sensor for monitoring mental-related conditions has also recently emerged as an important research topic  such as monitoring suicidal behavior in Australia [24] or studying the influence of seasons to depression [38]. Similarly  mining the blogs of military personnel involved in operation enduring freedom in Iraq was able to discern the emotional tone of blogs and detect service personnel experience and emotional reactions to combat exposure [17]. Language styles  based on the social and psychological meaning of words (such as in LIWC package)  have been explored across diverse research areas in sociology and psychology [35]. For example  LIWC features were found to be powerful predictors of influence and personality traits [23]. Similarly  language use has been found a predictor of depression. For example  in a research of detecting linguistic cues of depression in English and Spanish forums  the linguistic markers of depression found in conventional written text were also found in the language of depression online [29]. For a corpus of essays written by depressed colleague students  it was found that depressed students used more first person singular pronouns and more negative emotions words in their essays than those students who did not experience any depressive episode [30]. In a study on suicide  suicidal poets were found to use more first person pronouns and less first plural pronouns in their writing than did non-suicidal poets [33]. Further evidences of the value of word usage and expressive writing to physical and mental health is established in [26]. Investigations into depression in online setting have been conducted. For example  online search activity has been used to detect several depression phenomena. For example  Yang et al. [38] found the seasonality of online searching for depression terms; Ayers et al. [2] found the seasonal trends in Google searching for mental health terms  such as ADHD  anxiety  bipolar  depression  anorexia  schizophrenia and suicide. This finding provides health care research novel evidence of seasonal depression based on public searching trends. Tefft [36] found that there is a positive relationship between the Google Trends data on ‘depression’ and ‘anxiety’ with the unemployment rate. Similarly  strong associations were found between the Internet search query volumes on psychological distress terms  including ‘anxiety disorder’  ‘what is depression’  ‘signs of depression’  ‘depression symptoms’ and ‘symptoms of depression’  with mortgage delinquencies and unemployment statistics [3]. Noticeably  Sueki [34] found that the suicide death rate was not correlated with the searching volumes of ‘suicide’ but was correlated with the increase in searching for suiciderelated terms  such as ‘depression.’ For social networks in online setting  Choudhury et al. [7] attempted to predict depressed individuals based on their tweets  including the diurnal trends (or insomnia index  based on the number of posts made hourly). Also for Twitter data  Park et al. [25] found difference in the reason of joining the social network between those with and without depression. The non-depressed considered Twitter as an information consuming and sharing tool  whereas the depressed perceived it as a tool for social awareness and emotional interaction. For Facebook  subscribers’ status updates were found to reveal symptoms of major depressive episodes [19]  providing a source to identify potential patients  possibly suggesting a foundation for early warning systems. Similarly  Jeong et al. [32] showed that using social networks such as Facebook for major depressive disorder screening among college students is feasible and efficient. In a related problem  Cash et al. [6] found that adolescents reveal their suicidal thoughts and possible intentions on MySpace  providing opportunities to identify adolescents at risks. Online social networks have also been utilized for depression treatment. George et al. [9] demonstrated that Facebook can be used to treat stress and depression for firstyear medical students. It has also been demonstrated that using Internet-based interventions in post traumatic stress disorder treatment is feasible [11]. Kessler et al. [14] examined if a cognitive-behavioral therapy (CBT) delivered online incorporated into usual care can improve the treatment of depression. They found that the group with this intervention had a higher rate of patients recovered from depression  suggesting an effective method  in addition to traditional face-to-face  to deliver the CBT. Andrews et al. [1] concluded that these ways of CBT delivery still provided an effective  acceptable and practical treatment for anxiety and depressive disorder patients. his work. LiveJournal provides a platform for people of common interests to join community and discuss about their medical conditions. Moreover  in LiveJournal  users can label their posts with their moods at the time of writing  providing mood and sentiment data to understand the affective aspect of depression. Mishne [18] introduced one of the first mood classifications for this type of data. He used the length of journals  emphasized words and special symbols as features. The classification accuracy was modest  being slightly above baseline. A wider range of features  including cheap and effective features inspired from psychology study  were used in [21] for the problem of mood classification for LiveJournal posts. For the same problem  an approach using a hierarchy of possible moods was introduced in [12]  [13]  achieving better results than flat classifications. Furthermore  a link between mood and social capital was found in LiveJournal communities [28] and the mood-based features were used in the problem of community detection in the blog hosting site [22].,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://ieeexplore.ieee.org/abstract/document/6784326,7,4,data sets,"Nguyen, Phung, Dao, Venkatesh, Berk",5,0,0,0,2014,a) CLINICAL Communities: Communities who are interested in ‘depression’ and with at least 200 posts are extracted from LiveJournal. This is identified through the ‘Search communities by interest’2 provided by LiveJournal and results in 24 communities with 38 401 posts. The CLINICAL communities are grouped based on name and description of the individual community: depression  bipolar  self-harm  attachment/separation and suicide (See Table 1 for statistics). The earliest community creation date was in 2001  thus our data set spans over 10 years. b) CONTROL communities: We constructed a CONTROL data set using five popular categories of communities in the LiveJournal Directory.3 We select communities who have at least 200 posts  resulting in 23 communities with 229 563 posts. This set is called CONTROL  and the statistics of these 23 communities and their description are shown in Table 2.,2,2,2,2,0,1,0,0,0,0,,0,0,0,0,0,0,,0
https://ieeexplore.ieee.org/abstract/document/6784326,7,5,feature extraction,"Nguyen, Phung, Dao, Venkatesh, Berk",5,0,0,0,2014,To characterize the difference between CLINICAL and CONTROL communities  a variety of features are extracted: Affective features: We use the lexicon—Affective Norms for English Words (ANEW) [5]—to extract the sentiment conveyed in the content. This lexicon consists of 1034 words  rated in terms of valence and arousal  and is thus suitable for a quantitative estimation. The valence of ANEW words is on a scale of 1 (very unpleasant) to 9 (very pleasant). The arousal is measured on the same scale—1 (least active) to 9 (most active). A cloud visualization of ANEW words used in the blog posts made by CLINICAL and CONTROL groups is illustrated in Fig. 1. users to tag their posts from a list of 132 pre-defined mood labels.4 Thus in addition to the emotion expressed in the text of posts  the mood tag produced allows us direct access to the user sentiment. A cloud visualization of moods tagged on blog posts made by CLINICAL and CONTROL communities is illustrated in Fig. 2. LIWC features: We examine the proportions of words in psycholinguistic categories as defined in the LIWC package [27]: linguistic  social  affective  cognitive  perceptual  biological  relativity  personal concerns and spoken.5 Table 3 presents the mean of these LIWC psycholinguistic processes for the CLINICAL and CONTROL communities. Whilst similar in the use words with positive emotion  people in the CLINICAL communities tend to use words with more negative emotion—as examples  anxiety  anger and sadness. Further  they discuss more issues about health and death in comparison with the CONTROL group. On the other hand  the users in the CONTROL group discuss more neutral life related topics—ingestion  home and leisure words. Topics: For extracting topics  latent Dirichlet allocation (LDA) [4] is used as a Bayesian probabilistic modelling framework. LDA extracts the probabilities pðvocabulary jtopicÞ—that is  words in a topic  and then assigns a topic to each word in a document. For the inference part  we implemented Gibbs inference detailed in [10]. We set the number of topics to 50  run the Gibbs for 5 000 samples and use the last Gibbs sample to interpret the results.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,2,,0
https://ieeexplore.ieee.org/abstract/document/6784326,7,6,classification and feature selection,"Nguyen, Phung, Dao, Venkatesh, Berk",5,0,0,0,2014,We formulate two classification problems to examine the usefulness of extracted features: CLINICAL or CONTROL. Community classification  categories a particular community as either CLINICAL or CONTROL. Denote by B the corpus of all N blog posts. Given a blog post d 2 B  we are interested in predicting if the post belongs to either CLINICAL or COMMUNITY  based on textual features extracted from d  denoted as xðdÞ ¼ ½... ; xðdÞ i ; .... Since ANEW and mood have low coverage  or are not covered by a majority of posts  only topics or LIWC are used in the feature vector. When topics are the features  xðdÞ i is the probability of topic j in document d. If LIWC psycholinguistic processes are used as features  xðdÞ i represents the quantity of a specific psycholinguistic process i in document d. To create a balanced data set  10 000 posts made by users from the five CLINICAL and CONTROL categories (1 000 posts from each category) were selected for experiments. For diversity  we take an equal number of posts for each community in a category. For example  forDepression category  each community alonendepressed  depressedteens  depressionsucks and fightdepression contributed 250 posts. Latest posts were chosen to avoid introducing posts in early stages of communities. Let C ¼ f g c1; ... ; cM be a corpus of M communities (M N). When topics or LIWC psycholinguistic processes are the chosen features  cm is an average vector collated from all blog posts in B made by members of community m. When moods or ANEW words are chosen as features  collation across the community is done as follows: Mood: Let M ¼ {sad  happy  ...} be the predefined set of moods where jMj ¼ 132 is the total number of moods provided by LiveJournal. For each community  a 132-dimensional mood usage vector is constructed  whose kth element is the number of times the kth mood inMwas tagged within the community. ANEW: Each community is represented with a 1 034-dimensional feature vector  whose kth element is the number of times the kth ANEW word is used in the content of the posts made by users belonging to that community. We are interested in features sets that are strong predictive of CLINICAL communities. For this purpose  Lasso [8]  a regularized regression model  is chosen. Lasso performs logistic regression and selects features simultaneously  enabling an evaluation on both the classification performance and the importance of each feature in the classification process. Ten-fold cross validations are run on the feature sets  that is  for each of 10 runs  one held-out data fold is used for testing and other nine folds for training. Classification accuracy  the proportion of correctly classified cases  is used to evaluate the performance. The regularization parameter () is chosen such that it is the largest number and the accuracy is still within one standard error of the optimum (1se rule). This prevents overfitting since not too many features are included in the model while the accuracy of classification is still assured.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,2,0,,0
https://ieeexplore.ieee.org/abstract/document/6784326,7,7,classification,"Nguyen, Phung, Dao, Venkatesh, Berk",5,0,0,0,2014,The Lasso model as described above is used  considering four feature sets: ANEW  mood  topics and LIWC. For each set  the selected features and corresponding classification accuracy is shown in Table 4. Best results (100 percent accuracy) is achieved when Topics and LIWC are used as feature sets. Mood and ANEW features are also reasonably effective with accuracies of 96 and 89 percent  respectively. Only topics and LIWC are used as input feature sets  since ANEW and mood have low coverage for posts in the corpus. Topics as features achieve better performance with an accuracy of 93 percent (See Table 5). The LIWC features also perform well  achieving an accuracy of 88 percent. The predictive power of LIWC is thus slightly lesser than topics  but comes with lower computational cost  as extraction of topics requires pre-processing and sampling through LDA [4] or equivalent models.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,2,0,,0
https://ieeexplore.ieee.org/abstract/document/6784326,7,8,sentiment analysis,"Nguyen, Phung, Dao, Venkatesh, Berk",5,0,0,0,2014,This section analyzes the differences in the use of ANEW and mood tags across CLINICAL and CONTROL. ANEW. Fig. 3a shows the top ANEW words used differentially in CLINICAL and CONTROL groups. Of the top ten ANEW words whose usage is greater in CLINICAL group  seven have low valence  with negative connotations: cut (valence: 3.64)  depression (1.85)  hate (2.12)  depressed (1.83)  pain (2.13)  suicide (1.25) and alone (2.41). On the other hand  all 10 ANEW words whose usage is greater in CONTROL group have valence greater than 5.39 (black) and most of them are greater than 7—home (7.91)  food (7.65)  dog (7.57) or baby (8.22). Thus CLINICAL group uses more low valence ANEW words than the CONTROL group. Mood tags. Fig. 3b shows the top moods that are used differentially in CLINICAL and CONTROL groups. Most of the moods in the preference of CLINICAL group have low valence: depressed (valence: 1.83)  sad (1.61)  crushed (2.21)  lonely (2.17) and scared (2.78). On the other hand  CONTROL people experience high valence moods more frequently than CLINICAL  such as happy (8.21) or cheerful (8.10). In overall  for both mood tags and affective words derived from the content  the average valence is greater in CONTROL vs. CLINICAL group (Fig. 3c). This view from the data is corroborated through the Lasso model (Table 4) to predict CLINICAL communities: the largest positive weight is assigned to ‘depressed’ (a very low valence mood (1.83) and the largest negative weight is assigned to ‘ecstatic’ (a high valence mood (7.98)). This reflects that CLINICAL groups tag more low valence and less high valence moods than do CONTROL. In the model of ANEW to predict CLINICAL communities  ‘suicide’ (1.25)  a very low valence word  is assigned the second largest positive weight and ‘secure’ (7.57)  a high valence word  is assigned the second largest negative weight  somehow similar to the model of moods. To visualize the distance in the use of mood among communities  we project the mood-based representation for communities onto a 2D plane  using t-SNE [37]  a localitypreserving dimensionality reduction method (See Fig. 4). A separation between CONTROL and CLINICAL groups is visible  suggesting a difference in the use of mood tags between the two groups.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,2,0,,0
https://ieeexplore.ieee.org/abstract/document/6784326,7,9,analysis of topics,"Nguyen, Phung, Dao, Venkatesh, Berk",5,0,0,0,2014,We performed a nonparametric Wilcoxon test on the hypothesis of equal medians in the use of topics between CLINICAL and CONTROL groups. The null hypothesis H0 is considered to be rejected at p :05. Almost all of topics (46 of 50) were found to be statistically different in terms of use between the two groups. For community classification using topics as feature sets  Table 6 presents the top features selected by Lasso into the prediction model (Table 4). Of them  topic T6  a shoppingrelated topic  is a negative predictor of CLINICAL groups. On the other hand  topics related to depression are positive predictors of CLINICAL. For blog post classification  Table 7 presents the topics whose highest positive and negative weight in the model learned by Lasso (Table 5). The majority of topics positively predictive of CLINICAL posts are related to depression such as cutting  depression or suicide. On the other hand  all topics negatively predictive of CLINICAL posts (thus positive predictors of CONTROL posts) are more generic  such as computer  food or pets. We again use t-SNE to visualize the distance in the use of topics among communities (See Fig. 5). For CLINICAL  while Bipolar  Left and Cutters categories are well separated  Depression are close to Suicide communities. This indicates a high degree of topical commonality of discussion between Depression and Suicide communities.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://ieeexplore.ieee.org/abstract/document/6784326,7,10,analysis of linguistic styles,"Nguyen, Phung, Dao, Venkatesh, Berk",5,0,0,0,2014,We performed another nonparametric Wilcoxon test on the hypothesis of equal medians in linguistic styles between CLINICAL and CONTROL groups using the same data set in the topic case. The null hypothesis H0 is considered to be rejected at p :05. All 68 categories in LIWC are considered. We found rejection of this hypothesis in 63 of 68 of LIWC features. Therefore there was a significant difference in the use of LIWC features between CLINICAL and CONTROL population. For community classification using LIWC as feature sets  Tables 4 presents the linguistic features selected by Lasso into the model to predict CLINICAL communities. Sadness is a positive predictor of CLINICAL communities. This partially confirms the signs of depression: “persistent sad  anxious” [20]. In the model  words related to negative emotions— anxiety  anger and sadness words  are positive predictors. This supports the finding of [30] that depressed individuals express more negative emotion. For blog post classification (Table 5)  the occurrence of death related words (e.g.  bury  coffin  kill) increases in CLINICAL posts. This is in accordance with the findings in [30] that depressed individuals use more deathrelated words. On the other hand  ingestion (e.g.  dish  eat  pizza)  home (e.g.  apartment  kitchen  family) and sexual (e.g.  horny  love  incest) words are negative predictors of CLINICAL posts.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://ieeexplore.ieee.org/abstract/document/6784326,7,11,conclusion,"Nguyen, Phung, Dao, Venkatesh, Berk",5,0,0,0,2014,We have investigated online depression communities and studied their differential factors to other online communities. Three aspects have been examined: affect  psycholinguistic processes and topics within content. Machine learning and statistical methods were used to discriminate online messages between depression and control communities. All aspects—affect  the written content and writing style were found to be significantly different between these two groups. In addition  latent topics were found to have greater predictive power than linguistic features for prediction of depression communities. This study suggests that data mining of online blogs has the potential to detect meaningful data for depression studies. The result highlights the potential applicability of machine learning to psychiatric practice and research.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://www.jmir.org/2017/7/e243/,8,1,abstract,"Cheng, Li, Kwok, Zhu, Yip",5,0,0,0,2017,Early identification and intervention are imperative for suicide prevention. However  at-risk people often neither seek help nor take professional assessment. A tool to automatically assess their risk levels in natural settings can increase the opportunity for early intervention. The aim of this study was to explore whether computerized language analysis methods can be utilized to assess one’s suicide risk and emotional distress in Chinese social media. A Web-based survey of Chinese social media (ie  Weibo) users was conducted to measure their suicide risk factors including suicide probability  Weibo suicide communication (WSC)  depression  anxiety  and stress levels. Participants’ Weibo posts published in the public domain were also downloaded with their consent. The Weibo posts were parsed and fitted into Simplified Chinese-Linguistic Inquiry and Word Count (SC-LIWC) categories. The associations between SC-LIWC features and the 5 suicide risk factors were examined by logistic regression. Furthermore  the support vector machine (SVM) model was applied based on the language features to automatically classify whether a Weibo user exhibited any of the 5 risk factors. A total of 974 Weibo users participated in the survey. Those with high suicide probability were marked by a higher usage of pronoun (odds ratio  OR=1.18  P=.001)  prepend words (OR=1.49  P=.02)  multifunction words (OR=1.12  P=.04)  a lower usage of verb (OR=0.78  P<.001)  and a greater total word count (OR=1.007  P=.008). Second-person plural was positively associated with severe depression (OR=8.36  P=.01) and stress (OR=11  P=.005)  whereas work-related words were negatively associated with WSC (OR=0.71  P=.008)  severe depression (OR=0.56  P=.005)  and anxiety (OR=0.77  P=.02). Inconsistently  third-person plural was found to be negatively associated with WSC (OR=0.02  P=.047) but positively with severe stress (OR=41.3  P=.04). Achievement-related words were positively associated with depression (OR=1.68  P=.003)  whereas health- (OR=2.36  P=.004) and death-related (OR=2.60  P=.01) words positively associated with stress. The machine classifiers did not achieve satisfying performance in the full sample set but could classify high suicide probability (area under the curve  AUC=0.61  P=.04) and severe anxiety (AUC=0.75  P<.001) among those who have exhibited WSC. SC-LIWC is useful to examine language markers of suicide risk and emotional distress in Chinese social media and can identify characteristics different from previous findings in the English literature. Some findings are leading to new hypotheses for future verification. Machine classifiers based on SC-LIWC features are promising but still require further optimization for application in real life.,0,0,0,0,0,0,0,2,2,0,,0,0,0,0,0,0,,0
https://www.jmir.org/2017/7/e243/,8,2,background,"Cheng, Li, Kwok, Zhu, Yip",5,0,0,0,2017,Suicide is the second leading cause of death in 15-29-year-olds globally and the first for this age group in China In addition to suicide as the most extreme action  more young people are suffering from emotional distress  which not only reduces their quality of life but also becomes a risk factor for severe mental disorder and suicide Therefore  early identification and intervention in emotional distress and suicidal thoughts are imperative for preventing suicide deaths. To assess suicide risk and emotional distress  many tools have been developed and validated. Some examples of such tools include Adult Suicide Ideation Questionnaire Suicide Probability Scale (SPS) Depression Anxiety Stress Scales-21 (DASS-21) and the recently developed Suicidal Affect-Behavior-Cognition Scale These tools often require respondents to either fill in a questionnaire or participate in a professional interview. However  distressed or suicidal people often have low motivation to seek help from professionals In addition  a recent study found that taking a suicide assessment may lead to negative affect changes on individuals with depressive symptoms From the suicide prevention point of view  a tool that can assess one’s suicide risk and emotional distress in a natural setting without costing his or her efforts and attention is preferable and can increase the opportunities for early identification and intervention.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://www.jmir.org/2017/7/e243/,8,3,previous work,"Cheng, Li, Kwok, Zhu, Yip",5,0,0,0,2017,The wide use of Web-based social media has provided a natural setting where interpersonal communications can be well documented for studying suicide and mental health issues Cases of social media being used by individuals to express suicidal thoughts  look for suicide methods  or even live broadcast suicidal behaviors have been reported and studied in different countries including China With Twitter and Facebook blocked in China  Sina Weibo (referred to as Weibo hereafter; Sina is a company name and Weibo literally means Microblog) is one of the most popular social media platforms among the Chinese population. According to China Internet Watch  Weibo had more than 313 million of monthly active users by the end of 2016  which is close to the number of worldwide monthly active users of Twitter A recent study empirically demonstrated that Weibo users who have suicidal ideation or distressed mental states are very likely to tell others about their suicidal thoughts on Weibo This is in line with psycho-linguistic studies that see words or language as a meaningful marker to convey or predict different aspects of our minds Previous studies have demonstrated the potential to use social media data to assess suicide risk or depression in English There are relatively few studies on the same topic in Chinese  and only a handful of studies have explored the topic using Weibo data. These studies had several major limitations. First  some studies validated their machine learning models against human annotated suicide risk level The human annotators were often graduate students who were not systematically trained in suicide prevention. The validity of their annotation requires empirical examination Empirically validated assessment tools are a more rigorous way to validate machine classifier’s performance Second  most of the previous studies have artificially boosted the percentage of suicidal or depression cases in their total sample or their classifiers were trained to distinguish extremely high suicidal cases from extremely low suicidal ones but excluding those in the middle Such study designs have difficulty being applied to real life scenarios  where people with different levels of risk are mixed  and suicidal people often count for a small proportion of the total population. Last but not the least  previous Chinese studies have utilized a locally developed dictionary  namely  simplified Chinese micro-blog word count dictionary for analyzing Weibo posts The advantage of the locally developed dictionary is that it might have a higher coverage of Chinese Web-based language. However  the disadvantage is that the results can hardly be compared with other countries’ studies that often use the standardized linguistic inquiry and word count (LIWC) dictionary More importantly  when previous work used the local dictionary to classify a Weibo user’s suicide risk  the classifiers’ performance showed a large space for improvement or remained unclear In this case  it is worthy of empirical examination to find out whether using standardized LIWC dictionary can achieve comparable or even better performance than using a locally developed dictionary.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://www.jmir.org/2017/7/e243/,8,4,aim of the study,"Cheng, Li, Kwok, Zhu, Yip",5,0,0,0,2017,This study aimed to explore whether computerized language analysis methods can be utilized to assess Chinese individuals’ suicide risk and emotional distress based on their Weibo posts. Specifically  we not only analyzed what Simplified Chinese-Linguistic Inquiry and Word Count (SC-LIWC) categories were associated with suicide risk or emotional distress but also applied machine learning method to automatically classify whether a social media user was having suicide risk or emotional distress. We examined the computerized markers’ performance against conventional self-assessment tools to evaluate their utility.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://www.jmir.org/2017/7/e243/,8,5,data collection,"Cheng, Li, Kwok, Zhu, Yip",5,0,0,0,2017,A Web-based survey of Weibo users was conducted to assess the respondents’ suicide risk and emotional distress (ie  depression  anxiety  and stress). The invitation letter to participate in this survey was widely sent out to general Weibo users by various promotion activities. For a Weibo user to be eligible for the study  she or he had to be 18 years or older (by self-report). A 30 Renminbi incentive for each complete survey was provided to boost the respond rate. With the respondents’ consent  their Weibo posts that were posted in the public domain during the 12 months before the survey were downloaded by calling Weibo API. The survey fulfilled the Checklist for Reporting Results of Internet E-Surveys (CHERRIES) checklist and details of the procedure have been reported in previous publications In addition  when multiple survey feedback were submitted from the same Internet protocol addresses  only the first submission was used to avoid duplicate participation. In contrast to a previous study this study excluded those who posted nothing throughout the 12 months but not those who posted fewer than 100 posts. Eventually  data provided by 974 respondents remained for further analyses. The study has obtained ethical approvals from the Human Research Ethical Review Committee at the University of Hong Kong and the Institute Review Board of the Institute of Psychology at the Chinese Academy of Sciences. The survey measured respondents’ suicide probability score  depression  anxiety  stress  and Weibo suicide communication (WSC) as the outcome variables. In addition  the respondents’ Weibo posts language features were extracted as independent variables or features for machine learning. The details of how those data were obtained are elaborated in the following subsections. The Chinese version of the SPS was adopted to assess the respondents’ suicide probability. The SPS was originally developed in the United States and then translated and validated in China The Cronbach alpha coefficient of the scale in our study was .749. The Chinese version of the DASS-21 was used to measure the respondents’ emotional distress  which has been validated in China and has shown good construct validity and criterion-related validity The scale includes 3 subscales to measure depression  anxiety  and stress  respectively. In our study  the Cronbach alpha coefficient was .859 for the depression subscale  .767 for the anxiety subscale  and .821 for the stress subscale. WSC was measured by a single-item question on whether or not the respondent had told others via Weibo in the past 12 months that he or she wanted to kill himself or herself. Given Weibo’s multiple functions  WSC can be delivered by publishing Weibo posts  sending private messages to others  or expressing suicidal thoughts in a group chat. For this question  the respondent was not limited to any particular type of Weibo communication. Weibo posts were segmented using the Stanford word segmenter that resulted in 349 374 words and phrases. Thereafter  the SC-LIWC dictionary was applied to count the appearance of each category of words in every respondents’ Weibo posts. The SC-LIWC dictionary includes 7450 words that are grouped into 71 categories  including 7 main linguistic or psychological categories and 64 subcategories. In addition  the total number of words or phrases that each respondent published in the 12 months was counted as the 72nd category. Scores of the SC-LIWC categories were counted as percentages of the total number of words.,2,2,2,2,0,0,0,2,2,2,,0,0,2,0,0,0,,0
https://www.jmir.org/2017/7/e243/,8,6,data analysis,"Cheng, Li, Kwok, Zhu, Yip",5,0,0,0,2017,Five rounds of logistic regression analysis were applied by including the 5 suicide risk factors (SPS  depression  anxiety  stress  and WSC) as dependent variables  respectively. Binary classifications of the 5 risk factors were used in the logistic regression analyses. We followed previous studies to use the total score of 80 as the cut-off for the SPS 10 for severe depression  7 for severe anxiety  and 12 for severe stress to categorize the respondents to “at-risk” and “others” groups  respectively. As for WSC  the “at-risk” group is defined as exhibiting WSC  whereas the “others” group as not exhibiting WSC in the past 12 months. For each suicide risk factor  all 72 linguistic features of SC-LIWC were entered as independent variables to a stepwise regression for feature selection at a significance level of .05. The support vector machine (SVM)  a supervised machine learning model  was employed to build algorithms for automatically classifying whether a Weibo user is having suicide risk or emotional distress. SVM is a well-known and highly effective approach yielding high accuracy in affect and sentiment analysis in computer science The scores of the SC-LIWC categories were included as the features for SVM classification. SVM classification also requested the outcome variable to be binary  which was consistent with the logistic regression analysis. R version 3.0.0 (The R Project for Statistical Computing) with package “e1071” was used to conduct SVM training Furthermore  since our previous examination found that exhibiting WSC can be explained by suicidal ideation and negative affectivity we further used the WSC variable as a filter. Specifically  we only included those respondents who reported having WSC in the survey and then ran the SVM training solely on those respondents. It was expected that this screening method could further improve the performance of the SVM model. All the classification results were generated with leave-one-out cross validation that was found to be able to provide an almost unbiased estimator of the generalization properties of statistical models Receiver operating characteristic (ROC) curve analysis was operated for analyzing and comparing the diagnostic accuracy of the SVM classifications for the 5 risk factors. The primary outcomes of the study were the area under the ROC curves  sensitivities  and specificities of the SVM classifiers.,0,0,0,0,0,0,0,0,0,0,,2,0,0,0,0,0,,0
https://www.jmir.org/2017/7/e243/,8,7,sc liwc categories as markers,"Cheng, Li, Kwok, Zhu, Yip",5,0,0,0,2017,Table 1 presents the SC-LIWC categories that showed independent effects on differentiating those at-risk ones from the other respondents in the final regression model after stepwise selection. P<.05 was adopted as the cut-off for statistical significance. For example  as shown in Table 1  a 1% increase in usage of any pronoun would increase the risk of having high level of SPS by 18% (odds ratio  OR=1.18  P=.001). By contrast  more frequent use of verb was associated with lower risk (OR=0.78  P<.001). In short  Weibo users with high suicide probability were marked by a higher usage of pronoun  prepend and multifunction words  a lower usage of verb  and a greater total word count. The markers of the other 4 risk factors showed more commonalities. For example  second-person plural was positively associated with severe depression and stress  whereas work-related words were negatively associated with WSC  severe depression  and anxiety. Meanwhile  some special characteristics were associated with the different risk factors. Third-person plural was found to be negatively associated with WSC but positively with severe stress. Achievement-related words were positively associated with depression  whereas health- and death-related words were positively associated with stress.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://www.jmir.org/2017/7/e243/,8,8,automatic machine classifiers as markers,"Cheng, Li, Kwok, Zhu, Yip",5,0,0,0,2017,Table 2 demonstrates the AUCs  sensitivities  and specificities of the SVM classifiers for whether a Weibo user was at one of the five types of risk. There were no significant AUCs for the SVM classifiers of the total respondents for the 5 risk factors. However  when we filtered out those non-WSC respondents  SVM classification significantly identified those with high suicide probability or severe anxiety. The classification for severe stress was marginally significant  whereas the one for severe depression was still not significant. The performance characteristics of the 3 significant and marginally significant SVM classifiers are shown in Figure 1 as summarized by ROC curves.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://www.jmir.org/2017/7/e243/,8,9,principal findings,"Cheng, Li, Kwok, Zhu, Yip",5,0,0,0,2017,The study demonstrates the utility of natural language processing (NLP) methods to assess suicide risk and emotional distress in Chinese social media. Significant associations between certain SC-LIWC categories and suicide risk or emotional distress were identified. In addition  automatic machine classifiers achieved satisfying accuracy when classifying suicide probability and anxiety level among those who had expressed suicidal thoughts to others via Weibo. However  the classifiers’ performance on classifying depression and stress levels needs to be improved at large. The study sheds light on the potentials and challenges of developing automatic computerized program to assess mental risk based on natural language processing in Chinese. Although the study design is data-driven rather than hypothesis-driven  we will further discuss some key results by relating them to existing theories and previous research findings.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://www.jmir.org/2017/7/e243/,8,10,sc liwc categories as markers,"Cheng, Li, Kwok, Zhu, Yip",5,0,0,0,2017,It is noteworthy that this study did not find a significant association between first person singular pronouns (ie  I  me  and my) and suicide risk or emotional distress  which is inconsistent with a number of previous studies . The phenomenon might be related to the fact that first person singular in Chinese conveys an ambiguous meaning  which not merely refers to the addresser as himself or herself but also shows a tendency toward putting him or her in a whole community that the addresser belongs to  thus bringing the addresser a sense of empathy and friendly interpersonal relationship In this case  the use of first person singular in Chinese not necessarily indicates a self-focus mind and may not be able to mark suicide risk or emotional distress like in English. In addition  it is of note that a recent study examining linguistic characteristics of suicide related Tweets found that the first person pronouns can differentiate strongly-concerned Tweets from safe-to-ignore Tweets However  they have excluded possibly concerning Tweets from their original dataset that made their results not directly comparable with ours. In addition  those Chinese social media users with greater levels of depression and anxiety were more likely to write more of second person plural pronouns in their public posts. This suggests that they preferred referring or talking to a group of others directly in their posts  which was potentially inviting a direct communication with others. Suicide prevention professionals may make best of this opportunity to proactively engage with at-risk ones and offer help and support. The findings on third person plural’s association with the outcome variables were not consistent. While being negatively associated with WSC  it showed positive association with stress. No previous literature reported similar findings. Nevertheless  the inconsistency suggests that those having severe stress might be different from those having WSC in terms of how they relate themselves with third parties. Death-related words were associated with severe stress but not suicide probability. This finding is different from previous findings in English that suicidal poems talked about death-related more often as well as a Japanese study that showed tweeting “want to commit suicide” could predict suicidal ideation and attempt The divergence might be related with the different study design: our study compares people with greater suicide risk to those with lower risk  whereas the previous studies did comparisons either between those suicides deceased and alive nonsuicidal ones  or between those with history of suicide attempts with those without. Furthermore  our findings suggest that the Chinese Weibo users at high suicide probability might express their suicidal thoughts implicitly  rather than using words of death and suicide  in the public domain. By contrast  those with severe stress but not necessarily planning to kill themselves were more likely to disclose their emotional distress by using words relating to death and suicide. The usage of achievement-related words was positively associated with depression. This is in line with previous studies that found achievement-oriented to be often confounded with depressive symptoms However  a previous machine learning study based on Twitter users in the United States found that the greater usage of achievement-related words in Tweets was associated with being nonsuicidal Although the US study did not examine depression  the differences between our findings with theirs warrant more studies on the cross-cultural differences regarding the relationship between achievement and suicide or emotional distress. The use of work-related words was negatively associated with depression  anxiety  and WSC. The phenomenon might be interpreted from two different angles. First  it suggests that those distressed individuals were likely unemployed  which is known to be a risk factor for suicide and emotional distress. The alternative interpretation is that those who were more motivated by their work would demonstrate more positive mental states.,0,0,0,0,0,0,0,0,2,0,,0,0,0,0,0,0,,0
https://www.jmir.org/2017/7/e243/,8,11,automatic machine classifiers  ,"Cheng, Li, Kwok, Zhu, Yip",5,0,0,0,2017,The results of the machine learning analysis demonstrated the challenges of automatically assessing one’s suicide probability or emotional distress by NLP. This is related to the fact that prevalence of the outcome variables among the general population is somewhat low. However  by adding a filter of WSC  our machine classifiers’ performance has been improved  especially that of suicide probability and anxiety. This is because WSC was found to be highly correlated with the outcome variables which helps to boost the prevalence of the outcome variables among the filtered population. As discussed in the Introduction section  previous studies often artificially boosted the percentage of suicidal or depression cases in their total sample or purposely excluded those with medium level of risk from the sample Different from those studies  the filter of WSC used in this study indicated real behaviors of expressing one’s suicidal thoughts via Weibo to others. In real life scenarios  it is feasible to encourage those who have read or received Weibo posts or messages about suicidal thoughts to refer those posts to our algorithms for further assessment. There is certainly room to further optimize the machine classifiers’ sensitivity and specificity. Braithwaite and colleagues’ recent study using Twitter data in the United States adopted a similar study design as the presented study but their classifiers outperformed ours in terms of accuracy Braithwaite and colleagues used different scales to measure suicide risk and different machine learning model to develop their classifiers. It is worthy of our future efforts to find out whether following their approaches can improve the classification performance in the Chinese settings as well. Nonetheless  the performance of the suicide probability classifier and anxiety classifier with filter is promising. It is important that applying the classifiers to review and assessing the posts is much more efficient  convenient  and less costly compared with doing it manually or inviting those Weibo users to conduct questionnaire survey.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://www.jmir.org/2017/7/e243/,8,12,limitations,"Cheng, Li, Kwok, Zhu, Yip",5,0,0,0,2017,A few limitations of the study should be noted. The machine classifiers developed by this study need to be further optimized  especially the classifiers of depression and stress. More replicative studies are still needed to examine the transferable validity of our research findings. The Web-based survey adopted a random sampling approach. However  the respondents may have been self-selected because of their interest in psychological research. Nonetheless  we have compared the basic demographic characteristics (ie  age and gender) of the survey respondents with the general Weibo users and found no significant differences Last but not the least  the study was conducted in a data-driven manner that led to the results being less structured and some results difficult to interpret. In fact  the study has brought up more questions and new hypotheses for future studies rather than verifying or confirming existing theories.,0,0,0,0,2,0,1,0,0,0,,1,0,0,0,0,0,,0
https://www.jmir.org/2017/7/e243/,8,13,implications of future research,"Cheng, Li, Kwok, Zhu, Yip",5,0,0,0,2017,To apply the language markers and automatic classifiers in real life  we would suggest Weibo users to be more cautious when reading a post or message about suicide. When suspecting someone might be at risk  they can refer the person’s Weibo account to our classifiers that will automatically screen that person’s public posts and further assess his or her conditions. It will be beneficial if a longitudinal study can be carried out to apply the algorithms developed by this study to screen and assess Weibo posts continuously and provide the results to suicide prevention professionals for double check and follow-up. In turn  the experts’ feedback and follow-up results should be fed back to the model’s developers for optimization Some social media platforms  such as Facebook and Instagram  have developed “report” functions to allow users to flag those that are expressing suicidal thoughts. The report will be manually reviewed by in-house reviewers to decide whether the flagged person is indeed at risk. If automatic classifiers such as the ones developed by this study can be integrated into such kind of Web-based report function  it will improve review efficiency and better empower social media platforms and users to contribute to suicide prevention. As social media are rapidly penetrating into our daily life  the opportunities for detecting and engaging distressed individuals via social media should not be missed.,0,0,0,0,0,0,0,0,2,0,,0,0,0,0,0,0,,0
https://www.jmir.org/2017/7/e243/,8,14,conclusions,"Cheng, Li, Kwok, Zhu, Yip",5,0,0,0,2017,This study demonstrates that natural language in social media can be utilized as markers to differentiate those at-risk individuals from the general population and that the language markers are culturally sensitive. The automatic computer program shows potential for aiding human watchers to assess suicide probability and anxiety by improving the assessment efficiency but not compromising significant accuracy.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/3025453.3025909,9,1,abstract,"De Choudhury, Bagroy, Kumaraguru",3,1,0,0,2017,Psychological distress in the form of depression  anxiety and other mental health challenges among college students is a growing health concern. Dearth of accurate  continuous  and multi-campus data on mental well-being presents significant challenges to intervention and mitigation efforts in college campuses. We examine the potential of social media as a new “barometer” for quantifying the mental well-being of college populations. Utilizing student-contributed data in Reddit communities of over 100 universities  we first build and evaluate a transfer learning based classification approach that can detect mental health expressions with 97% accuracy. Thereafter  we propose a robust campus-specific Mental Well-being Index: MWI. We find that MWI is able to reveal meaningful temporal patterns of mental well-being in campuses  and to assess how their expressions relate to university attributes like size  academic prestige  and student demographics. We discuss the implications of our work for improving counselor efforts  and in the design of tools that can enable better assessment of the mental health climate of college campuses.,0,0,0,0,0,0,2,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/3025453.3025909,9,2,introduction,"De Choudhury, Bagroy, Kumaraguru",3,1,0,0,2017,College students confront many challenges in pursuit of their educational goals [51]. When such experiences are perceived as negative for a prolonged period of time  they can have an adverse effect on students’ well-being [1  3  66]. Beyond implications for personal health  the Federal Bureau of Investigation has reported that mental health concerns on college campuses can pertain directly to episodes of violence [22]. Yet few university students seek help related to mental illness. Only 18% of students with a past-year history of poor mental wellness are known to seek counseling  therapy or treat ment [9]. This arises due to a variety of barriers: limited health insurance coverage  paucity of knowledge about psychiatric services  social stigma  and lack of time [1  57]. It is recognized that campus-wide support measures  coping strategies  and mitigation programs might decrease the negative effects of mental illness in college students [3  54  66]. However  employing all of these interventions necessitates adequate assessment of the “mental health climate” within a university campus. Currently  such assessments are challenged due to the paucity of adequate and accurate data on students’ well-being. Local data is gathered through visits to the campus counseling center. However these services are often only availed by students when their mental well-being takes a downward turn [27]. Complementarily  many universities conduct periodic surveys [5] to gauge mental health challenges of students and supplement clinical data on students’ mental health [35  9]. However the large temporal gaps across which these measurements are made  the retrospective nature of recalling past experiences  and limited consistency of survey findings across multiple campuses make it difficult for authorities to act upon such information and influence campus mental health intervention programs. In this paper  we aim to bridge this gap by utilizing social media as an unobtrusive “lens” to gauge mental health expressions of students in university campuses. Our motivation stems from two observations. First  recent advances in HCI and social computing research has provided promising evidence that content shared on social media  especially its linguistic characteristics  can enable accurate inference  tracking  and understanding of mental health states [19  14  60]. Second  over 90% of young adults  or individuals of college going age  use social media [40]. In fact  many college students are appropriating these platforms to meet a variety of their needs  such as for self-disclosure  support seeking and social connectedness [30]. How can we build on these methodological advances and the pervasive use of social media by students to gauge their mental well-being? To answer this question  we focus on the following research aims: • Aim 1: Building and validating a machine learning methodology to identify mental health expressions of students in campus-geared online communities. • Aim 2: Analyzing the linguistic and temporal characteristics of the above inferred mental health expressions of students in different university campuses. • Aim 3: Developing an index of collective mental wellbeing in a campus  and examining its relationship to at tributes of the university  including academic prestige  enrollment size  and student demographic distribution. To accomplish these research aims  we use large-scale  passively gathered  longitudinal data shared in over 100 campusgeared communities on the social media Reddit. We then show that an inductive transfer learning approach [49] can help to detect mental health expressions in student populations with high precision and accuracy (97%). Analyzing these expressions over time  we find that they show a monotonically increasing trend through the academic year. However these expressions demonstrate a decreasing trend during the summer months. Alarmingly  we also observe that the relative proportion of these expressions have shown a 16% increase between 2011 and 2015. Then  we demonstrate that our transfer learning based classification approach can be used to develop a novel metric of college campus well-being  known as “Mental Well-being Index” (MWI). MWI enables us to discover both established as well as previously underexplored differences across college campuses. We find MWI to be lower in public universities with large undergraduate student bodies and female students; counter-intuitively  it is not lower in colleges with higher academic prestige. To the best of our knowledge  we present the first large-scale multi-campus study of college student mental health by leveraging social media data of over 100 campuses. Our findings bear implications for improving mental health support and counseling efforts within campuses. We conclude by discussing design considerations for technologies that enable unobtrusive  real-time tracking of collective mental health of college student populations.,0,0,0,0,0,0,2,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/3025453.3025909,9,3,mental health of college campuses,"De Choudhury, Bagroy, Kumaraguru",3,1,0,0,2017,There is a rich body of work in psychology  health policy  epidemiology  and public health around assessing and understanding mental health concerns of college students [31  45  59]. While some of these works focus on identifying the underlying factors that impact college students’ mental health [59]  the bulk focuses on assessing the mental health of students who seek help at college counseling centers [31]. Most college counseling centers persistently collect local data from students seeking their services on individual campuses  such as through survey instruments validated against the Diagnostic and Statistical Manual of Mental Disorders (DSM) [45]. However it has been noted that these data are rarely shared nationally. Further  Soet and Sevig [56] argued that college students who do not come to counseling centers need to be investigated to examine similarities and differences between clinical (those who seek mental health treatment) and non-clinical populations. By focusing on content shared on university campus social media communities  we are able to identify an alternative mechanism to gather information about mental health challenges in college campuses  including of those who may be unwilling or unable to seek professional help. Noting the challenges of solely focusing on clinical student populations  many universities have adopted a survey conducted by the American College Health Association [6]. The survey includes limited questions on mental health issues such as medication use  depression  and suicide. However these studies are limited in temporal granularity and involve significant resource commitment to conduct. To overcome limitations of survey approaches  researchers have employed wearable sensing technologies and experience sampling methods to obtain a variety of physiological and psychological signals in a continuous fashion from college student populations [62  28  65  64]. However most of these works have been done in a controlled lab setting or supervised real-life setting. Exception is the recent work of Wang et al [62]  who used smartphone sensing to validate attributes of campus life  including academic conduct  lifestyle  socialization  depression  stress  and mood [13  63  10]. Although these approaches capture rich  dense sensing data about people’s behaviors  activities and moods  they need considerable cooperation  intervention  and compliance from participants. This challenges conducting longitudinal  repeatable studies on student mental health spanning large populations. Social media data  on the other hand  can be passively collected  and therefore can scale to multiple campuses easily. Thus it can be more useful for longitudinal tracking of a specific university’s mental health climate. Various research studies have explored how mental health of college students relates to demographic and social factors [7]. Racial and ethnic minority students are reported to underuse mental health services  even while reporting more distress [67]. Further  studies report gender differences in psychological distress among young adults and students [45]. Therefore  it is posited that a focus on reaching diverse groups of college students  such as gender or racial minorities in specific campuses  is needed [24]. However  there has been limited research examining the manifestation of mental health concerns among students in the light of the academic setting  such as enrollment size  competitiveness  prestige  supportiveness of academic personnel  and field of study [56]. Hunt and Eisenberg [32] argued that the risk factors for mental disorders among students must be understood in the context of not only their vulnerabilities  but also how they interact with external factors in college. Our work attempts to fill this gap by analyzing the relationship between social media derived mental well-being of students  and a range of attributes of the corresponding institutions  such as the size and demographics of their student body  and academic prestige.,0,0,0,0,0,0,2,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/3025453.3025909,9,4,social media and mental health,"De Choudhury, Bagroy, Kumaraguru",3,1,0,0,2017,An emergent body of work in HCI and related disciplines has examined the relationship between mental well-being and self-disclosure on social media [18  2]. De Choudhury and De [18] explored individuals who appropriated the Reddit platform to express a variety of emotional and mental distress  and how these expressions are characterized by disinhibiting behavior. Andalibi et al. [2] qualitatively characterized the variety of emotional expressions that are shared on Instagram via the hashtag “#depression”. In other work  social media content analysis  specifically of linguistic cues and conversational patterns  has enabled novel mechanisms to predict risk to mental health concerns  ranging from depression [19]  substance abuse [47  41]  loneliness [36]  eating disorders [12]  and other mental health disorders [14  60]. We extend this body of work by developing an automated machine learning method that can detect mental health expressions in social media  specifically in the context of college student populations. There is limited work on college students’ social media use and their mental well-being. Ellison et al. [26] in a seminal study found that there is a positive relationship between college students’ Facebook use and the maintenance and creation of social capital. Similarly Manago et al. [42] found that social networking sites helped college students satisfy enduring human psychosocial needs. Other work has found that social media use may foster the development of intimate relationships including supportive friendships among college students [52]. Moreover the communication with friends that occurs on these platforms may help college students resolve key issues present during this transitional phase of life [44]. Given the pervasiveness of social media use among college students and its relationship with psychological well-being [30 43 61] we examine how data gathered from these platforms may lend insights into the collective mental well-being of college campuses. Leveraging social media researchers have also sought to develop quantifiable indices of emotional and mental well-being of large populations [46 34 29]. Kramer [37] developed a “Gross National Happiness” index with Facebook posts whereas Dodds et al. [21] developed a “happiness index” a hedonometer based on textual content shared on Twitter. More recently Schwartz et al. [55] adopted more sophisticated methods like topic models to study the patterns of county-specific levels of well-being and life satisfaction using Twitter data. Utilizing clinical self-reported information about depression De Choudhury et al. [17] also developed a Twitter-based state-level index of depression. Our contributions in this paper build on these investigations and methodologies. We examine how unobtrusive data gathered from students’ social media use may inform the development of a quantifiable index that tracks collective mental well-being of students in different college campuses.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/3025453.3025909,9,5,university data,"De Choudhury, Bagroy, Kumaraguru",3,1,0,0,2017,We first obtained a list of 150 ranked major universities in the United States by crawling the US News and World Report website [48]. This list is constructed based on the Carnegie classification  employed extensively by higher education researchers  and using a set of 16 indicators of academic excellence  defined by US News. The list includes a variety of universities spread across the US in different settings (e.g.  urban  rural)  and with a wide range of student enrollment sizes. Figure 1(a) shows their geographic distribution. As a part of this crawl  we also obtained university metadata: gender distribution of students  average tuition and fees  and academic calendar (semester/quarter). To obtain further information about the nature of the student body  we crawled the Wikipedia pages of all of the 150 universities. From these pages  we extracted the size of student enrollment  type (public/private)  and setting (rural/suburban/urban/city) at every institution. These definitions come from a formal categorization scheme used by the US Department of Education. The student body enrollment sizes ranged from 2 255 to 97 494  with 98 public and 52 private universities. 50 universities were reported to be urban  47 city  39 suburban  and 13 rural. Finally  we obtained information on racial diversity of the universities from a website known as Priceonomics [58]. The website calculates the Herfindahl-Hirschman Index (HHI)  by combining the race/ethnicity distribution of student bodies at different universities  with data given from the Department of Education. HHI ranges from 1 (the least diverse: a population of all one type) to 1/N (the most diverse)  where N is the number of different racial categories being analyzed. ,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/3025453.3025909,9,6,social media data and universities,"De Choudhury, Bagroy, Kumaraguru",3,1,0,0,2017,Next we obtained social media data of the above universities. Specifically  we focused on the social media Reddit. Why Reddit? Reddit is known to be a widely used online forum and social media site among the college student demographic [23]. Due to its forum structure  it is extensively used for both content sharing  as well as for obtaining feedback and information from communities of interest. Reddit harbors a variety of communities known as “subreddits”  including many dedicated to specific university campuses. This allows a large sample of posts shared by students of a university to be collected in one place. Our preliminary manual inspection of university subreddits (e.g.  r/gatech or r/KState) revealed that these subreddits are appropriated by students to discuss college topics (Table 1). Focusing on these public Reddit communities also does not require explicit data collection efforts to be coordinated at each of the 150 university sites. Although more students are likely to use Facebook  due to its largely privately shared content  it is challenging to obtain access to a large dataset of a university’s students. Next  while Twitter is also widely adopted  without explicit self-reported information  it is challenging to identify college student accounts. Finally  prior work [2  18] notes that semianonymity of Reddit enables candid self-disclosure around stigmatized topics like mental health. By utilizing Reddit’s subreddit search functionality  we crafted various search queries using names and acronyms of the university names in our above list. We were able to identify the public subreddit pages of 146 out of the 150 above identified universities. Thereafter we employed a multi-step approach to secure posts and associated metadata shared in these 146 subreddits: Initial Data Acquisition. We leveraged the archive of all of Reddit data made available on Google’s BigQuery [11]. BigQuery is a cloud based managed data warehouse  that allows third parties to access large publicly available dataset through simple SQL-type queries. Our queries grabbed all posts ranging between June 2011 and February 2016 available in the Reddit data archive. This included 424 984 posts from 153 378 unique users across all of the 146 universities  with a mean of 2 910.8 posts ( = 4329.6) and 1 050 unique users ( = 1407) per subreddit. Filling the Gaps in Subreddit Data. The second step of our data collection process focused on identifying subreddits with insufficient data  and supplementing them through additional alternative data collection. Through Reddit’s official API (https://www.reddit.com/dev/api/)  we obtained the most recent number of subscribers in the 146 university subreddits (as of July 2016). Then to investigate if and to what extent some subreddits may have had unusually low data as given in step 1  we determined the median unique user to subscriber ratio in each subreddit. This allows us to capture the subreddits where the subscriber count is high  however the data obtained is not sufficiently representative. For subreddits with unique user to subscriber ratio under median (.42) (73 in all)  we performed a one-time data collection using the Reddit API. This gave us a set of (at most) 1000 most recent posts for each subreddit  with a total of 39 824 posts added to the data obtained in step 1  following de-duplication. We note that this procedure did not skew the yearly distributions of data across the subreddits: The skew (yearly rate of change) before and after data filling were 4.86 and 5.05 respectively  which were found to be statistically equivalent based on a two-sample equivalence test (p = .013  p = .025)  a test that uses two one-sided t-tests on the before-after yearly rates of change from both sides of a chosen difference interval [1  1].,0,0,0,0,0,1,2,0,2,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/3025453.3025909,9,7,correcting for under adoption of reddit,"De Choudhury, Bagroy, Kumaraguru",3,1,0,0,2017,. Based on the Reddit data we collected  from Figure 1(b) we observe high positive correlation between a university’s size of student body (enrollment) and the number of users subscribing to the corresponding subreddit (R2 = .38; ⇢2 = .6; p<.05). For subreddits deviating from trend  it would imply that the associated university’s student body was under or overrepresented on Reddit. We therefore devised a method to identify these subreddits  to correct especially for underadoption bias. We first calculated the ratio between the number of subreddit subscribers to student enrollment for each subreddit and the corresponding university. If  for a subreddit  this ratio was less than the expected adoption of Reddit for the same demographic group (4-8% as of 20161)  we assumed that Reddit was under-adopted by the students in the corresponding university. We thus removed subreddits where this ratio was <4%. This brought down our subreddits from 146 to 109. In these 109 subreddits  the mean Reddit adoption was 8.6% ( =1.3)  which is close to the highest adoption reported by Pew. The final dataset employed in our ensuing analyses included 446 897 posts from 152 834 unique users (mean posts per subreddit: 4 100; mean users per subreddit: 1 402). Figure 2(a) gives a distribution of the volume of crawled posts over the years. Figure 2(b) gives the final distribution of subreddits over the unique user to subscriber count ratio. Figure 2(c-d) gives distribution of the posts and unique users across the final 109 subreddits.,0,0,0,0,0,0,2,0,0,0,,2,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/3025453.3025909,9,8,demographic representativeness,"De Choudhury, Bagroy, Kumaraguru",3,1,0,0,2017,We note  it is possible that the type of students who frequent the university subreddits could be consistently different from the student body at the same university. To examine the representativeness of our university subreddit data  we employed a random sample of 500 posts  distributed across the subreddits and the years  for manual examination of demographics. Two researchers then independently coded these posts for self-reported gender  race  or academic stage (undergraduate/graduate). For instance  from the post “I’m a junior transfer and this will be my second semester”  the researchers identified the post author to be an undergraduate  whereas from “Hi all! I’m a new grad student (male  22) here  the gender of the author can be inferred to be male. We found the interrater agreement to be high: Cohen’s = .84. The relative ratios between the gender  race  and academic stage distributions of the coded posts and the university student body (obtained based on our methodology in the subsection “University Data”) showed significant positive correlation: The mean undergrad/grad ratio in our labeled data was 2.9  while it was 2.6 in the universities. A two-sample test of equivalence gave p-values of .016 and .011 respectively  with respect to the difference interval [.4  .4]. The sex (male/female) ratio for our labeled data was 1.6  also observed to be statistically equivalent to that of the student body  1.1 (p = .02  p = .03  w.r.t. the difference interval [.5  .5]). This establishes the validity of our acquired Reddit data as a representative data source for studying mental health disclosures in university campuses.,0,0,0,0,2,0,2,0,0,0,,0,0,0,2,0,0,,0
https://dl.acm.org/doi/abs/10.1145/3025453.3025909,9,9,methods,"De Choudhury, Bagroy, Kumaraguru",3,1,0,0,2017,We now present a methodology of identifying posts shared in university subreddits that that are likely to be mental health expressions. Note that  our Reddit data does not contain any gold standard information around whether a post shared in a university subreddit is about one’s mental health experience or condition. Our proposed method overcomes this challenge by employing an inductive transfer learning approach [16]. First  we include (as ground truth data) Reddit posts made on various mental health support communities. Prior work has established that  in these communities  individuals selfdisclose a variety of mental health challenges explicitly [50]. Parallelly  we utilize another set of Reddit posts  made on generic subreddits unrelated to mental health  to be a control. Next  we build a machine learning classifier to distinguish between these two types of posts. Then we learn features that could detect whether an post shared in a university subreddit could be an expression of some mental health concern. We discuss these steps in detail in the following subsections,2,2,2,2,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/3025453.3025909,9,10,mental health and control data,"De Choudhury, Bagroy, Kumaraguru",3,1,0,0,2017,We gained access to a sample of 63 485 public posts from 35 038 unique users  shared between 2014 and 2016  in a variety of mental health subreddits—this repository of posts has been used in prior work to study mental health selfdisclosure and support seeking manifested in social media [18  50  39  20]. This dataset includes posts and associated metadata spanning 14 mental health related subreddits  such as r/depression  r/mentalhealth  and r/traumatoolbox  r/bipolarreddit. From this corpus  we excluded posts that contained only a title without a post body. This gave us 21 734 posts. We refer to these posts as MH posts. Our control data also relied on a dataset compiled and utilized in prior work [50]; it contains posts from subreddits such as r/WorldNews  r/food  and r/AskReddit. We randomly sampled an equal number of posts (21 734) as the MH posts above for our control dataset. We refer to these posts as CL posts.,0,0,0,0,2,0,0,0,2,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/3025453.3025909,9,11,automatic identification of mental health expression,"De Choudhury, Bagroy, Kumaraguru",3,1,0,0,2017,In order to automatically identify posts relating to mental health expressions  we adopted an inductive transfer learning approach [49]. As described above  we utilized the dataset of MH and CL posts as positive and negative examples in a binary classification framework. We utilized this data to train and test different classification techniques  including random forests  Ada Boost  Support Vector Machines (SVM)  and Logistic Regression. We also adopted k-fold cross validation to evaluate the optimality and robustness of the approach  and included as features the linguistic content of the posts (stopword eliminated uni-  bi-  and tri-grams).,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/3025453.3025909,9,12,mental well being index of universities,"De Choudhury, Bagroy, Kumaraguru",3,1,0,0,2017,Can the ability to predict whether or not a Reddit post involves an individual’s mental health expression provide the basis for an accurate  reliable  fine-grained model of mental well-being manifested in various university subreddits? To this end  we use our above developed machine learning classifier to automatically label the corpus of posts shared in the 109 university subreddits. Thereafter  we define a metric called the “Mental Well-being Index” (MWI). At a time t and for a university U  we define it as the standardized difference between the frequencies of users sharing non-mental health expression posts fn(t  U) observed until t in the subreddit of U  and that of the users sharing mental health expressions until t in the same subreddit (fp(t  U)): where µp (correspondingly µn) and p (correspondingly n) are the mean and standard deviations of the frequencies of mental health expression (correspondingly non-mental health expression) posts. Note that we consider separate terms for the two classes of posts. This allows mental health expression and other expressions in posts to be weighted equally (since their relative volumes are likely to be different in a subreddit). This way  we also focus on variation in each class separately. That is  even if per one’s behavior  some individuals dramatically under-express mental health concerns in their posts  the relative non-mental health expression compared to mental health expression will be informative.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/3025453.3025909,9,13,aim evaluating the mental health expression classifer,"De Choudhury, Bagroy, Kumaraguru",3,1,0,0,2017,First  we evaluate our transfer learning based classifier for detecting mental health expressions in Reddit posts. We split up our corpus of 43 486 posts shared on the MH and CL communities into a training set (on which 5-fold cross validation is applied) and a validation set  with 80% posts for training and 20% for validation. Following 5-fold cross validation with different classification approaches  we found our logistic regression model to yield the highest accuracy: 93.4%  with an average precision  recall  and F-1 score of .93 each (refer to the column on “Validation set” Table 2 for these metrics). Table 3 presents the confusion matrix corresponding to classification on this validation set. We also note the area-under-curve (AUC) value for this classifier  corresponding to the receiver operating characteristic (ROC) curve  to be high: .98 (see Figure 3). AUC is a widely used metric because it shows the tradeoff between true and false positive rates. Figure 3 shows that with only about 5% of FPR  we can achieve TPR of over 90%  illustrating high performance. We also evaluate this best performing logistic regression in terms of its explanatory power in the validation set over an equivalent Null model. We find that the difference between the deviance of the Null model and the deviance of our model approximately follows a 2 distribution  with degrees of freedom equal to the number of additional predictor features in the latter model: 2(250  000  N = 8692) = 123076 98476 = 2.46 ⇥ 104 p < 1010. Summarily  our model results in significant reduction of deviance in classifying MH and CL posts. Which n-gram features given by the logistic regression model are the most predictive of a post being classified to be MH or CL? To answer this question  we present  in Table 4  the 30 predictor features with positive coefficient weights (i.e.  they are correlated with MH posts)  and another 30 predictor features with negative coefficient weights (i.e.  they are correlated with CL posts). We illustrate the context of use of a sample of these features to understand their relationship to classification outcomes. Vast majority of the coefficients associated with positive weights pertain to expressions of a variety of different mental health challenges (“bpd”  “bipolar”  “ptsd”  “suicide”  “depressed”  “mental health”). Some of the other features with positive coefficients include mentions of stress and anxiety (“anxiety”  “anxious”  “nervous”)  some of the known concomitants of mental health concerns. Other features appear to be calls for help and support seeking on Reddit (“help”  “talk”  “request”). Some features also appear in discussions of treatment  coping strategies and outcomes (“meds”  “therapy”  “medication”  “diagnosed”  “psychiatrist”  “recovery”). Finally  a set of features with positive coefficients also include manifestation of negative emotions  hopelessness and dejection  pain  and even extreme thoughts of harming and killing oneself (“kill”  “tired”  “suicidal”  “die”  “worse”). On the other hand  the predictor features with large negative coefficients span a diverse range of topics  ranging from events and experiences (‘christmas”  ‘pregnant”)  lifestyle (“workout”  “gym”)  sports (“game”)  community participation and usage practices (“lpt request”  “want know”  “curious”)  to Reddit specific topics (“reddit”  redditors”  “edit”). In summary  we observe that our transfer learning based classifier is able to robustly detect and characterize MH expressions in Reddit posts with high accuracy. Using the above trained and validated MH-CL post classifier  we then examined its performance in identifying mental health (henceforth MH) expressions in the posts belonging to the 109 university subreddits. For the purpose  we first employed two raters to annotate a random sample of 500 university subreddit posts (balanced across the two classes) to be about MH expressions or not. We used this annotated sample as a test set on which we applied our trained classifier. Our qualitative annotation task proceeded as follows. Adopting an inductive semi-open coding approach  first  two raters  one a clinical psychologist and another a social media expert independently assigned binary annotations (MH expression or not) to a sub-sample of 100 posts. To arrive at these rules  they referred to prior work on qualitative and quantitative studies of mental health disclosures on social media [8  2]  and literature in psychology on markers of mental health expressions [53  15  33]. Following this initial rating exercise  the raters got together to resolve differences and constructed a final rulebook. Per this rulebook  a post had to satisfy one of more of these criteria to be annotated to be a MH expression: • Explicit expressions of first hand experience of psychological distress or mental health concerns (“i get overwhelmingly depressed”  “i get into a negative spiral”  “I think I am on the verge and feel like it’s the end”). • Explicit expressions of support  help  or advice seeking around difficult life challenges and experiences (“are there any resources I can use to talk to someone about depression?”  “I have been going to therapy for 2 days a week every week for my anxiety”). Using this rulebook  the raters then annotated the larger sample of remaining 400 posts. The final agreement was found to be high (Cohen’s = .83). We then applied our trained classifier to this annotated test set of 500 posts. We found our classifier to demonstrate consistent performance as before  in the task of distinguishing between university subreddit posts that are related to MH expressions and those that are not. We achieved a mean accuracy of 96.8%  with AUC of .97 (see the second column on “Test set” in Table 2). We  therefore  proceeded with using this classifier in machine labeling all of the 446 397 other university subreddits posts. Our classifier identified 13 914 posts (3.1%) to be MH expressions  whereas the rest of the 432 483 posts were marked not about the topic. This corresponded to 9010 unique users out of a total of 152 834 (mean=5%  std. dev.=1.5% across the 109 subreddits).,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/3025453.3025909,9,14,aim analyzing mh expressions in universities,"De Choudhury, Bagroy, Kumaraguru",3,1,0,0,2017,"As per our next research aim, we present analytical observations given by the above classification of university subreddit posts—specifically we seek to: 1) identify what linguistic constructs (n-grams) characterize posts predicted to be MH expressions, and 2) understand the temporal manifestations of MH expressions in the context of different universities. Linguistic Characteristics of MH Expressions In Table 5 we present the top 20 uni-, 20 bi-, and 20 tri-grams that appear uniquely in university subreddit posts identified to be MH expressions. For qualitative inspection of the context of use of these n-grams, we randomly sampled a set of 100 MH posts that contained at least one of these top n-grams. We performed qualitative semi-open coding on this sample, employing the same two raters as above. The rating task discovered various topical contexts in which these n-grams were used in the university subreddit posts: (a) (b) Figure 4: (a) Yearly trend of the proportion of mental health (MH) posts aggregated across all of the university subreddits. (b) Distribution of university subreddits over their respective positive or negative slopes of linear least squares fit to (a). We find that students appropriate the Reddit communities to converse on a number of college, academic, relationship, and personal life challenges that relate to their mental well-being (“go into debt”, “doing poorly in”, “only one homework”, “course an intro”, “up late”, “the jobs i”): I’m lost and overwhelmed. [...] I feel sick to my stomach because it made me go into debt and I can’t seem to bring myself to go out there and find a job. The n grams also indicate that certain posts contain explicit mentions of mental health challenges (“psychiatric”, “depression”, “killing myself”, “suicidal thoughts”), as well as the difficulties students face in their lives due to these experiences (“life isnt”, “issues with depression”, “was doing great”, “ruin”, “cheated”): New Fall transfer here. Can I use [some service] to get psychiatric help, including a diagnosis and meds IF necessary? There’s definitely some psychological issues I’ve been carrying around with me my entire life. Finally, some of the n-grams indicate that students tend to vent on the different subreddits about challenges of college life, or to share their personal stories, feelings, and experiences (“exhausted”, “isnt fair”, “my story”, “im just not”): [...] Since it’s too late to apply to any colleges for the fall semester, and since my entire social life is here at [some university], I would prefer to stick around here rather than going home. So that’s basically my story. Can anyone recommend me a mental health professional? Some of the top n-grams are  also used in the context of seeking support (“need help”, “i really need”, “could help me”): Anyone willing to help me with [some course] preparing for the final? I need help, this semester has been extremely stressful due to development of clinical depression and anxiety. Putting it together, this analysis help us validate that our classifier is able to reveal markers of mental health challenges in college students that are known to relate to their academic, personal or social lives [31, 45, 59]. Temporal Characteristics of MH Expressions Next, we present an analysis of how the identified MH expressions in different university subreddits change over time. First, we compute the proportion of MH posts for each of the 109 subreddits. Aggregating this fraction over each of the years in our datasets2, we study the relative temporal change in expression of mental health in the university subreddits under consideration. In Figure 4(a), we show this trend, along with a corresponding linear least squares model fit (R2 = .79,p < .05). We observe that, the proportion of posts with MH expressions has been on the rise—there is 16% increase in 2015, compared to that in 2011. Further, identifying groups of subreddits with strictly positive or negative slopes per their least square fits (Figure 4(b)), we find that although there are some university subreddits with negative slopes (i.e., they show a decreasing trend over the years), for the vast majority (71% of the 109 subreddits), there has been a continual increase over time. Next, we examine how MH expressions in university subreddits change over the course of a typical academic year. Since academic years differ in universities adopting the semester and the quarter system, we compare differences in the trends of MH expressions across these university groups as well. From Figure 5(a-b) we find that over the course of a typical academic year, the proportion of MH posts of universities in both the semester and quarter system show a monotonically increasing trend. Note the positive slopes of the two linear least squares model fits: R2 = .88,p < .05 and R2 = .78,p < .05 respectively. Between August and April, for the universities in the semester system, we observe an 18.5% increase in MH expression (p<.05); this percentage is much higher: 78% for those in the quarter system, when compared between September and May (p<.05). On the other hand, during the summer months (Figure 5(c-d)), for both semester system and quarter system universities, we observe a reverse trend for the proportion of MH posts, i.e., a trend with a negative slope: R2 = .99,p < .05 for both university groups.",0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/3025453.3025909,9,15,aim relating mwi to university attributes,"De Choudhury, Bagroy, Kumaraguru",3,1,0,0,2017,For our final investigation (aim 3)  we compute the MWI metric for each of the 109 university subreddits. We examine the relationship between the MWI of each university subreddit and the corresponding university’s attributes. We glean several interesting observations from Figure 6(a-h). From Figure 6(a) we find that MWI of the 66 public universities we consider  is lower  relative to that in the 43 private universities  by 332%. This difference is found to be statistically significant based on an independent sample t-test (t = 7.38 p < .05). Examining universities by their setting (Figure 6(b))  we find that MWI is lower in the 7 rural and 33 suburban universities by 40-266% compared to others (p<.05)  while it is the highest in the 31 universities categorized to be in cities (by 29 77%; p<.05). Next  Figure 6(c) and (d) show the relationship between MWI of the universities  and their academic prestige and tuition fees. We observe a negative slope in the scatter plot of the former (R2 = .09; p<.05)  while a positive slope in case of the latter (R2 = .17; p<.05). In essence  universities with higher academic prestige (or low absolute value rank) and higher tuition tend to be associated with higher MWI. Now we discuss the relationship of MWI of the universities with four attributes of their student body. Both Figure 6(e) and Figure 6(f) show that universities with larger student bodies (enrollment) as well as greater proportion of undergraduates in their student bodies tend to be associated with lower MWI (R2 = .15; p<.05 and R2 = .16; p<.05 respectively). Finally  examining two demography related attributes of student bodies  we find from Figure 6(g) and (h) that MWI tends to be lower in universities with more females (or sex ratio  male to female 1) by 850% (p<.01). Further  although our data shows a marginally lower MWI in universities with greater racial diversity  we did not find statistical significance to support this claim (R2 = .01; p = .2).,0,0,0,0,0,0,2,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/3025453.3025909,9,16,theoretical implications of the findings,"De Choudhury, Bagroy, Kumaraguru",3,1,0,0,2017,Mechanisms for collective assessment of mental health challenges in college populations are highly valued in the literature [56  6]  however they are rare in practice. By proposing an index of mental well-being in campuses that is derived from passively acquired social media data of students  we believe our work makes a contribution to close this gap. As our results have shown  with this kind of measurement  we are able to glean previously established as well as new insights into students’ mental well-being in different universities and types of student bodies. MH expressions of universities have been increasing over the years. A notable finding of our analysis is the monotonically increasing trend of (normalized) MH expressions across university campuses—there was a 16% rise between 2015 and 2011. This finding aligns with observations from nationwide surveys on mental health of college students. In a 2008 national survey of directors of campus counseling centers  95% of directors reported a significant increase in severe psychological problems among their students [4]. While our findings do not extrapolate to the same set of universities or the same timeframe  same directionality of the trend provides some validation of our MH expression detection method. MH expressions show increase during the academic year  but decrease over the summer. We also observe that the MH expressions consistently increase through the academic year  while consistently decrease during the summer. There is prior work that situates academic pressure as a notable contributing factor of mental health challenges in students [31  45]. We hypothesize that as the academic year advanced  the accumulating academic pressure may be one reason behind the monotonically increasing trend of MH expressions. On the other hand  students may have identified the summer months to be a time to unwind and relax  hence likely lowering the expression of MH challenges. However we suggest caution in deriving causal claims from these findings. MWI is lower for large  public universities with large undergraduate student bodies. Next  in relating student body attributes to a university’s MWI  we are able to confirm some known facts about college student mental health. The campuses most challenged by mental health issues tend to be public universities which have large student bodies and a greater proportion of undergraduate students. It is reported that the student bodies of these campuses include many who are the first in their families to attend college and therefore carry intense pressure to succeed [32]. Further  undergraduate students especially are known to be at an elevated risk [35  7]. We conjecture these prior findings may provide some explanation behind the observed low MWI in universities with large undergraduate student bodies. MWI is higher in high prestige universities. Further  we observe that there is a positive correlation with higher academic prestige (in terms of ranking) of a university and MWI. Despite reports of mental health challenges being more prevalent in top ranked colleges [25]  our results reveal an opposite trend. In a high prestige university  it is likely the student body is self-selected  in that they perhaps already have internalized the need to deal with the academic pressure and rigor needed for success. Therefore they might be unlikely to express being overwhelmed mentally and emotionally on a public social media community like Reddit. MWI is higher in universities with higher tuition. Relatedly  the other somewhat surprising finding is the positive relationship between a university’s tuition fees and its MWI. Financial stress is identified to be a major factor behind college students’ mental health issues [31]. Our finding deviates from this expected behavior. We conjecture it might be explained by the socio-economic support structure that many of the students at high tuition universities may come with—those who get admitted are likely to have friends  family  and a sound financial backbone that may be mediating their risk to wellbeing challenges [24]. MWI is lower in universities with a larger female student body. As a final observation  our results indicate that there is greater expression of mental health challenges in universities that have a larger proportion of female students over male. As noted in our literature review  female college students tend to seek mental health help more frequently compared to male students [45]. Hence it can be presumed that they also tend to be more expressive about their mental health challenges in social media  thereby explaining our finding for MWI. Taken together  through this paper  we introduced social media  for detecting campus-specific mental health expressions of students. This has enabled us to obtain a variety of insights  like the ones discussed above  into the mental well-being of a campus in a granularity and scale not possible before. Moreover  with our approach  it is possible to gather these insights through unobtrusive  inexpensive means  with little intrusion. Thus our work can expand and complement current survey based efforts of assessing student mental health and its relationship to attributes of the university or the student body. Broadly  we contribute to the emergent body of HCI research that leverages naturalistically shared population data on social media for mental health measurement [19]: We are able to identify a variety of college-student specific linguistic markers of mental health challenges by employing a novel data source of university-specific Reddit communities.,0,0,0,0,0,0,0,0,2,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/3025453.3025909,9,17,implications for design,"De Choudhury, Bagroy, Kumaraguru",3,1,0,0,2017,We believe that our work can enable technology design that promotes population-centric reflection of mental well-being in college campuses in ways not possible before. This can be accomplished in the following ways: Our work shows that when college students appropriate social media to express their mental health challenges  our method can accurately identify and measure such expressions. This observation and methodology can be incorporated into interactive applications for campus counselors. The application could surface specific linguistic attributes highly correlated with the mental health expressions of students on social media  as well as the temporal manifestations of these attributes in different student groups. This information can be highly beneficial to campus counseling centers and other campus health service providers in understanding the pervasiveness of mental health expressions  and the variety of topics that student attribute in these communities to be related to mental health challenges. They could act on this information to allocate their services and strategies to better reach and serve students. Our method and findings bear implications for the design of novel student mental well-being tracking interfaces  visualizations  and systems for use by campus administrators. These interfaces could provide stakeholders with an interactive way to identify temporally and in a near real-time fashion  the ebbs and highs of mental well-being  during a typical academic year as well as over extended periods of time. This information can then be utilized for routine assessments of campus morale  as well as to understand the impacts of academic events like examinations  regulations and policy decisions in campus life. Further  it can also contribute to improved preparedness in campus in case of an emergency and assessing mental resilience of the student body in response to adverse events that affect mental well-being of student. Finally  these systems can also empower campus administrators with collective information about students given by the students themselves  to identify how to proactively employ  allow and manage campus specific resources  mental health awareness and mitigation programs in order to best cater to the needs of the students  and improve campus mental well-being.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/3025453.3025909,9,18,limitations and future work,"De Choudhury, Bagroy, Kumaraguru",3,1,0,0,2017,We note that the Mental Well-being Index is not meant to be a diagnostic tool to assess who is at risk of mental illness. Thus we caution against appropriating the index as a mechanism to identify specific college students who could be suffering from mental health concerns. However  as future work  it will bworthwhile to examine to what extent MWI’s assessments of mental health challenges correlate with psychometric assessments obtained via instruments like the Patient Health Questionnaire (PHQ) [38]. We also caution against deriving causal claims between various university attributes and manifested MWI. Further  we only studied 150 ranked universities in the US. We caution against arbitrary generalizations. Prevalence. We found that the percentages of mental health expression users in Reddit were slightly lower (5%) than reported national statistics of college students with significant mental health concerns (7%) [6]. We hypothesize a couple of reasons behind this difference. 1) Students  due to the stigma of mental illness  may be underreporting their mental health concerns on a public platform like Reddit. 2) Students who appropriate social media for mental health needs may be ones who do not (or are not able to) seek professional help  accounting for the discrepancy. Evaluation. Although our validation approach for MWI derived trends that align with some known patterns of college students’ mental health challenges  one of the limitations of our work is more rigorous evaluation. Which are the campuses with the most mental health challenges  and does our MWI metric correlate with those statistics? Answering this question requires access to the normalized levels of mental health concerns in the universities studied here  which is not available for public use [56]. Moreover  university administrators may be hesitant to share such statistics more widely due to the stigma it may bring to a university student body. However  as we showed  MWI can be adopted to compare across subgroups of campuses that share similar attributes. Alternative Data Sources. Finally  we leveraged data from Reddit. While this social media is very popular in the college student demographic  it is likely that a variety of other social media are also used with data volunteering efforts of students  such as Instagram  Twitter  and Snapchat  as noted earlier. Future research could examine how data from these various platforms may be integrated to improve the assessment of mental health expressions in campuses.,0,0,0,0,0,0,2,0,2,0,,2,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/3025453.3025909,9,19,conclusions,"De Choudhury, Bagroy, Kumaraguru",3,1,0,0,2017,Many college students are appropriating online social platforms for mental health disclosure and support seeking needs. We used student-geared Reddit communities of over a hundred universities to build and evaluate a transfer learning based classification approach that can detect mental health expressions with 97% accuracy. Leveraging this classifier  we then developed a Mental Well-being Index (MWI) to evaluate the collective mental health status of over 100 university campuses in the US. We then showed the relationship between various attributes of the universities and their student bodies  and MWI. We believe our work can enbale technology design to tackle mental health challenges in college populations.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://ieeexplore.ieee.org/abstract/document/7752434,10,1,abstract,"Saravia, Chang, Lorenzo, Chen",4,0,0,0,2016,To apply the language markers and automatic classifiers in real life  we would suggest Weibo users to be more cautious when reading a post or message about suicide. When suspecting someone might be at risk  they can refer the person’s Weibo account to our classifiers that will automatically screen that person’s public posts and further assess his or her conditions. It will be beneficial if a longitudinal study can be carried out to apply the algorithms developed by this study to screen and assess Weibo posts continuously and provide the results to suicide prevention professionals for double check and follow-up. In turn  the experts’ feedback and follow-up results should be fed back to the model’s developers for optimization Some social media platforms  such as Facebook and Instagram  have developed “report” functions to allow users to flag those that are expressing suicidal thoughts. The report will be manually reviewed by in-house reviewers to decide whether the flagged person is indeed at risk. If automatic classifiers such as the ones developed by this study can be integrated into such kind of Web-based report function  it will improve review efficiency and better empower social media platforms and users to contribute to suicide prevention. As social media are rapidly penetrating into our daily life  the opportunities for detecting and engaging distressed individuals via social media should not be missed.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://ieeexplore.ieee.org/abstract/document/7752434,10,2,introduction,"Saravia, Chang, Lorenzo, Chen",4,0,0,0,2016,Most people suffering from mental illnesses have a prevalent behavior of wanting to be alone. Due to this isolation  mental disorder patients seek online venues  such as Twitter  to openly talk about their illnesses. These shared experiences become publicly accessible information containing reliable insights – linguistic and behavioral patterns – into a user’s personality and character. Such rich repositories of data have sparked the interest of researchers in the areas of both psychology and computational linguistics [3]  [1]  [2]. In this study  we use Twitter data to build predictive models that can detect if a user is suffering from various mental illnesses. These models are then leveraged to build an analytics and assessment tool  MIDAS  as briefly shown in Figure 1. Essentially  MIDAS provides statistics  in the form of visuals  to quickly assess the mental health of users via their Twitter feed. MIDAS can then be used by therapists and public health officials to quickly obtain information regarding a person’s mental health. In this regard  MIDAS has the potential to prevent tragedies commonly linked to people with undetected and untreated mental disorders (e.g.  suicide and addiction). On a broader scale  we envision systems such as MIDAS being able to discover vulnerable geographic regions and assisting in the efficient and timely distribution of medical treatment. Currently  there are various popular tools  such as the Center for Epidemiologic Studies Depression Scale Revised (CESDR)  that can provide a quick and free mental-health assessment test [4]. 1 Although CESD-R offers a reliable self-administered test  it requires time and careful reading to complete the 20- items survey. Additionally  users filling up the survey might not be in a recommended mental state to personally carry out the survey. In order to ensure that mental health tests are carried out much faster and efficiently  less human intervention and more automation is desired. By utilizing users Twitter feed  we design a system  MIDAS  that automatically performs a mental health checkup. This is beneficial for the patients that are physically or mentally unable to partake in traditional screening tests. As for therapists or primary care doctors  such system can complement their current techniques used for assessing a patient’s mental wellbeing. Consequently  the lack of on-demand  user-friendly mental-health screening tests has motivated us to first  build a system that can effectively predict mental disorders; and secondly  provide a web platform to quickly and effectively assess the mental health of online users. In short  this work employs an effective data collection mechanism to collect patient and non-patient accounts. We then extract two sets of features  which include term frequency-inverse document frequency (TF-IDF) and the Pattern of Life features (PLF) adopted from the work of Coppersmith et al.  [1]. Lastly  models are trained to produce statistics that are leveraged to build an online visualization tool to analyze several characteristics of Twitter users. With the aforementioned considerations and objectives in mind  we propose the MIDAS system. The main contributions and novelties of this work are: • An approach that employs an efficient data collection mechanism  to be used for the accurate prediction of two prevalent mental disorders  namely Borderline Personality Disorder and Bipolar Disorder. • A system built on top of the mental disorder pre-trained models  which offers real-time analytics aimed at investigating user characteristics as they relate to mental disorders.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://ieeexplore.ieee.org/abstract/document/7752434,10,3,data collection,"Saravia, Chang, Lorenzo, Chen",4,0,0,0,2016,Our system’s architecture  as shown in Figure 2  is divided into the following main components: data collection  data preprocessing  features extraction  prediction model  pre-trained model  REST API and visual interface. To train our models  we require information from two different types of users: patients and non-patients. Therefore  we employed a combined – manual effort and keyword matching – data collection approach to efficiently collect data for these users. For the collection of patients  we manually collect the community portals relevant to both mental disorders. 2 From these portals’ followers list  we select the self-reported users who explicitly state  in their profile description  that they suffer from a mental illness; i.e.  for a given user  we are checking if his/her profile contains any keyword related to a target disorder (e.g.  “borderline”  “bpd”  “bipolar”). Non-patients are referred to as random active Twitter users who are not explicitly stating that they are suffering from Bipolar disorder (hereinafter referred to as “BD”) or Borderline Personality Disorder (hereinafter referred to as “BPD”). To obtain these users  we randomly sampled Twitter IDs. Thereafter  we proceeded to download the tweets from the selected IDs. After the users have been identified  we manually label them into one of two categories: 1) Patient: a person who is suffering from a mental disorder  2) Not-related: any user who we don’t consider to be a patient. Lastly  after having obtained the final list of patients  we retrieve their tweets. These steps are applied for the collection of both BPD and BD patient datasets.,1,2,2,2,2,0,0,0,0,0,,0,0,0,0,0,0,,0
https://ieeexplore.ieee.org/abstract/document/7752434,10,4,preprocessing,"Saravia, Chang, Lorenzo, Chen",4,0,0,0,2016,This work only considered English user-authored content; we remove users who have more than 50% of posts containing hyperlinks. We also exclude users who have lower than 100 posts in total – they are mostly inactive users.,0,0,0,0,0,0,0,0,0,0,,0,0,2,0,0,0,,0
https://ieeexplore.ieee.org/abstract/document/7752434,10,5,feature extraction,"Saravia, Chang, Lorenzo, Chen",4,0,0,0,2016,In this work  we are focused on two main type of features (linguistic and behavioral). TF-IDF is adopted to model the linguist features of patients and Pattern of Life Features (PLF)  adopted from the work of Coppersmith et al.  [1]  is used to model the behavioral style of patients. TF-IDF Features: To capture the frequent and representative words used by the patients  TF-IDF is applied on the unigram and bigrams collected from all the patients’ tweets. Pattern of Life Features (PLF): These features reveal the emotional patterns and behavioral tendency of users  by measuring polarity  emotion and social interactions. In order to fully compose the PLF  we combined the following list of features: Age and Gender: Twitter does not publicly provide information about the age and gender of its users  mainly due to privacy concerns  so we adopted the work of Sap et al.  [5] to fill in this information. 2) Polarity Features: The Sentiment140 API 3 was used to label each tweet as either positive  negative or neutral. The polarity is furthermore transformed into five different values to capture the affective traits of each user: 1) Positive Ratio: the percentage of positive tweets  2) Negative Ratio: the percentage of negative tweets  3) Positive Combo: captures the mania and hypomania traits of patients  which is determined by the number of continuous positive posts appearing more than x amount of times within a period of time in minutes T. 4) Negative Combos: captures the depression traits of patients and is determined by the number of continuous negative posts appearing more than x amount of times within a period of time in minutes T. 5) Flips Ratio: quantifies the emotional unstableness and is determined by counting how frequently two continuous tweets  with different polarity (either positive to negative or negative to positive)  appear together within a period of time in minutes T. In our work  x is set to 2 and T is set to 30 minutes. 3) Social Features: These features can demonstrate how users are behaving with respect to their environment. The following are the social features designed for each user: 1) Tweeting Frequency: the frequency of daily posts; 2) Mention Ratio: the percentage of posts which contain at least one mention of another user; 3) Frequent Mentions: the number of Twitter users mentioned more than three times  which is a measurement of how many close friends a particular user may have; 4) Unique Mentions: the number of unique users mentioned  which is a measure of the width of a user’s social network.,1,2,2,2,0,0,0,0,0,0,,0,0,0,2,1,2,,0
https://ieeexplore.ieee.org/abstract/document/7752434,10,6,prediction model,"Saravia, Chang, Lorenzo, Chen",4,0,0,0,2016,In this study  we collected 17 BPD and 12 BD community portals. 5000 followers for each portal was collected  for a total of 145000 accounts. From these accounts  we manually picked a subset and annotated each user into positive examples (patient) or not-related examples. After filtering  we gathered a total of 278 BD accounts and 203 BPD accounts. A total of 548 random samples (negative examples) were obtained directly from the Twitter REST API. We experimentally chose Random Forest Classifier to be our main learning model. Consequently  we trained separate classifiers  one for each mental disorder  and equally distributed the random samples to each. We used a 10-fold cross validation to evaluate our models. Applying only the TD-IDF features  we achieved a precision of 96% for both the BP and BPD models. On the other hand  by applying the pattern of life features  we achieved a precision of 91% and 92% for the BD and BPD models  respectively. All models were further pre-trained and prepared as a REST-API service  which produce statistical outputs  further converted into insightful visuals.,1,2,2,2,0,0,0,0,0,0,,0,0,0,0,0,2,,0
https://ieeexplore.ieee.org/abstract/document/7752434,10,7,midas interface and demonstration,"Saravia, Chang, Lorenzo, Chen",4,0,0,0,2016,In this section  we discuss how the pre-trained models were used to extract various outputs that form the key components of the MIDAS visual interface. The components are divided into two groups  namely: User Profile and Mental Disorder Dimensions. Initially  the system provides options to either compare between existing Twitter accounts or compare with a BPD or Bipolar patient (obtained from training dataset). After providing the users to be analyzed  the system presents a comparison dashboard. The main idea of the comparison dashboard is to allow viewers to easily compare characteristics of users with respect to mental illness. Below  we briefly introduce each of the main components presented in the comparison dashboard along with a real-world scenario.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://ieeexplore.ieee.org/abstract/document/7752434,10,8,user profile,"Saravia, Chang, Lorenzo, Chen",4,0,0,0,2016,The upper part of the dashboard presents basic information about a Twitter user  which include the following: 1) Screen name: the screen name of the user; 2) Age Prediction: the age prediction obtained from the PLF model; 3) Gender Prediction: the gender prediction  also obtained from the PLF model. In this demonstration  it is important to note that there is a privacy issue in the sense that we are dealing with sensitive information. Therefore  we chose not to disclose the identity of the samples used in this demonstration and simply refer to them as sample A and sample B. However  the system allows to perform analysis on any Twitter public account or patient account). 4 As a demonstration example  sample A was chosen from the Bipolar disorder training dataset and sample B is the account of an active Twitter user  whose identity is protected as much as possible.,0,0,0,0,0,0,2,0,0,0,,0,0,0,0,0,0,,0
https://ieeexplore.ieee.org/abstract/document/7752434,10,9,mental disorder dimensions,"Saravia, Chang, Lorenzo, Chen",4,0,0,0,2016,Below the user information  two visuals are provided to show the probability of a user suffering from either BPD or Bipolar disorder. As shown in Figure 3  sample A (a bipolar patient) has a high probability of suffering from any of the two mental disorders. In the work Coppersmith et al.  [2]  this is referred to as the comorbidity or concomitance (overlap of disorders)  which is common in these type of patients. Sample B (a popular Twitter account) has significantly less probability of suffering from BPD but a somewhat noticeable probability of suffering from Bipolar disorder. At first glance  the statistics provided are interesting  but not detailed enough to understand  for example  why sample B has a higher probability of suffering from BD than BPD. In order to better understand each user  we offer additional exploratory tools to analyze user behavioral and linguistic styles. As presented earlier  there are various dimensions or features which are important for the PLF model to produce satisfactory predictions. The raw values of these features are obtained from the social and polarity measures produced by the PLF model. Social features refer to the engagement activity of users and polarity features refer to the affective traits of users. Social features include: tweeting frequency  tweet count  following  unique mentions  mentioning frequency and frequently mentioned. Polarity features include: negative combos  positive combos and flips ratio. In addition to the raw values provided  a sample sentiment timeline and sentiment ratio meter are demonstrated. The timeline contains a sequence of tweets for a given period of time and it demonstrates the sentiment trend of the user. The sentiment meter shows the distribution of each sentiment for a given user. As show in Figure 4  the polarity timeline and sentiment meter for both sample A (left) and sample B (right) are provided. In comparison  sample A expresses more negative thoughts than sample B; this is expected as sample A is a suffering patient. Conversely  sample B shares more positive thoughts than sample A; this is expected as this specific user is a widely known and joyful figure. Based on the sentiment meter and sentiment timeline  it is becoming a little clear as to why there might be a difference of behavioral styles between the two users. However  there is more to explore in terms of the linguistic style preferred by each user. To demonstrate the language (frequently used terms) by each user  we rely on the top k features provided by the TF-IDF models. As shown in Figure5 (A & B)  a word-cloud visual component is used to observe the preferred choice of words by each user. In both users’ world-clouds  we can observe the frequent use of affect-related words such as “love”  “hope”  “need” and “like”. Moreover  it is clear that sample A word-cloud is packed with more emotion or sentiment words. For those reasons  sentiment analysis is vital for these type of studies. Additionally  the word-cloud is able to visually give insights into the overall sentiment expressed by each user. For instance  sample B’s world-cloud shows more positive words and sample A shows more negative words. Besides exploring aggregated results  it is also important to study the trend of mental status of each patient with respect to time. As shown in Figure 5 (C & D)  the periodic mental status of each user is provided. The line chart was generated using the predictions obtained when using several batches of tweets for a given user; i.e.  each point represents the probability of a mental disorder for a specific period of time. This component gives insights into a patient’s onset period  which are essential for mental disorder studies as observed by Choudhoury et al.  [3]. For brevity  there is an apparent onset period for sample A (May) and one that is beginning to happen for sample B (year 2015).,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://ieeexplore.ieee.org/abstract/document/7752434,10,10,conclusion,"Saravia, Chang, Lorenzo, Chen",4,0,0,0,2016,Overall  we built an online system that allows for the exploration of various properties of a user with respect to two particular mental disorders. This system provides minimal results which can be exploited to built more complex systems to better understand a user behavior online. In addition  the system can be used to collect more data of patients through providing a means of returning feedback. Feedback is important to fine tune and further improve the learning models. For reference  we provide a short video demonstrating all the properties of the system and its usages (http://bit.ly/midasvideo).,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://www.nature.com/articles/s41598-020-68764-y,11,1,abstract,"Kim, Lee, Park, Han",4,0,0,0,2020,Users of social media often share their feelings or emotional states through their posts. In this study  we developed a deep learning model to identify a user’s mental state based on his/her posting information. To this end  we collected posts from mental health communities in Reddit. By analyzing and learning posting information written by users  our proposed model could accurately identify whether a user’s post belongs to a specific mental disorder  including depression  anxiety  bipolar  borderline personality disorder  schizophrenia  and autism. We believe our model can help identify potential sufferers with mental illness based on their posts. This study further discusses the implication of our proposed model  which can serve as a supplementary tool for monitoring mental health states of individuals who frequently use social media.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://www.nature.com/articles/s41598-020-68764-y,11,2,introduction,"Kim, Lee, Park, Han",4,0,0,0,2020,Social media is a popular space for expressing users’ feelings1 2. Through diverse social media or online social health communities  users often are likely to present their mental problems or illness with anonymity3. Such online health communities can be a network for expressing sympathy by communicating with others who have similar symptoms4. In addition  users often try to obtain health information related to their symptoms on social media as an attempt to diagnose themselves5 6. With this trend  several scholars have analyzed user-generated content on social media for observing users’ emotional state or mental illness  including depression  anxiety  or schizophrenia3 6 7 8 9 10. A recent study collected Twitter posts of users who reportedly had been diagnosed as depression7  analyzed the linguistic and emotional characteristics of the collected posts using the Linguistic Inquiry and Word Count (LIWC)11  and tracked their social engagement changes on Twitter. Another study attempted to predict users’ postpartum depression on Facebook  based on their posts and comments  and used specialized psychometric instruments to evaluate the level of postpartum depression between pre- and post-natal periods12. In addition  Reece et al.13 used image data to detect users’ depression on social network services. After collecting photos from Instagram uploaded by users  both face detection and colorimetric analysis were applied. To detect users’ anxiety disorders  prior research collected user data from Reddit and showed that N-gram language modeling and vector embedding procedures with topic analysis of users’ posts are efficient in finding potential users with anxiety disorders3. Several previous studies revealed that social media data is useful in observing or detecting users’ emotions or potential mental problems. This study goes one step further; by collecting various mental-health-related data from social media  we aim at developing a deep learning model that can identify a user’s mental disorder  including depression  anxiety  bipolar  borderline personality disorder (BPD)  schizophrenia  and autism. To this end  we collected users’ posts from Reddit  a popular social media that includes numerous mental-health-related communities (or so-called ‘subreddits’)  such as r/depression  r/bipolar  and r/schizophrenia8. As our aim is to identify whether a user suffers from a mental illness  such as depression and anxiety14  we collected data from the six subreddits: r/depression  r/Anxiety  r/bipolar  r/BPD  r/schizophrenia  and r/autism. Note that we employed the mental-health-related subreddits identified in prior work10. More specifically  among the popular 83 subreddits  6 subreddits were identified as mental-health-related ones by a statistical approach like a semi-supervised method as well as an assessment procedure by experts10. Each identified subreddit is associated with a specific mental condition  e.g.  r/depression is associated with the depression condition. By collecting and analyzing user’s posts uploaded in multiple mental-health-related subreddits in Reddit  we investigated whether specific posts of the user can be classified as relevant types of mental disorder. People who suffer from specific mental disorders may not know their most accurate diagnosis; for example  people with bipolar disorder can have a hard time distinguishing bipolar from depression since the symptom of both is similar6 15  or even it is strenuous to initially diagnose bipolar disorder16. We assumed that users attempt to search for mental health information on social media with general keywords  such as ‘mental health’  ‘mental illness’  or ‘mental status’  as if they reach out for help by opening up general stories about them at an early stage. Subsequently  many users are likely to communicate with other users in one of the general health-related channels in Reddit (e.g.  r/mentalhealth) in the beginning  but often fails to recognize their accurate problems. Therefore  we attempt to detect users’ potential mental disorders by their posts on social media. This study seeks to address the following research question. Can we identify whether a user’s post belongs to mental illnesses on social media?,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://www.nature.com/articles/s41598-020-68764-y,11,3,data collection,"Kim, Lee, Park, Han",4,0,0,0,2020,We collected post data from the following six mental-health-related subreddits  each of which is reported to be associated with a specific disorder10: r/depression  r/Anxiety  r/bipolar  r/BPD  r/schizophrenia  and r/autism. In addition  we further collected post data from the most popular health-related subreddit17  r/mentalhealth  to analyze posts with general health information. From each subreddit  we collected all the user IDs who had at least one post related to the mental health. Along with user IDs  we also collected titles and posts using the PushshiftAPI18. Note that all the user information is anonymized  hence no personally identifiable information was not included; we followed all the anonymization process guided by the Sungkyunkwan University Institutional Review Board (IRB). Overall  the current study collected information from 248 537 users  who wrote 633 385 posts in the seven subreddits from January 2017 to December 2018. Table 1 summarizes the information of collected data.,2,2,2,2,0,0,0,0,0,2,,0,2,2,0,0,0,,0
https://www.nature.com/articles/s41598-020-68764-y,11,4,data pre processing procedure,"Kim, Lee, Park, Han",4,0,0,0,2020,The data pre-processing procedure for the collected post data is presented in Fig. 1. After collecting the data  each title was combined with its corresponding post. We removed unnecessary punctuation marks and white spaces for each post. Then  we used the natural language toolkit (NLTK) implemented in Python to tokenize users’ posts and filter frequently employed words (stop words). Porter Stemmer  a tool used to define a series of guidelines for exploring word meaning and source  was employed on the tokenized words  to convert a word to its root meaning and to decrease the number of word corpus. After this procedure  data from 228 060 users with 488 472 posts in total were employed for the analysis.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,2,0,,0
https://www.nature.com/articles/s41598-020-68764-y,11,5,classification models,"Kim, Lee, Park, Han",4,0,0,0,2020,We developed six binary classification models  each of which categorizes a user’ specific post into one of the following subreddits: r/depression  r/Anxiety  r/bipolar  r/BPD  r/schizophrenia  and r/autism. Our conjecture is that a user who suffers from a specific mental problem writes a post on the corresponding subreddit that deals with the problem. A user can write posts across multiple subreddits if he/she suffers from multiple mental health problems  e.g.  a user suffering from both depression and anxiety. However  if the model is trained with the posts of users who have multiple symptoms like a prior study10  the classification model may suffer from noisy data. Therefore  we developed six independent binary classification models for each symptom to improve the performance. By developing six independent models for each mental disorder  each of which uses data where users suffer from only one particular mental problem  we were able to accurately identify a user’s potential mental state. For example  to develop a model for detecting depression  we labeled the posts written by users who upload posts only in the r/depression as the depression class; the opposite class is referred to as the non-depression class. To address a class-imbalance issue for the collected data  we applied the synthetic minority over-sampling technique (SMOTE) algorithm19. We divided our dataset into training (80%) and testing (20%) sets. Then  XGBoost and convolutional neural network (CNN) were employed. Morover  we excluded the posts of users who wrote posts across multiple subreddits in learning phase. To quantitatively represent each post  we converted the words in the training set to numerical representations (Fig. 2). For the XGBoost classifier  we used the TF-IDF vectorizer in the sckit-learn package20 to convert words into n-dimensional vectors. In the case of the CNN classifier  we applied word-embedding procedures from the pre-processed texts using the word2vec API of Python Package  Gensim21. The word vectors were pre-trained with the training dataset collected for the current study with continuous bag-of-words representation (CBOW) models  while the size of window was set to five. Note that by using the pre-trained word2vec model for representing each post for each subreddit  a language style used by users who write posts in a subreddit can be trained for the specific subreddit. An overview of the proposed CNN-based model is presented in Fig. 2. The model architecture is organized by the sequence of layers that includes an embedding layer  convolutional layer  max-pooling layer  dense layers  and the output. Fig. 2 illustrates how a post is trained in the given model. The first layer of the model is an embedding layer that represents the word embeddings of a pre-processed post with 20 dimensions  and its weight is initialized by the pre-trained word2vec. Second  a convolutional layer with input of word vectors has 128 filters  and each filter size is five. In addition  we applied a dropout rate of 0.25 to prevent over-fitting issues. The next layer is a max-pooling layer  which takes the maximum values within the CNN filters  and its dimension is 128. The output of the max-pooling layer is passed through two fully connected (dense) layers  and the final output is the probability of the classification through the sigmoid activation function  which ranges from 0 to 1. For training the neural network  we used both the binary cross-entropy loss function and Adam optimizer22  with a learning rate of 0.001. Our model was trained through 50 epochs and the batch size was set to 64.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://www.nature.com/articles/s41598-020-68764-y,11,6,ethics declarations,"Kim, Lee, Park, Han",4,0,0,0,2020,This study was approved by the Ethical Committee and Institutional Review Board of the Department of Applied Artificial Intelligence  Sungkyunkwan University (#H1AAI2020).,0,0,0,0,0,0,0,0,0,2,,0,0,0,0,0,0,,1
https://www.nature.com/articles/s41598-020-68764-y,11,7,results,"Kim, Lee, Park, Han",4,0,0,0,2020,Four evaluation metrics were employed to validate the performance of the models: accuracy (Eq. 1)  precision (Eq. 2)  recall (Eq. 3)  and F1-score (Eq. 4). TP  FN  TN  and FP represent true positive  false negative  true negative  and false positive  respectively. Table 2 summarizes the performance of the six binary classification models. Among the six different subreddits  r/autism showed the highest accuracy (96.96%) in the CNN  but had the lowest F1-score on the autism class (XGBoost: 38.31%  CNN: 48.73%)  which is due to the class imbalance problem. Overall  CNN models showed higher accuracy than XGBoost models across all the subreddits. One of the most class-balanced subreddits  r/depression  showed the highest performance scores in terms of precision (89.10%)  recall (71.75%)  and F1-score (79.49%) for the depression class. Three other subreddits  r/Anxiety  r/bipolar  and r/BPD  also showed high accuracy with CNN models  77.81%  90.20%  and 90.49%  respectively  and their F1-scores in identifying mental illnesses ranged from forties to fifties (%)  which are relatively lower than those with the class-balanced channels. In summary  our proposed model can accurately detect potential users who may have psychological disorders. We believe collecting more data may resolve the imbalanced data problem  resulting in a better performance.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://www.nature.com/articles/s41598-020-68764-y,11,8,discussion,"Kim, Lee, Park, Han",4,0,0,0,2020,Detecting mental illness problems in early stages and providing appropriate solutions can help potential mental disorder sufferers23. By collecting and analyzing data from mental-health-related subreddits in Reddit that focus on mental disorder issues  we introduced a deep learning model with natural language processing methods to identify the users with potential mental illness based on their posts. We believe that our method can open up a new research era where online social media can play a role as an efficient source for identifying potential mental illness based on users’ specific posts24. However  a majority of people who may have mental illness are still in social blind spots and lacks appropriate treatment due to several reasons such as difficulty in revealing their status to someone in person or having difficulties in physically accessing the clinics23. Based on the lessons learned  the following implications are presented. First  deep learning approaches with appropriate natural language processing methods can be used to detect users’ potential mental illnesses by their posts. With the employment of easily accessible social media data  the approaches used in this study can be adopted to alert the users who may be suffered from specific mental disorders before they visit counseling centers. Second  this study provides notable evidence supporting the possibility of utilizing online platforms that can help people in need of mental treatment. Specifically  for example  online platform service providers may ask a user’s consent first to access one’s account and if agreed  can provide the probabilities of each mental disorder predicted through our validated models based on the user’s posts. Lastly  the current study suggests detecting mental illness in social media can be a prominent research area in the future. The findings of the current study reveal the potential for social media platforms that can play a role in providing a space to interact with others who are suffered by mental disorder. However  there are a few limitations in this study. The current study did not consider several factors (e.g.  socio-demographic and regional differences) that could affect the classification models. These factors can be considered in future research  which can improve the quality or accuracy of the deep learning models. In addition  we collected the data from the public social media  Reddit  which may be different from the personal feed of social network services in expressing users’ emotions. We did not conduct additional validation procedures of our model with another independent dataset as mentioned above  which would need to be further investigated. Although postings in online social media could not explicitly tell the symptoms compared to posts in users’ personal pages that may say they are diagnosed with clinical mental illnesses  online social media have a potential to be used to identify mental disorder sufferers because they share their symptoms relatively accurately under the semi-anonymity system. Also  we trained our model on a specific mental state to directly classify the symptom and provide the predicted probabilities for each symptom. In this way  we could not accurately measure the co-morbid mental illness status  which is left for future work. ​​In future study  we could adopt an ensemble approach with our multiple binary classification models  which can be utilized to identify the real-world mental conditions  such as co-morbid illness. We also plan to validate our proposed model in posts of users who may have uncertain mental disease in other social network services such as Facebook or Twitter. In addition  a time-series user-level analysis that tracks a users’ longitudinal behavior pattern can help to develop a user-level detection model for mental illness using a recurrent neural network.,0,0,0,0,0,0,0,0,2,0,,2,0,0,0,0,0,,0
https://www.nature.com/articles/s41598-020-68764-y,11,9,data availability,"Kim, Lee, Park, Han",4,0,0,0,2020,The collected data in this paper can be achieved at https://jina-kim.github.io/dataset/20srep-mental. Other information used in this study can be accessed from the corresponding author with the reasonable request.,0,0,0,0,0,2,0,0,0,0,,0,0,0,0,0,0,,0
https://aclanthology.org/W19-3013.pdf,12,1,abstract,"Amir, Drezde, Ayers",3,0,1,0,2019,The ability to track mental health conditions via social media opened the doors for largescale  automated  mental health surveillance. However  inferring accurate population-level trends requires representative samples of the underlying population  which can be challenging given the biases inherent in social media data. While previous work has adjusted samples based on demographic estimates  the populations were selected based on specific outcomes  e.g. specific mental health conditions. We depart from these methods  by conducting analyses over demographically representative digital cohorts of social media users. To validated this approach  we constructed a cohort of US based Twitter users to measure the prevalence of depression and PTSD  and investigate how these illnesses manifest across demographic subpopulations. The analysis demonstrates that cohort-based studies can help control for sampling biases  contextualize outcomes  and provide deeper insights into the data.,0,0,0,0,0,0,2,0,0,0,,0,0,0,0,0,0,,0
https://aclanthology.org/W19-3013.pdf,12,2,introduction,"Amir, Drezde, Ayers",3,0,1,0,2019,The ability of social media analysis to support computational epidemiology and improve public health practices is well established (Culotta  2010; Paul and Dredze  2011; Salathe et al.  2012; Paul and Dredze  2017). The field has seen particular success around the diagnosis  quantification and tracking of mental illnesses (Hao et al.  2013; Schwartz et al.  2014; Coppersmith et al.  2014a  2015a c; Amir et al.  2017). These methods have utilized social media (Coppersmith et al.  2014b; Kumar et al.  2015; De Choudhury et al.  2016)  as well as other online data sources (Ayers et al.  2017  2013  2012; Arora et al.  2016)  to obtain population level estimates and trends around mental health topics. Accurately estimating population-level trends requires obtaining representative samples of the general population. However  social media has many well know biases  e.g. young adults tend be over-represented (demographic bias). Yet  most social media analyses tend ignore these issues  either by assuming that all the data is equally relevant  or by selecting data for specific outcome. For example  studying depression from users who talk about depression instead of first selecting a population and then measuring outcomes. Outcome based data selection can also introduce biases  such as over-representing individuals vocal about the topic of interest (self-selection bias). Consequently  trends or insights gleaned from these analyses might not be generalizable to the broader population. Fortunately  these problems are well understood in traditional health studies  and well-established techniques from polling and survey-based research are routinely used to correct for these biases. For example  medical studies frequently utilize a cohort based approach in which a group is pre-selected to study disease causes or to identify connections between risk factors and health outcomes (Prentice  1986). We can replicate these universally accepted approaches by conducting analyses over digital cohorts of social media users  characterized with respect to key demographic attributes. In this work  we propose to use such a social media based cohort for the purposes of mental health surveillance. We developed a digital cohort by sampling a large number of Twitter users at random (not based on outcomes)  and then using demographic inference techniques to infer key demographics for the users namely  the age  gender  location and race/ethnicity. Then  we used the cohort to measure relative rates of both depression and PTSD  using supervised classifiers for each mental health condition. The inferred demographic information allowed us to observe clear differences in how these illnesses manifest in the population. Moreover  the analysis demonstrates how social media based cohort studies can help to control for sampling biases and contextualize the outcomes.,0,0,0,0,2,0,2,0,0,0,,0,0,0,0,0,0,,0
https://aclanthology.org/W19-3013.pdf,12,3,methodology,"Amir, Drezde, Ayers",3,0,1,0,2019,We now briefly describe our approach for cohortbased studies over social media. A more detailed description of the proposed methodology will appear in a forthcoming publication. Most works on social media analysis estimate trends by aggregating document-level signals inferred from arbitrary (and biased) data samples selected to match a predefined outcome. While some recent work has begun incorporating demographic information to contextualize analyses (Mandel et al.  2012; Mitchell et al.  2013; Huang et al.  2017  2019) and to improve representativeness of the data (Coppersmith et al.  2015b; Dos Reis and Culotta  2015)  these studies still select on specific outcomes. We depart from these works by constructing a demographically representative digital cohort of social media users prior to the analyses  and then conducting cohort-based studies over this preselected population. While a significant undertaking in most medical studies  the vast quantities of available social media data make assembling social media cohorts feasible. Such cohorts can be used to support longitudinal and cross-sectional studies  allowing experts to contextualize the outcomes  produce externally valid trends from inherently biased samples and extrapolate those trends to a broader population. Similar strategies have been utilized in online surveys  which can have comparable validity to other survey modalities simply by controlling for basic demographic features such as the location  age  ethnicity and gender (Duffy et al.  2005).,0,0,0,0,0,0,2,0,0,0,,0,0,0,2,0,0,,0
https://aclanthology.org/W19-3013.pdf,12,4,building digital cohorts,"Amir, Drezde, Ayers",3,0,1,0,2019,Our cohort construction process entails two key steps: first  randomly selecting a large sample of Twitter users; and second  annotating those users with key demographic attributes. While such attributes are not provided by the API  automated methods can be used to infer such traits from data (Cesare et al.  2017). Following this approach  we develop a demographic inference pipeline to automatically infer age  gender  race/ethnicity and location for each cohort candidate. Age Identifying age based on the content of a user can be challenging  and exact age often cannot be determined based on language use alone. Therefore  we use discrete categories that provide a more accurate estimate of age: Teenager (below 19)  20s  30s  40s  50s (50 years or older). Gender The gender was inferred using Demographer  a supervised model that predicts the (binary) gender of Twitter users with features based on the name field on the user profile (Knowles et al.  2016). Race/Ethnicity The standard formulation of race and ethnicity is not well understood by the general public  so categorizing social media users along these two axes may not be reasonable. Therefore  we use a single measure of multicultural expression that includes five categories: White (W)  Asian (A)  Black (B)  Hispanic (H)  and Other. Location The location was inferred using Carmen  an open-source library for geolocating tweets that uses a series of rules to lookup location strings in a location knowledge-base (Dredze et al.  2013). We use the inferred location to select users that live in the United States. The age and race/ethnicity attributes were inferred with custom supervised classifiers based on Amir et al. (2017)’s user-level model. The classifiers were trained and evaluated on a dataset of 5K annotated users  attaining performances of 0.28 and 0.41 Average F1  respectively. See the supplemental notes for additional details on these experiments1 .,2,2,2,2,2,0,2,0,0,0,,0,0,2,2,0,1,,0
https://aclanthology.org/W19-3013.pdf,12,5,mental health classifiers,"Amir, Drezde, Ayers",3,0,1,0,2019,We build on prior work on supervised models for mental health inference over social media data. We focus on two mental health conditions — depression and PTSD — and develop classifiers with the self-reported datasets created for CLPysch 2015 (Mitchell et al.  2015; Coppersmith et al.  2015b). These labeled datasets derive from users that have publicly disclosed on Twitter a diagnosis of depression (327 users) or PTSD (246 users)  with an equal number of randomly selected demographically-matched (with respect to age and gender) users as controls. For each user  the associated metadata and posting history was also collected — up to the 3000 most recent tweets  per limitations of the Twitter API. The participants of the task proposed a host of methods ranging from rule-based systems to various supervised models (Pedersen  2015; PreotiucPietro et al.  2015; Coppersmith et al.  2015b). More recently  the neural user-level classifier proposed by Amir et al. (2017) showed not only good performance on this task  but also the ability to capture implicit similarities between users affected by the same diseases  thus opening the door to more interpretable analyses2 . Hence  we adopt their model for this analysis.,0,0,0,0,2,0,2,0,1,0,,2,0,0,0,0,0,,0
https://aclanthology.org/W19-3013.pdf,12,6,analysis,"Amir, Drezde, Ayers",3,0,1,0,2019,We constructed a cohort for our analysis by randomly selecting a sample of Twitter users and processing it with the aforementioned demographic inference pipeline. After discarding accounts from users located outside the United States  we obtained a cohort of 48K Twitter users with the demographic composition shown in Figure 1. Some demographic groups are over-represented (e.g. young adults) while others are grossly underrepresented (e.g. teenagers) which illustrates the need for methodologies that can take these disparities into account. We then processed the cohort through the mental-health classifiers to estimate the prevalence of depression and PTSD  and examine how these illnesses manifest across the population. The analysis revealed that 30.2% of the cohort members are likely to suffer from depression  30.8% from PTSD  and 20% from both. We observe a significant overlap between people affected by depression and PTSD  which is not surprising given that the comorbidity of these disorders is wellknown  with approximately half of people with PTSD also having a diagnosis of major depressive disorder (Flory and Yehuda  2015). How do these conditions affect different parts of the population? To answer this question  we looked at the affected users and measured how the demographics of individual sub-populations differ from those of the cohort as a whole. Figures 2 and 3 show the estimates for depression  PTSD and both  controlled for the cohort demographics. We observe large generational differences — PTSD seems to be more prevalent among older people whereas depression affects predominantly younger people. We also observe that in all cases Women are more susceptible than Men  and Blacks and Hispanics are more likely to be affected than Whites. This may represent a bias in the underlying data used to construct the classifiers  or a difference in how social media is used by different demographic groups. For example  models that were trained with a majority of data from White users maybe oversensitive to specific dialects used by other communities.,0,0,0,0,2,0,2,0,0,0,,2,0,0,0,0,0,,0
https://aclanthology.org/W19-3013.pdf,12,7,discussion,"Amir, Drezde, Ayers",3,0,1,0,2019,Comparing our estimates with the current statistics provided by the NIH — a prevalence of 6.7% for depression3 and 3.6% for PTSD4 —  we can see that ours are much higher. It should be noted however  that the NIH reports refers to Major Depression episodes whereas our classifiers maybe also be sensitive to mild depressions which may never be diagnosed as such. Moreover  these estimates are not directly comparable since the NIH statistics are outdated (the estimates are from 2003 and 2015 for PTSD and depression  respectively) and our cohort was not adjusted to match the demographics of the US population. Nevertheless  it is worth noting that the relative prevalence rates  per demographic group  we obtained correlate with the NIH reports. For example  we observe similar distributions in terms of age and gender. However  we found that Blacks and Hispanics are more likely to be affected by mental illnesses  whereas the NIH reports a higher prevalence among Whites. One possible reason for these disparities is that racial minorities are more likely to come from communities with lower education rates and socioeconomic status (SES)  and to be in a position where they lack proper health coverage and mental-health care. Reports from the NIH and other US governmental agencies show that 46.3% of Whites suffering from a mental-illness were subjected to some form treatment  but this was case for only 29.8% of Blacks and 27.3% of Hispanics5 . There may also be a bias in reporting within different racial and ethnic groups  as prevalence estimates can be biased by access to mental health care and social stigma. Recent studies show that factors such as discrimination and perceived inequality have a stronger influence on mentalhealth than it was previously supposed  even when controlling for the SES (Budhwani et al.  2015). Others have found that acute and chronic discrimination causes racial disparities in health to be even more pronounced at the upper ends of the socioeconomic spectrum. One of the reasons being that for Whites  improvements in SES result in improved health and significantly less exposure to discrimination  whereas for Blacks and Hispanics upwards mobility significantly increases the likelihood of discrimination and unfair treatment  as they move into predominantly White neighborhoods and work environments (Colen et al.  2017). While an in-depth analysis of this issue is beyond the scope of this work  these results suggest that it deserves further investigation. A followup study to investigate the role of discrimination in mental-health could be conducted by adding a model to identify users who reported instances of discrimination and compare the prevalence of mental-illness with a control group.,0,0,0,0,0,0,2,0,0,0,,2,0,0,0,0,0,,0
https://aclanthology.org/W19-3013.pdf,12,8,conclusions,"Amir, Drezde, Ayers",3,0,1,0,2019,We have presented the first cohort based study of mental health trends on Twitter. Instead of conducting the analysis over arbitrary data samples selected to match a given outcome  we first developed a digital cohort of social media users characterized with respect to key demographic traits. We used this cohort to measure relative rates of depression and PTSD  and examine how these illnesses affect different demographic strata. The ability to disaggregate the estimates per demographic group allowed us to observe clear differences in how these illnesses manifest across different parts of the population — something that would not be possible with typical social media analysis methodologies. This brings social media analysis methodologies closer to universally accepted practices in surveillance based research. Information about how different subpopulations perceive or are affected by certain health issues  could also improve public health policies and inform intervention campaigns targeted for different demographics. Moreover  the fact that some of our estimates correlate with statistics obtained through traditional methodologies suggests that this might be a promising approach to complement current epidemiology practices. Indeed  this opens the door to more responsive and deliberate public health interventions  and allow experts to track the progress or the effects of targeted interventions  in near real-time.,0,0,0,0,0,0,2,0,0,0,,0,0,0,0,0,0,,0
https://aclanthology.org/W19-3013.pdf,12,9,privacy and ethical considerations,"Amir, Drezde, Ayers",3,0,1,0,2019,The majority of social media analysis approaches try to extract signals from individual posts and thus do not need to record any personal information. However  as we start moving towards userlevel analyses  we are collecting and storing complete records of social media users communications. Even though this information is publicly available  people might not be consciously aware of the implications of sharing all their data and certainly have not given explicit consent for their data to be analyzed in aggregate. This is even more pertinent for analyses involving sensitive information (e.g. health related issues). As it has been demonstrated by the recent incidents involving companies inadvertently sharing or failing to protect users personal data  there is a serious danger of abuse and exploitation for systems that collect and store large amounts of personal data. Even though this is in large part an ethical question  there are technical solutions that can be used to partially address this issue. One is to use anonymization techniques to obfuscate any details that allow third parties (even analysts) to identify the individuals that are involved in the study. Another is to store only abstract representations — which can still be updated and consumed by predictive models —   and discard the actual content. In regards to consent  there are initiatives to support voluntary data donation for research purposes  e.g. the Our Data Helps program6 .,0,0,0,0,0,1,0,2,0,0,,0,1,0,0,0,0,,2
https://link.springer.com/chapter/10.1007/978-3-319-67186-4_6,13,1,abstract,"Toulis, Golab",2,0,0,0,2017,In this paper  we apply text mining and topic modelling to understand public mental health. We focus on identifying common mental health topics across two anonymous social media platforms: Reddit and a mobile journalling/mood-tracking app. Furthermore  we analyze journals from the app to uncover relationships between topics  journal visibility (private vs. visible to other users of the app)  and user-labelled sentiment. Our main findings are that (1) anxiety and depression are shared on both platforms; (2) users of the journalling app keep routine topics such as eating private  and these topics rarely appear on Reddit; and (3) sleep was a critical theme on the journalling app and had an unexpectedly negative sentiment.,0,0,0,0,0,0,0,0,0,0,,0,2,0,0,0,0,,0
https://link.springer.com/chapter/10.1007/978-3-319-67186-4_6,13,2,introduction,"Toulis, Golab",2,0,0,0,2017,Many applications of social media involve text mining  such as understanding user interests  customer reviews  and sentiment around news events. We discuss an application of social media text mining in the context of understanding public mental health. This is an increasingly important application domain: prevalence of mental health conditions is increasing  and so is the amount of data we have to understand these conditions [2]. While text data has been analyzed in great depth for marketing purposes  there remains a large opportunity in using text data to understand public mental health. Several researchers have identified social media data  such as Twitter posts  as a valuable source for mental health signals [3  6]. However  there remain critical gaps in our ability to understand mental health. People may not be willing to share mental health content publicly  especially on the largest social media platforms  which are associated with a personal identity. In this paper  we perform a mental health analysis on stigmatized topics by taking advantage of a unique dataset from a social media app for posting journal entires and sharing and tracking moods (referred to as the journalling app). We discover issues that are not widely discussed on other social media such as sleep. In addition  the journalling app requires users to track their moods in each journal. Hence  we are equipped with user-labeled sentiment  which otherwise is difficult to estimate. By comparing the journalling app dataset to other social media (Reddit)  we identify unique discussions that the mood-tracking community attracts. Furthermore  the journalling dataset is split into two segments: users may share their journals publicly or keep them private. Hence  we are able to understand which mental health issues are shared more publicly than others. The questions we seek to answer include: Does the journalling dataset cover a different set of topics than those discussed on other social media such as Reddit? Are some topics shared more publicly than others? Are some topics kept private? How is sentiment related to topics? Which topics elicit sad or happy feelings? To answer these questions  we use a text mining methodology to derive topics from journals. Furthermore  we take advantage of already labelled moods to perform sentiment analysis. To summarize  we make the following contributions: We apply text mining to a unique dataset of journals and associated moods that has not been studied before. We discover a set of mental health topics that are not frequent on other social media. We quantify which topics are more public than others to identify gaps in available social media data for analyzing public mental health. We compare how user-labeled moods vary across topics to identify important aspects of mental health that require attention. The remainder of this paper is organized as follows. We discuss context and related work in Sect. 2; we describe our datasets in Sect. 3; we explain our methodology in Sect. 4 followed by our results in Sect. 5; and we conclude in Sect. 6. This paper is related to two bodies of work: social media text mining and studies of public mental health. In the context of text mining  there are standard analysis techniques that enable topic modelling and sentiment analysis of social media posts. In the mental health domain  these techniques have had several successes including detecting users expressing suicidal thoughts on social media [11]. We also use standard topic modelling techniques but we utilize them to perform novel topic comparisons between datasets. Mental health studies traditionally collect information via health care professionals  which is a costly process and only allows for analysis of a small subset of the public. A significant opportunity for understanding mental health through social media data has been identified by Harman et al. [6]. They focused on specific mental health conditions  and despite low incident rates they found a wealth of data on social media. They concluded that individual and population-level mental health analysis can be made significantly cheaper and more efficient than current methods. Traditional therapy and studies of mental health conditions heavily utilize linguistic signals. In Diederich et al. [5]  text processing is used to detect mental health conditions such as schizophrenia by analyzing conversations between patients and their psychiatrists using clustering algorithms and sentiment classifiers. The drawback of most studies utilizing doctor-patient data is privacy concerns and smaller datasets. These studies tend to be more ad-hoc due to the size of data. Another large opportunity for mental health data analysis is in electronic medical records. For example  natural language processing (NLP) was used to improve classification accuracy of depression in mood states of patients based on medical records [13]. The creation of a social media corpus for mental health data could significantly improve mental health research [2]. There are two main methodologies for analyzing mental health signals in social media using linguistic signals. The first relies on hand-crafted lexicons containing connotations and strengths of words. For example  the Linguistic Inquiry Word Count (LIWC) lexicon has been used to help clinicians understand mental states given a patient’s writings [6]. The disadvantage of this method is that lexicons like LIWC cover a very small portion of possible language used in informal contexts such as social media. The second common method is to train a language classifier model. This technique is limited when ground-truth labels are not available. Existing work has attempted to approximate labels  and a conservative labeling approach is to filter for users who self-identify with a condition. In particular  previous work searched for statements such as “I was diagnosed with X” [2]. However  there are caveats that the authors identify with this approach. In particular  only a small sample of people would publicly self-identify with a mental health condition. Despite this  through a language model they were able to compare language uses across specific mental health conditions [2]. Alternative pipelines for acquiring labels to model social media text include crowd-sourcing and developing custom apps [2]. Crowd-sourcing involves surveying users. In a previous study  surveys were used to study mental health trends in undergraduate students [4]. In our study  we also identify school-related issues (among other things) as a frequent topic discussed by journallers. While successes with surveys have been made  having users agree to honestly share their personal information is difficult and it can be costly to solicit other data such as social media from surveyed users. On the other hand  apps that interact with social media such as Facebook can be used to collect personality information and grant access to public status updates. However  signals that are important for mental health analysis are not typically shared on Facebook. While existing work has focused on traditional social media  talking about difficult issues is not common on these platforms. On the other hand  the dataset we are studying is specifically designed for mood tracking. The journalling app’s goal is to de-stigmatize the expression of mental health. It is fully anonymous  and hence includes topics that are typically considered taboo on personally identifiable social media platforms. Moreover  the dataset is a combination of both public and private journals  allowing for more private topics to be mentioned frequently. Furthermore  instead of focusing on specific mental health conditions  we choose to take a broader look into the state public mental health. We demonstrate that simple  interpretable signals can be derived from our dataset. Furthermore  we use sentiment labeled by users to avoid relying on custom lexicons. In particular  one of our most important findings is a large issue with sleep. In a study that correlated sleep problems with mental health problems  it was found that patients are much more able to identify when they have an issue with their sleep and more willing to reveal it to their doctors than a potential mental health concern [9]. Furthermore  patient self-perception of sleep issues was strongly associated with health issues  which demonstrates that people are able to accurately identify when a real problem is present. While wide-scale studies of sleep data using social media have not been performed  there is an increasing prevalence of sleep-tracking mobile apps and tools for analyzing the quality of an individual’s sleep [7]. While traditional social media has helped people connect with friends and family  anonymous social media services are becoming increasingly used by people for sharing personal stories and looking for advice [14]. These communities are growing as the general public becomes comfortable sharing more information online  and benefit people who are unable or do not want to see a doctor in person to talk about mental health [14]. We believe that these types of datasets will become increasingly important to analyze for researchers. As such  we explore another anonymous social media platform  Reddit  and compare mental health topics discussed on Reddit to those written about on the journalling app.,0,0,0,0,0,0,0,0,2,0,,0,2,0,0,0,0,,0
https://link.springer.com/chapter/10.1007/978-3-319-67186-4_6,13,3,data,"Toulis, Golab",2,0,0,0,2017,We analyze two datasets: (1) user communities on Reddit and (2) journals from a mental health journalling mobile app. We omit the name of the app for privacy  and we refer to it as the “journalling app”. Reddit is a social media platform that was originally used for sharing and rating content such as news  documentaries and music. Users post in and subscribe to self-organized communities known as subreddits; subscribing to a subreddit allows a user to view all posts from that subreddit. An advantage of analyzing Reddit data is that the subreddits are labelled according to their topics. Utilizing curated lists from volunteer Reddit users  we crawled all subreddits related to mental health  as well as all subreddits linked by these communities. The second dataset consists of anonymized journal posts from a mobile app designed to help people track their moods and share them anonymously if they desire. For each journal post  the app requires the user to label the journal post with at least one mood selected from a pre-populated list including “happy”  “sad”  etc. We obtained all journals  and the associated moods  written between January 2016 and January 2017. This amounts to over 1.2 million journals written by approximately 75 000 users. Figure 1 plots the number of journals posted over time. Most of the journals were written in the first half of 2016  although we inspected topic distributions per month and did not find seasonal effects. Towards the beginning of 2016  many new users registered on the app and eventually stopped using it. Like weight-loss and productivity apps  we believe this influx is tied to users looking to improve their habits as a New Year’s resolution. Each journal can be set to be private or public (visible to all other users of the app). Roughly one third of all journals are public. Figure 2 plots the number of users on the y-axis versus the percentage of journals they posted publicly. Most users are either mostly private or mostly public. Most journals are relatively short  just like Twitter posts that are at most 140 characters. The average length of a journal with text in it is 128 characters; there are roughly 100 000 journal that have no text  only a mood label. We observed that private users tend to write journals that are slightly  but statistically significantly  longer than those written by public users by approximately 10 characters. Figure 3 shows the distribution of journal lengths  where the spikes correspond to 0 length (mood only)  200 characters (the default limit set by the app) and 300 characters (set as the maximum for visualization purposes). Users of the app can optionally enter their location  age and gender. While most users did not enter this information  we found that those who revealed their location are mostly from North America  those who revealed their gender are predominantly female  and those who revealed their age have an average age of 25.,2,1,1,2,0,1,2,0,2,0,,0,2,1,0,0,2,,0
https://link.springer.com/chapter/10.1007/978-3-319-67186-4_6,13,4,methodology,"Toulis, Golab",2,0,0,0,2017,The goal of this analysis is to understand public mental health by mining social media. We want to identify common topics discussed publicly (Reddit plus public journals from the journalling app) and privately (private journals). For the Reddit dataset  we simply count the number of subscribers in each subreddit related to mental health to discover popular topics and issues. Recall that each subreddit is labelled with its topic  so topic modelling is not necessary. On the other hand  for the journalling app  each journal post is labelled with a mood but not with a topic. Below  we describe our methodology for assigning topics to journals.,0,0,0,0,0,0,0,0,2,0,,0,0,0,0,0,0,,0
https://link.springer.com/chapter/10.1007/978-3-319-67186-4_6,13,5,topic modelling,"Toulis, Golab",2,0,0,0,2017,First  we removed journals with no text and those with fewer than 20 characters1  leaving 1.1 million journals for topic modelling. Next  we pre-processed the text using the Stanford Tweet Tokenizer  which is a “Twitter-aware” tokenizer designed to handle short  informal text [1]. We used the option that truncates characters repeating 3 or more times  converting phrases such as “I’m sooooo happyy” to “I’m soo happyy”. On average  the number of tokens per journal was 27.7. Since we are interested in topics  we removed stopwords and tokens with fewer than two letters  and we only retained nouns which appear in the WordNet corpus [10]. After this filtering  the average number of nouns per journal was 7. Examples of frequently appearing nouns  in alphabetical order  include “anxiety”  “class”  “dinner”  “family”  “god”  “job”  “lunch”  “miss”  “school”  “sick”  “sleep”  and “work”. We then iteratively clustered the journals into topics (details below) and removed nouns that do not refer to topics such as numbers  timings (e.g.  “today”  “yesterday”)  general feelings (e.g.  “feel”  “like”)  proper nouns  and nouns that have ambiguous meanings (e.g.  “overall”  “true”). Lastly  we only retained nouns that appeared more than ten times in the dataset. This process resulted in a vocabulary of 8386 words for topic modelling. Each journal is represented as a 8386-dimensional term frequency vector  with each component denoting the term-frequency/ inverse-document-frequency (TF-IDF) of the corresponding term. Algorithm 1 summarizes our topic modelling methodology. Given a TF-IDF term frequency vector for each journal  we run non-negative matrix factorization (NMF) [8]  implemented in Python’s scikit-learn package [12]. The objective of NMF is to find two matrices whose product approximates the original matrix. In our case  one matrix is the weighted set of topics in each journal  and the other is the weighted set of words that belong to each topic. Hence  each journal is represented as a combination of topics which are themselves composed of a weighted combination of words. We chose NMF because its non-negativity constraint aids with interpretability. In the context of analyzing word frequencies  negative presence of a word would not be interpretable. This is because we only track word occurrences and not semantics or syntax. Unlike other matrix factorization methods  NMF reconstructs each document from a sum of positive parts  which enables us to easily manually label the discovered topics. Iterating from 4 to 40 topics  we derived 37 different topic matrices (steps 1 and 2 of Algorithm 1). Each matrix consists of one topic per row. Each topic has a positive weight for each word in the vocabulary. Stronger weights indicate higher relevance to the topic. The final topic matrix we used has 14 topics and is shown in Table 1. We show the first six words in this table for simplicity  where we sorted the words associated with each topic from highest relevance to lowest. When judging the topic matrices  we considered the top twenty most important words per topic. Using this information  we manually labeled each row in the matrix with a corresponding topic. Furthermore  we manually evaluated each matrix based on the distinctness between topics  consistency within topics  and interpretability. During this process  we compiled a custom list of removed words that we mentioned earlier in this section. The groups of words we removed appeared as stand-alone topics that did not offer information about what the journal was about. For example  proper nouns appeared as a stand-alone topic. Other words  which we deemed too general or ambiguous  appeared across several topics and hence did not provide discriminative information. We note that by default NMF does not enforce words to be assigned a non-zero weight to only a single topic. Using our pruning procedure  we ensured words that appeared across too many topics were removed. We did permit words with multiple meanings (for example  “high”) and words that apply in different settings (for example yoga “class” versus academic “class”). We note that the most important words (based on weights) for each topic generally did not overlap  with “ate” being the exception. Our validation procedure  outlined in Sect. 4.2  ensured that the two topics “Dinner” and “Meals” were indeed distinct despite both assigning high weights to “ate”. We tested different levels of regularization to enforce sparseness in our models (see [8] for a discussion)  but did not find significant differences. However  one important modification we made to regularize each topic was to make their first words only as strong as their second ones (by default  first words are stronger than second words  which are stronger than third words  and so on). This is since the most relevant word for each topic tended to be too strong of a signal  regardless of how we changed the number of topics  pre-processing procedure  or regularization in the objective function. For example  the word “love” in a journal about sports would be so strong that the journal would be labeled as relating to romantic love. Lowering the importance of first words was sufficient to eliminate the false positives we identified. Given the final topic matrix (summarized in Table 1)  the next step is to use it to assign labels to journals (steps 3 and 4 of Algorithm 1). We plotted the distribution of how important each topic was to all journals in the dataset  with importance ranging from zero to one. Each distribution had a similar shape with a clear inflection point between 0.05 to 0.15 importance. Figure 4 shows an example importance distribution for the topic “Work”  where the inflection point occurs at 0.1 importance. We used these inflection points to set minimum thresholds of importance for each topic. We ignored any topic assignments below the thresholds. Then  we obtained the top two topics per journal  if any. We chose a maximum of two topics per journal due to the generally short length of journals. Using this topic modelling procedure  we assigned at least one topic to 430 000 (35% of all available) journals. The number of journals with one topic was 334 000  while 96 000 had two topics (the maximum).,0,0,0,0,0,0,0,0,0,0,,0,0,0,1,2,2,,0
https://link.springer.com/chapter/10.1007/978-3-319-67186-4_6,13,6,validation,"Toulis, Golab",2,0,0,0,2017,To evaluate the effectiveness of our topic modelling methodology  we selected random subsets of 60 public journals for each topic and 100 public journals with no assigned topic. We manually labeled the sampled journals  looking for one of the 14 available topics  no topic or “other” topic. Including “other” allowed us to validate whether our list of manually labeled topic names were accurate and complete. We then compared our labels with those assigned by the model. For journals which were assigned two topics by the model  we considered the model correct if either one of the assigned topics was equal to the topic we chose manually. Table 2 shows the topic accuracies of our model. Overall  our model works well  with an average accuracy well above 80%. Journals without topics were much shorter in length. The average journal length of a journal with no topic was 114 characters  while one topic was 142 and two topics was 185. Manual inspection confirmed that these journals indeed did not contain any topic more than 70% of the time. Instead  they mostly contained sentiment that was already available from mood labels. We conclude this section by remarking that we analyzed activity around significant events such as the 2016 American Election. We did not find statistically significant anomalies in topics mentioned since the topics we derived are mostly related to day-to-day activities.,0,0,0,0,2,0,0,0,2,0,,0,0,0,0,0,0,,0
https://link.springer.com/chapter/10.1007/978-3-319-67186-4_6,13,7,results,"Toulis, Golab",2,0,0,0,2017,This section presents the results of our analysis. As a reminder  the input consists of: (1) the number of Reddit users subscribed to various mental-health-related subreddits and (2) journals from the journalling app  each labeled with a timestamp  mood (entered by the user)  visibility (public vs. private; set by the user)  and up to two topics (assigned by our topic modelling algorithm). We begin by comparing commonly subscribed topics on Reddit to common topics discussed on the journalling app both privately and publicly. Table 3 shows the most subscribed mental health related subreddits. Table 4 lists various statistics for the 14 topics we identified in the journalling app  including: Happiness percentage  corresponding to the percentage of journals whose associated mood was “happy”. Publicness percentage  corresponding to the percentage of journals whose visibility was set to public. Number of journals per topic (1000s). Number of users who posted at least one journal on the given topic (1000s). Average journal length per topic. While scanning for health-related communities on Reddit  we immediately noticed that physical health (exercise  weight loss) is a much larger theme compared to the journalling dataset. This is likely since there are other apps for tracking exercise. On the other hand  communities focused on mental health were relatively small given Reddit’s large user base. Self-identified depression was the largest subreddit focused on a mental health condition  which in the journalling dataset was also a common topic. Additionally  Reddit includes smaller communities  such as “High School”  “Sleep” and “Family” that correspond to important topics found in the journalling dataset. Notably  people with ADHD formed a very large community on Reddit  which was not a major theme in the journalling dataset and which could be a unique dataset for researchers interested in ADHD. On Reddit  sleep-related communities are very small while in the journalling dataset it is a major theme. Sleep is a daily need that is critical to mood  which is what the journalling app is designed to track. Sleep is the third most common topic  and  as discussed later  it has a relatively negative sentiment. Based on manual inspection of a random subset of public journals  mentions of sleep are not mainly related to insomnia. Instead  we found that most mentions of sleep include commentaries on the quality of sleep  looking forward to go to sleep due to exhaustion  and (non-chronic) lack of sleep. To further understand how users are logging their sleep  we analyzed the timing of journals that mentioned sleep. Figures 5 and 6 show the times of day that sleep and non-sleep related journals  respectively  were written across all journals posted in 2016. Sleep was uniquely mentioned in the mornings  whereas all other topics followed a very similar distribution (“Dinner” was mentioned later in the day than other topics and was removed for clarity). In agreement with our manual inspection  sleep is mentioned before common hours of sleep and in the morning after waking up. In addition  Reddit’s community does not appropriately address specific issues that are affecting people in the journalling dataset  including family and school-related stress. Also  while a large subreddit exists for career advice  it does not specifically target job-related stress and workplace conflicts that are mentioned in the journalling dataset. Overall  one third of all journals are public. Based on Table 4  we find that social media has a gap in its ability to fulfill our social needs when expressing day to day activities. In particular  “Dinner” and “Meals” are topics that are shared (set to public) less than 30% of the time. Based on manual inspection of a random sample of public journals  those labelled with the topic “Dinner” tend to be about dates and family gatherings. On the other hand  “Meals” are generally short journals that are used to track how much was eaten and whether it was healthy or not. By creating a private medium  the journalling app helps people reflect upon these moments. On the other hand  more public topics which were shared 35% or more of the time were “Missing Someone”  “Sleep”  “Career & Finances”  “Love” and “Anxiety/Depression”. Anxiety and depression are talked about the most publicly  which shows that users are aware of and comfortable sharing their mental state on the journalling app. In comparison  these topics are not usually found on traditional social media since there is a stigma around them. Table 4 also contains the average mood of each topic  as labeled by users. While most topics are generally quite happy  there are some that are unexpectedly sad. Most surprisingly  “Sleep” is just as negative as “Missing Someone”  with only 43% of journals happy  compared to the average happiness across the dataset of 60%. “Dinner” and “Meals” were especially happy  which also happened to be the most private topics.,0,0,0,0,2,0,0,0,2,0,,0,0,0,0,0,0,,0
https://link.springer.com/chapter/10.1007/978-3-319-67186-4_6,13,8,conclusions,"Toulis, Golab",2,0,0,0,2017,In this paper  we used text mining to analyze a unique dataset of public and private journals in order to understand public mental health. We uncovered core themes affecting users. Based on user-labeled moods  we analyzed sentiment  revealing that the most private topics had the most positive mood. Despite being a very low mood topic  anxiety and depression were frequently publicly shared  showing the stigma around these issues can be mitigated in an anonymous environment. By comparing public and private journals  we determined which topics are shared more than others  identifying new themes not available in currently analyzed social media. Routine topics such as eating meals are kept private by users. Across the dataset  most journals and topics were mostly private  suggesting that traditional social media cannot fulfill the need to express emotions during these moments. We also compared the journalling app to Reddit  another service for anonymous sharing. We found that mental health topics such as family  school and work-related issues were missing from Reddit  perhaps because people are uncomfortable discussing these issues in a public forum  even anonymously. We believe there is an unfilled need for this user base. Future social media services may wish to offer a place to talk about these problems and make people comfortable enough to express emotions about them publicly. An interesting finding is that sleep was a critical theme in the journalling dataset. This topic was frequently mentioned before and after sleeping. Sleep had an unexpected negative sentiment that is comparable with the topic of missing someone. Sleep is a daily activity that has a large impact on mood and is impacted by external factors such as stress. Hence  sleep monitoring data is critical for understanding public mental health. In future work  we plan to collect more data to analyze issues related to sleep in more detail. For example  Twitter data offers a chance to study sleep patterns in users who post daily.,0,0,0,0,0,0,0,0,2,0,,0,2,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/2556288.2557214,14,1,abstract,"De Choudhury, Morris, White",3,1,0,0,2014,Search engines and social media are two of the most commonly used online services; in this paper  we examine how users appropriate these platforms for online health activities via both large-scale log analysis and a survey of 210 people. While users often turn to search engines to learn about serious or highly stigmatic conditions  a surprising amount of sensitive health information is also sought and shared via social media  in our case the public social platform Twitter. We contrast what health content people seek via search engines vs. share on social media  as well as why they choose a particular platform for online health activities. We reflect on the implications of our results for designing search engines  social media  and social search tools that better support people’s health information seeking and sharing needs.,0,0,0,0,0,0,0,0,2,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/2556288.2557214,14,2,introduction,"De Choudhury, Morris, White",3,1,0,0,2014,The Internet is a popular place to learn about health matters. According to a January 2013 Pew survey [10]  59% of U.S. adults reported using online resources to obtain health information in the past year. The Web is used for a range of purposes  including seeking advice [17 32]  connecting with experts and individuals with similar experiences [9 24]  sharing questions and concerns around treatment options [27]  or understanding professional diagnoses [4]. Online health content can enhance coping and self-efficacy [8]  affect health-related decisions and behavior of users and their friends and family [11]  enable better management of chronic health conditions [1]  and fuel discussions with healthcare providers [15]. Besides electronic mail  the use of search engines and social media are the most common online activities of adult U.S. Internet users [27]; in this work  we compare and contrast the use of these platforms for health activities. People often use general purpose search engines (e.g.  Bing  Google  or Yahoo!) to find online health information [29]. Recently  social media (e.g.  Twitter  Facebook) has emerged as an alternative platform for sharing  and even seeking  health information. A recent survey [11] indicated that as many as 39% of online health information seekers used social media  and a fraction of them had also followed their contacts’ health experiences or updates  posted their own health-related comments  gathered health information  or joined a health-related group. Other research has shown that Twitter is used for health-oriented question-and-answer tasks [26]. Disease-specific exchanges on social sites can provide new sources of knowledge  support  and engagement  particularly important for patients with chronic conditions [12]. Such emergent practices around seeking and sharing health information online indicate a shifting landscape. Search engines and social media platforms form an important continuum in terms of how people (privately) seek healthrelated information  as well as (publicly) share such information  respectively. As these mechanisms for seeking and sharing health information continue to grow in accessibility and popularity  and individuals utilize them to take a more active role in managing their health  it is imperative to understand the nature of health information sought or shared via the two platforms  as well as individuals’ motivations and intentions. We address this challenge in the research described in this paper  focusing on the following three research questions: RQ1: What is the relative prevalence of health activity on a search engine vs. social media? (We focus on Twitter here given its public nature). What motivates people to seek and share health information via each platform? RQ2: What are the characteristics of health activities on search engines and on Twitter? Are there differences in the severity of health conditions on which information is sought or shared  or in terms of the social stigma associated with the health condition? What topical contexts typically distinguish (private) health information seeking on search engines from (public) health information sharing on Twitter? RQ3: How do people evaluate both: (1) the information that they seek or share via search engines and Twitter  and (2) the risks associated with health activities on these platforms? We adopt a mixed-methods approach  combining data from a survey with 210 respondents  with 15 months of log data from a major Web search engine (anonymized for blind review) and from Twitter. Our findings indicate that there are considerable differences in health activity between the platforms. We show that people prefer search engines when seeking information on serious medical conditions  disabilities  and conditions known to bear social stigma  while Twitter is used more often to share information around symptoms of different health issues  and on conditions with benign explanations. Regardless  our finding that people use Twitter to seek or share information on some health concerns shows that they may underestimate the privacy implications of pursuing health content in such public channels. The differences in how people use search engines and social media suggest that they tailor their healthcare needs to the accepted norms and characteristics of the two platforms  and has implications for the design of nextgeneration search and social systems.,0,0,0,0,0,0,0,0,2,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/2556288.2557214,14,3,related work,"De Choudhury, Morris, White",3,1,0,0,2014,Searching the Web for health-related information is a common pursuit [10]. Studies have examined how people find and appraise health information online [8]  and its connection to healthcare utilization and health-related behaviors [1 31]. Sillence et al. [31] showed that examining online content influences health-related decision making and improves patient-physician communication. Ayers and Kronenfeld [1] found a positive correlation between online health information seeking and changes in health behavior. A common goal in health searches is self-diagnosis. One recent study estimated that 35% of online health seekers performed this activity in particular [10]. Studies have shown that during diagnostic searches people pursue both evidence-based search (focused on symptoms) and hypothesis-based search (focused on conditions and treatments) [4]. Recent research indicated a positive correlation between the frequency and placement of serious illnesses on result pages and negative emotions  e.g.  feeling overwhelmed and frightened [18]. People afflicted by medical conditions also find support via online health communities (OHCs) [9 30]. One study suggests that 30% of U.S. Web users have participated in medical or health-related groups [17]. In this light  approaches to community building have been proposed  e.g.  [13 34]. In this work  we focus on how search engines and social media (two of the most heavily used online platforms [27]) are used for health activities  rather than dedicated forums and OHCs. Recent research has demonstrated that social media provides a way for people to communicate with their contacts regarding health concerns [7 10 25]. Newman et al. [23] interviewed people with significant health concerns who participated in both OHCs and Facebook. They showed that people consider the target and the means of sharing information as they pursue social goals related to their personal health  including emotional support  motivation  accountability  and advice. Oh et al. [24] examined people’s use of Facebook for health purposes and showed that the level of emotional support was a significant predictor of health selfefficacy. New practices raise concerns about matters such as privacy. The implications of sharing health information in open fora such as social media have been examined in general [21]  and specifically in the context of health [14 35]. Young et al. [35] studied factors influencing the disclosure of health information on Facebook and steps that people took to protect their privacy. Hartzler et al. [14] showed that people often made errors in determining what health information was shared with whom in their social network. Closely related to information disclosure on online social platforms is the inherent stigma ascribed to many health conditions [5]. Social stigma describes negative feelings towards an individual or group on socially-characteristic grounds that distinguish them from others [6 20]. Berger et al. [3] showed that compared with those with nonstigmatized conditions  those with stigmatized illness were more likely to find health information online. Liu et al. [19] showed that video logs (to help people share stories  experiences  and knowledge) could support the disclosure of serious illnesses such as HIV  helping those afflicted overcome aspects of social stigma. Our research extends prior work in several ways. We are the first to directly compare and contrast health activities on search engines and social media  two of the most-used online tools. Second  we describe how people assess the health information they encounter  including motivations for selecting a particular platform. Third  we examine the characteristics of the information sought  including social stigma and topical context. Fourth  we examine in detail the nature of health information sharing on Twitter. Finally  we complement a survey with large-scale data captured in naturalistic settings; the aforementioned prior studies mainly used only one method (e.g.  surveys  focus groups  interviews)  or have small sample sizes  limiting their generalizability.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/2556288.2557214,14,4,data on health conditions,"De Choudhury, Morris, White",3,1,0,0,2014,A key challenge of this research was to identify the different health conditions on which people seek and share information via search engines and Twitter  respectively. We identified four broad categories of conditions based on their severity and types: (1) symptoms of major diseases  (2) benign explanations (non-life-threatening illnesses)  (3) serious illnesses  and (4) disabilities. We also characterized each condition by the degree of perceived social stigma provided by third-party judges. Our final list contained 165 conditions.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/2556288.2557214,14,5,health condition severity and types,"De Choudhury, Morris, White",3,1,0,0,2014,: We filtered logs based on a symptom list from the online version of the Merck medical dictionary. Starting with the Merck list  we removed duplicates (e.g.  multiple references to the same condition with different cohorts)  and split symptom pairs into singletons (e.g.  “Nausea and Vomiting in Adults” and “Nausea and Vomiting in Infants and Children” became “nausea” and “vomiting”). The list has 58 symptoms (e.g.  chest pain  headache  twitching)  and has been used in prior work on health search analysis [4].,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/2556288.2557214,14,6,benign explanations,"De Choudhury, Morris, White",3,1,0,0,2014,We used a list of 43 non-lifethreatening conditions  defined in a prior log-based analysis of search behavior [33]. The list comprised a range of commonly-occurring conditions selected from across the International Classification of Diseases 10th Edition (ICD10: www.who.int/classifications/icd/en/) published by the World Health Organization. Examples of the conditions chosen include caffeine withdrawal  common cold  and pregnancy.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/2556288.2557214,14,7,serious illnesses,"De Choudhury, Morris, White",3,1,0,0,2014,We utilized a list of 58 serious conditions defined in [33]  again based on the ICD-10. Note that these are well-known conditions that were likely to appear in our data. Examples of serious illnesses chosen included “heart failure”  “multiple sclerosis”  and “hepatitis”.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/2556288.2557214,14,8,disabilities,"De Choudhury, Morris, White",3,1,0,0,2014,: In addition to the acquired health conditions listed above  we also included six common disabilities (“autism”  “attention deficit hyperactivity disorder”  “deafness”  “blindness”  “cerebral palsy”  “dyslexia”)  to provide insight on the use of search and social media for seeking and sharing information related to this class of chronic health disorders. Stigma of Health Conditions To understand how stigma impacts people’s health activities on search engines and social media  we characterized each condition in terms of its level of social stigma. We used Amazon Mechanical Turk (www.mturk.com) to obtain ratings on the degree of social stigma for each condition on a three-point scale: 1 = low stigma  2 = moderate stigma  and 3 = high stigma. For each condition we obtained 10 ratings from crowdworkers and three ratings from three human factors researchers. For conditions with agreement exceeding 50% (seven or more raters agreed on a single rating)  we used the majority rating as the final measure of condition stigma. 126 out of the 165 conditions (76%) met this criterion. For the other conditions  with no clear consensus  we set stigma to moderate. This resulted in 81 conditions with low stigma (e.g.  headache)  72 with moderate stigma (e.g.  malaria)  and 12 conditions with high stigma (e.g.  AIDS).,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/2556288.2557214,14,9,log data collection,"De Choudhury, Morris, White",3,1,0,0,2014,We focus on the social media platform Twitter  a popular microblogging service used by 18% of U.S. Internet users  and whose popularity continues to increase [28]. Twitter is particularly interesting to study since nearly all posts are public; the public nature of tweets provides an interesting counterpoint to the private nature of search engine activity. We gathered a 15-month sample of Twitter’s Firehose stream (which includes all public tweets) between November 1  2011 and March 31  2013  made available to us under contract  focusing on English-language tweets. Twitter post count and unique user count were computed for each condition  and aggregated over the full time period. Specifically  we considered a post to belong to a certain health condition if there was a regular expression match of the condition to the text of the post (this would not permit substring matches within terms). To reduce noise  we excluded posts that were retweets or contained hyperlinks  since they were likely related to general news and not a user’s personal health. Using this method  we obtained 125 166 549 tweets on the 165 health conditions from 62 269 225 users in the time period of interest. The median number of posts was 51 687 per condition  from a median of 40 152 users per condition.,2,2,2,2,0,1,0,0,2,0,,0,0,2,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/2556288.2557214,14,10,search engine,"De Choudhury, Morris, White",3,1,0,0,2014,We mined the logs of a popular search engine over the same 15-month period used in the Twitter analysis  focusing on English-language queries. We processed billions of queries from which the queries for the health conditions of interest were extracted. We searched for queries that were either an exact match with one of the symptoms or conditions  or those where the condition was some subset of the query terms. As with Twitter  substring matches within query terms were not permitted to reduce noise. We also used synonyms of symptoms and conditions to increase coverage. Synonyms were identified via a two-step walk on the search engine click-graph using an approach similar to [2]. Example synonyms for “abdominal pain” included “sore stomach” and “belly ache.” We applied this procedure to all of the conditions. Note that  synonyms were not used in the data collection process from Twitter to ensure efficiency by limiting the size of the substring match space (Twitter posts are considerably longer than search queries). We observed 174 605 024 searches on health conditions from 38 676 368 users. The median number of searches was 293 505  from a median of 85 848 users  per condition.,0,0,0,0,0,0,0,0,0,0,,0,0,2,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/2556288.2557214,14,11,health information seeking and sharing survey,"De Choudhury, Morris, White",3,1,0,0,2014,To gain qualitative insight into people’s health information seeking and sharing practices  we conducted an online survey during June 2013  using a recruiting service (Cint) that offers “Census representative sampling” in terms of gender and age throughout regions of the U.S. Respondents were paid approximately 4 USD to complete the survey  and were required to have a Twitter account to qualify to participate. The survey comprised 37 questions  and took approximately 10 minutes to complete. Participants were asked if they had ever sought health information on a search engine such as Google or Bing  or on Twitter  and whether they had ever shared health information on Twitter. If they answered affirmatively  they were asked to describe the most recent occasion on which they did so  including the health condition that motivated them to search or share  and their objective in performing the activity. The survey included questions about how often participants used search engines and Twitter for various types of information seeking or sharing  views about risks associated with each platform  and basic demographics. In total  237 respondents completed the survey. After discarding low quality responses (as determined by nonsensical or sarcastic responses to open questions)  210 valid survey responses were analyzed. Of these respondents  53% were female  they resided in 38 U.S. states and the District of Columbia  and 43% had a college degree or higher. Ages ranged from 18 to 70 years (median = 35 years). Respondents reported using search engines frequently (76% at least once per day  with only 7% using them less than once per week). 71% of respondents had public Twitter accounts  while the remaining 29% had “protected” accounts (i.e.  only approved followers could view their postings).,0,0,0,0,0,0,2,0,2,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/2556288.2557214,14,12,results prevalence intent motivations,"De Choudhury, Morris, White",3,1,0,0,2014,To answer RQ1  we turn to our survey data to examine the relative prevalence of search engine and Twitter use for health activity  and users’ motivations and intents in using these platforms. Note that the health activities we focus on are seeking health information and sharing health information. The seeking activity is relevant to both search engines (i.e.  issuing health-related queries) and Twitter (i.e.  asking health questions)  whereas the sharing activity is relevant only to Twitter (i.e.  posting health-related tweets). Accurately determining seeking vs. sharing distinctions from Twitter data is challenging: while presence of a question mark may provide some indication  it still captures many non-information-seeking tweets [26]  requiring human labeling to accurately determine intent [16]; an approach that does not scale to our large dataset. Consequently  our log analyses considered all health-related Twitter posts to be incidents of sharing (since even seeking information on Twitter is a type of sharing due to the public nature of posts); however  our survey allowed us to ask users for insight on the distinction between their Twitter seeking and sharing activities.,0,0,0,0,0,0,0,0,2,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/2556288.2557214,14,13,prevalence,"De Choudhury, Morris, White",3,1,0,0,2014,The survey is useful for understanding prevalence  because it allows us to include those who did not search or tweet (who would not be visible in the log data). Recall  however  that all survey respondents had Twitter accounts  so the perspectives of non-Twitter users are not represented in survey data. Overall  94% of respondents (n=197) reported having used a search engine to seek health-related information. 11% sought health information for themselves on search engines at least once per day  rising to 40% doing so at least once per week. 13% sought health information on behalf of family or friends at least once per day  with 34% doing so at least once a week. Many fewer respondents  19%  reported having sought personal health information on Twitter. 23% reported having used Twitter to share information related to their health.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/2556288.2557214,14,14,intent of health activiteis,"De Choudhury, Morris, White",3,1,0,0,2014,The 197 survey respondents who reported seeking health information using a search engine were asked to recall the most recent instance  and to answer questions related to that specific incident. Respondents also described their search objective. These responses were classified using an iterative open coding process. A second rater used this scheme to rate a random sample of 30 responses; inter-rater reliability (Cohen’s ) indicated substantial agreement ( = .72). The same verification strategy is used in the rest of this section. We coded the intent for 183 respondents (the others had unclear intent). Some searches had multiple intents and received multiple categorizations  so the percentages sum to more than 100%. The most common motivation for using a search engine was to identify treatment options (53.0%)  e.g.  “stretches to cure or ease [tight hamstring].” Alternative and holistic treatment was a popular sub-category  comprising 13.5% of treatment searches  e.g.  “alternative treatment [hypothyroidism].” The next most common motivation was diagnosis of a health condition (26.8%) (e.g.  “whether or not the symptoms matched my behavior [depression]”) or interpreting the symptoms that they experienced (e.g.  “what may be a cause of this and if it may mean something more may be wrong [very heavy menstrual cycle]”). A third motivation was general understanding of a health condition or procedure (20.8%)  including understanding what a medical procedure might entail (e.g.  “more about the surgery process  healing time [umbilical hernia]”)  understanding the causes of an illness (e.g.  “the caused [sic] of it… [infertility]”)  or other general learning about a condition (e.g.  “prognosis [congestive heart failure]”). 7.1% of recalled searches were motivated by understanding medications  such as understanding side effects (e.g.  “to be able to learn the side effects [cholesterol medications]”)  comparing and contrasting medications (e.g.  “effectiveness [of cancer treatments]”)  or seeking information on available medications  such as whether non-prescription options are available or learning more about how a medication worked. 6.0% sought lifestyle information for chronic concerns  particularly nutrition information for managing diabetes  cholesterol  or weight loss (“special diet [cholesterol]”). Beyond these broad categories  participants also described other intents behind their search activity. 2.2% sought recent medical research findings on conditions or their treatments  and 1.1% sought social support such as online support groups for people with their diagnosis. Intent of Twitter Use for Health Information Seeking In a similar way to engine use  the 40 respondents who indicated they had sought health information on Twitter were asked to recall the most recent event. They explained their objectives in free-text. Open coding was used to categorize responses; the same categories were used for why people used search engines  with additional categories added as needed. Once again  substantial agreement was observed with the second coder (30 ratings  =.77). Three responses were unclassifiable; percentages are of the remaining 37. As with search engines  the most common objective was locating treatment information (56.8%)  e.g.  “how to help relieve the pain [numbness in the legs]”. 8.1% specifically sought natural or alternative treatments  e.g.  “natural remedies to headaches.” 16.2% of respondents sought information about healthy lifestyles  such as nutrition  dieting  or fitness (e.g.  “different ways to lose weight”)  and 13.5% sought to gain a general understanding of a health condition  e.g.  causes (e.g.  “why it happens…” [enlarged prostate])  consequences  or general knowledge. 5.4% sought new research about conditions or treatments  and 5.4% looked to find others with a similar situation to offer support or advice (e.g.  “if anyone else hsd [sic] allergy problems”). Only 2.7% reported seeking diagnostic information on Twitter. Intent of Sharing Health Information on Twitter The 48 respondents who recalled sharing information related to their health on Twitter were asked to consider the most recent incident  and to answer a series of questions with that specific occurrence in mind. Participants explained in free-text what their intent was behind sharing health information on Twitter (substantial agreement with second coder  =.65). 10 were not coded due to vagueness or missing responses. Of the remaining 38 responses  63.2% reported that they intended to share information about their immediate health status or symptoms (e.g.  “I was having a few teeth removed and I may not be online for a few days”). 34.2% wanted to share information or news about a condition (e.g.  “treatments that work for me [fibromyalgia and neuropathy]”).,0,0,0,0,2,1,0,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/2556288.2557214,14,15,motivations for health activities,"De Choudhury, Morris, White",3,1,0,0,2014,Respondents also explained why they chose to use a search engine  rather than alternatives such as asking a family member  consulting a health professional  or using a Q&A or social networking site. Free-text responses were classified into themes using an iterative open coding method (substantial second-coder agreement  =.63). Percentages are reported of the 151 responses that were classifiable  and the sum may exceed 100% since respondents provided several rationales. Convenience (speed  ease  availability) was the most commonly cited motivator for using a search engine to find health info (e.g.  “easy to access”); routine comfort with using search engines was another factor for 6.6% (e.g.  “I use google very often  and I trust it”). The next largest motivator  for 15.2%  was the plurality of results returned for any given inquiry (e.g.  “The internet has way more answers than just 1 person  or even 10”). Privacy of the information seeking experience was also cited as a benefit of using a search engine to find health information by 9.9% (e.g.  “because its [sic] awkward to ask someone else”). Overall  11.9% of respondents indicated that they had consulted a health care provider and needed more detailed info or were dissatisfied with what a health care professional had previously told them about the issue  and turned to search in response (e.g.  “I have consulted my physician and was not satisfied”). Another 9.3% indicated that they were performing research that they intended to later share with a health care provider (e.g.  “wanted to know what kinds of questions to ask the doctor at my appointment”). Sometimes (3.3%) a search was also used when medical care was not available (e.g. “it was after-hours for my doctor”). For 3.3% monetary costs determined whether to use search versus talk to a professional (e.g.  “too expensive to go to a doctor”). Motivations for Twitter Use for Seeking Health Information Like with search  participants were asked to explain in freetext why they used Twitter rather than alternatives such as health professionals and search engines). As previously  open coding was used to categorize responses (substantial agreement was attained with the second coder  =.72). 14 of the 40 responses were unclassifiable due to vagueness; reported percentages are of the remaining 26 responses. As with search engines  convenience (ease of use  speed) was the most common reason for seeking health information via Twitter (46.2%) (e.g.  “I [already] had it opened”). The next reason  specific to Twitter but perhaps analogous to the perception that search engines return a large variety of relevant results  was the perceived large audience of Twitter (23.1%) (e.g.  “lot of people there”). 15.4% indicated using Twitter because they were trying something different – they had exhausted more conventional options or were trying to corroborate information found elsewhere (e.g.  “second source”). 11.5% sought other people’s recommendations  advice  or opinions on treatments or managing health conditions (e.g.  “just to see what advice i could get…”)  and 7.7% sought social support (e.g.  “find other people who can relate”). Motivations for Twitter Use for Sharing Health Information Participants were also asked to explain in free-text why they chose to use Twitter to share information. A total of 38 participants had codable responses. Most commonly  respondents wanted to reach a large audience (e.g.  “because I wanted my friends to know”; “just to inform the regular crowd of a hashtag”) (22%). Some specifically wanted other people to benefit from health information they personally had found useful (e.g.  “there are other mothers out there that are looking for the same option…”) (8%). A few (4%) found Twitter to be a useful platform for complaining (e.g.  “place to vent”); some (4%) noted Twitter had some privacy benefits (e.g.  “Facebook has too many of my family members”).,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/2556288.2557214,14,16,summary,"De Choudhury, Morris, White",3,1,0,0,2014,The findings presented in this section indicate that search engines are more extensively used for health activities than social media like Twitter. While learning about the treatment and the diagnosis process of a health condition was a common purpose of health searches  gathering knowledge about the impact of health conditions on lifestyle and deriving general understanding of a medical procedure were popular goals behind using Twitter for health activities. Respondents indicated the plurality of search engine results  perceived large social media audience for feedback  and the diversity of the information available via search engines and social media to be primary motivating factors behind these practices  beyond the obvious convenience of the two platforms.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/2556288.2557214,14,17,results comparing health information,"De Choudhury, Morris, White",3,1,0,0,2014,To answer RQ2  we use log data to study the aspects of health that people engage with on search engines versus on Twitter. We first introduce several measures used in our analysis. Relative use – For a given health condition  its relative use on Twitter or the search engine is given by the ratio of its volume of use in that medium  to the volume of the health term that is most used in that medium. For example  the most searched condition on the search engine was “cancer” appearing in 31 443 735 queries  and the second-most queried was “pregnancy” found in 25 721 056 queries; the relative use for “cancer” on the engine=1  it was .82 for “pregnancy.” Rank – A relative ranking based on normalized relative mention of a health condition (on Twitter or the search engine)—lower ranks mean greater use. For example  on Twitter  the most-used term from our list is “headache”  that appeared in 24 607 507 posts; it would receive a rank=1. Rank difference – We compute rank difference between the search engine and Twitter for each health term – large negative values indicate that a term is searched relatively more than tweeted  whereas large positive values mean the reverse. For example  “cough” has rank 4 on Twitter and rank 26 on the search engine – its rank difference is 22  reflecting its relative prominence on Twitter as compared to search. Based on these definitions  in Table 1 we report the top 20 most searched and most shared health conditions in terms of their rank and relative use on each platform. The table also shows the top 20 most positive rank difference (relatively more tweeted than searched) and most negative rank difference (relatively more searched than tweeted) conditions  along with severity/type and stigma level of each conditions.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,1,,0
https://dl.acm.org/doi/abs/10.1145/2556288.2557214,14,18,definitions,"De Choudhury, Morris, White",3,1,0,0,2014,We first report on a comparison of the nature of health conditions appearing in search engine queries and Twitter posts  in the light of their severity and type. From Figure 1 we observe that search engines are used more frequently to seek information on serious conditions  compared to Twitter (per a Wilcoxon test  this difference is statistically significant: z=4.98  p<.001). Examples can be found in Table 1: in the 20 top-ranked conditions shared on Twitter and sought on the search engine respectively  there are six serious conditions in the former (e.g.  “diabetes”)  while nine in the latter. Benign conditions show relatively similar use on both search engines and Twitter (in Table 1  seven of 20 are benign conditions in the 20 top rank conditions for search as well as Twitter  e.g.  “pregnancy”  “anxiety” appear in both)—a Wilcoxon test reveals only marginal significant differences (z=1.42  p=.02). Next  per Figure 1  relative use of symptoms suggests symptoms to be shared more on Twitter than searched (per Wilcoxon test  this is a statistically significant difference: z=-2.95  p<.01). Taking examples from Table 1  seven of 20 top ranked conditions for Twitter are symptoms while it is only three in the case of the search engine. Finally  disabilities are searched more than they are shared on Twitter (per Wilcoxon test  z=1.76  p<0.01). In Table 1  none of the disabilities appear in the 20 top rank conditions for Twitter; in the case of search  “autism” is found to be ranked 12. The sensitive nature of many of the disabilities studied (mean stigma rating of the six disabilities=1.83  leaning toward the higher-stigma end of the scale) means that people may be more comfortable privately searching about their treatments  diagnosis  or coping mechanisms  than discussing them publicly on Twitter. We elaborate on these observations by analyzing rank differences (Table 1). We first observe those terms that are shared more on Twitter than searched. For the 20 most positive rank difference values  11 are benign (e.g.  “insomnia”  “toothache”  “sore throat”  “food poisoning”) and only one is serious and eight are symptoms. Supporting the observations from Figure 1  social media may be preferred for sharing benign conditions and symptoms  rather than serious illnesses. Next  we examine which terms that are searched relatively more than shared on Twitter. For the top 20 most negative rank difference values  12 are found to be serious conditions (e.g.  “kidney disease”  “bipolar disorder’”  “sexually transmitted disease”)  three are benign  and five are symptoms. This suggests a tendency to prefer search engines to Twitter for online activities related to serious health conditions.,0,0,0,0,0,0,0,0,2,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/2556288.2557214,14,19,comparison severity and type of conditions,"De Choudhury, Morris, White",3,1,0,0,2014,We now focus on understanding usage of health conditions on search engines and social media  and their levels of social stigma. Comparing across the platforms  from Table 1 (columns one and two)  we observe notable differences between Twitter and the search engine  e.g.  conditions perceived to have higher stigma are searched more (nine of the 20 top rank conditions for search have moderate or high stigma; three of them are extremely stigmatic  e.g.  “autism”  “AIDS”) than they are shared on Twitter (six of the 20 for Twitter have moderate or high stigma; two extremely stigmatic). Examination of the rank differences (Table 1) reveals the same pattern more prominently. Among the top conditions searched more than tweeted  four are highly stigmatic (e.g.  “vaginitis”  “myopathy”)  whereas there are no highly stigmatic conditions tweeted more than searched (although there are moderately stigmatic ones such as “wheezing”  “dysmenorrhea”). These considerable differences regarding stigma span all 165 health conditions (not just the top 20 shown in Table 1). Wilcoxon tests reveal that the relative use of moderate or highly stigmatic conditions is higher on search engines than on Twitter (for moderate stigma: z=4.57  p<.001; for high stigma: z=1.89  p<.01). Not surprisingly  but important to show  people prefer search engines over Twitter for online activity related to health conditions that may be associated with social taboo. This is reasonable given the nature of the platforms  but it is interesting that people do use Twitter to share some information on stigmatic conditions.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/2556288.2557214,14,20,comparison context of use,"De Choudhury, Morris, White",3,1,0,0,2014,From our log data  we examined the set of all non-stopword unigrams that co-occur (≥10 times) with each of the health conditions in Twitter posts and search queries. We cluster the unigrams based on LIWC’s (Linguistic Inquiry and Word Count: www.liwc.net) categories (style categories  e.g.  pronoun use  are excluded)  to obtain a general sense of the context of use of each term. For comparison across Twitter and search  we use a relative measure of use of each linguistic category to account for different usages in the two sources—defined in the same way as the relative use of a condition. Across Condition Severity We first compute the Jensen-Shannon divergences between the unigram category distributions of the condition severities/types (Table 2). Next we show differences between unigram distributions on Twitter and search (Table 3). From Table 2 we observe that for both Twitter and the search engine  there are significant differences in the context of use of different conditions. For example  the top unigram categories for benign conditions on Twitter and the search engine are: past and future tense words  social  work  anxiety  negative emotion  and anger (example unigrams include “hate”  “relieve”  “tomorrow”  “now”  “cry”  “worries”  “school”). This suggests that people are referring to their benign conditions with negativity  in terms of how they are disrupting their current and future social or professional life. On the other hand  mentions of serious conditions on Twitter and the search engine tend to include categories like: see  hear (perception terms)  body  family  death  and numbers (e.g.  “awareness”  “treatment”  “signs”  “survival”  “prognosis”  “rate”  “pain”). People may be seeking information related to the physical ordeals associated with serious health concerns  or specific treatment information. Next  the context of use of symptoms Twitter and the search engine spans the categories health  insight  cognitive mechanisms (e.g.  “pain”  “hard”  “remedies”  “medicines”  “outbreak”  “bad”  “sleep”  “woke”  “feel”  “ugh”  “damn”). These indicate people may be seeking information to do a self-diagnosis on their symptoms  as well as expressing frustration on the inconvenience the symptoms may be causing. Finally  examining the context of use of disability terms  we find that on Twitter categories like present and future  money  religion  social  and friends are common (e.g.  “god”  “day”  “children”  “support”  “month”  “special”  “today”  “please”  “money”). This indicates that through their postings  people share disability related information in the context of monetary costs and challenges  and perhaps even seek comfort through sharing religious thoughts and reaching out to their social audiences. For search these span over motion  health  space  work (e.g.  “disorder”  “school”  “banned”  “therapy”  “treatment”  “walk”  “speak”  “children”). Disabilities can be a personal and social challenge and impact people’s life activities and work; hence through search people appear to seek coping mechanisms to deal with their condition and experiences. Thus  the context of use of disabilities is distinct from that of acute conditions. In fact  comparing the unigram category usage around disabilities for Twitter and search  we find a distinctive contrast (ref. Table 3). Perhaps individuals are less inhibited in their searches on disabilities  since search engines provide a more private experience while seeking for information on a stigmatic experience (mean stigma rating of the six disabilities=1.83). Across Stigma Levels Next we analyze the context of use of the health conditions in the light of their level of social stigma. We find that the context of use of high stigma conditions is considerably different from that of low stigma conditions. This is consistent for both Twitter and the search engine (Table 4). For instance  for low stigma terms  Twitter and search engine use spans context that includes categories such as past  present  and future tense  insight  time  and work (e.g.  “life”  “sleep”  “week”  “ugh”  “now”  “tomorrow”  “people”  “test”  “effects”  “work”  “signs”  “sick”). It appears that individuals are seeking or sharing information on these conditions in the light of their mundane day to day experiences and how they are affecting their lives and work conditions. However  for high stigma conditions  usage context on Twitter and search engines comprises categories such as anger  sadness  anxiety  health  body  and perception words like feel (e.g.  “surgery”  “HIV”  “treatment”  “hospital”  “children”  “prognosis”  “die”  “lost”  “mad”  “cure”  “hate”  “fight”). Highly stigmatic conditions appear to present themselves within contexts associated with mental stress and anxiety in Twitter posts and search queries; people also attempt to seek information on these high-stigma conditions for educational and treatment purposes. However  as one would expect  Table 5 indicates that the context of use of high stigma conditions on Twitter and the search engine is statistically significantly different. For Twitter  the unigram categories for high stigma conditions span: sadness  anxiety  social  perception words like see  feel (e.g.  “hope”  “feel”  “shit”  “support”  “please”  “family”  “suck”  “mad”  “fight”). In contrast  for search they include health  body  feel  motion (e.g.  “signs”  “treatment”  “hospital”  “children”  “prognosis”  “die”  “surgery”  “rate”  “awareness”). This aligns with our prior observation: individuals are more cautious in the way they discuss highly stigmatic conditions on a public platform like Twitter  perhaps for fear of being judged by their audiences and contacts.,0,0,0,0,0,0,0,0,2,0,,0,0,0,0,2,0,,0
https://dl.acm.org/doi/abs/10.1145/2556288.2557214,14,21,summary,"De Choudhury, Morris, White",3,1,0,0,2014,This log study comparing the nature of online health information sought and shared reveals self-censorship—serious health conditions  disabilities  and highly stigmatic conditions are generally searched considerably more than they are mentioned in Twitter postings. Symptoms of health conditions  however  were more frequently present in Twitter posts than in search queries. This perhaps indicates people’s propensity to use social media as a broadcasting platform to express the ordeals and inconveniences that are caused by symptoms they face in their day to day lives.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/2556288.2557214,14,22,results privacy quality social support,"De Choudhury, Morris, White",3,1,0,0,2014,Turning to RQ3  we investigate how people evaluate their health information seeking or sharing experiences as manifested through their use of search engines and social media. We consider three aspects: (1) any privacy risks involved  (2) the quality of information they access  and (3) the availability of social support from others through this activity. Respondents indicated on a seven-point Likert scale (ranging from 7 = strongly agree to 1 = strongly disagree)  whether they agreed that using a search engine to seek health information was a privacy risk. Respondents slightly agreed (mean = 3.5  median = 3)  but were more concerned about the privacy risk on Twitter (mean = 4.7  median = 5; Wilcoxon signed-rank test: z=7.87  p<.001). Similarly  in terms of health information sharing through Twitter  people slightly agreed that using Twitter to share health information is a privacy risk (mean = 4.9  median = 5) and more so than seeking on Twitter (Wilcoxon test: z=3.96  p<.001). Respondents were in slight agreement that health information available via search engines is of high quality (mean = 4.8  median = 5) and moderately agreed that search engines were useful for finding sources of social support (mean = 5.6  median = 6). Conversely  respondents were neutral in agreement that health information available via Twitter is of high quality (mean = 3.7  median = 4). Surprisingly  people were neutral about Twitter’s utility for finding social support related to health issues (mean = 4.6  median = 4). These differences in perceptions of information quality were statistically significant  with search engines being perceived as providing higher-quality information than Twitter (median 5 vs. 4)  z=-8.81  p<.001. Surprisingly  search engines were also viewed as more useful for finding social support for health issues than Twitter (median 6 vs. 4)  z=-8.54  p< .001  perhaps because they are useful for surfacing forums and OHCs  which prior work has shown are seen as key venues for social support for health concerns [9 17]. Note  however  that respondents who did report searching for information on Twitter viewed it as less of a privacy risk (median 4) than those who had not (median 5) (MannWhitney test  z=-2.46  p=.01). These people also viewed health information on Twitter as of higher quality  z=4.12  p<.001  and viewed Twitter as better for finding social support on health issues (median 6 vs. 4)  z=4.78  p < .001. Similarly  respondents who reported having shared health information on Twitter viewed it as less of a privacy risk (median 4 vs. 5) than those who had not (z=-3.74  p< .001). It is unclear whether this view of Twitter as a higher-quality  lowerrisk venue for health activity results from positive experiences in engaging in health activity on Twitter despite initial skepticism  or whether these users were more likely to engage in health tweeting because they held these positive views.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/2556288.2557214,14,23,discussion,"De Choudhury, Morris, White",3,1,0,0,2014,Our results indicate that online health activity as manifested via search engine queries and tweets provides insight into users’ health information needs  as well as norms of use of these two prominent online platforms with regard to this sensitive topic. The complementary nature of the two media (public vs. private  seeking vs. sharing) help develop a more complete picture of the range of online health activities. Analyzing health trends based on public Twitter posts is an important emerging research topic [25]  but our findings suggest a need for caution if using Twitter to infer health trends for high-stigma conditions—there is evidence of selfcensorship. Combining Twitter data with an alternative data source (such as search logs) could give a better understanding of health information seeking and sharing practices online than using either source alone. Differential prevalence of health term use on different platforms  or differential contexts (e.g.  co-occurring unigrams) might be a useful indicator of the perceived stigma associated with a given health condition.,0,0,0,0,0,0,0,0,2,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/2556288.2557214,14,24,design implications,"De Choudhury, Morris, White",3,1,0,0,2014,The popularity of online venues for seeking information around health issues also raises the point of developing appropriate credibility indicators. Credibility seems particularly important given the relatively high level of confidence our survey respondents reported placing in the content found online  a level of confidence health professionals consider misplaced [22]. Potentially  credibility-specific analogues of Twitter’s “verified account” seal (currently used to label elite users) may be developed for health accounts (or websites  similar to healthonnet.org  but with verification of content accuracy). Systems that support easy sharing of online content with a user’s healthcare provider might also be helpful. New kinds of health information search systems maybe built that support standing queries over search and/or social media to keep users apprised of new developments related to different common health concerns  since seeking new research about conditions and diversity of health content were the goals of many respondents. Such queries might even be personalized based on a user’s medical history (perhaps using data from electronic health records) to increase the likelihood that users learn about relevant health information in a timely fashion (although such personalization has serious privacy implications). Besides  users’ interest in finding information about medications (e.g.  drug interactions  side effects)  diagnostic and treatment information for specific conditions suggest that search engines might serve users well by introducing new categories of “instant answers” that return such content directly in response to medication or illness queries. Although our findings indicate some degree of riskawareness (as evidenced via self-report in the survey and differential activities for high-stigma and serious conditions across the two platforms)  there is need for work on educating users about the privacy risks of seeking and sharing health information online (building on some initial efforts in this area [14]). Participants’ self-reporting and logged behavior suggests that they view search engines as quite private  and they may be unaware of how some search and advertising companies may collect and distribute their information. Even though less high-stigma content was shared on Twitter  the presence of any level of health information sharing in such a public venue may have serious repercussions (e.g.  higher insurance rates  denial of employment  etc.). Developing interfaces to remind users of these risks (perhaps by showing an “are you sure?” dialogue upon detection of sensitive terms in a query or tweet) is an important area for further research.,0,0,0,0,0,0,0,0,2,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/2556288.2557214,14,25,future directions,"De Choudhury, Morris, White",3,1,0,0,2014,Finally  we outline some limitations of this work. In our survey  94% (197) reported using a search engine to seek information related to their health. This fraction is much higher than the figure reported in the Jan 2013 Pew health survey [10]  which showed that 59% of U.S. adults have looked online for health information in the past year. Possible reasons for discrepancies include a slight difference in the question (we asked “have you ever” versus “past year” in Pew) and the characteristics of individuals who volunteered for our online survey: This population may be more Internet-literate than the pool in Pew where participants are recruited via telephone  and where having a Twitter account was not a prerequisite for participation. Though popular  Twitter is only one of several key social media platforms; understanding the role of other social network sites such as Facebook  in online health activity would be a valuable complement to this work. More research is needed to understand the characteristics of online health information seekers and those who seek such information from offline sources. Although we presented a comparison of the use of health conditions in searches and Twitter posts  in the absence of demographics (age  gender  etc.) and corresponding incidence rates of each of the 165 conditions for those populations  it is difficult to infer differences in their norms of use in searches or Twitter alone or in the prevalence of a term’s use online compared to its physical manifestation. Differences in the demographics of the user base between the two platforms may introduce confounds in the cross-platform comparisons. Considering the impact of these factors are interesting extensions to our research.,0,0,0,0,0,0,2,0,0,0,,2,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/2556288.2557214,14,26,conclusion,"De Choudhury, Morris, White",3,1,0,0,2014,Search engines and social media are popular tools for seeking and sharing information about a range of health conditions. We presented an in-depth study around the prevalence of these practices  the nature of health information sought  and why people are increasingly choosing to use such tools for their health information needs. We demonstrated that the prevalence and characteristics of health information that are sought via search engines or shared via social media are considerably distinct. People modify their information seeking and sharing practices depending on condition type: whether it is a serious condition  a disability  or simply an issue with a benign explanation. Despite being aware of the privacy risks of search engine use or public social media use  people did in fact use both  though differentially  to seek and share information on conditions which are socially considered to be stigmatic. We believe  through the findings in this paper  we have been able to shed new light on understanding health seeking and sharing practices both on search engines and on social media  and how these findings might influence the design of future iterations of these platforms,0,0,0,0,0,0,0,0,2,0,,0,0,0,0,0,0,,0
https://www.aaai.org/ocs/index.php/ICWSM/ICWSM14/paper/viewPaper/8075,15,1,abstract,"De Choudhury, De",2,1,0,0,2014,"Social media is continually emerging as a platform of information exchange around health challenges. We study mental health discourse on the popular social media: reddit. Building on findings about health information seeking and sharing practices in online forums, and social media like Twitter, we address three research challenges. First, we present a characterization of self-disclosure in mental illness communities on reddit. We observe individuals discussing a variety of concerns ranging from the daily grind to specific queries about diagnosis and treatment. Second, we build a statistical model to examine the factors that drive social support on mental health reddit communities. We also develop language models to characterize mental health social support, which are observed to bear emotional, informational, instrumental, and prescriptive information. Finally, we study disinhibition in the light of the dissociative anonymity that reddit’s throwaway accounts provide. Apart from promoting open conversations, such anonymity surprisingly is found to gather feedback that is more involving and emotionally engaging. Our findings reveal, for the first time, the kind of unique information needs that a social media like reddit might be fulfilling when it comes to a stigmatic illness. They also expand our understanding of the role of the social web in behavioral therapy.",0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://www.aaai.org/ocs/index.php/ICWSM/ICWSM14/paper/viewPaper/8075,15,2,introduction,"De Choudhury, De",2,1,0,0,2014,Health information seeking and sharing practices online are known to be effective in helping people cope with respective problems (Fox  2013). Studies have shown that online fora and support groups provide a conducive environment allowing people to get connected with others who share similar difficulties  misery  pain  condition  or distress and thus act as inexpensive and convenient vehicles for obtaining help and advice around health challenges (Eysenbach et al. 2004). Moreover  honesty and selfdisclosure are important therapeutic ingredients of these health communities (Johnson & Ambrose  2006). Recently  social media sites have begun to emerge as increasingly adopted platforms wherein health information seeking and sharing practices are apparent. Distinct from online fora  these social systems are more holistic  in the sense that millions of people use them to post about the mundane goings on of their lives. Additionally  unlike many online communities  most social media sites have a personal permanent identity associated with user profiles. Consequently  these platforms provide a rich ecosystem to study the variety of self-disclosure  social support  and disinhibition that engender health related discourse. Specifically  we examine a highly popular social news and entertainment media: reddit (http://www.reddit.com/). reddit is an interesting online social system that has the attributes of a forum: it allows sharing blurbs of text and media as posts that invite votes and commentary. At the same time it is often used as a social feed of information broadcasted from people’s contacts and audiences. Another distinctive aspect of reddit is its “throwaway” accounts. These are temporary identities often used as an “anonymity cloak” to discuss uninhibited feelings  sensitive information  or socially unacceptable thoughts momentarily; information otherwise considered unsuitable for the mainstream. Given the unique characteristics of reddit  in this paper we are interested in studying the nature of discourse around the important health challenge of mental illness. Mental illness  in particular  is a kind of health concern where the value of emotional and pragmatic support as well as selfdisclosure has been recognized over the years. Studies have demonstrated that both self-disclosure and social support are beneficial in improving perceived self-efficacy  and helping improve quality of life (Turner et al. 1983). Further  note that mental illness is regarded as a social stigma—there is evidence that people with mental illness tend to be guarded about what they reveal about their condition (Corrigan  2004; De Choudhury et al. 2014). The dissociative anonymity cover that reddit allows makes study of such a stigmatic illness interesting because of the disinhibition effect induced by anonymity (Caplan & Turner  2007). In the light of the above discussion  the following are the research challenges we address in this paper: RQ 1: What kind of language attributes and content characterize self-disclosure in reddit communities relating to mental health? RQ 2: What factors drive social support on mental health oriented posts? What are the various forms of social support that characterize commentary on mental health postings? RQ 3: What kind of online disinhibition do we observe in the mental health information seeking and sharing practices of redditors who choose to be anonymous versus others who do not? Based on a large dataset of several thousand reddit users  posts  and comments  our findings show that the reddit communities we study allow a high degree of information exchange around a range of issues concerning mental health. These range from self-disclosure of challenges faced in dayto-day activities  work  personal relationships  to specific queries about mental illness diagnosis and treatment. Feedback on mental health postings also ranges a wide spectrum  from emotional and instrumental commentary  to informational and prescriptive advice. In fact  lowered inhibition and self-attention focused posts receive greater support. Finally  looking through the lens of anonymity as enabled by throwaway accounts in reddit  we find that a small but notable fraction of redditors use the feature as a cover for more intimate and open conversations around their experiences of mental illness. In fact  despite the negative or caustic nature of content shared by anonymous redditors  online disinhibition of this nature garners more emotional and instrumental feedback through commentary. Our observations demonstrate that reddit fills an interesting gap between online health forums and social media and networks like Twitter and Facebook  when it comes to mental health discourse. Moreover  it is established that any psychological consequence depends on the activities a technology enables  attributes of the user  and how the two interact. This research  hence forms a crucial point when trying to determine the effect of the social web on a grave and stigmatic concern like mental illness.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://www.aaai.org/ocs/index.php/ICWSM/ICWSM14/paper/viewPaper/8075,15,3,related work,"De Choudhury, De",2,1,0,0,2014,Prior research in psychology has examined the important role of social support in combating depression (George et al. 1989). It is argued that social intimacy  social integration  nature of social networks as well as individual perception of being supported by others are important and indispensable in encouraging mental illness recovery (Turner et al. 1983). The internet is increasingly used for seeking and sharing health information online  and such activity is known to have connections to healthcare utilization and health-related behaviors (Sillence et al. 2007; Liu et al. 2013). Literature on online support groups notes that they are popular sources of information and support for many internet users (White & Dorman  2001). These forums tend to have sharp contrast when compared with similar offline groups; for instance  people are likely to discuss problems that they do not feel comfortable discussing face-to-face (Johnson & Ambrose  2006). Moreover such online health communities (OHCs) are known to foster well-being  a sense of control  selfconfidence  social interactions  and improved feelings. In this light  approaches to community building have also been proposed (Wicks et al. 2010). Turning to research on social media  a growing body of work has demonstrated that social media is an increasingly adopted platform allowing users to communicate around a variety of health concerns (Paul & Dredze  2011; Fox  2013; De Choudhury et al. 2014). Newman et al. (2011) interviewed people with significant health concerns who participated in both OHCs and Facebook. Oh et al. (2013) examined people’s use of Facebook for health purposes and showed that emotional support was a significant predictor of health self-efficacy. In the context of mental health in particular  Moreno et al.  (2011) demonstrated that status updates on Facebook could reveal symptoms of major depressive episodes  while Park et al. (2013) found differences in the perception of Twitter use between depressed and non-depressed users— the former found value in Twitter due to the ability to garner social awareness and engage in emotional interaction. On similar lines  De Choudhury et al. (2013) examined Twitter postings of individuals suffering from depression to build models that predict the future occurrence of depression. Weaving the above threads together  health information seeking and sharing on OHCs have been studied extensively in the light of the benefits of self-help  social support and empathy  however little investigation about the same has been done on social media. Today  many social media platforms are becoming a routine go-to place for broadcasting and discussing topics of daily life. It is therefore useful to examine how this kind of habitual disinhibition on social media impacts the manner in which people derive help on their health concerns  especially on ones like mental illness  known to be socially stigmatic (for definition see Corrigan (2004)). Focusing on reddit  note that we use a broad definition of “mental health” to span shared content around any aspect of the ailment experience. Finally  while prior work has shown the potential of social media in bearing markers of mental health concerns  our knowledge about the nature of self-disclosure on health concerns is rather limited  so is the nature of emotional support and advice that social media communities provide. Through the lens of reddit  hence we are interested in investigating the language and content of people’s selfdisclosure  what characterizes the nature of community feedback  and the attributes that drive such social support.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://www.aaai.org/ocs/index.php/ICWSM/ICWSM14/paper/viewPaper/8075,15,4,data and methods,"De Choudhury, De",2,1,0,0,2014,reddit is a social news website where registered users submit content in the form of links or text posts. Users  also known as “redditors”  can then vote each submission “up” or “down” to rank the post and determine its position or prominence on the site’s pages. These two attributes associated with a post are referred to as “upvotes” and “downvotes”. Redditors can also comment on posts  and respond back in a conversation tree of comments. Content entries  that is the posts  are organized by areas of interest or sub-communities called “subreddits”  such as politics  programming  or science. As of 2013  reddit’s official statistics included 56 billion page views  731 million unique visitors  40 855 032 posts  and 404 603 286 comments (http://blog.reddit.com/2013/ 12/top-posts-of-2013-stats-and-snoo-years.html). We used of reddit’s official API (http://www.reddit.com/ dev /api) to collect posts  comments and associated metadata from several mental health subreddits: specifically using a Python wrapper PRAW (https://praw.readthedocs.org/en/ latest/index.html). The subreddits we crawled were: alcoholism  anxiety  bipolarreddit  depression  mentalhealth  MMFB (Make Me Feel Better)  socialanxiety  SuicideWatch. All of these subreddits host public content. In order to arrive at a comprehensive list of subreddits to focus on  we utilized reddit’s native subreddit search feature (http://www.reddit.com/reddits) and searched for subreddits on “mental health”. Two researchers familiar with reddit employed an initial filtering step on the search results returned  so that we focus on high precision subreddits discussing mental health concerns and issues. Thereafter  we focused on a snowball approach in which starting with a few seed subreddits (mentalhealth  depression)  we compiled a second list of “related” or “similar” subreddits that are listed in the profile pages of the seed subreddits. Following a second filtering step  we arrived at the list of subreddits listed above. For each of these subreddits  we obtained daily crawls of their posts in the New category. Corresponding to each post we collected information on the title of the post  the body or textual content  id  timestamp when the post was made  author id  and the number of upvotes and downvotes it obtained. Since posts gather comments over a period of time following the time of sharing  we crawled all of the comments per post that were shared over a three day period after the post was made. Qualitative examinations of the subreddits of interest revealed that 90% or more of the comments to any post were typically made in a three day window following the time the post is made—hence the choice. The crawl of the subreddits used in this paper spanned between Nov 8  2013 and Dec 28  2013. We present some descriptive statistics of our crawled data. Our dataset contained 20 411 posts with at least one comment  and 97 661 comments in all  with 27 102 unique users who made posts  comments or both. A set of 7 823 users (28.79%) were found to write both at least one post and comment. CDF of the user distribution over posts and comments is given in Figure 1. The figure shows the expected heavy tail trend observed in several social phenomena. Also see Figure 2 for the distribution of comments over time following post share. It illustrates the quick responsivity culture in the communities we study (peak at 3 hours). Some of the additional statistics of our dataset are given in Table 1. Further  example titles of a few posts in our dataset are given in Table 2.,2,2,2,2,0,1,0,0,2,0,,0,0,0,0,0,0,,0
https://www.aaai.org/ocs/index.php/ICWSM/ICWSM14/paper/viewPaper/8075,15,5,general linguistic attributes,"De Choudhury, De",2,1,0,0,2014,To understand the nature of self-disclosure in reddit posts  we first examine the general linguistic attributes manifested in their content. In Table 3  we first present a list of the most popular (stopword eliminated) unigrams that appear in reddit postings. We intended to look at these highly shared unigrams more deeply and systematically  hence we organized these unigrams (stopword inclusive) in various semantic categories provided by the psycholinguistic lexicon LIWC (http://www.liwc.net/). We find that among the unigrams in Table 3  there are words that extensively span emotional or affective expressions (happy  love  bad  anxiety  good  hate) e.g.: “I've been recently wondering if my love to numb the world around me has turned me into an alcoholic…” “Has anyone else battled numbness  loss of feeling during recovery? Does it ever get better? i've been sober for about 2 years and still have pretty severe anxiety at times”. We observe presence of relationships and social life words too (family  friends  people  person  parents) e.g.: “i get really anxious out when i go home for big events.” “i do love my family  they're just really loud and argumentative sometimes”. e.g.  time  day  years  months: “hi all  i'm ten weeks sober today and while i wish i could say i'm physically and mentally in great shape  the truth is i judge my days by how i feel less bad  as oppose to good”. Work and daily grind oriented words are common as well  because lifestyle irregularities are often associated with the psychopathology of mental illness (Prigerson et al. 1995)—e.g.  life  school  work  job: “I am completely broke  can't afford rehab  and can't take time off work”. We also find a fair number of cognitive words in these highly used unigrams (felt  hard  feeling  lot)  e.g.: “I'm new here  but having anxiety like I haven't felt in a long time” “I find a lot of strength in going to a concert. I have been understanding of my anxiety and depression since i was about 8  and i hated it”. These observations are supported by psychology literature  where cognitive biases as manifested through dysfunctional attitudes  depressive attributional biases  and negative automatic thoughts were found to be characteristic of mental illness (Eaves & Rush  1984). Further  inhibition words like avoid  deny  safe demonstrate that redditors are perhaps using the platform to broadcast their thoughts to an audience of strangers or weak ties  on issues and topics they might consider to be socially stigmatic to be discussed elsewhere: “i can't escape the feeling of fright i have at all times. i don't feel safe in my own home” “ive been denying my (assumed) depression symptoms for close to two years now writing them off …” Comparing across different LIWC semantic categories over all posts  we observe noticeable differences— KruskalWallis one-way analysis of variance indicated the differences across categories to be significant (χ2 (39; N=20411)=9.24; p<10-4). Table 4 reports the top 8 most common LIWC categories  the mean proportion of words from each category in the posts  and the corresponding standard deviation. Note that the percentages over all categories sum to greater than 100%  since a word could belong to multiple categories. Observing closely  many of the categories whose corresponding unigrams appeared in Table 3  are also highly prominent categories globally over all posts  as given in Table 4. Not shown in Table 4  perhaps intuitively  negative emotion  anger  and sadness words were considerably more prominent than positive emotion words (a Wilcoxon signed rank test reveals that the differences are statistically significant (z=-6.08  p<.001)). Likely  these redditors experience several negative emotions: hence mental instability  helplessness  loneliness  restlessness manifest in their postings (Rude et al. 2004). Non-first person personal pronouns were the second most prominent category  implying the tendency of the authors of these posts to be interactive with their audiences perhaps to seek social support and advice. We also find the 1st person singular pronouns to be extremely common: perhaps indicating that many of the posts that redditors share on mental health are about themselves  their experiences  and the health and social issues they are facing. In fact  high selfattentional focus is a known psychological attribute of mental illness sufferers (Chung & Pennebaker  2007).,0,0,0,0,0,0,0,0,0,0,,2,0,0,0,0,0,,0
https://www.aaai.org/ocs/index.php/ICWSM/ICWSM14/paper/viewPaper/8075,15,6,mental health information sharing,"De Choudhury, De",2,1,0,0,2014,Beyond the linguistic attributes that indicate self-disclosure  we intended to examine whether redditors sought and shared information specific to their mental health concerns  e.g.  symptoms or treatment related information in their posts. For this purpose  we utilized the lexicon of “depression” terms from (De Choudhury et al.  2013). The lexicon consists of words derived from the “Mental Health” category of Yahoo! Answers. However  since we were primarily interested in keywords in this lexicon (comprising 1000 words) that would indicate symptoms or treatment aspects of mental illness  we obtained crowdsourced labels on each word on whether it is a symptom of a mental illness  a treatment related information  or otherwise. Particularly  per word  we obtained five independent judgments from crowdworkers on Amazon’s Mechanical Turk. We considered English proficient crowdworkers with more than 95% approval rating. To better guide their judgment  they were instructed to refer to the following online resources: RightDiagnosis.com (http://www.rightdiagnosis.com/m/ mental_illness/symptoms.htm)  WebMD (http://www. webmd.com/depression/mental-health-warning-signs)  and for treatment information  the list of antidepressant drug names from Wikipedia (http://en.wikipedia.org/wiki/List _of_ antidepressants). The labeling task coded a set of 467 words which were either symptoms or treatment specific (interrater agreement Fleiss’ κ=.68)—a word was considered to be a treatment or a symptom if three or more raters agreed. One researcher went through the coded words as a final step of sanity check. In Table 5  we present a few common symptom keywords and common treatment keywords found in post content. We observe that symptoms related posts revolve around details about sleep  eating habits  and other forms of physical ailment—all of which are known to be associated with occurrence of a depressive episode (Posternak et al.  2006). “Heart/rib/chest pains  worse than usual… hi everyone  i have been suffering mild panic attacks…” “Insomnia and binge drinking i've had periodic insomnia for a lot of my life. lately  it seems to have gotten worse”. Analyzing further  example post excerpts around treatment include: “Cognitive behavioural therapy? Does it work? Have you tried it? the university offers it for free where i live…” “How I know the medicine is working. bipolar ii  currently on 1500 mg lithium  400 mg lamictal  .5 mg risperdal  and as of one month ago  150 mg wellbutrin.” Thus users also appear to discuss their therapy and treatment  even dosage levels of medication  as well as seek advice on particular treatment options. Summarily  RQ 1 investigated disclosure in mental health subreddits: we found that the postings ranged from sharing the challenges faced in day-to-day activities  to work and personal relationships  as well as seeking detailed information on symptoms and treatment of mental illness. which redditors can provide feedback on a post  i.e.  voting and commenting  we define the following two measures of social support: 1. “Karma”. One of the dependent variables of social support we attempt to predict  measures the “karma” i.e.  net votes that a post receives. Since every reddit post receives a certain number of upvotes (positive feedback) and downvotes (negative feedback)  we define the measure of karma to be the difference between the number of upvotes and downvotes. Note that “karma” is a reddit defined measure (http://www.reddit.com/wiki/ faq#wiki_what_is_that_number_next_to_usernames.3Fa nd_what_is_karma.3F)  and the authors do not intend to convey any philosophical or suggestive meaning. 2.Comments. The second dependent variable of social support is the number of comments on a reddit post. Predictive variables. In Table 6 we present the independent variables which are the various semantic categories of words (from LIWC) computed on post content (ref. Previous section). We additionally use three different independent variables for this prediction task: the length of a post (number of whitespace delimited words)  number of emoticons (http://en.wikipedia.org/wiki/List_of_emoticons) in post content  and the number of question-centric words (what  where  when  which  who  whose  why  how). The rationale to get emoticons is to observe the degree of emotional expressivity  while for the question words it is to understand to what extent posts on mental health seek explicit feedback or suggestions from the reddit community. Statistical technique. We use negative binomial regression as our prediction method because both our dependent variables  karma and comments are counts  and negative binomial regression is typically well-suited to handle overdispersed count outcome variables. We use a measure called deviance to evaluate goodness of fit  since this model has no direct analog of the proportion of variance explained by the predictors (R 2 ) in linear regression. Deviance is a measure of the lack of fit to the data in a negative binomial regression model—lower numbers are better. It is calculated by comparing a model with the saturated model—a model with a theoretically perfect fit (the intercept only model here). Prediction results. Now we analyze the results of our two prediction tasks—predicting the measure of karma of posts  and the number of comments. Table 7 presents the results of negative binomial regression in predicting karma. We observe that compared to a baseline null model (intercept only model)  the LIWC semantic categories as well as post attributes (ref. Table 6) provide considerable explanatory power  with an improvement in deviance (328.351- 69.246=259.105). This difference in deviances follows a χ 2 distribution  hence using a χ 2 test we show significant statistical power of our model over baseline in explaining our data: χ2 (44; N=20411)=328.35-69.24=259.11; p<10-5 . Next  Table 8 presents the results of another negative binomial regression  with comment count of posts as the dependent variable. The overall model explains a considerable amount of deviance compared to the null model  χ 2 (44; N=20411)= 294.896-78.252=216.644; p<10-5 . The contribution of the different independent variables in the two prediction tasks is notable. In both  greater use of 1 st person pronouns in posts garners more karma and comments. We conjecture that personal accounts of mental health concerns engages more redditors. Interestingly  while both negative and positive emotion are significant predictors of karma and comment count  greater NA hinders karma but drives more comments. At the same time  greater PA leads to more karma though with fewer comments. Next  posts with lowered sense of inhibition receive greater karma and comments  likely because the post authors are discussing their mental health concerns in a candid and unrestrained way. Posts about relationships  social aspects  and health also receive greater social support in the form of karma and comments. Shorter posts and with lesser swear content receive more karma  however posts which are longer or more elaborate in nature garner greater commentary. Finally  presence of more emoticons in mental health posts is associated with higher karma  while posts that are more question-centric receive more comments  likely because the latter seek explicit feedback and advice from the greater reddit audience,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://www.aaai.org/ocs/index.php/ICWSM/ICWSM14/paper/viewPaper/8075,15,7,rq results social support,"De Choudhury, De",2,1,0,0,2014,"In our second research question, we first examine whether attributes of the content of posts, are predictive of the extent of social support posts receive. Response variables. Since there are two different ways in which redditors can provide feedback on a post, i.e., voting and commenting, we define the following two measures of social support: 1. “Karma”. One of the dependent variables of social support we attempt to predict, measures the “karma” i.e., net votes that a post receives. Since every reddit post receives a certain number of upvotes (positive feedback) and downvotes (negative feedback), we define the measure of karma to be the difference between the number of upvotes and downvotes. Note that “karma” is a reddit defined measure (http://www.reddit.com/wiki/ faq#wiki_what_is_that_number_next_to_usernames.3Fa nd_what_is_karma.3F), and the authors do not intend to convey any philosophical or suggestive meaning. 2.Comments. The second dependent variable of social support is the number of comments on a reddit post. Predictive variables. In Table 6 we present the independent variables which are the various semantic categories of words (from LIWC) computed on post content (ref. previous section). We additionally use three different independent variables for this prediction task: the length of a post (number of whitespace delimited words), number of emoticons (http://en.wikipedia.org/wiki/List_of_emoticons) in post content, and the number of question-centric words (what, where, when, which, who, whose, why, how). The rationale to get emoticons is to observe the degree of emotional expressivity, while for the question words it is to understand to what extent posts on mental health seek explicit feedback or suggestions from the reddit community. Statistical technique. We use negative binomial regression as our prediction method because both our dependent variables, karma and comments are counts, and negative binomial regression is typically well-suited to handle overdispersed count outcome variables. We use a measure called deviance to evaluate goodness of fit, since this model has no direct analog of the proportion of variance explained by the predictors (R 2 ) in linear regression. Deviance is a measure of the lack of fit to the data in a negative binomial regression model—lower numbers are better. It is calculated by comparing a model with the saturated model—a model with a theoretically perfect fit (the intercept only model here). Prediction results. Now we analyze the results of our two prediction tasks—predicting the measure of karma of posts, and the number of comments. Table 7 presents the results of negative binomial regression in predicting karma. We observe that compared to a baseline null model (intercept only model), the LIWC semantic categories as well as post attributes (ref. Table 6) provide considerable explanatory power, with an improvement in deviance (328.351- 69.246=259.105). This difference in deviances follows a χ 2 distribution, hence using a χ 2 test we show significant statistical power of our model over baseline in explaining our data: χ2 (44; N=20411)=328.35-69.24=259.11; p<10-5 . Next, Table 8 presents the results of another negative binomial regression, with comment count of posts as the dependent variable. The overall model explains a considerable amount of deviance compared to the null model, χ 2 (44; N=20411)= 294.896-78.252=216.644; p<10-5 . The contribution of the different independent variables in the two prediction tasks is notable. In both, greater use of 1 st person pronouns in posts garners more karma and comments. We conjecture that personal accounts of mental health concerns engages more redditors. Interestingly, while both negative and positive emotion are significant predictors of karma and comment count, greater NA hinders karma but drives more comments. At the same time, greater PA leads to more karma though with fewer comments. Next, posts with lowered sense of inhibition receive greater karma and comments, likely because the post authors are discussing their mental health concerns in a candid and unrestrained way. Posts about relationships, social aspects, and health also receive greater social support in the form of karma and comments. Shorter posts and with lesser swear content receive more karma, however posts which are longer or more elaborate in nature garner greater commentary. Finally, presence of more emoticons in mental health posts is associated with higher karma, while posts that are more question-centric receive more comments, likely because the latter seek explicit feedback and advice from the greater reddit audience. Our next question of investigation is  what is the type of social support that reddit users provide on postings in mental health subreddits? Our observation stems from prior health literature  where social support concerning illnesses has been characterized (Turner et al. 1983; George et al. 1989). From the literature  we identified a set of four categories of social support around health concerns— prescriptive  informational  instrumental  and emotional. Identifying types of social support in reddit commentary involved characterizing the content of comments into thematic clusters. Therefore we built a language model  that makes use of the unigrams and bigrams present in comment content to automatically infer clusters of support types. Note that unsupervised learning is more appropriate here because of the lack of ground truth. Our model is based on Latent Dirichlet Allocation (LDA) (Blei et al. 2003)  a widely popular probabilistic topic model that yields clusters of word tokens (unigrams and bigrams here). For the purposes of this paper  we made use of the LDA implementation provided by Stanford Topic Modeling Toolbox (http://nlp.stanford.edu/software/tmt/ tmt-0.2/). The model yielded a clustering of the comments into 20 different topics. Over the LDA output  two researchers performed qualitative labeling of the clusters  so as to identify those clusters whose comment content revolved around social support and advice  and if so  what was the type of support that the community was providing in them. That is  we intended to identify the clusters in which the comments bore prescriptive  informational  instrumental  and emotional information. Following the labeling task to identify which four clusters reflected the four types of social support above  the Fleiss’ κ for interrater agreement was found to be .77  and disagreements were resolved through discussion. We observe that there is considerable variation in the presence of each type of social support: there are more emotional (36%) and prescriptive comments (32%)  than those which are informational (13%) or instrumental (19%). A Kruskal-Wallis one-way analysis of variance indicated the differences to be significant (χ2 (3; N=97661)=5.88; p<10-3). To demonstrate differences across the four social support themes  we show context of use of one popular unigram/bigram from each category in Table 9. In short  in characterizing social support in RQ 2  we observed that lowered inhibition postings that are more selfattention focused and discuss relationship and health issues seemed to gather greater community support through votes and comments. Somewhat surprisingly greater negative affect was associated with more commentary. Not all social support were of the same type either—our findings demonstrated comments to bear emotional and instrumental  to information and prescriptive advice.",0,0,0,0,0,0,0,0,0,0,,0,0,0,0,2,0,,0
https://www.aaai.org/ocs/index.php/ICWSM/ICWSM14/paper/viewPaper/8075,15,8,rq results anonymity and disinhibition,"De Choudhury, De",2,1,0,0,2014,Our third and final research question revolves around examining how the ability to be anonymous on reddit impacts sharing of information around stigmatic health concerns like mental illness. An interesting feature of reddit  is the ‘throwaway account’. By way of this feature  reddit enables an individual create accounts in a matter of minutes without giving out an email address. They can thus make the choice to dissociate from their reddit identity by simply using an alternate pseudonym and then leaving it behind. Prior literature demonstrates that researchers recognize dissociative anonymity of this nature (a resistance to attach to offline identity or to their actual reddit account/online persona) as the foundation of online disinhibition (LapidotLefler & Barak 2012). Online disinhibition leads people to act differently than they would in identifiable online settings (Chester & O’Hara  2007). This led us to examine the behavior of throwaway accounts in our data. First  based on our qualitative observations  we used a simple technique to identify such anonymous throwaway accounts. We matched the regex “*throw*” to all usernames to construct a high precision set of anonymous posters. Note that  there could be users our technique cannot detect  e.g.  ones who may not have this regex in their temporary identities; however our technique gives us a reliable and precise user set to work with. To our surprise  mental illness despite being stigmatic  our statistics reveal that a rather small percentage of users in our dataset used throwaway accounts (1 209 users; 4.46%). Nevertheless  we do notice that in this small percentage of throwaway account owners  a greater fraction (92% or 1114 users) are authors of one or more posts  while about 42% (508 users) are ones who have authored one or more comments. This shows that sharing seemingly personal information on mental health issues through posts is considered relatively more of a sensitive activity than providing support through commentary. Further  733 users (61%) we found to post exactly one item (i.e.  post or comment). Such “one-time” usage of throwaway accounts by a large majority shows that redditors use it as a mechanism to discuss on topics they feel guarded about  and a one-time use gives them the opportunity to not leave any trails behind  and walk away from further discussion.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://www.aaai.org/ocs/index.php/ICWSM/ICWSM14/paper/viewPaper/8075,15,9,differences in language use,"De Choudhury, De",2,1,0,0,2014,Is there any observable difference in behavior  e.g.  in the light of the linguistic attributes (from LIWC) between the set of redditors who post with throwaway accounts and those who do not? Table 10 presents the mean use of top 10 LIWC semantic categories along which the two cohorts differ the most—note that for negative emotion (NA) we show a breakdown of three types of emotion  anger  anxiety  and sadness. We also present statistical hypothesis testing between the cohorts using Wilcoxon signed rank tests. We observe notable differences—anonymous redditors tend to be more disinhibited in the content they share in their postings (21.2% less inhibition words); share more about personal relationships e.g.  friends  family (14-16% more); converse more on death and health related issues (10-14% more); are more self-attention focused i.e.  greater use of 1 st person pronouns (6.8% more); are less interactive with others i.e.  lower use of 2 nd person pronouns (4.1% less); present more cognitive biases in their content (3.6% more cognitive words); and finally  are more negative and less positive (1.8% higher and 1.2% lower respectively)—also note greater use of anger  anxiety  sadness words. To summarize  these observations tell us that anonymity through these throwaway reddit accounts results in decreased feelings of vulnerability and increased selfdisclosure when it comes to discourse on mental health.,0,0,0,0,0,0,0,0,0,0,,2,0,0,0,0,0,,0
https://www.aaai.org/ocs/index.php/ICWSM/ICWSM14/paper/viewPaper/8075,15,10,differences in social support,"De Choudhury, De",2,1,0,0,2014,Does the nature of feedback or social support from the greater reddit community also differ in the case of posts from anonymous accounts? We observe from Figure 3 that throwaway accounts are endorsed to a lesser extent (lower karma)  perhaps due to their negative or caustic content. instrumental support through comments  in comparison to non-anonymous ones. Perhaps the reddit audience tends to sympathize more with the posters  and provide more helpful and contributory feedback  suggestions  and opinions to the throwaway posters because of their honest confessions. However  throwaway accounts garner fewer comments that are informational or prescriptive in nature. Perhaps when individuals share information through their primary (nonanonymous) accounts  they are more objective in nature and seek information and help on concrete challenges around mental illness. Consequently  the audience responds through support that is laden with information about e.g.  treatment  coping etc. as well as that provide propositions around ways to deal with the challenges. In essence  our findings of RQ 3 show that anonymity by way of throwaway accounts allows greater self-disclosure around the stigmatic topic of mental health without worrying about being identified; at the same time they let other redditors to communicate prudently  or give honest advice on sensitive issues around mental health.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://www.aaai.org/ocs/index.php/ICWSM/ICWSM14/paper/viewPaper/8075,15,11,theoretical implications,"De Choudhury, De",2,1,0,0,2014,Our findings show that reddit fills an interesting gap between online health forums  and social media and social networks like Twitter and Facebook  when it comes to mental health related discourse. We find that reddit users in these communities share  quite explicitly  information about mental health issues providing evidence of selecting this non-conventional tool as a medium that fulfils certain needs. Self-disclosure. The clinical literature reports that a variety of factors or states are associated with mental health concerns in people; these include mood disturbances  selfderogatory thoughts  cognitive impairments  attention  communication  and judgment (Rabkin & Struening  1976). Our observations from self-disclosure in reddit postings align with these prior findings—mood disturbances indicated by higher use of negative emotion words  selfderogatory thoughts and self-care indicated by work  time  and relativity words  cognitive impairments and judgment issues indicated by the use of cognitive words  attention to self as indicated by greater use of 1st person singular pronoun  and finally communication attributes reflected in the use of social words and increased non-first person personal pronoun use. In essence  our results demonstrate that redditors sharing posts in mental health subreddits often use it as a venue of self-expression of their experiences around their illness challenges  as well as the impact of those experiences on their work  life  and relationships. Aside from that  seeking concrete diagnosis or treatment information is also not uncommon in these communities. Social support. Our findings show that certain types of disclosure  e.g.  posts with lowered inhibition  invite greater social support than others. In fact  even though redditors are not compensated for their actions  we observe the feedback manifested in the comments to be of surprisingly high quality  and ranges from emotional and instrumental  to information and prescriptive advice. This is an important contrast to social media like Twitter  where sharing health information is most times a broadcast or an emotional outburst (Paul & Dredze  2011; De Choudhury et al.  2013)  and not necessarily around seeking specific or quality information around treatment and diagnosis. We also note the contrast of reddit use for mental health with the social network Facebook. While prior research has shown that Facebook status updates bear some degree of health-oriented information  privacy concerns around revealing “too much to people you know” often preclude one from seeking out for actionable advice around stigmatic health concerns (Newman et al. 2011; Young & QuanHaase  2009). In that light  reddit acts as a convenient medium because of its lack of personally identifiable information in user profiles. In comparison to online health forums too  our findings on reddit are distinct and complementary. Health forums often focus on specific health needs of individuals  and provide tools for easy sharing of health and illness information  ranging from current treatments  to symptoms and outcomes (Eysenbach et al. 2004). We observe that users do use reddit to fulfil some of these types of needs  however they often also use it simply for emotional support—an aspect considered very valuable in mental illness therapy (Cohen & Wills  1985). Anonymity. Finally  the small but distinctive use of throwaway accounts in the subreddits we study indicates that this feature of anonymity allows individuals to express views and thoughts relatively freely—something that may be considered to be extreme or unacceptable to the mainstream. That more than half of these throwaway account owners (61%) have used a particular throwaway account to post exactly once shows their propensity to make sure they do not leave any activity trails behind. However  we observe that such anonymity does not hinder the quality of social support redditors receive—in fact they garner more comments on such postings  and as we observe  tend to provide greater emotional sustenance  and are generally more involving and helpful in their suggestions and feedback. In essence  our findings align with prior literature (Bernstein et al. 2011; Schoenebeck et al. 2013)  where anonymity has been found to be a positive feature of some online communities  because when it comes to mental illness  disinhibition in the form of journaling and discourse can be an effective healing process (Rude et al. 2004). Nevertheless  as popular a social site as reddit is  further research is needed to understand these anonymity practices around mental health concerns. Compared to analogous anonymous sites like the discussion board /b/ on 4chan on one end of the continuum  and social networks like Facebook which harvest physical world contacts (e.g.  family  friends  coworkers) and identities on the other  it would be interesting to examine what factors affect disinhibition and self-perception of anonymity of users in these communities discussing stigmatic health concerns  and could be valuable insights for social media design.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://www.aaai.org/ocs/index.php/ICWSM/ICWSM14/paper/viewPaper/8075,15,12,limitations,"De Choudhury, De",2,1,0,0,2014,Our work is of course not free from limitations. We acknowledge that there are likely other subreddits beyond the ones we study where stigmatic information and topics as mental health are discussed. This work is a preliminary exploration  focusing on a set of high precision reddit communities  however expanding to other subreddits is a ripe area of future research. Also  understanding the extent to which the greater reddit population engaged in mental illness discourse embodies the observed behavior  is also valuable from a generalization perspective. Further note that by no means the goal of this research is to claim all of the individuals posting in the subreddits of interest actually suffered from mental illness: we can only make a weak inference about it from their interest in the forums we study. Future research will benefit from a mixed methods approach of backing up our quantitative analysis with qualitative investigations about the health status of the communities.,0,0,0,0,0,0,0,0,0,0,,2,0,0,0,0,0,,0
https://www.aaai.org/ocs/index.php/ICWSM/ICWSM14/paper/viewPaper/8075,15,13,conclusion,"De Choudhury, De",2,1,0,0,2014,We have presented  to the best of our knowledge  the first comprehensive study of mental health discourse on the social media reddit. Our results showed evidence of considerable self-disclosure around mental health issues. In characterizing the nature of social support in comments on the mental health subreddits we studied  we observed that it can span several nuanced categories  providing emotional to prescriptive feedback. Lastly  use of dissociative anonymity as featured via throwaway accounts  although limited  was found to be adopted as an information sharing practice for open conversations relating to mental illness. Interestingly  despite the negative or caustic nature of content shared by anonymous redditors  disinhibition led to more emotional and instrumental feedback. This research reveals how social media like reddit are fulfilling unique information and social needs of a cohort challenged with a stigmatic health concern looking through the lenses of disclosure  social support  and disinhibition. Potentially  our work may provide a wealth of resources to clinicians  health practitioners  caregivers  and policy makers to identify communities at risk.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/3025453.3025932,16,1,abstract,"Manikonda, De Choudhury",2,1,0,0,2017,Content shared on social media platforms has been identified to be valuable in gaining insights into people’s mental health experiences. Although there has been widespread adoption of photo-sharing platforms such as Instagram in recent years  the role of visual imagery as a mechanism of self-disclosure is less understood. We study the nature of visual attributes manifested in images relating to mental health disclosures on Instagram. Employing computer vision techniques on a corpus of thousands of posts  we extract and examine three visual attributes: visual features (e.g.  color)  themes  and emotions in images. Our findings indicate the use of imagery for unique self-disclosure needs  quantitatively and qualitatively distinct from those shared via the textual modality: expressions of emotional distress  calls for help  and explicit display of vulnerability. We discuss the relationship of our findings to literature in visual sociology  in mental health self-disclosure  and implications for the design of health interventions.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/3025453.3025932,16,2,introduction,"Manikonda, De Choudhury",2,1,0,0,2017,Social media platforms have emerged to be conducive means of social exchange and support seeking around stigmatized concerns like mental health. The benefits of such practices are situated in the literature on self-disclosure—the “process of making the self known to others” [11]. Self-disclosure can be an important therapeutic ingredient and is linked to improved physical and psychological well-being [30]. Moreover  since many social media platforms  like Instagram  Tumblr or Reddit allow anonymous or semi-anonymous discourse  they have come to be adopted widely in helping cope with mental health challenges [13  4]; conditions known to be associated with high social stigma. Self-disclosure can happen via the adoption of many diverse interaction modalities. For instance  expressive writing is identified to play a prominent role in supporting mental health therapeutic processes [43]. In fact  recent research has studied mental health disclosures through the lens of social media [3  10  14]  and has largely explored the ways in which linguistic attributes  such as affect  cognition  and linguistic style may reveal cues about one’s psychological state. We note that other modalities of mental health disclosure  such as visual imagery shared on social media  are under-explored. The rich literature in visual sociology situates imagery to be a powerful means of enabling emotional expression related to mental illnesses  especially those feelings and experiences that individuals may struggle to express verbally or through written communication [45]. It has been found that the parts of the brain that process visual information are evolutionarily older than the parts that process verbal information [33]. Thus  visual imagery are likely to evoke deeper elements of psychological consciousness than do words or writing. Mental health disclosures based on words alone utilize less of the brain’s capacity than do those in which the brain is processing imagery as well as words. Given these considerations  sharing and reflecting on visual narratives are a known psychiatric approach to tackle mental illness [26]. Extracting and characterizing the expressive meanings conveyed in the imagery shared around mental health disclosures on social media can provide rich information grounded in individuals’ everyday experiences and interactions. Thus these approaches could raise the quality of language-only studies of mental health disclosures. We leverage the recent uptake of photo sharing practices on different social media platforms  such as Instagram and Tumblr to investigate this research problem [15]. We are observing a shift in online user-generated content from predominantly text-based data to richer forms of image-based media. As Tifentale and Manovich [51] rightly noted  these image sharing practices open up fascinating opportunities for the study of “digital visual culture”. Our broad research goal in this paper revolves around investigating how social media disclosures of mental health challenges could be characterized via shared visual imagery. Specifically  we address the following three research questions: (RQ 1) What visual features characterize images of mental health disclosures shared on social media? (RQ 2) What are the kinds of visual themes manifested in these images  and what is the nature of emotional expression associated with these visual themes? (RQ 3) How do visual themes of mental health images complement and contrast with themes manifested in the language of these social media posts? To address these research questions  we leverage a large dataset of over two million public posts associated with ten mental health challenges shared on Instagram. We present some of the first quantitative insights into the nature of visual features  themes and emotion expressed in these images. For the purpose  we employ computer vision techniques of image analysis and unsupervised machine learning methods to identify visual and linguistic themes. Our findings indicate the prominence of a visual channel supporting candid and disinhibited social exchange around mental health. Specifically  we find that the visual and emotional markers of mental health images capture unique characteristics of self-disclosure  beyond those expressed via the sharing of linguistic content. These include  expressions of distress  personal struggles  explicit graphical content  as well as calls for help  and supportive advice toward improved well-being. We find that many of these markers align with forms of selfdisclosure reported in the psychology literature. We situate our findings in literature on visual sociology and the role of visual narratives in mental well-being. We discuss how our work can inspire further research on visual cues of mental health disclosures on social media. We also present design and ethical considerations toward building humancentered technologies and tools to provide support and scaffolding around this new self-disclosure medium.,0,0,0,0,0,0,0,0,2,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/3025453.3025932,16,3,self disclosure and mental health,"Manikonda, De Choudhury",2,1,0,0,2017,Goffman posited the importance of “sympathetic others” in helping people cope with difficult experiences  as well in enabling self-disclosure [22]. Self-disclosure has been widely investigated both in the psychology and the computer mediated communication (CMC) literature. This body of work has argued self-disclosure to be beneficial: having been linked to trust and group identity  as well as playing an important role in social interactions by reducing uncertainty [2  11  30]. In the context of mental health  Ellis [16] reported that discourse on emotionally laden traumatic experiences can be a safe way of confronting mental illness. Jourard [32] also reported that self-disclosure was a basic element in the attainment of improved mental health. This is because  painful events that are not structured into a narrative format  may contribute to the continued experience of negative thoughts and feelings that underlie many mental illnesses. Self-disclosure facilitates a sense of resolution  which results in less rumination and eventually allows disturbing experiences to subside gradually from conscious thought. A seminal work [42] found that participants assigned to a trauma-writing condition showed immune system benefits. Self-disclosure has also been associated with reduced visits to medical centers and psychological benefits in the form of improved affect [48]. Our work builds on these observations and examines the manner in which individuals might be appropriating the photosharing capability of social media platforms like Instagram to self-disclose about mental health challenges.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/3025453.3025932,16,4,visual methods and visual sociology,"Manikonda, De Choudhury",2,1,0,0,2017,Visual methods have widely been employed in the study of psychosocial aspects of health and well-being [25  45]. According to Harrison [26]  ‘visual methods’ describe any research design  which utilizes visual evidence  including the use of photographs  video recordings  drawings  and art. Often  these approaches connect “core definitions of the self” to society  culture and history via the examination of imagery. In fact  Harrison [26] distinguished between the visual as topic (i.e. the visual itself as the subject of investigation) and the visual as resource (i.e. the visual as a means of accessing data about other topics of investigation). Visual methods have been found to be very powerful  since certain emotions  thoughts  feelings  experiences  events  and relationships are more easily or variously expressed in a visual  rather than verbal form (see Gillies et al. [21]). They can also act as a tool for an individual’s identity and communication [47]. In particular  positive affect manifested in visual imagery can be indicative of an individual’s well-being  as well as provide insight into their social  cognitive  and behavioral tendencies and responses [33]. Imagery is also known to help portray stories or narratives relating to the intimate dimensions of the social—family  or one’s own body [47]. In our work  we leverage the observation that vulnerable individuals might be taking on to social media platforms to engage in mental health self-disclosure via visual imagery. We aim to examine some of the complexities typically explored through qualitative visual methods  via large-scale characterization of such mental health imagery shared on social media.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/3025453.3025932,16,5,social media imagery analysis,"Manikonda, De Choudhury",2,1,0,0,2017,Users of social media platforms are sharing large volumes of images around their daily lives  personal life events  or opinions  often in the form of personal photographs  selfies  memes  gifs and so on. Recently  a growing body of research has examined and characterized such imagery to identify sentiment and emotion [31]  societal happiness [1]  geographic landmarks [34]  abusive behaviors such as alcohol use [41]  public health challenges such as obesity [19]  and fitness [52]. Many of these works combine both text and imagery features toward the problem domain under consideration. For instance  Pang et al. [41] mined markers of underage drinking by first inferring age and gender of users from their Instagram profile pictures  and then analyzing the associated hashtags to discover the existence of drinking patterns in terms of time  frequency and location. Similarly  Abdullah et al. [1] developed a measure of population-scale happiness  known as Smile Index  by analyzing the visual cues of Instagram images  and then went on to validate it against text-only measurements as well as self-reported happiness. Another interrelated line of research [6  39] has also examined visual features present in these images  for instance  color palettes  and their relationship to social engagement. In this paper  we borrow several computational social media image analysis methodologies employed in the above body of research  in order to extract and characterize visual cues relating to mental health disclosures on Instagram.,0,0,0,0,0,0,0,0,2,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/3025453.3025932,16,6,social media and mental wellbeing,"Manikonda, De Choudhury",2,1,0,0,2017,Recently  a growing body of research has focused on understanding how large-scale social media activities can be used to understand  infer  and improve the wellbeing of people  including mental health concerns [12  13  27]. A common thread in this research is how computational techniques may be applied to naturalistic data  that people share on today’s online social platforms  to make sense of their health behaviors and related experiences. However  these works have primarily focused on the computational analysis of text and language for the purpose: including psycholinguistic analysis  topic modeling  and supervised and unsupervised language modeling. As noted earlier  many social media platforms today allow sharing of rich media objects  such as images  beyond text. We extend current stateof-the-art by examining the nature of mental health related visual cues manifested in Instagram posts. Another complementary line of research has also examined how content  primarily text  shared on social media and online communities may enable self-disclosure and help seeking  specifically toward facilitating wellbeing [17  29  18]. In this light  approaches to community building have been proposed [53]  and the role of participation and self-disclosure in such communities toward promoting health recovery and coping has been examined in domains as cancer  diabetes  and drug abuse [37  38  46]. In the mental health domain  Balani and De Choudhury [7] developed a classifier to automatically infer levels of self-disclosures in different mental health forums  whereas in [13]  the authors found that self-disclosure around a stigmatized condition like mental illness tends to be higher in platforms that allow anonymity. Close to our work are the works of Andalibi et al. ([3  4]) and Reece et al. ([44]). In the former work  Instagram images shared on #depression were analyzed through qualitative coding and did not study the visual cues of the images. In the latter work  Instagram user profile data collected through responses to a standardized clinical depression survey were utilized to reveal predictive markers of depression. However  these works did not characterize mental health disclosures facilitated uniquely by the visual modality. We employ computational methods to examine themes emerging out of the visual content of images spanning different mental health disorders. Thus our work extends this larger body of work by characterizing a form of online content  visual imagery  hitherto under-investigated in the context of mental health.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/3025453.3025932,16,7,data collection,"Manikonda, De Choudhury",2,1,0,0,2017,We utilized Instagram’s official API1 to obtain the dataset used in this paper. Each post in this dataset is public and contains post-related information  such as  the image  caption  likes  comments  hashtags  filter and geolocation  if tagged. Referring to prior literature [10]  we adopted an iterative approach to first identify a set of appropriate  distinguishing hashtags around different prominent mental illnesses prevalent in social media. With the seed tags  we performed an initial data collection of 1.5 million posts shared on Instagram between Dec 2010 and Nov 2015. Then by leveraging an association rule mining approach  we compiled the top k (k = 39  frequency ≥ 5000) co-occurring tags in the 1.5M posts  and then appended them to the original seed tag list for further data collection. Table 1 lists a sample set of tags used to crawl the dataset. This final list of 45 tags was thereafter passed on to a psychiatry researcher to be categorized into different disorder types. For tags that described experiences or symptoms crosscutting across different conditions (e.g.  “anxiety”)  they were counted toward each disorder type. Table 2 gives a list of the ten different disorders identified in our data. We additionally consulted the Diagnostic and Statistical Manual of Mental Health Disorders (DSM-V [5])  that indicates these disorders to be prominent mental health challenges in populations. This categorization of the mental health challenges was conducted to ensure that our data used in the ensuing analysis focused on well-validated and clinically recognized conditions. At the same time  it allowed us to focus on a diverse range of disorders expressed on social media  rather than specific ones studied in prior work [12  13  27]; thus enabling us to discover generalized patterns in visual disclosures of mental health challenges in social media. Our final crawl included 2 757 044 posts from 151 638 users spanning these disorders.,2,2,2,2,0,1,0,0,2,0,,0,0,0,0,0,2,,0
https://dl.acm.org/doi/abs/10.1145/3025453.3025932,16,8,data reliability,"Manikonda, De Choudhury",2,1,0,0,2017,Next  we assessed the suitability and reliability of our collected corpus of Instagram posts and users for our later analyses. For the purpose  we extracted n-grams (n=3) from the profile biographies of users. The top 10 uni-  bi- and trigrams are shown in Table 3. They show that users are appropriating Instagram to seek and provide social and emotional support around different mental health concerns (“need someone talk”  “feel free dm”). There are also explicit mentions of specific psychological challenges around mental health (“depression anxiety”  “telling suicidal kids”)  including warnings for profile visitors (“trigger warning”)  and personal experiences of the condition (“alone alone alone”). We corroborated these observations with a licensed psychiatrist  and concluded that the users in our dataset are engaging in genuine mental health disclosures  tend to demonstrate disinhibition towards sharing their mental health experiences  and are appropriating the platform specifically for this purpose via the chosen account.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,2,,0
https://dl.acm.org/doi/abs/10.1145/3025453.3025932,16,9,visual features,"Manikonda, De Choudhury",2,1,0,0,2017,Towards our first research goal RQ 1  to examine the visual features of images relating to mental health disorders  we employ the extraction of color profiles  i.e.  grayscale histograms [50]. Grayscale histograms provide us intuition about the brightness  saturation  and contrast distribution of images. In these histograms  images with high contrast pixels are binned in bins with lower numbers (near 0)  whereas images with brighter pixels are binned in higher number bins (near 255). We utilize the OpenCV library2 to extract these color histograms of images in our dataset. We also assess the visual saliency of images (using OpenCV) – a distinct subjective perceptual quality that makes some images stand out from their neighbors [24]. A typical image in our dataset is of size 612px × 612px  so by using a saliency metric  we obtain a 612 × 612 grid matrix. For each image in these three visual feature categories  we obtain an empirical threshold that ensures 1/3 rd of the pixels will be greater than this value  when sorted based on their saliency.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/3025453.3025932,16,10,visual themes,"Manikonda, De Choudhury",2,1,0,0,2017,Identifying meaningful themes from images is known to be challenging as they contain richer features compared to text [23]. To examine the themes manifested in different mental health images that is posed as RQ 2  we used a 2-step human-machine approach. The first step employed automated computer vision techniques to perform initial clustering. The second step involved human raters to refine and label the automatically generated clusters  wherein they independently reorganized the clusters to obtain coherent descriptor labels. Our human-machine methodology is motivated by two observations: Human coding can help extract semantically meaningful and contextually relevant image themes  but is difficult to scale in the face of datasets as large as ours. Automated clustering techniques can address the issue of scalability  but  on their own  may not provide reliable or meaningful themes. We describe our two step method below: Step I. In the first step  we used OpenCV to extract the Speeded Up Robust Features of the mental health images (SURF: [8]). SURF is a speeded-up local feature detector and descriptor that is good at handling images with rotation and blurring. More elaborately  the method uses a blob detector based on the Hessian matrix to find points of interest. It then develops a unique and robust description of an image feature  e.g.  by describing the intensity distribution of the pixels within the neighborhood of the point of interest. It is typically used to locate and recognize objects  people or faces  to make 3D scenes  to track objects and to extract points of interest. Thus  these are likely to be helpful in characterizing the visual attributes of our mental health image data. The extracted SURF vectors for all images are of 64 dimensions. Following the standard image vector quantization approach (i.e.  SURF feature clustering) [8]  we obtained the codebook vector for each image3 . Finally  we used the kmeans clustering algorithm (with Euclidean distance metric) to obtain 20 clusters  where we determine k in an empirical data driven manner  that improves cluster consistency. Step II. Next  with the help of two researchers familiar with mental health content on social media  the images in the 20 clusters and the affinity of themes were independently examined  so as to refine the clusters  as well as develop semantic descriptors characterizing them. The researchers adopted a semi-open coding approach  borrowing from the literature on mental health self-disclosure [30  2  11] and recent work in characterizing mental health images shared on different social media platforms [3  4]. The annotators first independently coded all of the 20 clusters. Then following mutual discussion and resolution of inconsistencies  they merged the 20 clusters and readjusted them (shifting some images to other appropriate clusters) to eventually identify six major visual themes of mental health images.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/3025453.3025932,16,11,linguistic emotions of visual themes,"Manikonda, De Choudhury",2,1,0,0,2017,Next  we employ the psycholinguistic lexicon LIWC (http://liwc.wpengine.com/) on the text associated with our mental health images spanning the different visual themes. We use the following five emotional attributes  motivated from prior work on mental health and social media [13  7] – anger  anxiety  sadness  positive affect and negative affect  and a measure of attributions to loss of life  indicated by the death category.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,2,0,,0
https://dl.acm.org/doi/abs/10.1145/3025453.3025932,16,12,linguistic themes,"Manikonda, De Choudhury",2,1,0,0,2017,Finally  to complement the visual themes (our research goal RQ 3)  we identify themes from the captions and hashtags (textual data) associated with the Instagram images in our dataset. We refer to these latent topics as linguistic themes. Existing literature [42] emphasizes the importance of studying language  since it reflects a variety of thoughts  functions as a signal of identity  and emphasizes the social distance. We believe the linguistic themes may therefore help us contrast the visual themes around how individuals engage in mental health disclosure on Instagram. We used TwitterLDA4 to extract these linguistic themes. This method was developed for topic modeling of short text corpora for mining the latent topics. As typically done in topic modeling  we pre-processed the data by removing a standard list of stop words words with very high frequency and words that occur fewer than five times since lda is an unsupervised learning approach identifying the correct number of topics is challenging we used the default hyper-parameter settings and topics which we determined based on the value of averaging corpus likelihood over ten runs these topics constituted what is known as lifted forms of linguistic vocabulary on these extracted linguistic vocabulary to arrive at interpretative descriptions we call them linguistic themes we adopted a similar semi open coding approach as the visual themes that involved the same two researchers as above the raters referred to the mental health literature and identified the best possible description that characterized the tokens in the linguistic vocabulary corresponding to each of the linguistic topics to characterize and represent each of these visual and the linguistic themes we propose a measure of visual diversity this measure estimates how coherent images are with respect to the each other in a theme to measure the diversity in terms of the latent visual features images are expressed in terms of their principal components within a theme in the component space distance between a pair of images are computed by employing the cosine theta similarity function to perform these set of operations we utilize the python scikit learn library,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,2,0,,0
https://dl.acm.org/doi/abs/10.1145/3025453.3025932,16,13,rq visual features,"Manikonda, De Choudhury",2,1,0,0,2017,Toward our first research goal we examine a variety of visual features manifested through the mental health imagery shared on instagram first we explore the types of color profiles grayscale histograms of the mental health images in figure we show histogram plots associated with three categories images of high contrast those with high saturation and those with high brightness next in table we show the proportion of posts belonging to the three color categories we observe that a large number of images across the disorders are of high saturation these images contain different types of colors however a considerable fraction does belong to the extreme ends as well high contrast and high brightness categories thus unlike prior findings were of instagram images were observed to not have dominant colors in our case we observe a contrasting pattern we further find that the high saturation mental health images have higher saliency compared to the other categories squared estimate of effect size based on a kruskal wallis test this implies that these mental health images are likely to trigger greater cognitive and perceptual stimulus to viewers further high contrast and high brightness images tend to have more hashtags attached indicating that the authors of these posts attempt to engage with the instagram audience by associating their posts with a wide range of topics and content indicators we conjecture this might be a way for the authors of these posts to increase their likelihood of discoverability and visibility on instagram,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/3025453.3025932,16,14,rq visual themes,"Manikonda, De Choudhury",2,1,0,0,2017,Next per rq we examine the types of visual thematic content present in mental health images in our instagram dataset figure gives heat map visualizations of all images that were clustered into six different visual themes the heatmaps were generated from a dimensionality reduced via principal component analysis representation of the images in each visual cluster specifically we considered each heatmap as a symmetric matrix with the row and column corresponding to two images from a given visual theme the pixel value to a given row and a column represents the visual diversity value between the two images that are marked on the axes for a given set of images visual diversity is thus computed for all possible combinations of image pairs in a visual theme alongside the heatmaps we also include the manually curated labels of the themes and their percentage representation in our instagram dataset we find that within the themes themselves there is low visual diversity as expected based on a cosine similarity metric among the visual themes social has lowest and captioned images has highest visual diversity with what kind of textual cues characterize these visual clusters in table we present the top tags associated with each of these themes we do not observe as much difference across them for instance inspection of the content of these tags include the tags we used to collect our data around the mental health disorders depression anxiety eating disorder suicide moreover tags like pian vorken lonely death appear consistently across multiple themes likely because hte content associated with the different themes relate to the topic of mental health concerns our observations can further be quantified through the high value of hte mean spearman rank correlation between the tags and their frequency ranks in each visual theme we now present a descriptive analysis of the visual themes,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/3025453.3025932,16,15,captioned images,"Manikonda, De Choudhury",2,1,0,0,2017,First we find that more than half of the mental health images contain embedded textual data we conjecture that individuals use this medium as a way to share motivational and encouraging thoughts quotes and ideas such as calls for support one such embedded text image says life is way too short to spend another day at war with yourself additionally use of the tags like warrior and dontgiveup in images of this theme indicate individuals desire to express their opinions and thoughts relating to tackling mental health challenges nevertheless we do also observe people to share confessions in this theme or as a way to converse with or reach out to an audience it is starting to hurt too much again you killed what was left of the good in me the tags associated with this theme include broken lonely help our inspection of images in this theme reveals high self focus and self preoccupation most of the embedded text are in first person singular pronouns and concern ones personal thoughts and experiences these observations align with prior work that analyzed social media language relating to mental health,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/3025453.3025932,16,16,selfie images,"Manikonda, De Choudhury",2,1,0,0,2017,Next although selfie images are observed to be a visual theme in our study they span less than a fifth of all types of mental health images on the contrary in generic instagram images prior work reports selfies to be one of the most notable types marwick found that selfies shared by instagram users show glimpses of their lives to others connect with audiences and receive instant feedback while these motivations are likely still present in the mental health communities on instagram the images in this visual theme tend to share considerable negative perspectives or signs of distress tags like ugly mentalillness anxiety fat selfharmmm these tags contrast the typical tags appearing in generic selfies further tifnetale and manovich noted that by sharing their selfies instagram users construct their identities and simultaneously express their belongingness to a certain community hence usage of the variety of mental health tags in the images of this theme might be a mechanism for individuals to find communities that relate to similar difficult to disclose experiences or as a way for them to define their identity around stigmatized conditions,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/3025453.3025932,16,17,social images,"Manikonda, De Choudhury",2,1,0,0,2017,The third largest visual theme in our dataset is found to visualize social settings in people everyday lives the post images associated with this theme all tend to have more than one human face present in the same frame this indicates that some individuals in our data may be choosing to share public information about their social contexts or their association with friends and family our conjecture stems from observing tags like happy support and people that appear frequently in images of this theme it is known that feelings of social isolation and loneliness are predominant in individuals with mental health concerns,0,0,0,0,0,0,0,0,2,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/3025453.3025932,16,18,food,"Manikonda, De Choudhury",2,1,0,0,2017,The fourth visual theme spans of the image posts and revolves around aesthetic visuals of plated food while many generic mental health tags tend to be associated with this theme we observe the presence of distinctive tags relating to dietary practices and physical health fat fitness calories some of the mental health disorders we consider in this paper relate specifically to unusual or dangerous dietary habits such as eating disorders and anorexia this may explain the presence of diet of ingestion specific tags contrastively tags like recoveryfood healthyfood highprotein plantstrong foodisfuel in the images associated with this time may indicate recovery trajectories or intentions to cope with these mental health challenges,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/3025453.3025932,16,19,physical perceptions,"Manikonda, De Choudhury",2,1,0,0,2017,Next the visual theme around physical perceptions includes content that elucidate detailed perspectives about ones own body tags in this cluster include skinny ugly thin it is known that mental health conditions like eating disorders and anorexia are associated with manifestation of a desire to be unusually skinny  by adhering to normative perceptions of body image [20]. Thus  certain individuals might be appropriating the visual communication channel of Instagram to craft  reinforce  advocate or share particular body image perceptions. This observation is further supported in the usage of various tags that illustrate injurious attitudes and beliefs about one’s body  such as “face”  “fatfatfatfat”  “notskinnyenough”  “fatty”  “overweight”.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/3025453.3025932,16,20,graphic images,"Manikonda, De Choudhury",2,1,0,0,2017,Finally  despite being a smaller share (2%)  we observe a noted visual theme of highly graphic images  wherein individuals share images of damaging their own body. Tags like “cut”  “blood”  “blades”  “bruise” uniquely appear in the images associated with this theme. While the specific intent behind the sharing of these images needs further investigation  usage of tags like “pathetic”  “empty”  “numb”  “hated” does indicate the range of self-deprecating thoughts that characterize images in this theme. The visual expressivity of Instagram may be providing individuals with an outlet to showcase and release their emotional pain [35].,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/3025453.3025932,16,21,rq b emotions manifested in visual themes,"Manikonda, De Choudhury",2,1,0,0,2017,Next  we present the expression of emotions in the mental health images spanning the six visual themes. Figure 3 summarizes the distributions of the measures of six emotions across each visual theme described above. Broadly  the different visual themes express diverse emotions. Expectedly  levels of Negative Affect (NA)  Anger  Anxiety  Sadness and Death are relatively higher in all themes  compared to Positive Affect (PA). However  we observe notable differences in how specific emotions are expressed in the different visual themes. We discuss them below: First  NA is consistently the largest emotion expressed in all the six visual themes (H(1433663  6) = 5.7; p < .001 based on a Kruskal Wallis test). It is highest in the Graphic Images visual theme (+8.7%)  followed by Captioned Images (+8.0%). We note similar trends for Sadness; it is higher by +28% in the Captioned Images theme  compared to others. As observed earlier (also see Table 6)  the images associated with the Captioned Images tend to act as an outlet of deepseated feelings and emotional distress—this can explain the high measures of NA and Sadness in it. Anxiety is the highest in the Social theme (+60%; H(1433663  6) = 6.4; p < .001); its second highest value is observed for the Graphic Images theme (+48%). Since per Table 6  many of the tags associated with Graphic Images relate to self-injurious behaviors known to be commonly associated with anxiety challenges [35]  we see that manifested via the Anxiety measure. Next  we find that Anger is highest in the Food and Social themes (+18% and +12% respectively; H(1433663  6) = 2.7; p < .01). Recall that our data consists of images associated with the topics of eating disorders and anorexia; hence the manifested anger in the Food theme may indicate selfconflicting thoughts about diet and food. On the other hand  high Anger in the Social theme may be attributed to limited access to social support; an aspect that characterizes many mental health related content on social media [14]. Somewhat surprisingly  we observe that the Food theme also includes the highest manifestation of PA (+108%; H(1433663  6) = 10.5; p < .0001). This shows that  for some individuals  sharing Food related content may relate to a desire to adopt healthy and functional dietary habits  and positive perspectives towards physical health. Moreover  many individuals in mental health recovery tend to share diet images as a way to identify with this behavior change process (ref. tags in Table 6). This might also be the underlying reason behind high PA. Next  PA is lowest in the Graphic Images (-52%; H(1433663  6) = −5.9; p < .001). Due to the large volume of images in the Graphic Images theme relating to deliberate harm to one’s bodies  the emotion expressed in these images tends to be of largely negative tonality (and thus low PA). Finally the theme of Physical Perceptions stands out from the rest of the themes with respect to the expression of Death related emotions (+50%; H(1433663  6) = 6.2; p < .001).,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/3025453.3025932,16,22,rq linguistic themes,"Manikonda, De Choudhury",2,1,0,0,2017,As a final analysis (per RQ 3)  we present and contrast the observations gleaned from the visual themes and their emotions  with linguistic themes obtained from the same set of mental health images. The extracted 10 linguistic themes and their associated vocabulary is presented in Table 8. All of the linguistic themes are highly semantically coherent within themselves  as noted from the themes’ annotations in Table 8. Further  none of the linguistic themes overlap conceptually with any of the visual themes  as noted in the human annotations. In fact  mean Spearman rank correlation between the top 100 tags of each linguistic and visual theme is only .14 (p < .01)  indicating that the sets of themes provide complementary perspective in understanding the different mental health disclosures of individuals on Instagram. Going deeper into specific linguistic themes  we notice two themes (7 and 8) specifically expressing positive and negative emotion respectively. Example tags for the two themes include “sadness”  “emo”  “emogirl” and “good”  “happy”  “fun”  “beautiful” respectively. Expectedly  two themes (2 and 5) relate to specific mental health challenges  ranging from anorexia and self-harm (tags like “blithe”  “selfhate”  “anorexia”) to expressions of suicidality (“cutting”  “worthless”  “killme”). (Table 7). At the same time  we find the presence of a few linguistic themes that do not particularly relate to mental health issues. For instance  theme 6 spans content shared with the typical Instagram audience [28] (note tags like “instadaily”  “smile”  “bestoftheday”  “instamood”  “selfie”  “tagsforlikes”). Another example is theme 4 that expresses feelings and thoughts around everyday activities and experiences (example tags include “life”  “people”  “today”  “good”  hope”  school”). Together  these themes indicate that despite primarily maintaining mental health focused accounts on Instagram (ref. Table 3)  certain individuals do involve themselves in generic discourse as well. Again  this is in contrast to the visual themes  where we observed some mental health challenge manifested in every theme. Finally  we find two linguistic themes that relate specifically to more uplifting content  such as relating to fitness (theme 3) and mental health recovery (theme 10). The former consists of tags like “workout”  “healthy”  “support”  “motivation”  “gym”  “exercise” and “mentalhealthawareness”. This indicates that the posts associated with this theme encourage and promote behaviors around improved physical health  known to bear links to better mental well-being [49]. Theme 10 includes majority of content around recovery from eating disorder behaviors  as observed through tags like “anorexiarecovery”  “eatingdisorderrecovery” and “staystrong”. By sharing the posts associated with this theme  individuals may be aiming to seek and provide emotional support  or to share their personal stories and experiences. Further inspection reveals that some of the posts associated with these two themes (3 and 10) tend to also be generated by a range of mental health support groups on Instagram. We note that such recovery related information was not discoverable through the visual themes.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/3025453.3025932,16,23,relationship of findings to visual sociology,"Manikonda, De Choudhury",2,1,0,0,2017,Our work indicates the adoption of the visual modality of photo-sharing social media platforms for mental health disclosure. In fact  many of the shared mental health images bear specific visual signatures  such as with high brightness or high contrast pixels. To explain this finding  we draw on Berger [25]: “black-and-white photography is paradoxically more evocative than colour photography. It stimulates a faster onrush of memories because less has been given  more has been left out”. Individuals might be choosing these minimalist visual techniques to draw attention to their psychological state. The specific visual signatures may also indicate that the individuals want their emotions and experiences to be visible to others [45]  beyond linguistic descriptions  although displaying these emotions can make them susceptible to both judgment and encouragement. Further  through the analysis of visual themes  we found that images with a variety of distinct visual cues serve as a vehicle of expression of distress  helplessness and social isolation to certain individuals. From the theme “Physical Perceptions”  we can learn that shared visual imagery on Instagram may be allowing some individuals to seek feedback on atypical perceptions of their own physical image [20]. Further  we observed the use of imagery in sharing graphic content (theme: “Graphic Images”). Research identifies many underlying reasons behind such physically damaging graphic expression  such as normalization of behavior as a way to deal with emotional distress [9]. At the same time  we observed mental health images on Instagram also being mobilized to seek and provide psychosocial support and as “safety valves” [21]. This is observable in the theme Captioned Images  that includes explicit calls for help. Goffman [22] posited the desire of individuals with socially stigmatized experiences to look for “sympathetic others”. Adopting the visual modality  individuals may be intending to bond around mental health topics. One of our less expected findings is that the visual and the linguistic themes were considerably distinct. These differences can be ascribed to the ways that the themes capture not just different forms of mental health disclosures on Instagram. They also capture the disinhibiting nature of people’s discourse with their audiences  as well as in expressing aspects of their experiences that may not be easily communicated via either of the modalities. For instance  the presence of the visual themes  Graphic Images and Physical Perceptions indicates that individuals are taking to the photo-sharing affordance of Instagram as a way for emotional release around a distressful experience. As Keltner noted  such tendencies of emotional expression via the visual modality are a known attribute of many mental health sufferers [33]. At the same time  linguistic themes provide us with contextual groupings around the shared visual imagery. We found the presence of a linguistic theme around mental health awareness and recovery  and others around specific positive and negative emotions. Together  the two modalities provide us a comprehensive picture of the characteristics of social media based mental health disclosure practices.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/3025453.3025932,16,24,implications for hci and design,"Manikonda, De Choudhury",2,1,0,0,2017,An important goal of this paper has been to open up new discussions in the social media and mental health research communities about the role of image-sharing behaviors on social media in addressing mental health challenges. Can we develop mechanisms that can sense  based on one’s shared visual imagery  their vulnerability  and extend timely  tailored and helpful support to those in need? We discuss HCI and design implications relating to intervention tool development  technologies for emotional self-reflection  and capabilities that enable access to social and emotional support  in the light of mental health challenges.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/3025453.3025932,16,25,semi automated intervention tools,"Manikonda, De Choudhury",2,1,0,0,2017,  or automated tools that use domain expert help  can be built leveraging our visual theme extraction method. These tools can trigger a warning  in a privacy-honoring way  to individuals when imagery with visual signatures related to unusual physical and mental vulnerability are shared. This can include imagery relating to the themes “Physical Perceptions” or “Graphic Images”  that contained many vulnerable tags (“selfharmmm”)  and expressed high negative emotion. Note that the role of visual cues is critical here  since the usage of linguistic cues alone may not reveal the nuances of one’s mental health disclosure—the tag “depression” can appear in a variety of posts  ranging from the na¨ıve to those that can describe potentially dangerous behaviors. In fact  combining the characteristics of the visual and linguistic cues  psychologists can assess the gravity or severity of the mental health disclosures made on social media platforms  including understanding their temporal trends in the larger community. Although Instagram and other social media platforms have put in place some intervention policies to bring help to those users who engage in mental health disclosure  at best  they can be called “blanket” strategies. This is because the interventions are neither tailored to the individual or the context  nor do they leverage nuanced and subtle cues manifested in shared content. For instance  Instagram bans certain mental health tags (e.g.  ‘suicide’  ‘thinspiration’)  whereas Tumblr issues a public service announcement for all searches on a set of terms (e.g.  “depressed”  ‘proana’). Our methods can help improve such efforts by discovering  analyzing  and characterizing the diverse range of information shared in visual imagery  aside from textual data.,0,0,0,0,0,0,0,0,2,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/3025453.3025932,16,26,technologies for self relflection,"Manikonda, De Choudhury",2,1,0,0,2017,Leveraging our methods of characterizing visual and linguistic attributes of mental health disclosures  we believe that social media platforms can provide individuals with capabilities for emotional selfreflection. These capabilities can include personal visualizations and displays: individuals can analyze historical trends of the different visual themes manifested through their shared social media content  and associated linguistic constructs. For instance  temporal patterns of themes like “Physical Perceptions”  “Graphic Images”  or “Selfies” can be shown to end users  alongside the associated tags and the expressed levels of PA  NA  Anger  Anxiety  Sadness and Death  derived via our emotion extraction method. Interpretable summaries of the visual features of shared images can also be included in these self-reflection enabling systems—such as  color or saliency based information that compare one’s social media visual signature with typical Instagram content. Those intending to cope with or manage mental health challenges can especially benefit from such self-awareness.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/3025453.3025932,16,27,capabilities to avail social and emotional support,"Manikonda, De Choudhury",2,1,0,0,2017,Exclusive mechanisms to seek support around mental health issues can also be developed utilizing our visual and linguistic content characterization framework. Individuals who engage in consistent sharing of imagery with negative body image perceptions  graphic images or content associated with extreme negative emotion can be algorithmically recommended to access recovery related content shared on the same platform  that may be residing outside of their “echo chambers”. Additionally  pointers to help resources can be incorporated  such as ways to avail online therapy  pop-ups to reach out to a friend  or a self-care expert.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/3025453.3025932,16,28,ethical considerations,"Manikonda, De Choudhury",2,1,0,0,2017,Due to the sensitivities around the topic of investigation in this paper  there are many important ethical implications to consider. For this work  we used public posts shared on Instagram  and we did not have any interaction with the users. Therefore our work did not qualify for approval from the relevant Institutional Review Board. Nevertheless  we acknowledge that employing our proposed methods and approaches in the design of the above outlined interventions presents some ethical challenges. How can these automated approaches  that are themselves prone to errors  be made to act fairly  as well as secure one’s privacy  their rights on the platforms  and their freedom of speech? Further  how can we address potential risks of these automated approaches misinterpreting and misrepresenting any of the shared visual or linguistic cues? To address these ethical challenges we propose the following guidelines to be incorporated in the design and deployment of the above proposed interventions and tools: a) Seeking voluntary consent from the population being studied and those likely to benefit from the technologies. b) Partnership with a trained domain expert  e.g.  a clinical psychologist or a psychiatrist so as to ensure that the tools bear potential to extend help and support to individuals engaging in significant mental health disclosures. c) Including extensive privacy and security protocols to protect the individuals being studied  starting from collection of social media data  to its analysis and modeling  and then during the development of the interventions and tools. And d) Adoption of user centered design approaches in intervention and technology development  to investigate specific needs and constraints of the target users  as well as their acceptability  utility  and interpretability.,0,0,0,0,0,0,0,0,2,2,,0,0,0,0,0,0,,2
https://dl.acm.org/doi/abs/10.1145/3025453.3025932,16,29,limitations and future work,"Manikonda, De Choudhury",2,1,0,0,2017,We acknowledge that there are some limitations to our work  as the analysis and the inferences obtained are purely data driven and relied on public posts shared on Instagram around mental health challenges. Specifically  to obtain disclosures of mental health issues  we utilized tags attached to posts. We presume self-selection biases in users who make public posts and link them to different mental health hashtags. We caution against applying our methods to arbitrary contexts. Moreover  although users might be voluntarily relating themselves to one of the mental health disorder categories  it is unclear to what extent this constitutes an online identity construction activity. Importantly  it is challenging to assess the gravity of a given user’s health condition using these posts or images alone  or more specifically if they are actually experiencing a clinical mental health concern. On a related note  although the tags we employed for obtaining our mental health data were verified through consultation with a licensed psychiatrist  we do not claim our methods reveal symptoms or diagnostic markers of mental illness in individuals. Therefore the methods we developed in this paper were not evaluated for their effectiveness in discovering mental health concerns  but rather as a principled and quantifiable way to understand the nature of mental health disclosures shared on social media. Putting it together  our findings should not be interpreted to be diagnostic claims about one’s mental health. To do so  we advocate for collaboration between clinicians and HCI researchers  along with voluntarily consenting patients. This constitutes one of our future research directions. Finally  qualitative methods would lend a deeper understanding of the motivations behind appropriating a public social media outlet for vulnerable and sensitive exchange. We also emphasize that the analyzed visual and linguistic patterns on Instagram are not the only patterns that can help study selfdisclosure. Indeed  current advancements in machine learning approaches like deep learning [36] have created a new thread of research in image classification  where image classification problems once difficult to solve  can now be solved very efficiently. We believe such methods can be incorporated to study mental health imagery shared on social media.,0,0,0,0,0,0,0,0,2,0,,2,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/3025453.3025932,16,30,conclusion,"Manikonda, De Choudhury",2,1,0,0,2017,In this paper  we presented one of the first quantitative analyses of visual imagery shared on the photo-sharing social media Instagram around a variety of mental health challenges. We characterized different forms of self-disclosure as enabled via the visual imagery medium  and contrasted them with that enabled via linguistic expression. We found that individuals were appropriating photo-sharing affordances of Instagram to vent their discontentment around mental health challenges  seek support  and to disclose sensitive and vulnerable information about their emotional distress. We believe our approach and findings can influence the design of new health interventions that leverage the rich information embedded in visual imagery of mental health disclosures.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://www.sciencedirect.com/science/article/pii/S0747563215300996,17,1,abstract,"Cavazos-Rehg, Krauss, Sowles, Connolly, Rosas, Bharadwaj, Bierut",7,0,0,0,2016,This study examines depression-related chatter on Twitter to glean insight into social networking about mental health. We assessed themes of a random sample (n = 2000) of depression-related tweets (sent 4–11 to 5-4-14). Tweets were coded for expression of DSM-5 symptoms for Major Depressive Disorder (MDD). Supportive or helpful tweets about depression was the most common theme (n = 787  40%)  closely followed by disclosing feelings of depression (n = 625; 32%). Two-thirds of tweets revealed one or more symptoms for the diagnosis of MDD and/or communicated thoughts or ideas that were consistent with struggles with depression after accounting for tweets that mentioned depression trivially. Health professionals can use our findings to tailor and target prevention and awareness messages to those Twitter users in need.,0,0,0,0,2,0,0,0,0,0,,0,0,0,0,0,0,,0
https://www.sciencedirect.com/science/article/pii/S0747563215300996,17,2,introduction,"Cavazos-Rehg, Krauss, Sowles, Connolly, Rosas, Bharadwaj, Bierut",7,0,0,0,2016,The use of social media platforms has steadily risen over the past decade  and recent surveys estimate that 90% of online adolescents and young adults in the United States use some kind of social networking site (Duggan & Smith  2013). One of the most popular social networking sites is Twitter with 19% of online American adults using its platform (Duggan & Smith  2013). In fact  35% of internet users of ages 18–29 use Twitter  and American teenagers named it the “most important social media network” in a 2013 market research survey (Brenner  2013  Edwards  2013). Additionally  latest data from a 2014 survey from the Pew Research Center found that 23% of online adults currently use Twitter  which is a 5% increase from 2013 (Duggan  Ellison  Lampe  Lenhart  & Madden  2015). Twitter is a microblogging platform where users create short updates that are less than 140 characters. These “tweets” are then viewed by a network of “followers” that choose to follow the user's account (Marwick  2011). Because of the ease with which Twitter allows users to connect with a large audience of acquaintances and strangers  its popularity has grown  especially with teens and young adults (Brenner  2013  De Cristofaro et al.  2012). In fact  Twitter contrasts with other popular social media platforms like Facebook  because Twitter users tend to keep their posts public while Facebook profiles often use privacy settings. Adults between the ages of 55–64 are now the fastest growing demographic on Facebook (Tappin  2014) and  consequently  it is considered by many teens to have a prominent adult presence (Madden et al.  2013). In contrast  most of the Twitter users are young (66% are 25 years old and under) (Bennett  2014  De Cristofaro et al.  2012). In terms of the demographic specifics among teen Twitter users  Twitter is more popular among girls versus boys (37% versus 30%). Additionally  African American teens use Twitter to a higher degree than Caucasian counterparts (45% versus 31%) (Lenhart et al.  2015). Given that there is less interaction or oversight from older adults/parents  in general on Twitter  young Twitter users can feel free to openly communicate with friends and acquaintances about virtually anything  even topics as traditionally private as mental illness. Depression tends to be a stigmatized condition (Eisenberg  Downs  Golberstein  & Zivin  2009) and many people consider the struggle of such symptoms to be “private matters” and/or choose not to seek treatment because they do not want to be labeled as a psychiatric patient (Bland et al.  1997  Dew et al.  1988  Sirey et al.  2001). Moreover  self-harm  defined as intentional self-poisoning or self-injury  irrespective of type of motive or the extent of suicidal intent  is a grave public health problem among young people (Hawton  Saunders  & O'Connor  2012). Thoughts about self-harm are also a risk factor for suicide and are strongly associated with mental illnesses  especially Major Depressive Disorder (MDD) (Evans et al.  2004  Kessler et al.  2005  Pfaff and Almeida  2004). Several studies have examined references to depression  self-harm  and suicidality on social media in an effort to better understand the information being shared and discussed  however the research on this topic is still in its infancy. For instance  existing research has identified that posts about stress and depressive symptoms are common on Facebook profiles (Egan and Moreno  2011  Moreno et al.  2011). In addition  an individual's social media posts about feeling depressed corresponded well with the depression symptoms they self-reported on a depression screening tool (Moreno et al.  2012). Studies focused on depression-related tweets have been relatively few in number but nonetheless signal that such risk factors as suicidality  self-harm  and depression are posted on Twitter. For instance  De Choudhury  Gamon  Counts  and Horvitz (2013) studied the tweets of individuals who had been diagnosed with MDD according to self-report responses using the Center for Epidemiologic Studies Depression Scale (CES-D). Based on their retrospective study evaluating tweets for one year prior to the reported onset of MDD  the researchers detected lowered social activity  greater negative emotion  high self-attentional focus  increased relational and medicinal concerns  and heightened religious thoughts among individuals who scored positively for depression when compared against individuals who scored negatively for depression using ratings from the CES-D. In a related study  researchers examined the keyword “depression” on Twitter for two months in 2009 and yielded 20 000 tweets; the researchers found initial evidence that individuals tweet about their depression and even disclosed updates about their mental health treatment on Twitter (Park  Cha  & Cha  2012). Likewise  a case study of tweets posted by a Twitter user prior to committing suicide found that suggestions of suicide were noted in the individual's tweets immediately prior to the suicide occurring (Gunn & Lester  2012). Another related study found associations between clusters of at-risk suicide Twitter conversations and increased prevalence of geographic-specific suicide rates reported in traditional data sources (Jashinsky et al.  2015). Lastly  a study of Tweeters in Japan found that self-reported lifetime suicide attempts were associated with tweets expressing suicidality (Sueki  2015). There is still much to learn about the content of depression-related tweets. Symptoms of depression have been observed in Facebook posts (Egan and Moreno  2011  Moreno et al.  2011); however  it is unknown if in some cases  clinical symptoms of depression can be identified in tweets. At the other extreme  while there is some indication that Tweeters provide encouragement to individuals with depression (Park et al.  2012)  the extent of supportive tweets (i.e.  educational and prevention messages) about depression is likewise unclear even though it is quite possible that Tweeters seek out or post supportive comments or advice about depression on Twitter. Furthermore  as stated in a recent editorial published in this journal  the need for researchers to develop a deeper understanding of this type of cyberbehavior has never been more timely or more important (Guitton  2014). In response  the present study answers the following research questions: What are the most common themes of depression-related chatter on Twitter and how well do some tweets correspond with clinical symptoms of depression? In addition  what are the demographic characteristics of Twitter users posting this type of content? In addressing these research aims  our exploratory study gleans insight into the conversations and social networking that is occurring about depression on Twitter  and who is participating in these conversations  which offers a unique contribution to an emerging field of research.,0,0,0,0,0,0,2,0,2,0,,0,0,0,0,0,0,,0
https://www.sciencedirect.com/science/article/pii/S0747563215300996,17,3,methods,"Cavazos-Rehg, Krauss, Sowles, Connolly, Rosas, Bharadwaj, Bierut",7,0,0,0,2016,The Twitter data in the current study is public. The study protocol was approved by the Washington University Institutional Review Board.,0,0,0,0,0,0,0,0,2,2,,0,0,0,0,0,0,,0
https://www.sciencedirect.com/science/article/pii/S0747563215300996,17,4,tweets related to depression,"Cavazos-Rehg, Krauss, Sowles, Connolly, Rosas, Bharadwaj, Bierut",7,0,0,0,2016,Tweets about depression were collected by Simply Measured  a company that specializes in social media measurement and analytics (Simply Measured  2014). Simply Measured has access to the Twitter “firehose” (or full volume of tweets) via Gnip  a licensed company that can retrieve the full Twitter data stream. All tweets in the English language that contained at least either “depressed”  “#depressed”  “depression ” or “#depression” were collected between April 11 and May 4  2014. We scanned a random sample of the tweets to identify common phrases that included our keywords of interest but were not about mental health. In SAS version 9.3 (SAS Institute  Inc.  Cary  NC)  we used the index function  which searches a character expression (in this case  the text of the tweet) for a specific string of characters  to locate and remove such tweets from our sample. We removed tweets that included the following terms  regardless of capitalization: “Great Depression”  “economic depression”  “during the depression”  “depression era”  “tropical depression”  and “depressed real estate”. ​​The popularity and influence of the Tweeters was described using the distribution of followers and Klout Scores. While number of followers is a measure of popularity  Klout Score is a measure of influence. Klout Scores range from 0 to 100  with a higher score indicating higher influence. Klout Score is calculated based on an algorithm that considers over 400 signals from eight different online networks. Examples of signals include the amount of retweets a person generates in relation to the amount of tweets shared and the amount of engagement a user drives from unique individuals (e.g.  lots of retweets from different individuals as opposed to lots of retweets from one person) (Klout Inc.  2014).,1,2,2,2,2,1,0,0,0,0,,0,0,0,0,0,0,,0
https://www.sciencedirect.com/science/article/pii/S0747563215300996,17,5,themes,"Cavazos-Rehg, Krauss, Sowles, Connolly, Rosas, Bharadwaj, Bierut",7,0,0,0,2016,Using the SAS surveyselect procedure  we selected a simple random sample of 2000 tweets (that were not direct @replies) from the total volume of depression-related tweets. Two members of the research team  each with graduate level degrees (i.e. Ph.D. and M.P.H) and more than 10 years of experience in mental health research  scanned approximately 300 random tweets in order to determine their most common themes and generate a codebook. We defined themes as topics that occur and reoccur (Ryan & Bernard  2003). Tweets were coded for presence of the following themes: 1) Tweeter discloses feelings of depression; 2) Tweet is a supportive or helpful message about depression; 3) Tweeter discloses feeling school or work-related pressures related to depression; 4) Tweeter engages in substance use to deal with depression; and 5) Tweeter discloses self-harm or suicidal thoughts. Tweets could be assigned as many themes as were pertinent. For tweets where the user indicated that he or she was feeling depressed  those that appeared trivial (i.e.  not concerning) were identified. These were tweets where the depression terms were used casually or in a humorous manner or referenced depression caused by trivial things  such as being depressed after finishing a good book  seeing a concert  or watching a sad movie. In addition  we ascertained the source of the tweet by viewing the Tweeter's profile picture and studying their Twitter handle name. The source was then coded into one of the three following categories: clinician/therapist  health-focused handle (e.g.  health/government organization  handle focused on healthy lifestyle  etc.)  or regular person or other (handle did not fall into the above categories). Using this codebook  the 2000 randomly sampled tweets were coded in teams of two trained student research interns who coded the tweets together  discussing each tweet and coming to an agreement on the final assigned codes. A sample of 150 tweets was also coded by a senior team member (Ph.D. clinician) with extensive mental health research experience. Inter-coder reliability for each theme was as follows: 1) Tweeter discloses feelings of depression: percent agreement 85%  kappa 0.67; 2) Tweet is a supportive or helpful message about depression: percent agreement 85%  kappa 0.70; 3) Tweeter discloses feeling school or work-related pressures related to depression: percent agreement 98%  kappa 0.72; 4) Tweeter engages in substance use to deal with depression: percent agreement 100%  kappa 1.0; 5) Tweeter discloses self-harm or suicidal thoughts: percent agreement 98%  kappa 0.56; 6) trivial disclosures of depression: percent agreement 90%  kappa 0.70; 7) source of Tweet: percent agreement 91%  kappa 0.51. Because both prevalence of and kappa for diminished ability to think/concentrate were low  we chose not to report on this code.,0,0,0,0,2,0,0,0,0,0,,0,0,0,0,0,0,,0
https://www.sciencedirect.com/science/article/pii/S0747563215300996,17,6,presence of depression symptoms,"Cavazos-Rehg, Krauss, Sowles, Connolly, Rosas, Bharadwaj, Bierut",7,0,0,0,2016,Tweets where the Tweeter expressed feelings of depression  and were not deemed trivial  were further coded to identify whether symptoms of depression were also expressed. Symptoms of MDD  according to Diagnostic and Statistical Manual of Mental Disorders  Fifth Edition (DSM-5)  included depressed mood or irritable most of the day  nearly every day  decreased interest or pleasure in most activities  significant weight change or change in appetite  change in sleep  psychomotor agitation or retardation  fatigue or loss of energy  guilt or worthlessness  diminished ability to think or concentrate or indecisiveness  and self-harm/suicidality (American Psychiatric Association [APA]  2013). The research team also coded tweets that disclosed thoughts about self-harm as endorsing a symptom of depression. Because most Tweeters did not specify the length of time that they had been experiencing a symptom  we did not use the criteria of “nearly every day” for the symptoms. For example  if the Tweeter mentioned feeling decreased interest or pleasure in most activities  we coded this Tweet as endorsing a symptom of depression even though it was not clear if the Tweeter had been experiencing this symptom “nearly every day”. However  when only general feelings of depression were expressed (i.e.  depressed mood or irritability)  we did take the length of the depressed mood into consideration in order to distinguish the depression-related tweets that described a chronic or long-term struggle with depression. Specifically  we differentiated between tweets that provided no information about a length of time for the depressed mood (e.g. “I'm depressed”) from those gle that indicated that the Tweeter was feeling depressed for an extended length of time such as “I'm tired of being depressed all the time.” Psychomotor agitation or retardation must be objective (i.e.  observable by others) and was therefore not coded. Two team members with graduate degrees (Ph.D. and M.P.H.) and expertise in mental health research separately coded each of the tweets for symptoms of MDD symptomatology and/or self-harm. First the presence of any symptom was coded  with good agreement (percent agreement 87%; kappa 0.65). The team members then discussed the remaining tweets where there was disagreement to come to a consensus on whether any symptoms were present. Then the team members further reviewed the tweets where at least one symptom was present in order to code the specific symptoms present in the tweets. Inter-coder agreement for each symptom was as follows: 1) depressed mood or irritable: percent agreement 82%  kappa 0.62; 2) decreased interest or pleasure in most activities: percent agreement 97%  kappa 0.65; 3) significant weight change or change in appetite: percent agreement 100%  kappa 1.0; 4) change in sleep: percent agreement 98%  kappa 0.76; 5) fatigue or loss of energy: percent agreement 98%  kappa 0.83; 6) guilt or worthlessness: percent agreement 89%  kappa 0.69; 7) self-harm: percent agreement 97%  kappa 0.73; 8) suicidality: percent agreement 98%  kappa 0.89.; agreement was good (median percent agreement across specific symptoms 97%  range 82%–100%; median kappa 0.71  range 0.32–1.0). We chose not to report on diminished ability to think/concentrate in our results because both the prevalence of and kappa for this symptom were low. Any disagreements were then discussed in detail by the two researchers to determine whether the symptom was expressed in the tweet and a consensus reached.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://www.sciencedirect.com/science/article/pii/S0747563215300996,17,7,demographic characteristics,"Cavazos-Rehg, Krauss, Sowles, Connolly, Rosas, Bharadwaj, Bierut",7,0,0,0,2016,Demographics Pro was used to infer the demographic characteristics of the individuals who disclosed feeling depressed in the tweets that were examined in the present study (Demographics Pro for Twitter  2014). The demographic characteristics examined include: age  gender  race/ethnicity  marital status  income  occupation  and location of the Tweeter. In order to assess for patterns in Twitter interests the most likely Twitter handles that are being followed by the Tweeters were also provided. Demographics Pro uses a series of proprietary algorithms to estimate or infer likely demographic characteristics of Twitter handles based on Twitter behavior/usage. Their predictions rely on multiple data signals from networks (signals imparted by the nature and strength of ties between individuals on Twitter)  consumption (consumption of information on Twitter revealed by accounts followed and real-world consumption revealed by Twitter usage)  and language (words and phrased used in tweets and bios). Demographics Pro has used their methodology to profile some 300 million Twitter users to date and requires confidence of 95% or above to make an estimate of a single demographic characteristic. For example  if 10 000 predictions are made  9500 would need to be correct in order to accept the methodology used to make the prediction. The success of the Demographics Pro analytic predictions relies on the relatively low covariance of multiple amplified signals. Iterative evaluation testing the methodologies on training sets of established samples of Twitter users with verified demographics allows the calibration of balance between depth of coverage (the number of demographic predictions made) and required accuracy. The size of these established samples of Twitter users with verified demographics varies from 10 000 to 200 000 people depending on the specific demographic characteristic to be inferred. For comparison purposes  inferred demographic characteristics across a sample of 20 000 randomly selected English-language Twitter users in the U.S. and Canada were also provided by Demographics Pro. We used Pearson chi square tests to compare the inferred characteristics of Tweeters from our sample who expressed feelings of depression versus the typical Twitter user. P < .05 was considered statistically significant.,0,0,0,0,2,0,2,0,0,0,,0,0,0,0,0,0,,0
https://www.sciencedirect.com/science/article/pii/S0747563215300996,17,8,results,"Cavazos-Rehg, Krauss, Sowles, Connolly, Rosas, Bharadwaj, Bierut",7,0,0,0,2016,From April 11 to May 4  2014  approximately 12 000 000 000 tweets were posted on Twitter (Simply Measured  2014) and of this full stream of Twitter data  a total of 1 562 941 depression-related tweets were collected using our keywords of interest. The median number of followers of the tweets was 335 (inter-quartile range 137–825) and the median Klout score (or measure of influence) was 39.4 (inter-quartile range 30.5–44.2).,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://www.sciencedirect.com/science/article/pii/S0747563215300996,17,9,themes,"Cavazos-Rehg, Krauss, Sowles, Connolly, Rosas, Bharadwaj, Bierut",7,0,0,0,2016,The 2000 randomly sampled tweets had similar follower and Klout distribution as the full sample (follower's median 335  inter-quartile range 139–821; Klout score median 38.4  inter-quartile range 29.8–43.9). Of the 2000 randomly sampled tweets  22 (1%) were not about depression in humans and were thus excluded from qualitative analysis. Of the 1978 tweets that were about depression  97% (n = 1910) were from unique Twitter handles. Themes identified in the 1978 randomly sampled tweets are presented in Table 1. Supportive or helpful tweets about depression was the most common theme (n = 787  40%) and included such messages as how to prevent depression  supportive or inspirational quotes for individuals struggling with depression  and how to help loved ones who are depressed. Nearly 1/3 of the tweets were coded by our research team as being from individuals who disclosed feeling depressed (n = 625  32%). All of the tweets expressing feelings of depression were from unique Twitter handles. The other themes identified were much less common. School or work-related pressures were mentioned in 3% of the tweets (n = 54) and substance use in the context of feeling depressed was found in 1% of the tweets (n = 26).,0,0,0,0,2,0,0,0,0,0,,0,0,0,0,0,0,,0
https://www.sciencedirect.com/science/article/pii/S0747563215300996,17,10,source of tweets,"Cavazos-Rehg, Krauss, Sowles, Connolly, Rosas, Bharadwaj, Bierut",7,0,0,0,2016,Most of the tweets (n = 1 806  92%) were from ordinary people or other Twitter accounts that did not fall into the above categories. Only 6% (n = 112) of the tweets were from health focused handles such as health or government organizations or handles promoting healthy lifestyles (e.g.  Doctor's Nutrition @drsnutritionetx  Natural Health @naturalhealth92). In addition  only 3% (n = 60) were from clinicians or therapists.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://www.sciencedirect.com/science/article/pii/S0747563215300996,17,11,symptoms of depression conveyed in tweets,"Cavazos-Rehg, Krauss, Sowles, Connolly, Rosas, Bharadwaj, Bierut",7,0,0,0,2016,​​From the 625 tweets that disclosed feelings of depression  we identified 419 tweets where the Tweeter reported struggles that were consistent with depression (versus using “depression” or “depressed” in a trivial and/or humorous way; n = 206). The 419 tweets were further coded for the presence of one or more DSM-5 symptom(s) of MDD or indications of self-harm. Of the 419 tweets  we identified 125 (30%) that included ≥1 symptom of MDD or self-harm. A flowchart of our study is provided in Fig. 1 and examples of such tweets that included ≥1 symptom of MDD or self-harm are provided in Table 2. The most common DSM-5 symptoms of MDD that were disclosed within a tweet were feeling depressed mood or irritable (n = 73/125  58%) and guilt or worthlessness (n = 30/125  24%). MDD symptoms that were endorsed less frequently (<10%) included self-harm (n = 10/125  8%)  contemplating suicide or expressing a desire for death (n = 10/125  8%)  decreased interest or pleasure in most activities (n = 9/125  7%)  fatigue or loss of energy (n = 9/125  7%)  change in sleep (n = 8/125  6%)  and significant weight change or change in appetite (n = 1/125  <1%).,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://www.sciencedirect.com/science/article/pii/S0747563215300996,17,12,demographics of twitter users,"Cavazos-Rehg, Krauss, Sowles, Connolly, Rosas, Bharadwaj, Bierut",7,0,0,0,2016,Tweets where individuals expressed feelings of depression (and were not deemed trivial) was a popular theme of the tweets in our random sample and given the potential clinical implications associated with these social media messages  our research team had interest in characterizing the demographic characteristics of these individuals. Accordingly  Table 3 presents the inferred characteristics of Twitter users who tweeted about feeling depressed (n = 419) compared to the inferred demographics of a random sample of 20 000 Tweeters in the US and Canada. Those tweeting about feeling depressed tended to be ≤ 25 years old (94%)  which is similar to the typical Twitter user (94% ≤ 25 years old)  but the distribution of ages under the age of 25 appeared to differ between the two groups (see Table 3). The vast majority of individuals who tweeted about feeling depressed were girls/women (77%) in comparison to the typical Twitter user (58%). Most of the individuals who tweeted about feeling depressed were inferred to be Caucasian (52%). This prevalence is lower when compared against the typical Twitter user (64% Caucasian). More tweeters who disclosed feeling depressed were African American than the typical Twitter user (35% versus 24%).,0,0,0,0,2,0,2,0,0,0,,0,0,0,0,0,0,,0
https://www.sciencedirect.com/science/article/pii/S0747563215300996,17,13,discussion,"Cavazos-Rehg, Krauss, Sowles, Connolly, Rosas, Bharadwaj, Bierut",7,0,0,0,2016,The present study examined the depression-related chatter on Twitter over the course of about one month (April 11 to May 4  2014) in order to glean insight into the conversation and social networking that is occurring about depression on this popular social media platform. Over this time period  over 1.5 million tweets were collected (over 70 000 per day)  showing the considerable presence of social networking about depression on Twitter. We additionally studied the content of a random sample of these tweets  and inferred within our research team that the most common theme was supportive or helpful tweets about depression. Past studies have found that the internet is widely used for information seeking and/or to engage in peer support exchanges for many health-related issues including depression (Moreno et al.  2011  Moreno et al.  2012). Our findings add to this emerging field of research by identifying Twitter as a social media platform where individuals likewise deliver and/or garner support or helpful advice about depression. We do not know the extent to which supportive/helpful tweets help individuals to cope with depression but we do see this as an important next step. Another common theme that we identified was that the Tweeter was disclosing feelings of depression. Furthermore  two-thirds of these tweets appeared to reveal one or more symptoms consistent with the diagnosis of MDD and/or communicated thoughts or ideas that were consistent with struggles of depression. The most common symptoms we observed in these tweets included chronic feelings of a depressed or irritable mood and guilt/worthlessness. Thus  our findings indicate that some individuals are forthcoming about their mental health struggles via postings on Twitter which corresponds with Park et al. (2012) who likewise found that individuals tweet about their depression and even disclosed updates about their mental health treatment on Twitter. Moreover  we found tweets where thoughts about self-harm and/or suicide were mentioned. When extended to the full sample of tweets  we estimate nearly 700 tweets per day are posted that entail thoughts about self-harm and/or suicide. We are unable to determine the degree to which the tweets that we examined do correspond with self-reported depression and/or suicidal intent  and a more in-depth study on this topic is an important next step in this line of research. Nevertheless  it is indeed worrisome that individuals are socially networking about these topics  especially when the posts involve self-harm and suicidal thoughts. This is especially concerning given the surmounting evidence supporting the social contagion of suicidality (Daine et al.  2013  Gould and Davidson  1988  Niedzwiedz et al.  2014  O'Carroll and Potter  1994). Our study is exploratory and as a whole  social media research is a relatively new field of research. Thus  it is relevant to understand the effects of these posts among individuals who view them because young people tend to be highly impressionable to media messages and peer influences  in general. Given the relatively high prevalence of tweets that reflected one or more symptoms for the diagnosis of MDD and/or communicated thoughts or ideas that were consistent with feeling depressed  we focused our demographic analysis on these Tweeters. We note that Caucasian girls/women were inferred to be the most likely Tweeters of depression-related content  and African Americans also had a dominant presence among the Tweeters. Epidemiological studies similarly note MDD prevalence rates that are in-line with the gender and racial/ethnic differences we observed among the Tweeters (Howell et al.  2001  Kessler  2003). In fact  the prevalence of depression has been consistently documented at about twice as high in females versus males (Kessler  2003). Our findings are also consistent with research that has found differences in social media use between African Americans versus Caucasians (Madden et al.  2013). In general  African Americans have a high presence on Twitter (Madden et al.  2013). African American youth are less likely to disclose their real name on a social media profile  and this anonymity may facilitate their openness to discussing depression-related content on Twitter (Madden et al.  2013). Whatever the case may be  our study provides a snapshot into the inferred demographic characteristics of individuals whose tweets are disclosing struggles with depression. We additionally found that the Tweeters in our study tended to follow the same popular culture celebrities. Best practices for incorporating social media trends and insights health-related interventions have not yet been established. However  it may be useful for mental health-related organizations that currently use Twitter to strategize ways to tailor their tweets to match the ages and hobbies/interests of individuals who are disclosing feelings of depression in their tweets. For example  on August 30  2014  the National Suicide Prevention Lifeline tweeted  “Musician Rhett Miller talks about his suicide attempt and why he is glad the Lifeline is here to help. http://ow.ly/ASkwI”. Another example is Demi Lovato who is a young adult celebrity who is popular with a young demographic and has advocated on behalf of mental health issues  including tweets such as “It's NAMI's National Day of Action!! Pass comprehensive mental health legislation #mentalhealth #Act4MentalHealth” which was tweeted on September 4  2014. Suicide prevention organizations are encouraged to strategize ways to incorporate music  education  or popular culture into their efforts perhaps by partnering with celebrities to deliver prevention messages about suicide and depression. Because Twitter limits user's posts to 140 characters  the depth of content can be lacking. This restricted the amount of information users could give about their feelings of depression  and therefore  all causes about the Tweeter's depression were often not given within the tweets that we examined. The findings of the study are based on a random sample of 2000 depression-related tweets sent in a 24-day period. Therefore  more tweets examined over an extended time period could produce more thorough findings. We did not monitor popular terms like “sad”  “unhappy”  and “miserable” because they are commonly used to describe temporary states or moods and would have likely generated a much higher number of trivial tweets. However  a more extensive list of keywords to monitor could have led to more comprehensive findings. We are unable to make any conclusions about the type of individuals who are willing to share personal information about depression openly online compared with those who do not choose to socially network in this way. For instance  there are some individuals with depression who likely suffer from social anxiety (Keller  2006) and use Twitter to socially network because they experience less social anxiety interacting online when compared to offline interactions (Yen et al.  2011). Finally  Demographics Pro infers demographic characteristics of Twitter users  so although their methods are deemed to be accurate  it's possible that the inferred characteristics of the Tweeters do not represent the actual Twitter users in our sample of tweets. Similarly  the “typical Twitter user” comparison group is limited to the United States and Canada and is not an exact match of our sample of Tweeters; most of our Tweeters are inferred to be from the United States and Canada (78%) while 9% are from United Kingdom and 13% are from another country. Despite these limitations  the present study offers a unique understanding of the role of social media in the expression of depression. Twitter is one of the most widely used social media platforms especially among young people  and we found that many individuals are posting supportive or helpful tweets about depression. However  we also found a relatively high prevalence of tweets that revealed one or more symptoms for the diagnosis of MDD and/or communicated thoughts or ideas that were consistent with feeling depressed. Our study is unique in its examination of a broad sample of depression-related tweets for identification of common themes and symptoms as well as Tweeter's demographic characteristics and interests. We hope that our findings can be used by mental health professionals to tailor and target prevention and awareness messages to those Twitter users who are most in need.,0,0,0,0,2,0,2,0,0,0,,1,0,0,0,0,0,,0
https://aclanthology.org/W15-1202.pdf,18,1,abstract,"Mitchell, Hollingshead, Coppersmith",3,0,0,1,2016,Analyzing symptoms of schizophrenia has traditionally been challenging given the low prevalence of the condition  affecting around 1% of the U.S. population. We explore potential linguistic markers of schizophrenia using the tweets1 of self-identified schizophrenia sufferers  and describe several natural language processing (NLP) methods to analyze the language of schizophrenia. We examine how these signals compare with the widely used LIWC categories for understanding mental health (Pennebaker et al.  2007)  and provide preliminary evidence of additional linguistic signals that may aid in identifying and getting help to people suffering from schizophrenia.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://aclanthology.org/W15-1202.pdf,18,2,introduction,"Mitchell, Hollingshead, Coppersmith",3,0,0,1,2016,Schizophrenia is a group of mental disorders that affect thinking and emotional responsiveness  documented throughout history (e.g.  The Book of Hearts  1550 BCE). Today it is diagnosed and monitored leveraging self-reported experiences.2 This may be challenging to elicit from schizophrenia sufferers  as a hallmark of the disease is the sufferer’s belief that he or she does not have it (Rickelman  2004; National Alliance on Mental Illness  2015). Schizophrenia sufferers are therefore particularly atrisk for not leveraging help (Pacific Institute of Medical Research  2015). This suggests that techniques that leverage social language shared by schizophrenia sufferers could be greatly beneficial in treatment of the disease. Early identification and monitoring of schizophrenia can increase the chances of successful management of the condition  reducing the chance of psychotic episodes (Hafner and Maurer  ¨ 2006) and helping a schizophrenia sufferer lead a more comfortable life. We focus on unsupervised groupings of the words used by people on the social media platform Twitter  and see how well they discriminate between matched schizophrenia sufferers and controls. We find several potential linguistic indicators of schizophrenia  including words that mark an irrealis mood (“think”  “believe”)  and a lack of emoticons (a potential signature of flat affect). We also demonstrate that a support vector machine (SVM) learning approach to distinguish schizophrenia sufferers from matched controls works reasonably well  reaching 82.3% classification accuracy. To our knowledge  no previous work has sought out linguistic markers of schizophrenia that can be automatically identified. Schizophrenia is a relatively rare mental health condition  estimated to affect around 1% of the population in the U.S. (The National Institute of Mental Health  2015; Peral¨ a et ¨ al.  2007; Saha et al.  2005)  or some 3.2 million people. Other mental health conditions with a high prevalence rate such as depression3 have recently received increased attention (Schwartz et al.  2014; De Choudhury et al.  2013b; Resnik et al.  2013; Coppersmith et al.  2014a). However  similar studies for schizophrenia have been hard to pursue  given the rarity of the condition and thus the inherent difficulty in collecting data. We follow the method from Coppersmith et al. (2014a) to create a relatively large corpus of users diagnosed with schizophrenia from publicly available Twitter data  and match them to Twitter controls. This provides a view of the social language that a schizophrenia sufferer may choose to share with a clinician or counselor  and may be used to shed light on the illness and the effect of treatments.,0,0,0,0,0,0,0,0,2,0,,0,0,0,0,0,0,,0
https://aclanthology.org/W15-1202.pdf,18,3,background and motivation,"Mitchell, Hollingshead, Coppersmith",3,0,0,1,2016,There has been a recent growth in work using language to automatically identify people who may have mental illness and quantifying its progression  including work to help people suffering from depression (Howes et al.  2014; Hohman et al.  2014; Park et al.  In press; Schwartz et al.  2014; Schwartz et al.  2013; De Choudhury et al.  2013a; De Choudhury et al.  2013b; De Choudhury et al.  2011; Nguyen et al.  2014) and post-traumatic stress disorder (Coppersmith et al.  2014b). Related work has also shown it is possible to aid clinicians in identifying patients who suffer from Alzheimer’s (Roark et al.  2011; Orimaye et al.  2014) and autism (Rouhizadeh et al.  2014). The time is ripe to begin exploring an illness that deeply affects an estimated 51 million people. The term schizophrenia  derived from the Greek words for “split mind”  was introduced in the early 1900s to categorize patients whose thoughts and emotional responses seemed disconnected. Schizophrenia is often described in terms of symptoms from three broad categories: positive  negative  and cognitive. Positive symptoms include disordered thinking  disordered moving  delusions  and hallucinations. Negative symptoms include a flat affect and lack of ability to begin and sustain planned activities. Cognitive symptoms include poor ability to understand information and make decisions  as well as trouble focusing. Some symptoms of schizophrenia may be straightforward to detect in social media. For example  the positive symptoms of neologisms  or creating new words  and word salad  where words and sentences are strung together without a clear syntactic or semantic structure  may be expressed in the text written by some schizophrenia sufferers. Negative symptoms may also be possible to find  for example  a lack of emoticons can reflect a flat affect  or a lower proportion of commonly used terms may reflect cognitive difficulties. As we discuss below  natural language processing (NLP) techniques can be used to produce features similar to these markers of schizophrenia. For example  perplexity may be useful in measuring how unexpected a user’s language is  while latent Dirichlet allocation (Blei et al.  2003) may be useful in characterizing the difference in general themes that schizophrenia sufferers discuss vs. control users. All NLP features we describe are either automatically constructed or unsupervised  meaning that no manual annotation is required to create them. It is important to note that although these features are inspired by the literature on schizophrenia  they are not direct correlates of standard schizophrenia markers.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://aclanthology.org/W15-1202.pdf,18,4,data,"Mitchell, Hollingshead, Coppersmith",3,0,0,1,2016,We follow the data acquisition and curation process of Coppersmith et al. (2014a)  summarizing the major points here: Social media  such as Twitter  contains frequent public statements by users reporting diagnoses for various medical conditions. Many talk about physical health conditions (e.g.  cancer  flu) but some also discuss mental illness  including schizophrenia. There are a variety of motivations for users to share this information on social media: to offer or seek support  to fight the stigma of mental illness  or perhaps to offer an explanation for certain behaviors.4 We obtain messages with these self-reported diagnoses using the Twitter API  and filtered via (caseinsensitive) regular expression to require “schizo” or a close phonetic approximation to be present; our expression matched “schizophrenia”  its subtypes  and various approximations: “schizo”  “skitzo”  “skitso”  “schizotypal”  “schizoid”  etc. All data we collect are public posts made between 2008 and 2015  and exclude any message marked as ‘private’ by the author. All use of the data reported in this paper has been approved by the appropriate Institutional Review Board (IRB). Each self-stated diagnosis included in this study was examined by a human annotator (one of the authors) to verify that it appeared to be a genuine statement of a schizophrenia diagnosis  excluding jokes  quotes  or disingenuous statements. We obtained 174 users with an apparently genuine selfstated diagnosis of a schizophrenia-related condition. Note that we cannot be certain that the Twitter user was actually diagnosed with schizophrenia  only that their statement of being diagnosed appears to be genuine. Previous work indicates that interannotator agreement for this task is good: κ = 0.77 (Coppersmith et al.  2014a). For each user  we obtained a set of their public Twitter posts via the Twitter API  collecting up to 3200 tweets.5 As we wish to focus on user-authored content  we exclude from analysis all retweets and any tweets that contain a URL (which often contain text that the user did not author). We lowercase all words and convert any non-standard characters (including emoji) to a systematic ASCII representation via Unidecode.6 For our community controls  we used randomlyselected Twitter users who primarily tweet in English. Specifically  during a two week period in early 2014  each Twitter user who was included in Twitter’s 1% “spritzer” sample had an equal chance for inclusion in our pool of community controls. We then collected some of their historic tweets and assessed the language(s) they tweeted in according to the Chromium Compact Language Detector.7 Users were excluded from our community controls if their tweets were less than 75% English.8,2,2,2,2,2,0,0,0,2,2,,0,0,2,0,0,0,,0
https://aclanthology.org/W15-1202.pdf,18,5,age and gender matched controls,"Mitchell, Hollingshead, Coppersmith",3,0,0,1,2016,Since mental health conditions  including schizophrenia  have different prevalence rates depending on age and gender (among other demographic variables)  controlling for these will be important when examining systematic differences between schizophrenic users and community controls. In particular  we would like to be able to attribute any quantifiable signals we observe to the presence or absence of schizophrenia  rather than to a confounding age or gender divergence between the populations (Dos Reis and Culotta  2015). To that end  we estimated the age and gender of all our users (from their language usage) via the tools graciously made available by the World Well-Being Project (Sap et al.  2014). For each user  we applied a hard threshold to the gender prediction to obtain a binary ‘Female’ or ‘Male’ label. Then  in order to select the best match for each schizophrenia user  we selected the community control that had the same gender label and was closest in age (without replacement).,0,0,0,0,0,0,2,0,0,0,,0,0,0,0,0,0,,0
https://aclanthology.org/W15-1202.pdf,18,6,drawbacks of a balanced dataset,"Mitchell, Hollingshead, Coppersmith",3,0,0,1,2016,We use a balanced dataset here for our analysis (an equal number of schizophrenia users and community controls). This 50/50 split makes the machine learning and analysis easier  and will allow us to focus more on emergent linguistics that are related to schizophrenia than if we had examined a dataset more representative of the population (more like 1/99). Moreover  we have not factored in the cost of false negatives or false positives (how should the consequences of misclassifying a schizophrenia user as non-schizophrenic be weighed against the consequences of misclassifying a non-schizophrenic user as schizophrenic?). All our classification results should be taken as validation that the differences in language we observe are relevant to schizophrenia  but only one step towards applying something derived from this technology in a real world scenario.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://aclanthology.org/W15-1202.pdf,18,7,concomitance,"Mitchell, Hollingshead, Coppersmith",3,0,0,1,2016,Often  people suffering from mental illness have a diagnosis for more than one disorder  and schizophrenia is no exception. Of our 174 users with a genuine self-statement of diagnosis of a schizophrenia-related condition  41 also state a diagnosis of at least one other mental illness (30%)  while 15 of those state that they have a diagnosis of more than one other mental illness (11%). The vast majority of these concomitances are with bipolar (25 users)  followed by depression (14)  post traumatic stress disorder (8) and generalized anxiety disorder (6). These comorbidity rates are notably lower than the generally accepted prevalence rates  which may be due to one of several factors. First  we rely on stated diagnoses to calculate comorbidity  and the users may not be stating each of their diagnosed conditions  either because they have not been diagnosed as such  or they choose to identify most strongly with the stated diagnosed conditions  or they simply ran out of space (given Twitter’s 140-character limit). Second  we are analyzing Twitter users  which consists of only a subset of the population  and the users that choose to state  publicly  on Twitter  their schizophrenia diagnosis  may not be an accurate representation of the population of schizophrenia sufferers. The noted concomitance of schizophrenia and bipolar disorder is frequently labeled as “schizoaffective disorder with a bipolar subtype”  with some recent research indicating shared impairments in functional connectivity across patients with schizophrenia and bipolar disorders (Meda et al.  2012). It is worth keeping in mind throughout this paper that we examine all subtypes of schizophrenia together here  and further in-depth analysis between subtypes is warranted.,0,0,0,0,0,0,0,0,2,0,,0,0,0,0,0,0,,0
https://aclanthology.org/W15-1202.pdf,18,8,methods,"Mitchell, Hollingshead, Coppersmith",3,0,0,1,2016,We first define features relevant to mental health in general and schizophrenia in particular  and explore how well each feature distinguishes between schizophrenia-positive users and community controls. We then design and describe classifiers capable of separating the two groups based on the values for these features in their tweets. We reflect on and analyze the signals extracted by these automatic NLP methods and find some interesting patterns relevant to schizophrenia.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://aclanthology.org/W15-1202.pdf,18,9,lexicon based approaches,"Mitchell, Hollingshead, Coppersmith",3,0,0,1,2016,​​We used the Linguistic Inquiry Word Count (LIWC  Pennebaker et al. (2007)) to analyze the systematic language differences between our schizophreniapositive users and their matched community controls. LIWC is a psychometrically validated lexicon mapping words to psychological concepts  and has been used extensively to examine language (and even social media language) to understand mental health. LIWC provides lists of words for categories such as FUTURE  ANGER  ARTICLES  etc. We treat each category as a feature; the feature values for a user are then the proportion of words in each category (e.g.  the number of times a user writes “I” or “me”  divided by the total number of words they have written is encoded as the LIWC “first person pronoun” category).,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,2,0,,0
https://aclanthology.org/W15-1202.pdf,18,10,open vocabulary approaches,"Mitchell, Hollingshead, Coppersmith",3,0,0,1,2016,In addition to the manually defined lexicon-based features described above  we also investigate some open-vocabulary approaches. This includes latent Dirichlet allocation (LDA) (Blei et al.  2003)  Brown clustering (Brown et al.  1992)  character n-gram language modeling (McNamee and Mayfield  2004)  and perplexity.9 We now turn to a brief discussion of each approach.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,2,0,,0
https://aclanthology.org/W15-1202.pdf,18,11,latent dirichlet allocation,"Mitchell, Hollingshead, Coppersmith",3,0,0,1,2016,LDA operates on data represented as “documents” to infer “topics”. The idea behind LDA is that each document can be viewed as a mixture of topics  where each topic uses words with different probabilities (e.g.  “health” would be likely to come from a psychology topic or an oncology topic  but “schizophrenia” is more common from the former). LDA infers these topics automatically from the text – they do not have labels to start with  but often a human reading the most frequent words in the topic can see the semantic relationship and assign one. In our case  all tweets from a user make up a “document”  and we use collapsed Gibbs sampling to learn the distribution over topics for each document. In other words  given a specific number of topics k (in our work  k=20)  LDA estimates the probability of each word given a topic and the probability of each topic given a document. Tweets from a user can then be featurized as a distribution over the topics: Each topic is a feature  whose feature value is the probability of that topic in the user’s tweets. The LDA implementation we use is available in the MALLET package (McCallum  2002).,0,0,0,0,0,0,0,0,0,0,,0,0,0,1,2,0,,0
https://aclanthology.org/W15-1202.pdf,18,12,brown clustering,"Mitchell, Hollingshead, Coppersmith",3,0,0,1,2016,Words in context often provide more meaning than the words in isolation  so we use methods for grouping together words that occur in similar linguistic constructions. Brown clustering is a greedy hierarchical algorithm that finds a clustering of words that maximizes the mutual information between adjacent clusters; in other words  words that are preceded by similar words are grouped together to form clusters  and then these clusters are merged based on having similar preceding words  and then these clusters are further merged  etc. Each word is therefore associated to clusters of increasing granularity. We define all leaf clusters10 as features  and the feature value of each for a user is the proportion of words from the user in that cluster. The Brown clustering implementation we use is currently available on github 11 and is used with default parameter settings  including a limit of 100 clusters.,0,0,0,0,0,2,0,0,0,0,,0,0,0,0,2,0,,0
https://aclanthology.org/W15-1202.pdf,18,13,character ngrams,"Mitchell, Hollingshead, Coppersmith",3,0,0,1,2016,Character n-gram language models are models built on sequences (n-grams) of characters. Here  we use 5-grams: for all the tweets a user authored  we count the number of times each sequence of 5 characters is observed. For example  for this sentence we would observe the sequences: “for e”  “or ex”  “r exa”  “ exam”  and so on. The general approach is to examine how likely a sequence of characters is to be generated by a given type of user (schizophrenic or non-schizophrenic). To featurize character n-grams  for each character 5-gram in the training data  we calculate its probability in schizophrenic users and its probability in control users. At test time  we search for sets of 50 sequential tweets that look “most schizophrenic” by comparing the schizophrenic and control probabilities estimated from the training data for all the 5-grams in those tweets. We experimented with different window sizes for the number of tweets and different n for n-grams; for brevity  we report only the highest performing parameter settings at low false alarm rates: 5-grams and a window size of 50 tweets. An example of this can be found in Figure 1  where one schizophrenic and one control user’s score over time is plotted (top). To show the overall trend  we plot the same for all users in this study (bottom)  where separation between the schizophrenics (in red) and control users (in blue) is apparent. The highest score from this windowed analysis becomes the feature value. Note that this feature corresponds to only a subset of a user’s timeline. For schizophrenia sufferers  this is perhaps when their symptoms were most severe  a subtle but critical distinction when one considers that many of these people are receiving treatment of some sort  and thus may have their symptoms change or subside over the course of our data.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,2,0,,0
https://aclanthology.org/W15-1202.pdf,18,14,perplexity,"Mitchell, Hollingshead, Coppersmith",3,0,0,1,2016,The breadth of language used (to include vocabulary  topic areas  and syntactic construction) can be measured via perplexity – a measurement based on entropy  and roughly interpreted as a measurement of how predictable the language is. We train a trigram language model on one million randomly selected tweets from the 2014 1% feed  and then use this model to score the perplexity on all the tweets for each user. If a user’s language wanders broadly (and potentially has the word salad effect sometimes a symptom of schizophrenia)  we would expect a high perplexity score for the user. This gives us a single feature value for the perplexity feature for each user.,0,0,0,0,2,0,0,0,0,0,,0,0,0,0,2,0,,0
https://aclanthology.org/W15-1202.pdf,18,15,isolated features,"Mitchell, Hollingshead, Coppersmith",3,0,0,1,2016,We examine differences in the language between schizophrenia sufferers and matched controls by mapping the words they use to broader categories  as discussed above  and measuring the relative frequencies of these categories in their tweets. Different approaches produce different word categories: We focus on LIWC vectors  topics from latent Dirichlet allocation (LDA)  and clusters from Brown clustering. We compare whether the difference in the relative frequencies of each category is significant using an independent sample t-test 12 Bonferroni-corrected.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,2,0,,0
https://aclanthology.org/W15-1202.pdf,18,16,liwc vectors,"Mitchell, Hollingshead, Coppersmith",3,0,0,1,2016,We did not make predictions about which LIWC categories might show deviations between our schizophrenia and control users  but instead examine all the LIWC categories (72 categories  corrected α = 0.0007). We find that the language of schizophrenia users had significantly more words from the following major categories: COGNITIVE MECHANISMS  DEATH  FUNCTION WORDS  NEGATIVE EMOTION  and in the following subcategories: ARTICLE  AUXILIARY VERBS  CONJUGATIONS  DISCREPANCIES  EXCL  HEALTH  I  INCL  INSIGHT  IPRON  PPRON  PRO1  PRONOUN  TENTATIVE  and THEY. Schizophrenia users had significantly fewer words in the major categories of HOME  LEISURE  and POSITIVE EMOTION  and in the subcategories of ASSENT  MOTION  RELATIVE  SEE  and TIME.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,2,0,,0
https://aclanthology.org/W15-1202.pdf,18,17,latent dirichlet allocation,"Mitchell, Hollingshead, Coppersmith",3,0,0,1,2016,We find that the difference between the two groups is statistically significant for 8 of the 20 topics  i.e.  the relative frequency of the topic per user is significantly different between groups (corrected α = 0.0025). Significant topics and top words are shown in Table 1  with the condition with the highest mean proportion shown in the leftmost column and indicated by color: red for schizophrenia (Sch) and blue for control (Con) topics. We then find the topic t with the maximum estimated probability for each user. To see the prevalence of each topic for each condition  see Figure 2  where each user is represented only by their LDA topic t.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,2,0,,0
https://aclanthology.org/W15-1202.pdf,18,18,brown clustering,"Mitchell, Hollingshead, Coppersmith",3,0,0,1,2016,To narrow in on a set of Brown clusters that may distinguish between schizophrenia sufferers and controls  we sum the relative frequency of each cluster per user  and extract those clusters with at least a 20% difference between groups. This yields 29 clusters. From these  we find that the difference between most of the clusters is statistically significant (corrected α = 0.0017). Example significant clusters and top words are shown in Table 2.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://aclanthology.org/W15-1202.pdf,18,19,perplexity,"Mitchell, Hollingshead, Coppersmith",3,0,0,1,2016,We find this to be only marginally different between groups (p-value = 0.07872)  suggesting that a more in-depth and rigorous analysis of this measure and its relationship to the word salad effect is warranted.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://aclanthology.org/W15-1202.pdf,18,20,machine learning,"Mitchell, Hollingshead, Coppersmith",3,0,0,1,2016,In Section 4  we discussed how we featurized LIWC categories  LDA topics  Brown clusters  Character Language Models  and perplexity. We now report machine learning experiments using these features. We compare two machine learning methods: Support Vector Machines (SVM) and Maximum Entropy (MaxEnt). All methods are imported with default parameter settings from python’s scikit-learn (Pedregosa et al.  2011). As shown in Table 3  the character language model (‘CLM’) method performs reasonably well at classifying users in isolation  and the features based on the distribution over Brown clusters (‘BDist’) performs well in a maximum entropy model. An SVM model with features created from LIWC categories and a distribution over LDA topics (‘LIWC+TDist’) works best at discovering schizophrenia sufferers in our experiments  reaching 82.3% classification accuracy on our balanced test set. Featurizing the distribution over topics provided by LDA increases classification accuracy over using linguistically-informed LIWC categories alone by 13.5 percentage points. The CLM method performed surprisingly well  given its relative simplicity  and outperformed the LIWC features by nearly ten percentage points when used in isolation  perhaps indicating that the open-vocabulary approach made possible by the CLM is more robust to the type of data we see in Twitter. Combining the LIWC and CLM features  though  only gives a small bump in performance over CLMs alone. Given the fairly distinct distribution of LDA topics by condition as shown in Figure 2  we expected that the ID of the LDA topic t would serve well as a feature  but found that we needed to use the distribution over topics (TDist) in order to perform above chance. This topic distribution feature was the best-performing individual feature  and also performed well in combination with other features  thus seeming to provide a complementary signal. Interestingly  while the CLM model out-performed the LIWC model  the combination of LIWC and TDist features outperformed the combination of CLM and TDist features  yielding our best-performing model.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://aclanthology.org/W15-1202.pdf,18,21,analysis of language based signals lda and brown clustering,"Mitchell, Hollingshead, Coppersmith",3,0,0,1,2016,In the previous section  we examined how well the signals we define discriminate between schizophrenia sufferers and controls in a balanced dataset. We now turn to an exploratory discussion of the language markers discovered with the unsupervised NLP techniques of LDA and Brown clustering  in the hopes of shedding some light on language-based differences between the two groups. Refer to Tables 1 and 2. Both LDA and Brown clustering produce groups of related words  with different views of the data. We find that both methods group together words for laughing – “haha”  “lol”  etc. – and these discriminate between schizophrenia sufferers and controls. In LDA  this is Topic 6; in Brown clustering  this is Cluster 0001001.13 Controls are much more likely to ask someone to retweet (“rt”)  pulled out in both methods as well (Topics 7 and 11; Cluster 00001). The two approaches produce word groups with time words like “today” and “tonight” that discriminate between schizophrenia sufferers and controls differently; the word “today” in particular is found in a topic and in a cluster that is more common for controls (Topic 19 and Cluster 01011011010). LDA pulls out positive sentiment words such as “love”  “awesome”  “amazing”  “happy”  “good”  etc. (Topics 11 and 19)  and topics with these words are significantly more common in controls. It also finds groups for negated words like “don’t”  “didn’t”  “won’t”  etc. (Topic 2)  and this is significantly more common in the language of schizophrenia sufferers. Both decreased occurrence of positive sentiment topics and increase of negated word topics is suggestive of the flat affect common to schizophrenics. Topic 12 contains a group of words specific to mental health  including the words “mental”  “health”  and “medical”  as well as  interestingly  “schizophrenia” and “schizophrenic” – unsurprisingly occurring significantly more under the schizophrenia condition. Recall that we remove the original diagnosis tweet from our analysis  but this topic indicates much more talk about the condition. One wonders whether this might extend to other mental health conditions  and whether the stigma of discussing mental health is reduced within the anonymity provided by the Internet and social media. Figure 2 furthermore indicates that only schizophrenia sufferers have this Topic 12 as their LDA topic t. Brown clustering pulls out the first person pronoun ‘I’ as a main cluster  and we find that this is significantly more frequent in schizophrenia sufferers than in controls. This is comparable to the LIWC category ‘I’  which we also find to be proportionally higher in the language of schizophrenia sufferers. Interestingly  Brown clustering pulls out words that mark hedging and irrealis moods in English (Cluster 010100111). This is found in phrases such as “I think”  “I believe”  “I guess”  etc. We find that this cluster is significantly more common in the language of schizophrenia sufferers  perhaps related to the dissociation from reality common to the disorder. We also find a Brown cluster for connectives (words like “but”  “because”  “except”) in Cluster 01011111111; and this is also significantly more common in schizophrenia sufferers. The use of an exclamation point (Cluster 0010111) also differs between schizophrenia sufferers and controls. Note that markers << and >> are also common in this cluster. This is an artifact of our text processing of emojis; in other words  both emojis and exclamation points are significantly less likely in the language of schizophrenics. This is potentially another reflection of the flat affect negative symptom of schizophrenia.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://aclanthology.org/W15-1202.pdf,18,22,conclusion,"Mitchell, Hollingshead, Coppersmith",3,0,0,1,2016,Given its relative rarity compared to other mental health conditions like depression or anxiety disorders  schizophrenia has been harder to obtain enough data to leverage state-of-the-art natural language processing techniques. Many such techniques depend on large amounts of text data for adequate training  and such data has largely been unavailable. However  we can discover a sufficient amount of schizophrenia sufferers via publicly available social media data  and from here we can begin to explore text-based markers of the illness. This comes with a notable caveat: These users battling schizophrenia may be different in some systematic ways from the schizophrenic population as a whole – they are Twitter users  and they are speaking publicly about their condition. This suggests that replication of these findings in more controlled settings is warranted before hard conclusions are drawn. By applying a wide range of natural language processing techniques to users who state a diagnosis of schizophrenia  age- and gender-matched to community controls  we discovered several significant signals for schizophrenia. We demonstrated that character n-grams featurized over specific tweets in a user’s history performs reasonably well at separating schizophrenia sufferers from controls  and further  featurizing the distribution over topics provided by latent Dirichlet allocation increases classification accuracy over using linguisticallyinformed LIWC categories alone by 13.5 percentage points in an SVM machine learning approach. Moreover  the features produced by these unsupervised NLP methods provided some known  some intuitive  and some novel linguistic differences between schizophrenia and control users. Our cursory inspection here is only capturing a fraction of the insights into schizophrenia from text-based analysis  and we see great potential from future analyses of this sort. Identifying quantifiable signals and classifying users is a step towards a deeper understanding of language differences associated with schizophrenia  and hopefully  an advancement in available technology to help those battling with the illness.,0,0,0,0,0,0,0,0,2,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/2700171.2791026,19,1,abstract,"Kumar, Drezde, Coppersmith, De Choudhury",4,1,1,1,2015,The Werther effect describes the increased rate of completed or attempted suicides following the depiction of an individual’s suicide in the media  typically a celebrity. We present findings on the prevalence of this effect in an online platform: r/SuicideWatch on Reddit. We examine both the posting activity and post content after the death of ten high-profile suicides. Posting activity increases following reports of celebrity suicides  and post content exhibits considerable changes that indicate increased suicidal ideation. Specifically  we observe that post-celebrity suicide content is more likely to be inward focused  manifest decreased social concerns  and laden with greater anxiety  anger  and negative emotion. Topic model analysis further reveals content in this period to switch to a more derogatory tone that bears evidence of self-harm and suicidal tendencies. We discuss the implications of our findings in enabling better community support to psychologically vulnerable populations  and the potential of building suicide prevention interventions following high-profile suicides.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/2700171.2791026,19,2,introduction,"Kumar, Drezde, Coppersmith, De Choudhury",4,1,1,1,2015,The “Werther effect” describes the increased rate of completed or attempted suicide following media reported incidents or depiction of celebrity suicides [19]. Naturally  the Werther effect is a highly regarded phenomenon in media effect research — prior literature has examined ways of exploiting findings on the effect to inform journalistic practices  as well as to guide suicide prevention programs [32]. However  key to the success of these interventions is the ability to measure the prevalence of the phenomenon as well as quantifying its manifestation in psychologically vulnerable populations. Unfortunately  scientific evidence in support of the Werther effect Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise  or republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. has been either qualitative  specific to particular populations  selfreport based  or based on government or agency reported suicide rates [5  16]. Due to the sensitive nature of suicide  gathering data on suicidal ideation or attempts to corroborate existence of the Werther effect  especially the epidemiological extent of the phenomenon  has been challenging [32]. Online social media platforms  such as Twitter  Facebook  Reddit  and Tumblr  are popular outlets for people seeking information and social support  including issues around a variety of psychological and health challenges [37  38  13]. An attractive feature of these platforms is that they allow for anonymous or pseudonymous participation. Thus  they provide individuals with a candid platform of expression  especially around conditions that are considered socially stigmatized  such as suicide and mental illness [6]. In this paper  we leverage data derived from a widely adopted suicide support forum called “SuicideWatch”1 . This community is hosted on the popular social media Reddit. Through analysis of historical posts and associated metadata  we examine attributes of suicidal ideation and suicide interest  specifically relating to the Werther effect. Our underlying assumption is that such a forum can provide us information about individuals who are likely prone to suicidal thoughts or tendencies. Moreover  this comes at a scale previously unavailable  spanning thousands of individuals  thus enabling rigorous statistical analysis. The ability to analyze the content of the messages shared on this forum further allows us derive language and behavior related attributes associated with the Werther effect. The main contributions of this paper are: • We show notable increases in the posting frequency on the forum following reports of celebrity suicides. This change is persistent after accounting for baseline expected variability in posting activity  and in contrast to other mental health forums on Reddit. • We include linguistic measures of behavior  obtained from content shared on the forum. We find that posts following celebrity suicides express greater negativity  raised cognitive bias  increased self-attentional focus  and lowered social integration in the aftermath of celebrity suicides. • We utilize n-gram and topic model analyses to show that expressions of self-derogatory behavior  depictions of suicidal tendencies  and detachments from the social realm are more frequently discussed after celebrity suicides. Broadly  we aim to aid suicide prevention  one of the leading causes of death in the United States. While suicide is the 10th leading cause of death2   it rises to the third position for people aged 15-24 3 a demographic known to extensively use social media.4 An ability to measure existence and prevalence of the Werther effect from large-scale online data may help moderation efforts on these platforms  especially in periods that succeed high-profile celebrity suicide. Online interventions may also be built to extend careful advice and help to individuals identifying with a particular suicide event and therefore at an increased risk of future suicide.,0,0,0,0,0,0,0,0,0,0,,2,2,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/2700171.2791026,19,3,privacy note,"Kumar, Drezde, Coppersmith, De Choudhury",4,1,1,1,2015,All social media data used in this paper are publicly available. At no time did we contact or interact with a user. Given the sensitive nature of this research  we took care to anonymize the data in analysis and presentation  so as to minimize any inadvertent disclosure of personal information  or information that may reveal cues about an individual’s online identity. Approval was obtained from the relevant institutional review boards.,0,0,0,0,0,0,0,0,2,2,,0,2,0,0,0,0,,2
https://dl.acm.org/doi/abs/10.1145/2700171.2791026,19,4,werther effect and copycat suicides,"Kumar, Drezde, Coppersmith, De Choudhury",4,1,1,1,2015,A number of studies have found evidence  both anecdotal and statistical  supporting the existence of the Werther effect in news and entertainment media [19  5  31  54]. These findings indicate that the effect varies in its persistence  peaking by about three days and leveling out by about two weeks [17]. The effect is also associated with the amount and prominence of media coverage  and it has consistently been observed to be prevalent in diverse cultures [43  63  16]. Hence literature has also critically examined the role of media as a risk factor of suicide [56  54]. A complementary line of work also found that repetitive or highprofile suicide events led to imitative behaviors [61] — especially with media consumers with similar characteristics to the celebrity. Explicit depiction of particular methods has been known to lead to increases in completed suicides employing that method — this phenomenon is also known as “copycat suicides” [33]. Previous work on the Werther effect has measured actual suicides  which do not lend direct insights into the nature of suicidal ideation. Moreover  little has been investigated in terms of the role of online suicide discussion and support platforms  and their relationship to the manifestation of the Werther Effect. Relatedly  much of the psychology research into suicidality and suicidal ideation relies on surveys and self-report. These methods can be intrusive  expensive  vulnerable to participants’ memory bias  vulnerable to experimental demand effects  and often lack temporal granularity. These are further complicated by impediments due to the stigma and sensitivity surrounding suicide. In contrast  social media  especially public online communities geared towards supporting suicidal individuals  can provide us with a rich  real-time source of information about the phenomena  without many of the drawbacks of more traditional approaches. Here  we measure suicide related interest and intent by looking at conversations on a prominent suicide forum on Reddit  called SuicideWatch. This focus on online communities allows us to look at content around suicide interest and intent  allowing complementary insight into the problem.,0,0,0,0,0,0,0,0,2,0,,2,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/2700171.2791026,19,5,psychology of suicide,"Kumar, Drezde, Coppersmith, De Choudhury",4,1,1,1,2015,Several studies have examined linguistic and behavioral attributes associated with suicide. Due to the inherent challenge of obtaining information shared by suicidal individuals  most prior work focuses on poetry as the source of content  analyzing suicidal tendencies of poets  or publicly available corpus of suicide notes [52]. [57] leveraged the popular and widely validated psycholinguistic lexicon LIWC to understand the various linguistic constructs manifested in poems written by suicidal poets. An important finding of this work was that the suicidal poets showed higher usage of firstperson singular nouns in their writing when compared to a control group of poets  interpreted as lowered social integration or stronger inward focus. In general they found that suicidal thoughts can be associated with detachment from the social realm and higher preoccupation with the self. Along similar lines  others [26  53  42  15  28  60] have independently found that suicidal poets showed ambivalence towards death in their writing  exhibited cognitive distortion and emotional inhibition  and over time their linguistic style shifted to a personal  expressive form and stronger inward focus. Authors in [44] found cognitive impairment to be a characteristic predictor of suicide ideation based on a standardized battery of questionnaires. Petrie and Brook found that repeated suicide attempts were associated with lowered coherence and self-esteem  and greater hopelessness [42]. Beyond suicide  literature in psychology has focused on attributes of mental illness and other behavioral health concerns [1]. Pennebaker and colleagues used LIWC in a number of different populations and scenarios to identify language related markers of anxiety  stress  neurotic tendencies and psychiatric disorders [41  47]. Taken together  this suggests the potential of leveraging behavioral and linguistic cues for understanding vulnerability in individuals and populations. While this paper does not focus on inferring suicidality of individuals based on the content they share online  we do examine how such behavioral and linguistic cues may be mined from data gathered from social platforms  with particular focus on the Werther Effect.,0,0,0,0,0,0,0,0,2,0,,0,0,0,0,1,0,,0
https://dl.acm.org/doi/abs/10.1145/2700171.2791026,19,6,health wellbeing and social media,"Kumar, Drezde, Coppersmith, De Choudhury",4,1,1,1,2015,Social media research has indicated that psychological states  health  well-being  and social support status of individuals may be gleaned via analysis of language and conversational patterns. These include utilizing social media to understand conditions and symptoms related to diseases [37  38]  influenza propagation [49] and prediction [39]  cyberbullying and teenage distress [14]  substance abuse [29  30]  postpartum depression [10]  mental health [12  36  22  8  9  50]  and insomnia [24]. Contrastively  research on suicide in social media is limited. Authors in [62] focused on South Korean blogs to predict nationwide suicide rate data. An interesting aspect of this work is that economic  meteorological  and celebrity suicide count variables were used as controls in the prediction model. Similarly  there is some evidence of announcements of suicidal thoughts on Facebook [48]  and public health consequences of suicide related content on social media was examined in [27]. This paper builds on this emergent body of research. We utilize data shared on a social media platform  Reddit  to probe the psyche and social milieus of individuals contemplating suicide or seeking support to fight such tendencies.,0,0,0,0,0,0,0,0,2,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/2700171.2791026,19,7,social media data,"Kumar, Drezde, Coppersmith, De Choudhury",4,1,1,1,2015,As mentioned earlier  we examine data from Reddit. We provide a description of the features of this social media platform  which are important to understand the context of our research problem. Reddit allows users to submit content in the form of links or text posts  organized by areas of interest or sub-communities called “subreddits” (e.g.  politics  programming  science). Users can voice their opinion on the post via a voting mechanism which allows more popular submissions to be featured more prominently according to their score: the difference between the “upvotes” and “downvotes” cast on each post (also known as “score”). Users can also engage with each other via a comment thread attached to each post. In 2014  Reddit had 71 billion page views  over 8  000 active communities  55 million posts  and 535 million comments5 . In this paper  we focus specifically on the subreddit called “SuicideWatch”  a forum for users contemplating suicide and who seek help  advice  and support. It is a strong support community with (currently) about 35K subscribers. The community is highly moderated  with many of its moderators and active subscribers adopting prominent roles in providing support to individuals showing vulnerability. In this subreddit  votes on posts are used as a proxy for support and to increase or decrease a post’s prominence  rather than as a statement of agreement/disagreement. We used Reddit’s official API6 to collect posts  comments  and associated metadata from r/SuicideWatch (hereafter SW). Our analysis in this paper is based on all posts made to SW between October 16  2013 and December 19  2014 – 66 059 posts from 19 159 unique users. Example (paraphrased) titles of posts from r/SuicideWatch are given in Table 1. We collected the title of the post  the body or textual content  ID  timestamp  author ID  the number of upvotes and downvotes the post obtained  including the difference between upvotes and downvotes on the post (i.e. score.),2,2,2,2,0,1,0,0,0,0,,0,0,0,0,0,2,,0
https://dl.acm.org/doi/abs/10.1145/2700171.2791026,19,8,wikipedia data,"Kumar, Drezde, Coppersmith, De Choudhury",4,1,1,1,2015,Next we compiled a list of reported celebrity suicides which fell within the time range of our Reddit data. Defining who is a “celebrity” is nontrivial  so we refer to the Wikipedia page listing celebrity suicides 7 as a way to measure who has sufficient celebrity status for inclusion. We obtained 10 reported celebrity suicides in the same period as our Reddit data; their names and reported suicides are shown in Table 2. We measure the prominence of a celebrity’s death by measuring the change in Wikipedia page views for the celebrity’s Wikipedia page. Wikipedia provides daily page view statistics for each page.8 We compare the number of page-views in the two weeks prior to their death with the two weeks following their death in terms of z-score (Figure 1). Here z-scores are computed by converting the page views to standard normal variable with 0-mean and standard deviation of 1. For 9/10 of the cases  we see a notable spike in number of views  showing that the suicides of these individuals were well-known enough to be viewable on such a macro scale and for examining the presence of Werther Effect in social media. We note two aspects related to the above analysis  and which will be used through the rest of this paper. First  since we are focusing on different types of data sources—Wikipedia and Reddit  we use z-score conversion as a normalization technique for the Wikipedia page views and Reddit’s SW posting activity volume. Further  the above observation in Wikipedia data  and the analyses that ensue  focus on observing changes over a two week window preceding and succeeding a celebrity suicide; this choice is motivated by our initial analyses and from the literature on Werther Effect [17].,2,2,2,2,0,1,0,0,0,0,,0,0,0,0,0,2,,0
https://dl.acm.org/doi/abs/10.1145/2700171.2791026,19,9,methods,"Kumar, Drezde, Coppersmith, De Choudhury",4,1,1,1,2015,Our goal is to measure the change in both the quantity and quality of posts to SW following a celebrity suicide. First  we will measure the volume of posts on SW preceding and succeeding a celebrity suicide to obtain a measure of increased interest in the topic of suicide. Next  we will use a series of content analysis techniques to examine the nature of these posts: how the topic of posts changed in the wake of the suicide.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/2700171.2791026,19,10,developing a baseline,"Kumar, Drezde, Coppersmith, De Choudhury",4,1,1,1,2015,We begin by constructing a baseline as to the expected variability in posts by measuring pairs of subsequent two week periods. Deviations from these expected trends following a celebrity suicide would provide evidence for the Werther effect. Our goal is therefore to identify  per celebrity  a set of k consecutive two-week time period pairs in the entire timeframe of our data. We refer to the first item of the pair (i.e.  the first two-week window) as the “preceding” window  and the immediately following two-week window as the “succeeding” window. Collectively  these baseline pairs yield an empirical distribution of the expected variation when there is not a celebrity suicide. Specifically  each of the k two-week window pairs (1) have no reported celebrity suicide  and (2) take periods that start on the same as the day of week. This accounts for day-ofweek related variations on SW. For the purposes of this paper  we choose k as 20.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,2,,0
https://dl.acm.org/doi/abs/10.1145/2700171.2791026,19,11,developing a control,"Kumar, Drezde, Coppersmith, De Choudhury",4,1,1,1,2015,Next  we develop a control to establish that changes in volume of posts in SW succeeding a celebrity suicide compared to that preceding it is attributed to the topic of suicide in particular  and such changes are not part of a broader shift in interest in mental health topics. For this purpose  we identified a set of “control group” subreddits  which are on topics related to mental health  but are unlikely to be specifically about suicide or suicidal ideation. These mental health subreddits (henceforth referred to as MH subreddits) were compiled based on our prior work in [11]; refer to the paper for details on how these subreddits are identified and crawled. Table 3 lists the control subreddits  crawled in the same timeframe as SW. We obtained 32 509 posts from 23 807 unique users. Like SW  all of these subreddits are public.,0,0,0,0,0,0,0,0,2,0,,0,0,0,0,0,2,,0
https://dl.acm.org/doi/abs/10.1145/2700171.2791026,19,12,linguistic measures,"Kumar, Drezde, Coppersmith, De Choudhury",4,1,1,1,2015,We propose four categories of linguistic and non-linguistic attributes to examine preceding/succeeding celebrity suicides. These are: (1) affective attributes  (2) cognitive attributes  (3) linguistic style attributes  and (4) social attributes. Measures belonging to all of these attribute categories are largely based on the psycholinguistic lexicon LIWC [40]  and were motivated from prior literature that examine associations between the behavioral expression of individuals and their responses to traumatic context and crises  including vulnerability due to mental illness [7  11]. Note that LIWC has been extensively validated to perform well on Internet language [7  18]. (1) We consider two measures of affect derived from LIWC: positive affect (PA)  and negative affect (NA)  and four other measures of emotional expression: anger  anxiety  sadness  and swear. (2) We use LIWC to define the cognitive measures as well: (a) cognition  comprising cognitive mech  discrepancies  inhibition  negation  death  causation  certainty  and tentativeness; and (b) perception  comprising set of words in LIWC around see  hear  feel  percept  insight  and relative. (3) Next  we consider four measures of linguistic style: (a) Lexical Density: consisting of words that are verbs  auxiliary verbs  nouns  adjectives (identified using NLTK’s [2] POS tagger)  and adverbs. (b) Temporal References: consisting of past  present  and future tenses. (c) Social/Personal Concerns: words belonging to family  friends  social  work  health  humans  religion  bio  body  money  achievement  home  and sexual. (d) Interpersonal Awareness and Focus: words that are 1st person singular  1st person plural  2nd person  and 3rd person pronouns. (4) For social attributes we utilized a variety of content sharing  social interaction  and social support indicators. These are: post length  number of comments  vote difference (difference between upvotes and downvotes  divided by total upvotes and downvotes)  comment arrival rate (average time difference between any two subsequent comments in a post’s comment thread)  time to first comment (time elapsed between the first comment and the timestamp of the corresponding post)  and median comment length9 . We compute each of the above linguistic measures of behavior at the post level – the value of a measure is given by the ratio of the number of words in a post that match words belonging to the measure  to the total number of words in the post. For each measure we take the average across all celebrities  to ensure each suicide event is equally weighted  i.e.  to avoid skew due to a single suicide. For statistical comparison  we used the Welch t-test; a negative tstatistic value means the measure increased after suicide.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,1,0,,0
https://dl.acm.org/doi/abs/10.1145/2700171.2791026,19,13,ngram analysis,"Kumar, Drezde, Coppersmith, De Choudhury",4,1,1,1,2015,We also present an analysis of the usage of various n-grams in posts shared succeeding and preceding celebrity suicides. Specifically  we focus on uni-  bi-  and tri-grams — we refer to them as n-grams throughout the paper. For comparison of the post and pre-celebrity suicide periods  we compute log likelihood ratios of each n-gram given as the logarithm of the ratio of the probability of occurrence of the n-gram in the post-suicide period  to the probability of the same n-gram over the pre-suicide period. Thus  when a n-gram is equally frequent  its log likelihood ratio will be zero; it would be greater than 1 if it is more frequent post celebrity suicides  whereas less than 1 if the reverse.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,1,0,,0
https://dl.acm.org/doi/abs/10.1145/2700171.2791026,19,14,topic model analysis,"Kumar, Drezde, Coppersmith, De Choudhury",4,1,1,1,2015,Our final content analysis method used a topic model  which have been commonly employed to analyze health data [45  37  38]. We obtain topics by running Latent Dirichlet Allocation (LDA) [3] over the combined set of posts shared in a two week period preceding and succeeding the celebrity suicide events. We use Gensim’s [46] implementation of online LDA from [21]. We use the default hyper-parameter settings and 50 topics  which we found to work well in initial experiments. To measure topic increases in post-suicide posts we first compute the posterior probability of each topic separately for the pre-suicide and post-suicide posts. We then compute the rate of increase for each topic as the difference between the posterior probability post-suicide and pre-suicide  divided by the probability of topic pre-suicide.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,1,0,,0
https://dl.acm.org/doi/abs/10.1145/2700171.2791026,19,15,measuring post volume,"Kumar, Drezde, Coppersmith, De Choudhury",4,1,1,1,2015,Figure 2 shows trends in posting activity on SW (raw frequencies of posts)  overlaid with the times of reported celebrity suicides. To quantify changes in the aftermath of the celebrity suicides  we use z-scores of the number of posts in the pre celebrity suicide and post celebrity suicide two week periods10; change per celebrity is given by the difference between post-suicide z-scored #posts and the presuicide z-scored #posts (Figure 3). Posting activity increased after most celebrity suicides; combining all suicides  we find a strong increase of SW posts (Table 4). Compared to the observed changes in the empirical background distribution (§4.1.1) we find that the actual change combined across all 10 celebrities is 3.64 (mean difference between post-celebrity suicide z-score of posts and pre-suicide z-score across all celebrities)  whereas  although positive  the same change is nearly half the mean baseline change (1.95) (Table 4). Further  paired t-tests indicate that the change associated with each celebrity suicide event is significantly different from the corresponding baselines (p < .001). Thus the observed increase in SW posts is unlikely due to a random fluctuation in posting volume. Finally  comparing the z-score posting change of SW (3.64) to the MH subreddits control group (0.43) we find that these increases are specific to SW (Table 4). These three findings support the manifestation of the Werther Effect in the forum.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,1,0,,0
https://dl.acm.org/doi/abs/10.1145/2700171.2791026,19,16,linguistic measures,"Kumar, Drezde, Coppersmith, De Choudhury",4,1,1,1,2015,Table 5 summarizes the linguistic measures of behavior derived from SW content  comparing the two week period following celebrity suicides to a period of same length before. (1) Affective Attributes: Post-suicide content is more negative  angry  sad  and anxious. Prior literature indicates increased negative affect to be associated with increased emotional vulnerability such as mental instability  helplessness  loneliness  and restlessness [59]. nitive biases. Posts are less certain  show increased negation  and use more perception centric words  such as words in the category ‘feel’. The psychology literature indicates such cognitive biases to be associated with lower emotional stability and increased selfconsciousness [44  25  12]. Additionally  post-suicide content has more death related conversation and shows lowered inhibition. These are known to be associated with greater health concerns as well as suicidal thoughts [58  7]. Lowered inhibition also indicates an increased tendency for self-disclosure [42  57] – suicidal ideation or suicide interest  lowered self-esteem and display of self-derogatory thoughts are extreme forms of self-disclosure and inhibition. (3) Linguistic Style Attributes: Post-suicide content has lower lexical density. Greater mental health challenges and suicidal tendency is known to show this characteristic – such content is mostly about the self  hence people attribute less to things  happenings or people around them [60  11]. The literature also associates lower lexical density to high drive states  which are typical in suicidal individuals [35]. Post-suicide content is less concerning about the future and more fixated on the past – likely due to the manifestation of suicidal tendencies [55]. Lowered future orientation is a known attribute of negative attitude towards one’s own life and actions. Additionally  post-suicide content shows little social and personal concerns. It is known that suicidal thoughts are accompanied by thoughts about the self and self-occupation  hence they are less likely to talk about words relating to social  work  and humans [7]. (4) Social Attributes: Post-suicide content shows more use of first person singular pronouns and fewer second and third person pronouns. This suggests that posters are less socially concerned or bothered. Suicidal ideation is associated with greater self-attentional focus [57  60  12]. In fact  together with the fact that they also exhibit fewer social and personal concerns  it is likely that the postsuicide cohort in SW share more personal stories and in general  high self-preoccupation [4]. Post-suicide content is longer – literature on self-disclosure on stigmatized topics  such as suicide interest  shows that greater self-disclosure is associated with longer and more verbose content [20  23  11]. Further  the SW community seems to provide more support through a greater number of and longer comments  likely because of their high degrees of expressed vulnerability. However  they get fewer upvotes – suicide interest or ideation related content are unlikely to garner positive approval. People also tend to comment faster – we presume due to the increased sensitive content  the community volunteers to provide help and advice quickly.,0,0,0,0,0,0,0,0,0,0,,2,0,0,0,1,0,,0
https://dl.acm.org/doi/abs/10.1145/2700171.2791026,19,17,ngram comparison,"Kumar, Drezde, Coppersmith, De Choudhury",4,1,1,1,2015,Going beyond the linguistic measures  we now investigate whether and how usage of n-grams change following celebrity suicides. Per section 4.2.2  we obtained frequencies and log likelihood ratios of all uni-  bi-  and tri-grams from the SW posts in the two weeks following the suicides and also that in the same period before. An analysis of the n-grams that exhibit the greatest changes preand post- celebrity suicide demonstrate a similar trend (Section 4.2.2). We also measured the adjusted mutual information between the frequencies of occurrence of the n-grams (frequencies > 50) after celebrity suicides and that before: yielding a score of 0.21. This indicates that there is little correlation in terms of frequencies of these n-grams before and after celebrity suicide reports. Further  a Welch t-test informs that this difference to be statistically significant – the t-statistic is found to be 3.7 (p < .001  df = 6892). Table 6 gives a list of 75 n-grams and their associated log likelihoods — we present them in three categories  25 n-grams each with highest and lowest log likelihoods  and most frequent 25 ngrams with log likelihood zero. The first column indicates those n-grams more frequent in the post celebrity suicide period versus before  the second indicates the reverse  and the third column is a set of n-grams which equally co-occur in both categories. Our findings align with those from the previous subsection — qualitative inspection of the n-grams in the first column suggest SW content shifts to a more vulnerable tone following the celebrity suicides (first column) compared to before (second column). We organize the inspected themes into various broad categories in line with literature on analysis of content in suicide notes [35  51  57]. In the aftermath of the celebrity suicides  we find evidence of expression of anxiety and depression  sense of guilt and regret (“i hate it”  “piece of shit”  “hate myself so”  “give a shit”  “without me. i”)  hopelessness (“i gave up”  “i ended”)  sorrow (“alone i”  “leave me”)  and explicit desire to end their life (“’to hang myself”  “wanting to kill”  “of suicide”  “tired of living”). During this period we also observe heightened conflicting thoughts (“but right now”  “i’m probably”)  and a sense of urgency and help seeking (“really just want”  “to let me”  “help me”). On the other hand  n-grams that are more frequent in the period before the celebrity suicides tend to be less sensitive or vulnerable. In many cases use of n-grams like “be happy. i”  “a good person”  “be happy”  “hope” indicate a tendency for individuals to strive towards maintaining a positive spirit. We also observe greater intent to seek help from the SW community — “if anyone has”  “feel free to”  “want people to”  “tell my parents”. Mentions of close friends and family are also widely common (“friends  no”  “family  i”  she says she”  “my father is”  “people  but”  “my dad and”  “to her. i”). To contrast with the n-gram usage following the suicides  it thus seems that individuals are typically keen to derive help and support from the SW community  however this reduces significantly in the short time period following the celebrity suicides. Compared to the above two categories of n-grams  we observe that certain types of content are prevalent in SW posts irrespective of the time of reports of celebrity suicides. These include reflection (“thinking of”  “don’t even know”  they wouldn’t”  “we’re all”  “reason to live”)  illustration of one’s experiences of coping with distress (“i’ve struggled”  “cope with”  “going through with”  “a therapist  ”)  and negative tone (“badly”  “unhappy”  feel bad for”). We also observe some n-grams suggesting call for help (“call me”  “helped me”  “phone”). This indicates the manner in which the broader SW community might be catering to some of the prime needs of this population in a consistent fashion irrespective of external agents or events.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,1,0,,0
https://dl.acm.org/doi/abs/10.1145/2700171.2791026,19,18,topic model analysis,"Kumar, Drezde, Coppersmith, De Choudhury",4,1,1,1,2015,Finally  we provide a topic analysis to determine how topics discussed in SW prior to celebrity suicides differ from those following the suicides. For the purpose  we use the method described in section 4.2.3. From Figure 4  we observe that the mean (absolute) change across all topics is 2.34% (±9.16%) in the two-week period succeeding the suicides compared to that before. Specifically  we observe that topics #4  50  30  8  17 (five topics with highest absolute change) show notable changes in use in the post-celebrity suicide period  compared to that preceding the suicides. Now we analyze change in the content of these topics  like with n-grams  based on prior literature on analysis of suicide notes [35  51  57]. We present top changing five topics along with a set of their representative words that capture the essence of the topics. (1) lost  useless  done  poisoning  alone  fucking  angry  shitty  hate  suffocate  damn: This topic (#4) describes self-derogatory and selfcritical thoughts relating to self destruction (increase: 13%). (2) suicidal  sorry  lifeless  kill  death  withdraw  horrible  anxiety  rough  afraid  hotline  flashes  numb  scars  harsh  scared: This topic (#50) manifests confessions and regrets of individuals and their desire to commit suicide  particularly via distress expression (increase: 7%). (3) maybe  though  probably  except  however  if  but  admit  reason  finally  support  wanted  hurt  bad  sure  nothing  pain  best: This topic (#30) illustrates the struggling and conflicting feelings that are often known to ensue suicidal ideation [51] — cognitive functioning under competing motives  e.g.  self-criticism vs. selfprotection  aggression vs. affection towards others (increase: 6%). (4) talk  help  ask  emergency  advice  happy  sober  smile  trying  comforted  believe  promise  hopeful  love  sure  friend  family  relationship  parents  school  domestic  guy  undergrad: This topic (#8) mixes requests for help  including constructs related to demand  command  and request. It expresses needs of the individual and requires some behavior on the part of the listener for their satisfaction [20]. We conjecture such requests are a prime reason why individuals join the SW community. We also notice manifestation of positive and compassionate cognitive thoughts  as well as content around societal  practical and familial concerns. Alarmingly  this topic decreases after celebrity suicide (decrease: 6%). (5) really  anything  want  always  never  everything  unable  still  every  unwilling: This topic (#17) reflects information under high drive or emotion and tend to be more extreme  polarized or ambivalent in their assertions [34]. This is indicated by use of terms that permit no exception (increase: 5%).,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/2700171.2791026,19,19,discussion,"Kumar, Drezde, Coppersmith, De Choudhury",4,1,1,1,2015,Our empirical findings suggest that social media (Reddit in this work) contain subtle yet significant changes in language and activity around suicide content following celebrity suicide events. To the best of our knowledge  this is one of first studies examining the prevalence of the Werther effect centered around suicidal ideation manifested online. However  we urge caution in deriving causal implications from our findings. While SW is a great resource for suicide related support  it is a platform wherein individuals discuss why they desire to end their lives  instead of a channel for communicating or sharing suicide notes. We cannot assess how increased activity on SW compares with increased attempted or completed suicides. In fact  participation in such online support groups may decrease suicide attempts. Nevertheless  our study provides insight into a previously difficult facet of suicide to explore — discussion and ideation. Next  one presumed explanation of the observed SW post changes might be that the users are  in fact  discussing the celebrity suicides themselves. To examine the validity of this conjecture  we calculated the number of times each celebrity was mentioned — 8/10 celebrities were never mentioned  L’Wren Scott was mentioned once  and Robin Williams was mentioned 52 times. This suggests that the ensuing linguistic changes of post content are not attributable to discussion of the suicide events of the celebrities themselves. Additionally  we note that one celebrity suicide is perhaps qualitatively different than the rest (as is evidenced in the preceding paragraph) – that of Robin Williams. To ensure that our observed effects were not driven by this singular event  we repeated our empirical investigation excluding his suicide; our findings remain true. For instance  even after disregarding this suicide  the changes in the period succeeding suicides compared to that before was significantly higher than the baselines and the control group (MH subreddits)  although it decreased to some extent (32% decrease for the former  and 8% for the latter). The different LIWC categories continued to show significance  while the difference between the posterior probabilities of the pre-celebrity suicide and post-celebrity suicide topics was also distinct. We also comment on the generalizability of our findings. We acknowledge that the results presented in this paper are limited to those individuals who participate in the SuicideWatch support forum on Reddit. It is possible there is a self-selection bias in this population. For instance  it is likely it is a set of individuals who are seeking help on this sensitive issue. Second  it is also a group who are choosing an online platform for seeking help  instead of  or in addition to other (offline) modalities of suicide support. Caution is advised given the known bias of Reddit user base as well—the average redditor is a 20-something male11  and perhaps more “techsavvy” than the general set of individuals contemplating suicide. Finally  even though some of the linguistic and content cues that we mined are Reddit-specific (language characteristic of the platform’s culture)  our methods could be extended to other social media platforms as well—especially to those online platforms which possess similar attributes implicitly or explicitly  and which allow sharing of textual content.,0,0,0,0,0,0,0,0,0,0,,2,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/2700171.2791026,19,20,implications of findings,"Kumar, Drezde, Coppersmith, De Choudhury",4,1,1,1,2015,There are many practical implications of this work. Community moderators may develop strategies for the aftermath of celebrity suicides  allowing them and other interested/committed volunteers to be more proactive with the help they offer  as well as provide some unstated context relevant to support during the period immediately following a celebrity suicide report. Awareness of this phenomenon on SW will also allow moderators and volunteers to pay specific attention to redditors who show increased signs of suffering exacerbated by the suicide event. Additionally  individuals whose content contain phrases and other linguistic constructs of high suicidality may be connected to community members who have volunteered to provide help and support— social support and higher levels of social capital can help individuals fight such vulnerable tendencies. Thus  adequately deployed interventions following celebrity suicides can actually motivate potentially suicidal individuals to steer away from their final decision  and encourage them to continue living.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/2700171.2791026,19,21,ethics,"Kumar, Drezde, Coppersmith, De Choudhury",4,1,1,1,2015,Finally  we identify the ethical challenges of this line of research. The interventions outlined above need to honor the privacy of the individuals and those who volunteer to provide help and support. Further  beyond the design suggestions outlined above  how to actually intervene  deploy  and offer support to individuals of high likelihood of suicide is a research and ethical question of its own. Especially given the (semi)-anonymous ecosystem that Reddit’s SW forum provides to this sensitive population  we need to inculcate utmost care in the manner in which help and support are catered to them following the evidence of existence of the Werther Effect. Unthoughtful interventions may actually lead to counterhelpful outcomes—for instance  chilling effects in participation in the community  or suicide ideation related expression moving on to other alternative or peripheral platforms online where such populations might be difficult to discover and therefore extend help to. Broadly  the ethical dimensions of interventions and their deploy ment need to ensure that communities like the SW can continue to be safe and powerful platforms for seeking help  advice  and support around suicidal tendencies  and for online psychotherapy.,0,0,0,0,0,0,0,0,0,0,,0,2,0,0,0,0,,2
https://dl.acm.org/doi/abs/10.1145/2700171.2791026,19,22,conslusion,"Kumar, Drezde, Coppersmith, De Choudhury",4,1,1,1,2015,We have presented preliminary findings on the manifestation of the Werther effect in the prominent Reddit suicide support forum r/SuicideWatch. We found significant changes manifested following reports of several celebrity suicides – impacting the frequency of posting activity as well as the nature of content shared. Our findings are among the first to demonstrate the Werther effect on suicidal ideation and have implications for building suicide prevention interventions following high-profile suicides. There are several interesting directions to future work. Examination of other suicide support forums and social media  observations over longer periods of time and more celebrities and across varied cultural contexts will help us generalize our findings on presence of Werther Effect online. We also intend to investigate to what extent Werther Effect in social media relates to actual changes in completed (government-reported) suicide numbers in regions where the Reddit platform is widely adopted. Causal relationships between the change observed following celebrity suicides and the reporting of these suicide events can be better inferred through a predictive setting  which also constitutes a promising future research direction. Finally  given our observation that more prominent suicidal thoughts are expressed in posts succeeding the celebrity suicides  automated or semi-automated suicide ideation detectors may be developed using machine learning approaches  that can be used to bring timely help and support to these vulnerable communities.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://aclanthology.org/W17-3110.pdf,20,1,abstract,"Loveys, Crutchley, Wyatt, Coppersmith",4,0,0,1,2017,Many psychological phenomena occur in small time windows  measured in minutes or hours. However  most computational linguistic techniques look at data on the order of weeks  months  or years. We explore micropatterns in sequences of messages occurring over a short time window for their prevalence and power for quantifying psychological phenomena  specifically  patterns in affect. We examine affective micropatterns in social media posts from users with anxiety  eating disorders  panic attacks  schizophrenia  suicidality  and matched controls.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://aclanthology.org/W17-3110.pdf,20,2,introduction,"Loveys, Crutchley, Wyatt, Coppersmith",4,0,0,1,2017,mental illness and suicide pose a significant public health problem. Each year approximately 800 000 people will die by suicide  and an estimated 16 million suicide attempts will occur (World Health Organization  2013). Mental illness is a similarly widespread problem  affecting almost one in four people worldwide during the course of their lifetime (World Health Organization  2013). Mental illness (including suicide) detrimentally affects quality of life  ranking as the fourth-largest contributor to disability-adjusted life years (Vigo et al.  2016). Moreover  five of the top twenty causes of global disease burden were from mental illness (Vigo et al.  2016). Little progress has been made over the past fifty years in terms of improving these figures (Franklin et al.  2016). A key step to reducing the global burden of mental illness and suicide deaths is to ensure that early risk detection and intervention occur (Insel  2009). Current systems of care struggle with scalability and measures of long term efficacy. Given recent advances in many industries by ubiquitous technology and data science  many hold out hope that a similar revolution is possible in mental health. Digital phenotyping  where data from everyday interactions with digital devices like smartphones and computers can be turned into quantifiable signals of mental health  holds promise for providing the real-time data needed for these advances. Real-time analysis of dispositional and discrete situational factors could help clinicians predict the onset or exacerbation of symptoms or suicidal behaviors (Nelson et al.  2017). This would transcend analysis and open the possibility for data-empowered interventions. Generally  computational linguistics uses techniques that examine significant portions of a user’s data  spanning a long period of time. The few exceptions still only examine subsets of the data on the order of days or weeks (Resnik et al.  2015; Coppersmith et al.  2016; Mitchell et al.  2015). However  there are meaningful psychological phenomena occurring at much smaller time scales that slip past current methods (Nelson et al.  2017). Micropatterns  inspired by Bryan et al. (in press)  are intended to focus on this neglected time window on the order of hours  by analyzing consecutive social media posts within such a window. Here we examine affective micropatterns in language produced by individuals with a self-reported diagnosis of mental illness  a panic attack or suicide history  and neurotypical controls. We evaluate the affective valence of sequences of three consecutive tweets produced by individuals in each user group to identify micropatterns characteristic of each group. We compared suicide  panic attack  and mental illness group micropatterns to those of neurotypical controls. We address two questions: [1] Are there meaningful signals in affective micropatterns relevant to mental health? [2] Do micropatterns hold more information than the labels that make up their components? This paper is the first time that affective micropatterns are examined directly  rather than as a component of a more complex learning system. This is also the first time that the relative power of micropatterns is explored beyond suicide risk.,0,0,0,0,0,0,0,0,2,0,,0,0,0,0,0,0,,0
https://aclanthology.org/W17-3110.pdf,20,3,why social media,"Loveys, Crutchley, Wyatt, Coppersmith",4,0,0,1,2017,One particularly compelling and rich source of data for digital phenotyping is language. Language provides a window into the perception  cognition  and other psychological processes at work in a person  and thus provides a useful lens through which we can understand  quantify  and eventually improve mental health. Social media  in particular  provides a trove of language data in a form conducive to computational analysis. Critically for this work  it also includes the time that a particular piece of language was authored by the user. Social media is  thus  one data source through which the early signs of mental illness and suicide can be detected (Reece et al.  2016; Coppersmith et al.  2016; Bryan et al.  in press). Quantifiable signals for a wide range of behavioral health conditions have been uncovered recently  and this provides a foothold into analysis and intervention empowered by data science. A wide array of conditions have been studied including major depressive disorder (Chung and Pennebaker  2007; De Choudhury et al.  2013)  post-traumatic stress disorder (Coppersmith et al.  2014b  2015b; Resnik et al.  2015; Preotiuc-Pietro et al.  2015; Pedersen  2015)  schizophrenia (Mitchell et al.  2015)  eating disorders (Walker et al.  2015; Chancellor et al.  2016)  generalized anxiety disorder  bipolar disorder (Coppersmith et al.  2014a)  suicide (Coppersmith et al.  2015c; Kumar et al.  2015; Wood et al.  2016; Kiciman et al.  2016)  borderline personality disorder  and others (Coppersmith et al.  2015a).,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://aclanthology.org/W17-3110.pdf,20,4,social media micropattern analysis,"Loveys, Crutchley, Wyatt, Coppersmith",4,0,0,1,2017,Micropatterns in short sequences of emotion  cognition  behavior and symptoms relevant to specific psychological states may be evident in social media data  reflecting dynamic shifts in internal situational factors. Many social media users report enough personal information on public feeds to be able to capture brief shifts in behaviors  cognitions  emotions  and symptoms relevant to particular psychological states. This information has been used to assess whether a user is declining into a suicidal state (Bryan et al.  in press). Bryan et al. (in press) found that distinct micropatterns in content of social media posts were predictive of proximity to a suicide death. One month prior to a suicide death  a seesaw-like effect was observed between social media posts about a maladaptive coping behavior and a negative belief  and at one week prior to a suicide death  this negative relationship grows stronger. Bryan et al. (in press) detected micropatterns from human-labeled posts and a complex model informed by dynamic systems theory. Here  we complement this work by adding automation to the labeling and exploring the micropatterns directly  rather than embedded in a larger system. No prior research has evaluated micropatterns in social media post content for psychological disorders other than suicidality. This technique of looking at short subsequent posts and the psychological phenomena present therein is relatively new  so we aim for simplicity and straightforwardness in our experimental design and features. While there are a number of potentially more interesting avenues of exploration involving fine-grained emotions  psychologically meaningful events  coping mechanisms  and decompensation  we eschew the added complexity in favor of exploring a fundamental unanswered question: Is there meaningful signal in the micropatterns relevant to mental health?,0,0,0,0,0,0,0,0,2,0,,0,0,0,0,0,0,,0
https://aclanthology.org/W17-3110.pdf,20,5,symptom dynamics,"Loveys, Crutchley, Wyatt, Coppersmith",4,0,0,1,2017,Broadly  the motivation for exploring micropatterns and data on the timescale of minutes and hours stems from the importance of temporal information in the assessment of psychological symptoms. Knowledge of symptom cooccurrence over specified time periods can determine whether a mental illness diagnosis is received  as well as inform assessments of treatment responsiveness and relapse (American Psychiatric Association  2013; Nelson et al.  2017). Temporal information is essential to detecting ongoing fluctuations in psychological symptoms  which may be key to predicting the onset of psychological disorders or increased suicide risk (McGorry and van Os  2013). Emotions  behavior  and cognitions fluctuate rapidly as an individual interacts with the environment (van Ockenburg et al.  2015; van Os  2013). People have tendencies to behave  think  or feel certain ways  however  conditions and interactions fluctuate and one might have a markedly different reaction to the same environment on a different day. These brief shifts in behaviors  emotions  cognitions  and physical symptoms relative to one another in an environment  over the course of seconds to hours  can determine a persons presentmoment psychological state (van Os  2013). The Fluid Vulnerability theory encapsulates this idea  suggesting that daily perturbations in situational factors interact with dispositional factors to trigger present-moment psychological states (Rudd  2006). Dispositional (or distal) factors establish baseline risk  and are relatively fixed variables such as demographics  trait characteristics  beliefs or life histories  which tend to indicate stable predispositions toward experiencing particular psychological states or disorders. Conversely  situational (or proximal) factors indicate the likelihood that a person experiences a mental illness episode or engages in self-harming behavior at a specific point in time. Examples could include events such as the onset of a troubling thought or an unpleasant social interaction in the workplace. The Fluid Vulnerability theory suggests that for individuals with low baseline risk  even a severe stressor will not elicit suicidality or exacerbations in mental illness symptoms; alternatively  for people with high baseline risk  situational factors conducive to suicidality or mental illness episodes need not be as high for an episode to be triggered (Rudd  2006). Most work at the intersection of natural language processing and social media has focused on assessing dispositional factors through examination of a large corpus of posts. However  assessing more situational risk factors will require a different set of methods. While existing bag of words approaches evaluate dispositional risk factors  temporal analyses are necessary to detect brief fluctuations in situational risk factors.,0,0,0,0,0,0,2,0,0,0,,0,0,0,0,0,0,,0
https://aclanthology.org/W17-3110.pdf,20,6,data,"Loveys, Crutchley, Wyatt, Coppersmith",4,0,0,1,2017,We briefly explain the data collection method here  but we refer the interested reader with further questions on the methodology to Coppersmith et al. (2016) for the suicide attempt data and Coppersmith et al. (2014a) for all other conditions. The data for these analyses are Twitter posts collected via two methods. Most of the data come from users who have publicly discussed their mental health conditions. These users are frequently referred to as “self-stated diagnosis” users  as they state publicly something like “I was diagnosed with schizophrenia”  or “I’m so thankful to have survived my suicide attempt last year”. The data for users with a suicide attempt was supplemented by data from OurDataHelps.org  a data donation site where people provide access to their public posts and fill out a short questionnaire about their mental health history. Data are then deidentified and made available to researchers addressing questions of interest to the mental health community. Donors provide consent for their data to be used in mental health research upon signup. Of the users who attempted suicide  146 came from OurDataHelps.org. Specifically  we examine generalized anxiety disorder  eating disorders  panic attacks  schizophrenia  and attempted suicides. These conditions were selected based on the theory that there are important timing aspects to their symptoms – ebbing and flowing of symptoms as treatment is effective (especially schizophrenia)  onset and exacerbation of symptoms by external events and stress  and punctuated events in time of psychological symptoms (suicide attempts  panic attacks  and binging/purging behavior with eating disorders). We use the Twitter streaming API to collect a sample of users who used a series of mental health words or phrases in their tweet text (e.g.  ‘schizophrenia‘ or ‘suicide attempt‘). Each tweet that uses one of these phrases is examined via regular expression to indicate that the user is talking about themselves. Finally  those tweets that pass the regular expression are examined by a human to confirm (to the best of our ability) that their selfstatement of diagnosis appears to be genuine. This results in a dataset with users that have a self-stated diagnosis of generalized anxiety disorder (n = 2408)  an eating disorder (749)  panic attacks (263)  schizophrenia (350)  or someone who would go on to attempt suicide (423). Some of these users do not exhibit the sort of posting behavior required to create micropatterns (i.e.  they rarely post multiple times within a 3 hour time window). We exclude these users from our analysis  which is 5-9% of users for most conditions  with the exception of those with a suicide attempt  where a little over half the users do not exhibit this posting behavior. The resultant dataset used for analyses is: generalized anxiety disorder (n = 2271)  eating disorders (687)  panic attacks (247)  schizophrenia (318)  suicide attempts (157). In order to allow comparisons of each condition to control users  we gather a random sample of 10 000 Twitter users for whom at least 75% of their posts are identified by Twitter as English. All the users with a self-stated diagnoses and all members of this control population have their age and gender estimated according to Sap et al. (2014). For each user with a self-stated diagnosis  we find a matched control through the following procedure: create a pool of users where the estimated gender matches and the estimated age is within the same 10-year bracket (the suggested accuracy of the age estimator). From that pool of age- and gender- matched users  we select the user whose tweets start and end over the most similar timeframe. We will refer to these age-  gender-  and time-matched controls simply as “matched controls” throughout the rest of this paper. All tweets were publicly posted by their author (i.e.  no users marked at “protected” or “private” were included). On average  users had 2949 tweets. The distribution of estimated age and genders for users with each self-stated condition can be seen in Figure 1. For most conditions  the population skews female  though for schizophrenia the genders are roughly balanced. The average age tends to be in the early-to-mid 20s.,2,2,2,2,2,0,0,2,2,0,,0,2,2,0,0,1,,0
https://aclanthology.org/W17-3110.pdf,20,7,caveats,"Loveys, Crutchley, Wyatt, Coppersmith",4,0,0,1,2017,All of the following analysis is subject to a few caveats emergent from the data and how the data were collected. The users with mental health conditions are all found data of one sort or another  so there are some inherent biases. We prefer to express these biases rather than add complexity by attempting to cleverly correct for them. Many of these users talk publicly about their mental health  which given the stigma and discrimination they face  is likely a distinct subpopulation of those with mental health conditions. It is possible that users with a psychological disorder or suicide history who did not publicly disclose this information could have been included in the control group for analyses  which may have the effect of artificially lowering the estimated power of any emergent differences. Users who donated data through OurDataHelps.org are likely biased differently  with over representation of altruism  since they are willing to do things for the public good without any obvious self gain. Another consideration is that all users who reported a suicide attempt within our dataset survived. There is a possibility that characteristic differences also exist between individuals who do and do not die by a suicide attempt. Note that this research was conducted on English-speaking social media users. The content of social media post micropatterns for psychological disorders and suicidality could differ between cultural contexts  due to differences in cross-cultural expressions of mental illness (Chentsova-Dutton et al.  2007). These are active Twitter users  which imparts a demographic skew compared to the rest of the world (in particular  these users skew young). We see more females in our user populations than the rough gender balance observed for general Twitter users (Greenwood et al.  2016). The language data itself is meant for public consumption  and may reflect how the authors wish to be perceived  and not what one would get from a more traditional journal study of internal and private thoughts and feelings. Finally  we include users who had a concomittant or comorbid mental health condition. Thus a small number of users appear in more than one category,0,0,0,0,0,0,2,0,2,0,,2,0,0,0,0,0,,0
https://aclanthology.org/W17-3110.pdf,20,8,methods,"Loveys, Crutchley, Wyatt, Coppersmith",4,0,0,1,2017,This study aimed to examine the prevalence of affective micropatterns in social media posts and highlight differences in micropattern occurrence that might be relevant to quantifying mental health. Primarily  we do this through comparison of users with anxiety disorders  eating disorders  schizophrenia  suicide attempt history  and their matched controls. We use a straightforward and well-understood method for sentiment analysis  VADER (Hutto and Gilbert  2014)  to produce a trinary label for each message: positive  neutral  or negative. VADER outputs a [0  1] score for each sentiment label; we use the label with the maximum score. Specifically  we examined trajectories of posted emotional content in three subsequent tweets  no more than three hours from earliest to latest. The same tweet will be counted in more than one over lapping micropattern if more than three tweets occur in the three-hour time window – so if 5 tweets occur in 3 hours  3 micropatterns will be recorded from those 5 tweets  likewise for 4 tweets  2 micropatterns will be recorded. The potential overlap exists for both patients and neurotypical users  and subsequent analyses (e.g.  classifying users based on proportion of micropatterns) were designed to be robust to this property of overlapping micropattern generation. The number of sequential tweets to examine was chosen to minimize the complexity of the analysis while allowing significant variability to be observed. Critically  we aimed for the resulting dimensions (i.e.  number of distinct micropatterns) to be small enough for meaningful interpretation by clinical psychologists.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,1,0,,0
https://aclanthology.org/W17-3110.pdf,20,9,results,"Loveys, Crutchley, Wyatt, Coppersmith",4,0,0,1,2017,Our results collectively suggest that (1) micropatterns are not random (2) there are some significant differences in the occurrence of micropatterns between users who have a given mental health condition and their matched controls and (3) there is some quantifiable predictive power for separating users with mental health conditions from their matched controls captured by the micropatterns  in excess of what power the labels that underlie the micropattern have alone.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://aclanthology.org/W17-3110.pdf,20,10,micropatterns are not randomly distributed,"Loveys, Crutchley, Wyatt, Coppersmith",4,0,0,1,2017,Before any analysis of the differences in micropattern occurrence between users with mental health conditions and their matched controls  we demonstrate that these micropatterns are not randomly distributed  nor are they an artifact of the different base rate of users with mental health conditions expressing negative sentiment more often. Previous work indicates that there are some expected variability in the proportion of messages from users in each condition  and significantly different from their matched control users (Coppersmith et al.  2015a). Specifically  it has been widely reported that users with certain behavioral health conditions use more words from the LIWC category Negative Emotion (Chung and Pennebaker  2007; Park et al.  2012; De Choudhury et al.  2012; Coppersmith et al.  2015a)   which in this case would have the effect of inflating the number and proportion of micropatterns involving negative labels  simply because the prevalence of these labels were higher. For each condition  we observe the distribution of labels for all messages from each condition. This establishes the base rate of each label occurring for that condition. Using these base rates  we randomly generate a label for each message from each user according to the base rate (i.e.  respecting the timestamps of each post  but randomly assigning a label rather than what VADER predicted from the text). We then  for each user  examine the observed micropatterns with these randomlyassigned labels. We repeat this procedure 10 000 times  thus providing a null distribution of what we would expect the number and proportion of micropatterns to be if the underlying sentiment labels were randomly distributed. When we compare the observed value from real data to this randomlygenerated population  the differences are stark and large. The observed z-scores for each micropattern’s deviation from normal range from 13.3 to 423859.1  with a median of 895.5. Since the significance for a z-score (at the p < 0.05 level) is 1.96  we can safely assume that the observed population of labels was not likely the result of a random process. This strongly suggests that the differences observed are not attributable merely to random fluctuations and a different base-rate of the underlying labels.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://aclanthology.org/W17-3110.pdf,20,11,differences in micropatterns,"Loveys, Crutchley, Wyatt, Coppersmith",4,0,0,1,2017,Figure 3 shows the deviation in each micropattern for users with mental health conditions relative to their matched neurotypical controls. This  taken with significant differences observed in matchedsample t-tests (omitted for brevity)  clearly indicates that there are significant differences in micropatterns for a range of mental health conditions. While there are some observed similarities between the changes in micropatterns across conditions  significant differences exist between the various mental health conditions and their deviations from controls. Note that the vast majority of the micropatterns observed in all conditions (> 80%) are (neutral neutral neutral). This is likely an overestimate of the number of neutral messages present  due to the closed-vocabulary nature of our lexicon-based labeling approach. Specifically  VADER depends on a lexicon of words and associated scores  and lexicon-based approaches generally provide higher precision (i.e.  fewer false alarms  which means fewer neutral messages tagged as valenced) at the cost of significantly decreased recall (i.e.  many valenced messages are tagged as neutral). This is exacerbated by the fact we are scoring individual tweets  which contain relatively few words. Thus  while there are often some parameters to adjust around the sensitivity of classifiers  the combination of the lexicon approach and the short document makes for a very sparse set of features to score from. In turn  this tends to create more neutral labeled messages. Some observed deviations line up with current psychological literature  providing some facevalidity to this approach. First  all mental health conditions show an increase in the number of (negative negative negative) affect micropatterns. This is consistent with the widelyfound phenomenon that those with mental health conditions tend to experience greater negative affect (Chung and Pennebaker  2007; Park et al.  2012; De Choudhury et al.  2012; Coppersmith et al.  2015a). This does suggest  though  that these are not necessarily randomly distributed negative posts  but in fact they are more likely to have concentrated and subsequent strings of negative posts. Second  users with schizophrenia were less likely than neurotypicals to show affect or affective variability between posts. This reflects research suggesting that individuals with schizophrenia display deficits in affective expression; a common negative symptom triggered by both disease pathophysiology and use of antipsychotic medication (Messinger et al.  2011). Third  we see increases in affective volatility by users prior to a suicide attempt (as evidenced by (positive negative positive) and (negative positive negative) micropatterns  consistent with many as-of-yet unpublished findings from the Jelenik Summer Workshop at Johns Hopkins University (Hollingshead et al.  in prep.). Fourth  users with an anxiety disorder were less likely than neurotypical controls to post consecutive positively-valenced tweets. This may be reflective of a negative attentional bias often associated with anxious emotion (Bar-Haim et al.  2007).,0,0,0,0,0,0,0,0,0,0,,2,0,0,0,0,0,,0
https://aclanthology.org/W17-3110.pdf,20,12,separating users,"Loveys, Crutchley, Wyatt, Coppersmith",4,0,0,1,2017,We also aim to understand if micropatterns convey some additional information about mental health and mental health status  above and beyond the labels that go into the micropattern (in this case  positive  negative  and neutral sentiment labels). Ideally  we would examine how well micropatterns could predict meaningful psychological events  but we lack significant data to do this more than anecdotally. Instead  we continue in line with previous work and compare performance on a binary prediction task. The task is to separate users with mental health conditions from their matched controls. Rather than examining absolute performance of this task as if it were a real world scenario  we aim to examine the relative performance of the micropatterns  the underlying sentiment labels  and a combination of the two  as a way of assessing how much unique information the micropatterns themselves impart1 . For each user  we created a feature vector where each entry was the proportion of micropatterns that a particular micropattern made up. Similarly  we made a feature vector for the proportion of sentiment labels that each sentiment label made up (the base rate). Figure 4 shows the accuracy results of a 10-fold cross validation binary classification experiment (balanced samples) using a random forest classifier. In all cases  the micropatterns outperform the base rate  which is often little better than chance. In most cases  using both signals together (by concatenating the feature vectors) provides no significant gain in performance over either one alone. This suggests that for most conditions  most of the information from the sentiment labels are captured as part of the micropatterns  but not all of it. Thus  we are led to conclude that micropatterns do provide additional information over the base rate of the sentiment labels alone.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://aclanthology.org/W17-3110.pdf,20,13,discussion,"Loveys, Crutchley, Wyatt, Coppersmith",4,0,0,1,2017,This paper presents foundational analysis of a relatively novel computational linguistic method that incorporates temporal information over short durations. Micropattern analysis provides information about common shifts in language content which may be useful for helping to distinguish between people with and without a psychological disorder or suicide risk. This study demonstrated that micropatterns in social media posts hold some power to distinguish between users who have a mental health condition or a history of suicide attempts or panic attacks from their matched controls. Despite potential limitations  this study provides promising evidence in support of using micropattern analysis to detect progressions in suicide risk and symptoms of psychological disorders in future research. While the present study demonstrated that differences in micropatterns exist between users with and without a particular psychological disorder  information was not gathered on whether specific micropatterns can indicate the severity of a psychological disorder. We also did not assess whether micropatterns can distinguish between clinical conditions  and this is a likely next step for future research. While there are a number of potentially more interesting avenues of exploration involving more fine-grained emotions  psychologically meaningful events  sleep disturbance  physical symptoms  coping mechanisms  decompensation  and their interplay  these bring with them an exponential complexity. We have done some preliminary examination of more fine-grained emotional labels  and found that interpretation and assessment was unwieldy and too complex for a reasonable human to undertake – 27 possible micropatterns are observed here (three labels  observed over three subsequent messages: 3 3 = 27). Extending this to the emotion classifier from Coppersmith et al. (2016)  for example  would bring this to 8 3 = 512 micropatterns. Careful thought is required for analysis as the depth of possible labels grows. Many avenues for future work seem apparent  as the veritable panoply of labels to augment the straightforward VADER sentiment labels opens up. However  first and foremost of those possibilities is to directly replicate the work of Bryan et al. (in press) and extend it to non-military populations  and populations of different demographics to assess generalizability. This paper strongly suggests that micropatterns hold power for a wide range of mental health conditions  not just suicide risk. Specifically  including some of the knownrelevant psychological phenomena that can be inferred from explicit self-reports seem a worthwhile next step  including: cognitive symptoms  physical symptoms  sleep disturbance  coping behavior  and suicidal thoughts and behavior. Ultimately  technology is only a small part of the solution  since humans  workflows  and incentives that make up the existing system of care will need to integrate these technological solutions into their processes.We gave careful consideration to the ethics and privacy surrounding this work  and employed the ethical guidelines from Benton et al. (2017)  and used social media data donated with consent for use in mental health research from OurDataHelps.Org. We strongly encourage researchers interested in working in this space to consider the ethical implications from the outset  both of the research itself and also for the possible resultant technology. Recently  Mikal et al. (2016) conducted focus groups around their perception of this vein of work  which has greatly informed our work  and we heartily recommend it for informing ethical discussions.,0,0,0,0,0,0,2,2,0,0,,2,0,0,0,0,0,,0
https://aclanthology.org/W17-3110.pdf,20,14,ethics and privacy,"Loveys, Crutchley, Wyatt, Coppersmith",4,0,0,1,2017,ion relevant to mental health can be found in examining subsequent posts in relatively short order (so-called micropatterns). Furthermore  we demonstrate that even with a simple and straightforward lexicon approach  signficant deviations in micropatterns can be found between users who have mental health conditions and their matched controls. While some of the observable differences have face validity and align with existing psychological literature  some remain unexplained. Moreover  micropatterns hold more predictive power than the sentiment labels that they rely upon  which suggests that they are capturing important information not captured by the sentiment of the message alone. The results here were presented on simple and straightforward lexiconbased linguistic analysis  but the evidence strongly suggests that increasing the variety of psychologically meaningful (e.g.  life changing events  coping mechanisms  decompensation) will lead to additional fruitful insights. Challenges remain about the sheer dimensionality of these more complex micropatterns  and how they should be best interpreted for synthesis with the psychological literature. While there is significant future work to understand why these micropatterns emerge and what value they hold for psychological understanding and intervention  we see this as a promising step  and a worthy avenue of future study,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,2
https://aclanthology.org/W17-3110.pdf,20,15,conclusion,"Loveys, Crutchley, Wyatt, Coppersmith",4,0,0,1,2017,The authors would like acknowledge the support of the 2016 Jelinek Memorial Workshop on Speech and Language Technology  at Johns Hopkins University  for providing the concerted time to perform this research. The authors would like to especially thank Craig and Annabelle Bryan for the inspiration for this work and the generosity with which they shared their time to mutually explore results. Finally and more importantly  the authors would like to thank the people who donated their data at OurDataHelps.org to support this and other research endeavors at the intersection of data science and mental health.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/2998181.2998220,21,1,abstract,"De Choudhury, Sharma, Logar, Eekhout, Nielsen",5,1,0,0,2017,Cultural and gender norms shape how mental illness and therapy are perceived. However  there is a paucity of adequate empirical evidence around gender and cultural dimensions of mental illness. In this paper we situate social media as a “lens” to examine these dimensions. We focus on a large dataset of individuals who self-disclose to have an underlying mental health concern on Twitter. Having identified genuine disclosures in this data via semi-supervised learning  we examine differences in their posts  as measured via linguistic attributes and topic models. Our findings reveal significant differences between the content shared by female and male users  and by users from two western and two majority world countries. Males express higher negativity and lower desire for social support  whereas majority world users demonstrate more inhibition in their expression. We discuss the implications of our work in providing insights into the relationship of gender and culture with mental health  and in the design of gender and culture-aware health interventions.,0,0,0,0,0,0,2,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/2998181.2998220,21,2,introduction,"De Choudhury, Sharma, Logar, Eekhout, Nielsen",5,1,0,0,2017,I do not know how to communicate with the experts. He told me that I have some kind of disease in my mind  but I think I am okay. He kept asking me to express my feelings toward the earthquake  but I feel embarrassed f I tell people my own feelings. — Taiwanese natural disaster victim  quoted in [34]. The United Nations’ Millennium Development Goals established in 2000 identify large-scale cultural  social  economic  demographical and political processes underlying differential health risks in the global population1 . In fact  it is established that rarely does biology act alone to determine health inequities [49]. In the context of mental health in particular  gender and culture based differences are significant. Such differences may emanate from a biomedical  social  economic  or an epidemiological perspective [7  54]. However  significant data gaps exist in the understanding of the gender and cultural dimensions of mental health2 . Gender and cross-cultural analysis is important to improve our understanding of the epidemiology of mental health problems  decisions and disclosures of these problems in different subgroups  and also to increase potential for greater public participation in health. It is noted that overlooking gender or culture based differences can have drastic consequences. This includes  misdiagnosis  misappropriation of interventions  and a “one-size-fits-all” approach to extend help to those who may have unique needs [49  47]. In recent years  a new research direction has established social media data as a way to understand mental health challenges in people [15  13  24]. At the same time  other work has established social media sites as powerful platforms of self-disclosure and social support-seeking around psychological distress [14  3  2]. However  identifying gender based and cross-cultural context is critical in the use of such passively sensed big data for making sense of mental health and well-being. Our motivation springs from the observation that cultural and gendered expression of different subgroups may differ markedly from the “typical”  largely western populations  on which current social media investigations of mental health are based [15  13]. For instance  depression in women  in particular  is an outstanding challenge in majority world countries like India where the cultural context indicates males and females to inhabit different social worlds. Gender inequality  family violence  and restrictions to independence of young women permeates family life in India. These norms are known to increase risk of mental illness in women [35]. Consequently  rarely are analytical insights and intervention mechanisms able to be transplanted without modification from one subgroup to another  and still provide the same value [27]. It is known that digital behaviors such as disclosure and expression of emotion in social media are influenced strongly by gender and culture based behavioral and linguistic norms. As sociologist Goffman also notes in his celebrated book “Stigma” [18]  individuals with a socially discredited attribute such as mental illness  tend to manage impressions of themselves in social settings in order to protect their identities. These actions might derive heavily from their gender and cultural context. In this paper  we present a gender based and cross-cultural quantitative examination of mental health content shared on social media. Specifically  we study mental illness related self-disclosures made on Twitter  and analyze and characterize them along gender based and cross-cultural dimensions. Our contributions include: • A machine learning approach to identify genuine selfdisclosures of mental illness from noisy social media posts. We use vector-representations of content shared on external mental health support communities as weak labels to infer genuine social media disclosures in a semi-supervised manner. Based on psychology expert consultation  this method is found to yield 96% accuracy. • Statistical comparisons between the disclosures of female and male users  and by users from four prominent English speaking countries: western countries U.S.  U.K.  and majority world countries India  and South Africa. Specifically  we examine different linguistic style  affective  behavioral and cognitive attributes  and content characteristics based on a topic modeling approach. For our work  we use a dataset of half a million Twitter users and nearly 1.5 million posts. Our findings reveal significant differences in how different gender and cultural subgroups  who disclose their mental health concerns  express themselves on Twitter  compared to equivalent control subgroups. Unlike their male peers  female users in our dataset express more positivity  greater involvement in social and familial concerns  higher propensity to engage in health discourse  and show a desire to seek help on the platform. Turning to different cultural subgroups  we find that users from majority world countries demonstrate more inhibition and express fewer negative emotions  compared to their peers from the western countries. Topically  content from the former cohort reveals signs of an avoidant attitude and shame  whereas that from the latter group expresses decreased self-esteem and heightened loneliness. We discuss how naturalistically generated and unobtrusively gathered social media data can help understand gender and culture based differences among individuals who disclose their mental health challenge. Our work bears implications for the design of gender and culturally aware interventions  so as to bring tailored  timely help to individuals in need.,0,0,0,0,0,0,2,0,2,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/2998181.2998220,21,3,privacy and ethics,"De Choudhury, Sharma, Logar, Eekhout, Nielsen",5,1,0,0,2017,We leverage public data from Twitter and Reddit for our work; hence our work did not qualify for approval from our Institutional Review Board. Nevertheless  we took greater care in de-identifying and paraphrasing any content we present as examples to support our investigation. Importantly  our work does not make any diagnostic claims about mental illness experiences of the population we study,0,0,0,0,0,0,0,0,2,2,,0,0,0,0,0,0,,2
https://dl.acm.org/doi/abs/10.1145/2998181.2998220,21,4,gaps in gender and cultural dimensions of mental health,"De Choudhury, Sharma, Logar, Eekhout, Nielsen",5,1,0,0,2017,Prior work has identified significant differences in the health challenges experienced by gender subgroup populations [38]. Prior research has specifically identified the need for data collection efforts that can address largely unreported causes of women’s excess disease burdens  and parse out the contributions of sex and gender  and their interaction  in the etiology  onset  progression and prevention of different health conditions [37]. Sexual violence and mental health issues among women are especially known to suffer from paucity of adequate data [46]. For a long time  clinicians have reported that women receive more services for mental illness in clinical settings than men [1]. There is therefore a need to understand the expression of mental illness in gender subgroups to identify those at greater risk. Other works exploring gender-based dimensions of mental illness include the work of Kawachi and Berkman [30]. The authors observed that the protective effects of social ties on mental health are not uniform across gender groups in society. Further  they found that social connections may paradoxically increase levels of mental illness symptoms among women with low resources. In general  Astbury found that gender differences in mental disorders extend beyond differences in the rates of various disorders or their differential time of onset or course  and include factors that can affect susceptibility  disclosure  diagnosis  and adjustment to mental disorder [1]. Culture. According to Shweder [45]  culture is “to be a member of a group is to think and act in a certain way  in the light of particular goals  values  pictures of the world; and to think and act so is to belong to a group.” Culture can be defined as any characteristic of a group of people  which can affect and shape their beliefs and behaviors  including mental wellbeing [45]. Cultures vary in the extent to which expression of distress is socially sanctioned and reported. However  cross-cultural and cross-national studies of mental illnesses are limited [50]. Guillemin et al. [20] noted that cultural groups vary in disease expression and in their use of various health care systems. They go on to argue for the need to develop mental health and quality of life assessment measures specifically geared toward populations in non English-speaking countries. Similarly  Yeomans and Forman noted that many diagnostic methods and models have been derived from studies of samples from industrialized countries; however their application to diverse cultural populations needs attention to reliability and validity [57]. In terms of specific cultural studies  Yeh [56] studied the particular and unique mental health concerns of Asian immigrant youths  whereas Stack [48] investigated the effect of media on imitative suicide in Japan and compared it to the American context. Further  Wang et al. [54] observed that Chinese individuals are generally reluctant to express distress and often attribute such distress to external or physical causes. In majority world countries  it is noted that socio-political and economic issues  inequalities in the workplace  societal and cultural expectations are often known to impact individuals’ risk to distress and mental illness. The 14-countrywide WHO Multinational study conducted in 1995 around the prevalence  nature and determinants of mental illnesses in general medical care settings provides some broad insights into some of these cross-national dimensions of mental health [52]. Patel et al. [39] reported that the startling finding of this study was that  despite the use of standardized methods in all countryspecific centers  there were enormous variations in most variables known to be linked to mental health. Indeed  the only similarities across centers were the general observations of the ubiquity of mental illnesses  and the association of mental illness and disability after adjustment for physical disease severity. On the other hand  specific variables showed substantial variations; the prevalence rates of mental illness ranged from 7-52%  clinician recognition of mental illness varied from 5-60%  and the association of key variables such as gender  physical ill-health and education with mental illness were in opposite directions in different centers. These findings demonstrate the need for comparative cross-cultural and cross-national studies that can identify disclosure practices of mental illnesses  the local needs and thereby inform local policy and interventions [40]. We envision that leveraging social media data to understand gender and culture based differences in mental health disclosures can contribute toward closing these data gaps. More data driven capabilities of mental health inferencing and monitoring can also increase the visibility of these issues and provide an impetus for vulnerable subgroups to seek help and for clinicians to offer more treatment or intervention options.,0,0,0,0,0,0,2,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/2998181.2998220,21,5,social media and health,"De Choudhury, Sharma, Logar, Eekhout, Nielsen",5,1,0,0,2017,Research in recent years has revealed that social media data  especially language and conversational patterns can be a powerful source of information toward understanding and detecting health challenges in individuals and populations. These efforts include utilizing social media to understand conditions and symptoms related to diseases [41]  substance abuse [36  33]  postpartum depression [13]  eating disorders [8]  and other mental health disorders [15  11  51]. Self-disclosure  the “process of making the self known to others”  has been noted to be a basic element in the attainment of improved health [28]. Thus  another line of research has examined how social media platforms might be allowing honest and candid expression of thoughts  experiences and beliefs around mental health concerns [14  3]. De Choudhury and De [14] explored how use of Reddit for mental health purposes is characterized by disinhibiting behavior. Andalibi et al. [3] qualitatively characterized the variety of selfdisclosures that are shared on Instagram via the hashtag “#depression”. While these works study self-disclosures in online communities of support  we extend this body of work by developing an automated method to identify mental illness related self-disclosures in arbitrary social media content  that can have significant implications for how social media platforms are utilized to extend help to those in need. We further note that gender and cross-cultural examinations of health states  health behaviors  and disclosures using social media are limited. UNICEF recently undertook a study of social media content in Eastern Europe to look at attitudes towards vaccination [53]. De Choudhury et al. [15] identified differences between experiences of depression between women and men as measured from Twitter. Andalibi et al. [2] found that men are significantly more likely to adopt anonymous social media identities when engaging in sexual abuse related self-disclosure. Tsugawa et al. [51] examined how individuals with non-English speaking backgrounds expressed depressive thoughts and emotions on Twitter  and thereafter built classification models to identify markers of depression among Japanese speaking users. In other work  RamirezEsparza et al. [43] analyzed linguistic attributes of English and Spanish posts shared in depression forums. Our work presents an in-depth large-scale data-driven study of individuals who choose to disclose their mental illness on Twitter. Adopting this lens of self-disclosure  we then examine gender based and cross-cultural differences in attributes of disclosure in four predominantly English speaking countries.,0,0,0,0,0,0,0,0,0,0,,0,2,0,0,2,0,,0
https://dl.acm.org/doi/abs/10.1145/2998181.2998220,21,6,twitter data,"De Choudhury, Sharma, Logar, Eekhout, Nielsen",5,1,0,0,2017,Our study uses publicly shared mental illness self-disclosure data collected from the social media Twitter. We started by obtaining a large sample of English language candidate selfdisclosure posts from the full archive of public Twitter data  around a variety of mental health concerns. Specifically  we filtered the Twitter posts shared in March 2015 containing any of the keyphrases included in Table 1. These keyphrases were collated by a combination of reference to prior work [11  12]  and consultation with a trained psychiatrist practitioner. Through these keyphrases  we sought to identify who publicly state that they have been diagnosed with  or suffering from some form of mental illness. As noted by Coppersmith et al.  users may make such a statement to seek support from others in their Twitter social network  to fight the stigma of mental illness  or perhaps as an explanation of some of their behavior [11]. We obtained 1 319 064 posts from 534 829 unique users at the end of this initial data collection phase. Parallelly  we obtained a candidate control data sample from Twitter’s Firehose stream  so as to allow robust statistical comparisons between Twitter users who choose to self-disclose their mental illness  and those who do not. This dataset included a random sample of 1 513 279 posts from 673 898 unique users made on Twitter during March 2015  ensuring that none of these posts matched any of the keyphrases given in Table 1. Thereafter  for both of the candidate mental health disclosure sample and the candidate control sample of posts  we utilized Twitter’s official API3 to obtain the last 3 200 posts for each of the unique users in both datasets. For the control dataset  if any of the users had any post in their crawled posts matching one of the keyphrases above  we disregard them from our analysis. Further  employing the Google Compact Language Detector4   we disregarded any users  if at least 75% of their posts were not written in English. Our final candidate disclosure sample contained 51 038 914 posts from 470 337 users (µ = 108.5)  while the control sample contained 66 214 850 posts from 480 685 users (µ = 137.7).,2,2,2,2,2,1,0,0,2,0,,0,0,2,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/2998181.2998220,21,7,gender and country inference,"De Choudhury, Sharma, Logar, Eekhout, Nielsen",5,1,0,0,2017,In order to allow gender based and cross-cultural comparisons of the above data  we now present an automated gender and country name inference method — Twitter does not allow individuals an ability to self-report their gender  whereas location information is known to be highly noisy [22]. We also note the need for such a method given the size of our two datasets  that makes human coding of gender and country names challenging from a practical perspective. Gender Inference. For inferring gender of a user in our two datasets  we compared account names to existing name databases. First of all  we started cleaning the account names and subsequently identifying the first and last name on the basis of a 1-gram lookup. Thereafter  we compared the names to country-specific name databases. For this  the country origin of a Twitter user was firstly retrieved using the Country Name Inference  giving the user’s specified location. If the gender could not be retrieved using the previous method it would look up the name over all the country-specific lookup tables. For analytical simplicity  we only consider binary gender (female/male) in this paper. Country Name Inference. For inferring the country name corresponding to a user  we adopted a stepwise approach as follows: First  we cleaned the location strings reported in the location field of Twitter user profiles — including normalization of character case and removal of non-English word roots. Then we performed a location matching exercise  wherein we split the cleaned location strings into single words  iteratively created all possible 5-to-1-gram substrings  and then matched each n-gram to a location database based on GeoNames (http://www.geonames.org)  preferring larger n-grams over smaller ones (“New York City” to “York”). Third  we performed disambiguation by computing geographic distances between matched text-adjacent places and assigned high likelihood to those matches that are close to each other geographically. We then sorted equally likely location alternatives by population size and choose the top one. We note that  compared to geo-located Twitter posts  this location field string lookup method has been known to yield better coverage in social media data [22]. Validation. Finally  we validated both our gender and country name inference methods based on annotations obtained from two independent raters on a sample of 100 users. We found agreement between the raters’ annotations and the one given by our method for 79% of the cases for gender  and 86% of the cases for country names (κ = .77). In our candidate disclosure and candidate control datasets  we were able to infer binary gender for 325 873 candidate mental health (of which 59% were female) and 439 224 candidate control users (of which 46% were female) respectively  and country information for 131 890 and 328 468 users for the same two groups respectively. Within the scope of this paper  we restrict our attention to four most populated countries where English is a predominant form of expression — western countries US  UK (GB)  and majority world countries India (IN)  and South Africa (ZA). All of these countries fare in the 25 countries with most population5 . Focusing on these countries provides us with a lens to examine cross-cultural differences in the disclosures of mental illnesses on social media. Handling Population and Internet Penetration Bias. We note that the populations of the four countries are widely different  along with their overall reported internet penetration rates6 . Hence we devise a subsampling strategy to filter users belonging to one of the four countries  from the set of users in the candidate disclosure and the control datasets with inferrable country information. First  for population based subsampling  we use the inverse of the population ranks of the countries4 as the respective rates of sampling. Then we use the internet penetration percentages of the countries5 to randomly sample that fraction of users from the population subsampled sets. In this manner  across both the datasets  we obtained 211 132 Twitter users from the US  61 816 users from the UK  10 808 from IN  and 5 769 from ZA.,0,0,0,0,2,0,0,0,0,0,,2,0,0,2,0,0,,0
https://dl.acm.org/doi/abs/10.1145/2998181.2998220,21,8,obtaining genuine mental health disclosure data,"De Choudhury, Sharma, Logar, Eekhout, Nielsen",5,1,0,0,2017,We note that the candidate mental health disclosure data sample is prone to significant noise. It is possible that although a user uses one of the mental health keyphrases in their Twitter post  it may not indicate a genuine disclosure (e.g.  “when I have to wake up at 6am  I feel like killing myself” does not indicate a person’s real intention of taking their life). To eliminate such users  we adopt a semi-supervised machine learning method [58] in which we compare the language of each user in our Twitter dataset  with the language of a selfidentified set of social media users suffering a from mental illness. For the purpose  we obtain a large sample of 79 833 posts from 44 262 users made on the Reddit subcommunities r/depression  r/mentalhealth  and r/SuicideWatch between February and November 2014. We use this dataset a weak signal of the language used by individuals identifying with a form of mental illness. Prior work has also indicated that the disclosures made on these forums are genuine disclosures of mental illnesses  and have also been validated through consultation with a psychiatrist [16]. Our approach proceeds as follows: Step 1. We create Twitter user-centric vector-representations by collating all of their posts – this would give us as many vectors as the number of users in the candidate disclosure sample. We also similarly create a single vector representation by collating all posts in our Reddit dataset. Step 2. Next we establish comparative validity across the Twitter vectors and the Reddit vector. This is an important step because the language of Twitter and Reddit cannot be directly compared due to the unique affordances of each site  and the seeming differences in the demographics of the two7 . For each of these vectors  we perform linguistic normalization of tokenized items in them8 . Then we compute the average automated readability index (ARI [44]) on all of the normalized Twitter vectors  and the Reddit vector. We observe the ARI differential between the two to be ∼6.7%9   indicating that the languages are close to standardized internet speak [25]  and hence comparable following normalization. Step 3. Next we build n-gram language models (n = 3) for both the normalized Twitter vectors  and the single Reddit vector. We then determine cosine similarity scores between the language models of each normalized Twitter vector and the Reddit vector. Step 4. Finally  we obtain the distribution of cosine similarities over all normalized Twitter vectors (see Figure 1(a)). We construct the “genuine disclosure dataset” of users to be those vectors (of users) for whom the cosine similarity of their language models with that of Reddit’s is greater than or equal to the median similarity across all vectors (median distance=.71 σ = .157). Our final dataset of genuine mental illness disclosures consisted of 231 611 users; we will refer to this set as the MID users.,0,0,0,0,0,0,0,0,0,0,,0,0,2,0,2,2,,0
https://dl.acm.org/doi/abs/10.1145/2998181.2998220,21,9,expert verification of mental health disclosures,"De Choudhury, Sharma, Logar, Eekhout, Nielsen",5,1,0,0,2017,Next we qualitatively verified whether posts from these users indeed were self-disclosures of mental health challenges. For the purpose  we consulted a licensed psychologist  and also included two researchers  who were familiar with mental health content shared on social media. Over a random sample of 100 posts complied from the timeline of 100 randomly selected MID users  we obtained independent binary annotations on whether a post was likely to be related to mental health. The Fleiss’ κ for interrater agreement was found to be high  .87  along with an accuracy of 96% in distinguishing users who engage in genuine disclosures from those who do not. This establishes adequacy of our approach. Table 2 gives some paraphrased mental health disclosure posts of Twitter users who were identified to be genuine mental health disclosers by our approach. In Figure 2 we show the pipeline of steps involved in our approach of arriving at this final dataset.,0,0,0,0,2,0,0,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/2998181.2998220,21,10,developing an accurate control dataset,"De Choudhury, Sharma, Logar, Eekhout, Nielsen",5,1,0,0,2017,Next we also note that it is possible that our candidate control dataset includes users who engage in mental illness in their posts  however did not use any of the keyphrases from Table 1. To eliminate such users  like above  we compare the language used in the Twitter posts of these users with that of the Reddit mental health posts. However in this case  we are interested in the users whose language is most distinct (or least similar) from that used in the Reddit content. Hence our final control dataset is obtained by filtering for those users whose language model based similarity is less than the median similarity across all control user vectors (238 860 users; median distance=.38; σ = .125). See Figure 1(b) for a distribution of the cosine cosine similarities. We will refer to this dataset as CTL users (ref. Figure 2 for the approach).,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/2998181.2998220,21,11,quantifyng differences in disclosures,"De Choudhury, Sharma, Logar, Eekhout, Nielsen",5,1,0,0,2017,In this subsection  we present methods to quantify the differences between the disclosure characteristics of females and males  and individuals reported to be from one of the four countries of interest: US  GB  IN  and ZA in the MID dataset. Linguistic Measures Language is a powerful source of expression [26]. A rich body of work  such as Boroditsky et al. [6] showed how the perception of objects in different languages can relate to as well as impact one’s social and pscyhological status. It is recognized that language  specifically one’s native language  shapes and drives one’s thoughts  actions  and social relationships [10]. Further  it is established that cross-cultural and sex differences exist in one’s underlying thought processes [21  55]. For instance  according to Kovecses [ ¨ 31]  cultural models are known to define one’s emotional concepts. To quantify gender and cross-cultural dimensions in the language of individuals who engage in mental health disclosure on social media  we propose three categories of measures: (1) affective attributes  (2) cognitive attributes  and (3) linguistic style attributes. Measures belonging to all of these attribute categories are largely based on the psycholinguistic lexicon LIWC [42]  and were motivated from prior literature that examines associations between the behavioral expression of individuals and their psychological distress  including vulnerability to mental illness [9  14]. Specifically  with LIWC  we are able to study the psychological value of language in gender and culture subgroups—such as  parts of speech that include pronouns  articles  prepositions  conjunctives  and auxiliary verbs [9]. (1) We consider two measures of affect derived from LIWC: positive affect (PA)  and negative affect (NA)  and four other measures of emotional expression: anger  anxiety  sadness  and swear. Literature in mental health [49] identifies emotional expression to be key to characterizing one’s psychological vulnerability. (2) We use LIWC to define the cognitive measures as well: (a) cognition  comprising cognitive mech  discrepancies  inhibition  negation  death  causation  certainty  and tentativeness; and (b) perception  comprising set of words in LIWC around see  hear  feel  percept  insight  and relative. Quantifying one’s cognition and perception  as manifested linguistically  can provide insights into emotional stability and cognitive complexity—these attributes are important with regard to understanding one’s mental well-being [19]. (3) Next  we consider four measures of linguistic style: (a) Lexical Density: consisting of words that are verbs  auxiliary verbs  nouns  adjectives (identified using NLTK’s [4] POS tagger)  and adverbs. (b) Temporal References: consisting of past  present  and future tenses. (c) Social/Personal Concerns: words belonging to family  friends  social  work  health  humans  religion  bio  body  money  achievement  home  and sexual. (d) Interpersonal Awareness and Focus: words that are 1st person singular  1st person plural  2nd person  and 3rd person pronouns. Together  linguistic styles are known to indicate one’s underlying psychological processes (lexical density)  personality (temporal references)  social support and connectivity (social/personal concerns)  and awareness of one’s surroundings and environment (interpersonal focus). Prior work identifies all of these cues to be valuable in understanding mental health  in both offline and online contexts  including social media [43].,0,0,0,0,0,0,2,0,0,0,,0,0,0,0,2,0,,0
https://dl.acm.org/doi/abs/10.1145/2998181.2998220,21,12,topic modeling,"De Choudhury, Sharma, Logar, Eekhout, Nielsen",5,1,0,0,2017,Our second method for comparing mental illness disclosures uses a topic model  which have been commonly employed to analyze health data [41]. We obtain topics by running Latent Dirichlet Allocation (LDA) [5] over all posts. We pre-processed the data by removing a standard list of Twitter-specific stop words  words with very high frequency (> 0.25× datasize)  and words that occur fewer than five times. Thereafter  we used Gensim’s implementation of online LDA from [23]. We used the default hyper-parameter settings and 100 topics  which we determined based on the value of average corpus likelihood over ten runs. To measure topic differences in one cohort (e.g.  IN MID users) over the other (e.g.  UK MID users)  we first compute the posterior probability of each topic separately for all posts in both cohorts. We then compute three comparison metrics: (1) the rate of change for each topic  given as the difference between the posterior topic probabilities of the cohorts  divided by the probability of the first cohort; (2) the pointwise mutual information between the posterior topic probabilities of the same cohorts; and (3) the Spearman’s rank correlation Figure 3: Mean absolute differences between female and male MID and CTL users per the various categories of linguistic measures. Difference for a specific measure is calculated as the ratio of the difference between the values of the measure for females and males  to the value of the measure among males. between the topic distributions for the two cohorts. Additionally  we compare all gender and culture cohorts based on significance tests (e.g.  Mann Whitney U test for gender  and the Kruskal Wallis test for cultural differences). We also present a method to qualitatively examine the differences between the topics used by different MID user cohorts. For the purpose  two researchers familiar with mental health content on social media independently inspected the words associated with each of the topics given by the above topic model. They used a semi-open coding approach to develop a codebook and extracted descriptive topical themes for the topics (Cohen’s κ=.74). During the codebook development  the two annotators referred to prior literature on gender and cultural differences in mental health [46  20  52]. In the results section  we will present an examination of these qualitative differences.,0,0,0,0,0,0,2,0,0,0,,0,0,0,0,2,0,,0
https://dl.acm.org/doi/abs/10.1145/2998181.2998220,21,13,gender differences,"De Choudhury, Sharma, Logar, Eekhout, Nielsen",5,1,0,0,2017,Linguistic Differences Based on Table 3  we observe considerable differences  in terms of the linguistic measures  in the Twitter posts of female and male MID users. Affective Attributes (Gender). Starting with the measures of affect  female MID users show higher sadness (z = −39.0; effect size Cohen’s d = 1.1; 15.4% higher) and anxiety (z = −26.4; Cohen’s d = 1.0; 10.7% higher). Prior literature indicates expression of these emotions to be associated with depression symptoms such as mental instability and helplessness  loneliness  and restlessness [13]. “My health has been defining me lately. #depression has invaded my peace and #anxiety has exhausted my thoughts. Pain isn’t always physical.” (↑ female) “Why am I even here ... No one needs or wants me ... I’m useless” (↑ female) Whereas we find that male MID users express more NA (z = 9.8; Cohen’s d = .5; 2.6% higher)  anger (z = 15.8; Cohen’s d = .6; 5.3% higher)  and use more swear language (z = 22.5; Cohen’s d = .8; 9.5% higher) in their posts. “The past week has been horrible. Depression is robbing me of the peace I have felt. I’m isolated in a house full of people.” (↑ female) “Honestly fuck everyone  y’all gonna miss me when I’m gone” (↑ male) Interestingly  female users also tend to use more PA in their shared content (z = −33.9; Cohen’s d = 1.1; 7.1% higher) perhaps as a way to demonstrate a positive outlook publicly  despite the mental health challenge that they might be experiencing. “There’s something about those eyes helping to wake me up everyday that makes the days brighter.” (↑ female) We also report the extent to which the female and male CTL users differ  along all of these affective attributes. Per Figure 3  this mean difference is found to be only 4.7%  which is considerably low compared to 8.5% in the case of the MID cohort. Cognitive Attributes (Gender). Next  female MID users show lowered cognition and perception  in other words  greater cognitive impairment in their Twitter posts compared to male MID users. For instance  inhibition (z = 29.9; Cohen’s d = 1.0) is lower in females relative to males by 6.5%. Through lower usage of certainty words  female peers tend to demonstrate heightened emotional instability (z = 4.7; Cohen’s d = .5). “So now here I am  confused and full of questions. Am I born to lose or is this just a lesson?” (↑ male) The female and male users in the CTL cohort  however  differ by only 2.3% across the various cognition and perception attributes  as shown in Figure 3. Lexical Density and Awareness (Gender). Next  lexical density of the social media content of female MID users is higher compared to their male peers  as observed through the usage of prepositions (z = −18.4; Cohen’s d = .7; 6.4% higher)  conjunctions (z = −4.6; Cohen’s d = .5; 1.6% higher)  adverbs (z = −18.1; Cohen’s d = .9; 3.0% higher)  inclusive (z = −7.1; Cohen’s d = .5; 2.3% higher)  and exclusive (z = −16.9; Cohen’s d = .7; 4.9% higher). However  compared to the male MID users  they show lowered awareness of objects and their surroundings  as measured via the proportion of verbs (z = 21.5; Cohen’s d = .6; 1.6% lower)  auxiliary verbs (z = 13.1; Cohen’s d = .9; 1.9% lower)  articles (z = 37.6; Cohen’s d = 1.1; 13.4% lower) in Twitter posts. Note that  in the case of the female and male users in the CTL cohort  per Figure 3  the mean difference across all of these measures is observed to be only 2.2%. This indicates that the female and male MID users show differences beyond that accounted for in the control sample. Temporal References (Gender). Female MID users tend to be more focused on the here and now  due to their greater use of present tense words (z = −33.3; Cohen’s d = .8; 3.4% higher). On the other hand  male MID users show a greater future orientation compared to the females  via the usage of more future tense in their Twitter language (z = 35.7; Cohen’s d = .9; 9.2% higher). Such differences in temporal references in language are however not observable between the female and male CTL cohort. Compared to a mean difference of 6.4% in the case of the MID cohort  it is only 2.6% for the CTL users. Social/Personal Concerns (Gender). There are a variety of differences in the social and personal concerns that manifest in the Twitter posts of female and male MID users. First  male MID users display lower sense of achievement (z = −32.2; Cohen’s d = 1.1; 8.1% lower)—a known sign of lowered self-esteem [8]. On the other hand  female MID users express greater concern about health (z = −18.0; Cohen’s d = 1.1; 6.0% higher) and body (z = −8.9; Cohen’s d = .9; 2.7% higher) compared to their male peers. This might indicate their greater self-awareness of their wellness status or perceptions of their physical health. “Over the past 2 years I have been hit with physical and mental pain. The pain is real. It is still there.” (↑ female) “My stomach sinks everytime.” (↑ female) An interesting finding here is the observation that male MID users exhibit lower use of social (z = −4.7; Cohen’s d = .1)  friends (z = −27.4; Cohen’s d = 1.1)  family (z = −23.2; Cohen’s d = .9)  and bio words (z = −21.3; Cohen’s d = 1.0). This may imply that these users are less socially concerned or bothered. By the same token  the female peers might be using such language more extensively in their Twitter posts in order to explicitly seek help from their social networks or to feel supported. “Hard to really feel sick with this support group. #Family” (↑ female) “I miss having someone  a friend to talk to all night” (↑ female) Finally  male MID users show a greater interest in religious discussions compared to females (z = 10.6; Cohen’s d = .8; 5.1% higher). “God is working things out for you  even when you don’t feel it. Have faith and be thankful.” While analyzing the social and personal concerns expressed by the female and male users in the CTL cohort  based on Figure 3 we do not observe such extensive differences: the mean difference between the cohorts per these measures is only 3.7%. Interpersonal Focus (Gender). Increased use of first person singular pronouns (z = 26.1; Cohen’s d = 1.1; 10.2% higher) in the posts of male MID users shows their selffocused behavior and disclosure of personal stories. “I decided yesterday evening to go back to this thinking place. there was no one else there  just me. I had let my parents know where I was” (↑ male) Additionally  lower use of second person pronouns (z = −8.0; Cohen’s d = .5; 3.0% lower) as well as that of third person pronouns (z = −6.9; Cohen’s d = .8; 3.4% lower) in the content of the male users tell us that they tend to be less interactive  and engage in lesser discourse about others. For the female and male users in the CTL cohort (Figure 3)  the interpersonal focus measures account for only 2.9% of the difference.,0,0,0,0,0,0,0,0,2,0,,0,0,0,0,2,0,,0
https://dl.acm.org/doi/abs/10.1145/2998181.2998220,21,14,topical differences,"De Choudhury, Sharma, Logar, Eekhout, Nielsen",5,1,0,0,2017,Digging deeper  between the female and male gender MID cohorts on Twitter  we find significant differences in topics of the posts. Figure 4(a) gives a distribution of the number of topics over their respective z-statistic values (based on a Mann-Whitney U test and Kruskal Wallis tests). These tests examine the differences in the likelihood of prevalence of top ics in the posts of female and male MID users. We find that several topics are associated with highly negative or highly positive z-scores (p < .0005; df = 99)  indicating significant topical differences between the gender groups. Next  we find that  per the three topic comparison metrics  the Spearman rank correlation for topical comparison between females and males is ρ =.31 (p < .01 based on a Mann Whitney U test)  the pointwise mutual information (PMI) is .38 (p < .01 based on a Mann Whitney U test)  while the mean percent difference in the likelihoods of the 100 topics is 59.3% (p < .001 based on a Mann Whitney U test). However  the respective metrics between males and females in the CTL cohort are ρ =.14  PMI=.61  and a 33.6% difference in the topic likelihoods. This reveals significant topical differences for the MID cohort  above and beyond the differences that exist due to underlying gender based latent factors. We now present some insights we gained by examining the posts associated with the topics that distinguish the female and male cohorts most distinctively. We first identify the two topics that are more likely to be prevalent in posts from male MID users than female. The posts associated with topic #13  for instance  express contemplative negative thoughts and hopelessness (e.g.  ‘never’  ‘low’  ‘mess’  ‘lonely’  ‘drunk’): “Sometimes I wonder if anyone still looks out for me. I am a mess that nobody wants to clean up. I’m a wreck” “Some things are better left unsaid. Lonely nights make for long nights. Being drunk sometimes makes it easier.” The second topic more prevalent among male MID users is topic #57  and it shows detachment from the social realm and hesitation to seek help (e.g.  ‘invisible’  ‘help’  ‘ask’  ‘relationship’): “If I were going to kill myself  I wouldn’t tell anyone. If I’m already invisible  why see me to favor your own self righteousness?” “I love my relationship but it’s fun to wonder who was just too shy or too afraid to ask me out. I have suspicions but I’m self centered.” Contrastively  we examine the top two topics that manifest more extensively in posts from female MID users  compared to the males. Posts associated with topic #86 indicate the with mental health challenges  and a desire for disclosure and help seeking (e.g.  ‘fear’  ‘rejection’  ‘bury’  ‘lose’  ‘stay’  ‘say’  ‘okay’): “you’re afraid to tell people how you feel because you fear rejection  so you bury it deep inside yourself where it only destroys you more.” “Sadly  you can’t lose what you never had  you can’t keep what’s not yours  and you can’t hold on to something that doesn’t want to stay.” Finally  through topic #45  the topic with the second highest likelihood of prevalence in female MID users’ posts over males  the female MID users share personal experiences around mental illness  including self-assessments and selfrealization (e.g.  ‘pain’  ‘control’  ‘realization’  ‘reminder’): “I used to hurt myself  because it was the only pain I could control.” “Daily reminder that life has taught me: don’t waste your time worrying about the people who don’t like you  you don’t live to please anyone.”,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/2998181.2998220,21,15,linguistic differences,"De Choudhury, Sharma, Logar, Eekhout, Nielsen",5,1,0,0,2017,The second part of our empirical investigation focused on characterizing the usage of different linguistic measures across the four cultures: western countries US  GB  and majority world countries IN and ZA. In Table 4  we report the mean values of each statistically significant linguistic measure in each of the four cultures  as well as the results of Kruskal-Wallis significance tests comparing them. For the purposes of the ensuing discussion  we performed post hoc multiple comparison Wilcoxon tests to report which specific pairs of cultures differed most in terms of the different linguistic measures. Further  we discuss our results by comparing the majority world cultures IN and ZA  with the western cultures US and GB. The linguistic differences between these two cohorts in the CTL group are shown in Figure 5. Affective Attributes (Culture). First  MID users from IN and ZA express relatively higher PA (H = 104.3; eta-squared effect size E2 r = .45) and lower NA (H = 17.5; eta-squared effect size E2 r = .19)  anger (H = 127.9; eta-squared effect size E2 r = .53)  anxiety (H = 87.9; eta-squared effect size E2 r = .42) and sadness (H = 79.6; eta-squared effect size E2 r = .44) compared to those from US and GB. In other words  users from the latter two cultures  US and GB  tend to express emotional content that is more emotionally outspoken in comparison with the majority world cultures (11.6% higher). We conjecture those in the majority world cohort might be expressing more positivity as a coping mechanism or as a face-saving mechanism [14]. This difference is also considerably larger than the affective differences observed for the cultures in the CTL cohort (6.8%); see Figure 5. “I hate myself with a burning passion right now  I should be out on the sled” (↑ US  GB) “I’ve been crying all afternoon I feel like a bag of shit” (↑ US  GB) “Being unhappy is the only thing I do right.” (↑ US  GB) Cognitive Attributes (Culture). We also observe greater manifestation of cognitive impairment and emotional instability among MID users from US and GB compared to those from IN and ZA. Mean difference between the groups is 6.3%  while the same in the case of the CTL cohort is only 2.9%. This is observable especially from the lower use of measures of cognitive mech (H = 150.8; eta-squared effect size E2 r = .64)  certainty (H = 41.2; eta-squared effect size E2 r = .33)  discrepancies (H = 25.5; eta-squared effect size E2 r = .31)  and percept (H = 120.1; eta-squared effect size E2 r = .55). “you think youre doing okay  then suddenly its a nighttime and you feel alone and youre not sure how to distract yourself anymore” (↑ IN  ZA) Lexical Density and Awareness (Culture). The lexical density measures show lower values in the posts of US and GB MID users  relative to those in the case of IN and ZA users. That is  the former group uses fewer prepositions (H = 210.6; eta-squared effect size E2 r = .63)  adverbs (H = 208.8; eta-squared effect size E2 r = .69)  and exclusive words (H = 38.4; eta-squared effect size E2 r = .37). Additionally  they also show reduced awareness of their social and environmental context  as measured via the usage of articles (H = 186.7; eta-squared effect size E2 r = .49). We note this to be more than twice relative to the differences in terms of lexical density and awareness observed in the two groups within the CTL cohort (ref. Figure 5). Temporal References (Culture). Next  the MID users from IN and ZA show an increased future orientation in their Twitter posts  compared to those from US and GB  as indicated in the usage of future tense words (H = 103.8; eta-squared effect size E2 r = .41). On the other hand  the latter group discussed more of their past experiences and events  via the use of past tense words (H = 369.9; eta-squared effect size E2 r = .65). This difference is also much larger than what is observed in the case of the CTL cohort (7.2%  compared to 14.1% int he former). “I felt so lonely and I started to cry but nobody understood.. Nobody saw just how broken I really was.” (↑ US  GB) Social/Personal Concerns (Culture). Increased levels of social concerns  measured via family (H = 154.1; eta-squared effect size E2 r = .58)  and social words (H = 1613.5; etasquared effect size E2 r = .87)  also tend to be observable in the posts shared by MID users from western cultures US and GB  compared to IN and ZA. These users also discuss more able health (H = 124.5; eta-squared effect size E2 r = .56)  body (H = 56.6; eta-squared effect size E2 r = .32)  and bio (H = 27.8; eta-squared effect size E2 r = .27) compared to the IN and ZA users. We conjecture the latter (majority world) group to be more self-conscious in utilizing a public platform like Twitter to discuss about the health aspects relating to a stigmatized illness. “I miss friendships with everyone that has ever held me close. Please be patient with me and I promise you that I’ll be back!” (↑ IN  ZA) “I go through these spouts of depression and I could lay in bed for weeks. I just dont care about a single thing about this health condition” (↑ US  GB) We further observe that the IN and ZA MID users converse less about sensitive or ‘taboo’ topics like religion (H = 162.2; eta-squared effect size E2 r = .68)  death (H = 38.4; eta-squared effect size E2 r = .28)  and sexual (H = 131.0; eta-squared effect size E2 r = .47) in their Twitter posts  compared to their peers from US and GB. This might indicate a latent social norm in the former cohort to avoid sensitive discourse on a publicly accessible platform like Twitter. Overall  the differences in these two sets of MID culture groups is much higher (11.7%) compared to the difference observable in the CTL cohort (4.9%). Interpersonal Focus (Culture). Finally  we observe the MID users from the western cultures US and GB to demonstrate a greater tendency of social interaction (use of second person pronoun words (H = 290.1))  as well as attention to people around them (use of third person pronouns (H = 49.9; eta-squared effect size E2 r = .36)). However  the users from IN and ZA express greater self pre-occupation and selfattentional focus  as measured in their use of more first person singular pronouns (H = 128.0; eta-squared effect size E2 r = .54). We also note that the aggregated interpersonal focus in these two sets of MID users (14.4%) is much higher in contrast to the CTL users (7.2%). “I suffered and was embarrassed to talk. When I spoke up my suffering lifted. There are people who will listen.” (↑ IN  ZA) Topical Differences We now move over to examining the differences in the topics prevalent in the four cultures based on the LDA topic model. We first report on the statistical differences between the likelihoods of prevalence of topics in the four culture groups. In Figure 4(b) we show a distribution of the number of topics over their respective H-statistic values  based on Kruskal Wallis tests of comparing the topics of the four cultures. The figure indicates that a large number of topics have large Hstatistic values (e.g.  over 10; p < .0005; df = 96)  in turn revealing topical differences between the culture groups. Next  investigating these topical differences further  we present the differences per the three different metrics: (1) Mean Percentage Difference in Topic Likelihood; (2) Spearman’s Rank Correlation; and (3) Pointwise Mutual Information (PMI). The results are given in Table 5. Across all pairwise differences for the three metrics  we observe that US and GB MID users aggregatively differ less topically compared to US-IN  US-ZA  GB-IN or GB-ZA. For instance  the Spearman’s rank correlation of topics used by US and IN MID users is only .17  however it is much higher for US-GB (.43). This shows that US-GB topics are more similar to each other in their usage  compared to the topics prevalent in US and IN posts. Another example is the measure of pointwise mutual information (PMI) for US and ZA. It is only .18  indicating their there are considerable differences in the likelihoods of various topics across these two cultures. Examining these numbers computed on the CTL users  we observe less significant differences in topic likelihoods. Together  these indicate topical differences in the posts of MID users from the four cultures to be higher than that observed in their CTL cohort peers. We now discuss the context of usage of the top two topics that have a greater likelihood of occurrence in western cultures (US and GB posts) versus that in the majority world cultures (IN and SA posts shared by the MID users). Topic #29 centers around words like ‘kill’  ‘stop’  ‘hate’  ‘pain’  ‘life’ all of which generally revolve around self-deprecating  self-critical thoughts and self-destruction. “my god life makes me want to kill myself” “everything good is taken from me. EVERYTHING” “I’d rather my heart stop beating and my lungs fail me that continue living through this constant pain” Next  topic #54 includes words like ‘alone’  ‘lonely’  ‘people’  indicative of loneliness and lack of social support. US and GB MID user posts associated with high likelihood of this topic tend to bear a tone of decreased self-esteem  and greater self-loathing. “lonely even when I’m not alone” “no one will ever be able to understand” “my family is supposed to be loving and accepting and they are the ones who trigger me the most” On the other hand  the two top topics that are more prevalent in MID users from majority world cultures  IN and ZA  compared to those from western cultures  US and GB  revolve around significantly different content. For instance  topic (#17) manifests confessions and regrets of individuals (e.g.  ‘faith’  ‘regret’  ‘strong’). Regret” “Will I ever beat this? I was doing so good but what happened to me” “I put so much faith in you being the one  I was so excited. It’s just depression  paranoia and panic got the better of me.” Through topic #86  the MID users in IN and ZA express bereavement and marginalization due to the stigma associated with mental illness (e.g.  words ‘pretend’  ‘ashamed’  ‘embarrassed’  ‘struggle’). We observe that  these MID users may be trying to resolve their problems through posts associated this topic  believing that mental health can be maintained by avoiding bad thoughts and exercising will power. In fact  Wang et al. [54] have suggested that talking to another individual or group (e.g.  a mental health worker) about psychological problems may be viewed by non-American sufferers as bringing disgrace on the family. “Beyond tired. I want this to end. I can’t pretend anymore  need to be strong” “I’m still so embarrassed  but I’m hurt more by him not being there for me. I don’t want to struggle alone.” “Fear is the biggest reason I don’t want to admit that I’m depressed. Need to get over this. I can do it.” Interaction Between Gender and Culture Our final empirical investigation considers the interaction between gender and culture  as measured linguistically. For the sake of simplicity  we do not include topical analysis for this last investigation. In Table 6 we present mean values of the different linguistic measures  grouped by categories  for male and female MID users in each of the four cultures. Since not all linguistic measures were significant based on KruskalWallis significance tests  we computed the means based on those that were indeed significant. We additionally report in the table the mean relative difference between male and female MID users grouped by the Western (US  GB) and Majority world (IN  ZA) cultures. We derive two major observations from this investigation. First  for the linguistic measure categories  females and males show systematic differences across the four cultures  that align with our findings from the gender analysis. For instance  as we had observed before  females for all of the cultures tend to express more positive affect and less negative affect compared to males from the corresponding cultures. They also show lower cognitive processing  express more social/personal concerns on Twitter  and tend to be more interpersonally focused — all of which we had observed before. This indicates that gender differences are persistent over cultures  at least in the context of the four cultures we study here. Our second observation revolves around assessing the extent of linguistic difference between females and males spanning the four cultures. Even in the light of the above findings around systematic differences in the linguistic expression of the two gender groups across cultures  not all cultures show the same extent of difference. Interestingly  the relative difference between females and males are greater for the two majority world cultures (IN  ZA)  in contrast to the west ern cultures (US  GB). For instance  positive affect difference between females and males in the former culture group is - 332.6%  whereas it is only -192.2% in the case of the latter. Causal links  e.g.  whether culture indeed impacts gendered expression of mental health on social media cannot be easily derived. However  putting it together  these observations indicate that  with regard to social media disclosures of mental health  culture and gender are closely intertwined.,0,0,0,0,0,0,0,0,2,0,,0,0,0,0,2,0,,0
https://dl.acm.org/doi/abs/10.1145/2998181.2998220,21,16,theoretical and practical implications,"De Choudhury, Sharma, Logar, Eekhout, Nielsen",5,1,0,0,2017,Our work has provided some of the first detailed insights into how gender and cultural attributes relate to the content shared by individuals who disclose about their mental health challenges on these platforms.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/2998181.2998220,21,17,reflecting on gender differences,"De Choudhury, Sharma, Logar, Eekhout, Nielsen",5,1,0,0,2017,In the sample of mental health disclosing Twitter users we studied in this paper  there were considerable differences in the manner in which females and males appropriated the platform. Specifically  based on our different linguistic measures  females expressed higher sadness and anxiety  but lower anger and NA. These observations align with prior work in social psychology. Lieberman and Goldstein [32] also used the LIWC program to find that women in online support groups who experienced depression used high volume of anxiety words. Female mental health disclosers in our data also expressed greater social and familial concerns  compared to males. Literature has indicated that women tend to rely more on the social network of their family and the community  rather than the individual; whereas men exhibit a relative orientation toward stoicism [20]. These observations provide credence to our findings. Per our topic model analysis  we further observed differences between female and male mental health disclosers. Male users appropriated Twitter to show greater detachment from the social realm and hesitation to seek help. Examining literature on mental health trauma  we find evidence supporting this observation. Norris et al. [37] found that the differences in PTSD severity were greater women compared to men. The authors speculated this to be attributable to adherence to more traditional gender roles  in which male “machismo” inhibits disclosure of distress.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,2,0,,0
https://dl.acm.org/doi/abs/10.1145/2998181.2998220,21,18,reflecting on cultural differences,"De Choudhury, Sharma, Logar, Eekhout, Nielsen",5,1,0,0,2017,Turning to cultural comparison of social media disclosers of mental health concerns  one of our findings revealed that individuals from the majority world countries  IN and ZA were less likely to express negative emotions in their Twitter posts compared to those from western countries  US and GB. The former group also expressed relatively less cognitive impairment and lowered tendency of social interaction as measured via pronoun use. As Marsella and Christopher [34] have argued  intrusive symptoms of many mental health concerns may be universal  however  avoidance or numbing may be more culturally based. Yeomans et al. [57] also discovered cultural factors in how people handle traumatic stress: for example  Asians are more reluctant to express distress in public  which aligns with our findings. Moreover  based on our topic analysis  the IN and ZA cohorts were less likely to be disinhibiting or candid in their social media discourse  compared to their western peers. For instance  they used fewer words relating to self-critical or selfloathing thoughts  but shared evidence of bereavement and shame. This finding supports observations in prior cross cultural investigations of mental health in eastern and western cultures; where individuals in the former culture were found to feel more socially stigmatized while sharing these experiences socially [17]. These practices have been attributed to social  political  economic  and communication barriers. Broadly  these observations indicate notable cultural differences in the Twitter activities of IN and ZA based mental health disclosers. They support the view that a conflict exists between traditional cultural values for these groups and the way in which psychological and social support are sought in western cultures.,0,0,0,0,0,0,2,0,2,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/2998181.2998220,21,19,gender and culture,"De Choudhury, Sharma, Logar, Eekhout, Nielsen",5,1,0,0,2017,Finally  we observed notable patterns in the interaction between gender and culture groups  as measured through language. Kovecses [ ¨ 31] posited that “Although we find a great deal of commonality in emotion concepts and metaphors both across languages/cultures and through time  we can see a great deal of variation as well”. Our findings of this interaction study align with this position of Kovecses — the female and male mental health disclosers ¨ showed similar linguistic characteristics across the four cultures  however  the extent to which they differed in each culture  was markedly distinct. Summarily  as pointed out earlier  gender and culture based dimensions of mental health are less understood today  primarily because of the challenges in obtaining reliable and normalized data on the same [49  47]. In this paper  we showed the potential of social media as a way to quantify and assess these differences. To do so  we proposed  developed  and evaluated a rigorous and principled methodology that automatically and accurately identified disclosures of mental health challenges on social media  specifically Twitter. Obtaining adequate access to gold standard data (aka ground truth) is of prime importance in quantitative studies of mental health and social media. The most reliable data is gathered via self-reported means like surveys and user studies  however are difficult to scale. Gathering public data around mental health topics from social media incurs a much lower overhead  but the false positive rates in such data acquisition approaches can be alarmingly high. Our technique balances this through a semi-supervised approach  in which we leverage a much more reliable source of mental health data to glean genuine disclosures in another large but noisy source. Domain expert validation revealed the effectiveness of our approach. In short  beyond mental health  we believe our method provides a generalized template for gathering reliable data in problem contexts where either data acquisition is difficult  or noise  social and psychological considerations pose significant challenge in obtaining and curating quality data. Besides  while our findings largely focus on validating known attributes of gender and culture in the context of mental health  our work brings to the fore novel mechanisms to do so. It proposes a rigorous quantitative framework through which the findings can be studied in large populations  tested for generalization  or adapted to multiple online  gender and cultural contexts. Finally  we were also able to discover nuances in mental health expression across the gender and culture subgroups  that may be challenging to quantify through traditional means. These primarily include recognizing the role of specific linguistic constructs and topics in mental health disclosures. Thus  a gender and cross-cultural approach to mental health  examined from naturalistic  unobtrusive data collected from social media  can provide guidance to the identification of appropriate responses from the mental healthcare system  as well as for healthcare policy globally.,0,0,0,0,0,0,2,0,2,0,,0,0,2,0,2,2,,0
https://dl.acm.org/doi/abs/10.1145/2998181.2998220,21,20,design implications and ethics,"De Choudhury, Sharma, Logar, Eekhout, Nielsen",5,1,0,0,2017,Social media data can be an unobtrusive mechanism to gather information about one’s emotional and psychological state  and our findings indicate that this rich information source may be employed to understand and characterize mental health disclosures. Hence our methods can influence the design and development of timely and personalized intervention tools and provisions that bring help to mental health disclosers on social media. Such interventions can include applications and services can serve as early warning systems  and tools that can provide personalized alerts and psychosocial support resources to individuals  based on their linguistic expression  topic usage  and emotional distress. By allowing individuals monitor their linguistic and topical trends  interventions can also include technologies that serve as a personal diary-type narrative for self-reflection and self-awareness of mental well-being over time. While design considerations along these lines have been proposed in prior work on western populations [13]  for these interventions to be viewed as credible and effective more broadly  they would need to modified for those assuming different gender role or belonging to different cultures. In fact  the relationship between gender and cultural attributes and technology use  such as social media have begun to constitute a major thrust in CSCW and social computing research. Below we describe some concrete design considerations that can help make mental health intervention technologies more sensitive and adaptive toward one’s gender and culture.,0,0,0,0,0,0,2,0,0,0,,0,0,0,0,2,0,,0
https://dl.acm.org/doi/abs/10.1145/2998181.2998220,21,21,making interventions culture aware,"De Choudhury, Sharma, Logar, Eekhout, Nielsen",5,1,0,0,2017,Virtually all cultures have therapeutic systems that reflect their particular world view and values [57]. Design considerations of mental health interventions delivered via online or social media channels could leverage these cultural constructs. For instance  many eastern cultures often take more of a “sociocentric” than an “egocentric” view of society [57]—this observation was notable in our comparisons of majority world and western cultures as well. Hence interventions can prioritize provisioning social support on social media to individuals from majority world cultures  such as through recommending supportive links  communities and groups  as well as by providing a communication channel for candid discourse. On the other hand  interventions could focus on alternative schemes  such as providing subtle nudges  promoting self-awareness and self-refleciton  and triggering personalized alerts to enable those in western cultures better cope with and manage their mental health challenges. It is also known that ethnic groups tend to show lower compliance to mental health interventions  possibly due to the implications of social stigma and shame involved in seeking mental health help  or perhaps due to deep-seated beliefs about mental illness [47]. In fact  our results showed that perceptions of stigma and emotional inhibition were higher in the cultural groups from the majority world countries. Hence appropriate efforts need to made in both designing and deploying interventions to these cultural groups in particular. Techniques such as gamification  or external incentive or reward structures could be incorporated in the above outlined interventions to increase motivation and compliance with systems geared towards improving one’s mental well-being.,0,0,0,0,0,0,2,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/2998181.2998220,21,22,making interventions gender aware,"De Choudhury, Sharma, Logar, Eekhout, Nielsen",5,1,0,0,2017,Heightened levels of self-introspection and hesitation to seek help are known to foster a sense of uncertainty and emotional vulnerability [30]. These tendencies are more pronounced in men [1]. In fact  our findings reveal that male mental health disclosers showed a detachment from their social realms  as well as were less willing to acknowledge their psychological problems. Therefore  social support based interventions can be adequately and specifically directed to male mental health disclosers on social media platforms. For instance  they could be connected with other help seekers in an online community  so that they could confide in each other  share experiences and challenges  and find emotional support through an informal channel. Certainly  none of these interventions would be successful without appropriate and sensible privacy and ethical considerations in place  due to the sensitivities that exist around a stigmatized condition like mental illness. Although our methods are based on public data collected from social media  we suggest caution in how our findings are interpreted in gendered and cultural contexts. Importantly  our method or findings should not be used for making diagnostic claims about mental illness  or as standalone detectors of mental illness in gender based or cultural subgroups. We also do not intend our work and findings to be leveraged in ways that amplify perceptions of gender or culture based stereotypes on social media  or use these attributes for discriminatory purposes around mental health topics.,0,0,0,0,0,0,2,0,2,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/2998181.2998220,21,23,limitations,"De Choudhury, Sharma, Logar, Eekhout, Nielsen",5,1,0,0,2017,There are some limitations to our work that one should consider. First  we acknowledge that our findings are limited by our data acquisition capabilities. We relied on a set of hand curated keyphrases to seed our Twitter data collection. Although these were validated via consultation with a psychologist  they do not possibly include all of the ways in which Twitter users self disclose their mental health concerns. We were also limited by our ability to accurately infer gender and country information for the users in our dataset. While the methods we used have been employed in prior work and yield high precision content  we cannot rule out that our method suffers from a low recall problem. Further  our method of obtaining mental health selfdisclosure information only captures a subpopulation of Twitter users with a form of mental illness (i.e.  those who are speaking publicly about what is usually a very private matter). This may not truly represent all aspects of the population as a whole. Moreover  this method in no way verifies whether this diagnosis is genuine (i.e.  people are not always truthful in self-reports on Twitter)  or whether they are clinically validated. However  given the stigma often associated with mental illness  it seems unlikely users would self-disclose that they are diagnosed with a condition they do not have. We also note caveats in our semi-supervised machine learning method of identifying genuine mental illness disclosures  and eliminating noisy control data samples. We used Reddit as a way to obtain weakly labeled mental health disclosure information. However  we do note that Reddit demographics and platform usage practices might be considerably different from that of Twitter. We believe our normalization step for comparative validity to be helpful in curbing the effects of these differences  but future work could explore other means to obtain labeled mental health content. Our results  while suggestive  are correlational by nature. Therefore  it is impossible to say whether the expression of specific linguistic or topical constructs was the driving factor behind different gender and cultural subgroups’ mental health disclosures on social media. Finally  we comment on the cultural grouping adopted in this work—US and GB which were defined as western cultures  and IN and ZA which were included as majority world countries. Our choice was driven by an attempt to coarsely identify broad cultural dimensions that may simplify our analytic approaches. However  we acknowledge that the four countries chosen are not culturally distinct— for instance  IN and ZA have a rich colonial history  which might explain some of the observed similarities between them and GB  in contrast to US. Certainly  other groupings are possible. For instance  several organizations use metrics of economic prosperity to identify developed and developing cultures1 . How our findings generalize over alternative cultural groupings is an interesting direction toward future research.,0,0,0,0,0,0,2,0,2,0,,2,0,0,0,0,0,,0
https://dl.acm.org/doi/abs/10.1145/2998181.2998220,21,24,conclusion,"De Choudhury, Sharma, Logar, Eekhout, Nielsen",5,1,0,0,2017,Psychology research recognizes considerable data gaps in gender based and cultural dimensions of global mental health. In this paper  we provided some of the first quantitative insights into gender and cultural differences associated with individuals who self-disclose to suffer from a mental health concern on Twitter. To identify genuine disclosures  we developed a semi-supervised learning based framework  which yielded high accuracy (96%) based on expert feedback. Thereafter  we explored the range of differences in the content shared by female and male users  and users who report on Twitter to be from US  UK  India or South Africa. We observed male users to express higher negativity and lower desire for social support  whereas majority world users (India and South Africa) demonstrated more inhibition in their expression. Our findings help validate  via use of social media data  a number of known characteristic differences in mental health experience of gender and cultural subgroups. We believe our work can encourage re-thinking of privacy-honoring health interventions to be more gender and culture aware  so that they could bring appropriate and personalized help and support to vulnerable individuals on social media.,0,0,0,0,0,0,2,0,0,0,,0,0,0,0,0,0,,0
https://www.jmir.org/2019/6/e14199/,22,1,abstract,"Leis, Ronzano, Mayer, Furlong, Sanz",5,0,0,0,2019,Mental disorders have become a major concern in public health  and they are one of the main causes of the overall disease burden worldwide. Social media platforms allow us to observe the activities  thoughts  and feelings of people’s daily lives  including those of patients suffering from mental disorders. There are studies that have analyzed the influence of mental disorders  including depression  in the behavior of social media users  but they have been usually focused on messages written in English. The study aimed to identify the linguistic features of tweets in Spanish and the behavioral patterns of Twitter users who generate them  which could suggest signs of depression. This study was developed in 2 steps. In the first step  the selection of users and the compilation of tweets were performed. A total of 3 datasets of tweets were created  a depressive users dataset (made up of the timeline of 90 users who explicitly mentioned that they suffer from depression)  a depressive tweets dataset (a manual selection of tweets from the previous users  which included expressions indicative of depression)  and a control dataset (made up of the timeline of 450 randomly selected users). In the second step  the comparison and analysis of the 3 datasets of tweets were carried out. In comparison with the control dataset  the depressive users are less active in posting tweets  doing it more frequently between 23:00 and 6:00 (P<.001). The percentage of nouns used by the control dataset almost doubles that of the depressive users (P<.001). By contrast  the use of verbs is more common in the depressive users dataset (P<.001). The first-person singular pronoun was by far the most used in the depressive users dataset (80%)  and the first- and the second-person plural pronouns were the least frequent (0.4% in both cases)  this distribution being different from that of the control dataset (P<.001). Emotions related to sadness  anger  and disgust were more common in the depressive users and depressive tweets datasets  with significant differences when comparing these datasets with the control dataset (P<.001). As for negation words  they were detected in 34% and 46% of tweets in among depressive users and in depressive tweets  respectively  which are significantly different from the control dataset (P<.001). Negative polarity was more frequent in the depressive users (54%) and depressive tweets (65%) datasets than in the control dataset (43.5%; P<.001). Twitter users who are potentially suffering from depression modify the general characteristics of their language and the way they interact on social media. On the basis of these changes  these users can be monitored and supported  thus introducing new opportunities for studying depression and providing additional health care services to people with this disorder.,0,0,0,0,2,0,0,0,0,0,,0,0,0,0,0,0,,0
https://www.jmir.org/2019/6/e14199/,22,2,background,"Leis, Ronzano, Mayer, Furlong, Sanz",5,0,0,0,2019,Mental health is an essential component of our health. The World Health Organization (WHO) defines mental health as a “state of well-being in which people realize their potential  cope with the normal stresses of life  work productively  and contribute to their communities” Good mental health is about being cognitive  emotionally and socially healthy and it helps to determine the way we think and feel  in relation with others and how we make choices. Several factors  such as genetic  sociocultural  economic  political and environmental aspects  shape and influence our mental health. In the last few years  mental disorders have become a major concern in public health  and they are one of the main causes of the overall disease burden worldwide. They have devastating consequences for both patients and their families . According to the WHO  depressive disorders are the most common among the mental illnesses . Such disorders conditions are characterized by sadness  loss of interest and pleasure  feelings of guilt or low self-worth  disturbed sleep or appetite  feelings of tiredness  and poor concentration In 2018  at the global level  more than 300 million people were suffering from depression  and it is the main contributor to global disability. Depression has several consequences  both personal and social costs In some cases  depression can lead to suicide ideation and attempts The prevalence of this disorder changes depending on age  but it affects the whole population  from children and adolescents to elderly people. From 2005 to 2015  the number of people with depression increased by around 18% In this context  social media platforms allow to observe the activities  thoughts  and feelings of people’s daily lives and thereby investigate their emotional well-being. This domain has become a new growing area of interest in public health and health care research People with depression often use social media to talk about their illness and treatment  share information and experiences  seek social support and advice  reduce social isolation  and manage their mental illness In addition  access to mobile devices facilitates the use of social media platforms  such as Twitter and Facebook  at any time and at any place. Social media  such as Twitter  is by nature social  and we can consequently find social patterns in Twitter feeds  thereby revealing key aspects of mental and affective disorders Social media has become an important source of health-related information  which allows us to detect and predict affective disorders and which can be used as an additional tool for mental health monitoring and infoveillance Furthermore  the application of different methodologies based on natural language processing and machine learning technologies has proved to be effective in supporting and automating the identification of early signs of mental illness by analyzing the content shared on the Web by individuals This human interaction with social media contributes to build the so-called digital phenotype  reshaping disease expression in terms of the lived experience of individuals and detecting early manifestations of several conditions . Twitter is an internet microblogging social media service that allows users to post short messages about facts  feelings and opinions  and  as shown in previous studies  users’ health conditions . Twitter is one of the most important social media platforms in terms of number of users  with more than 330 million active users worldwide . Since November 2017  the maximum number of characters of a tweet has been increased from 140 to 280. By analyzing huge amounts of text  researchers can link everyday language use with social behavior and personality Language  as a means of communication  constitutes an essential element for providing valuable insights about people’s interests  feelings and concerns For this reason  the analysis of the messages posted on social media platforms may provide information about many personality traits  lifestyles  and psychological disorders The potential anonymity of social media encourages its users to be more willing to report health information  such as details of their mental disorders and treatments received. In addition  it is seen as a way to communicate and receive support from others with similar experiences  avoiding the isolation and fighting the social stigma of these conditions Nevertheless  users suffering from depression may also feel uncomfortable socializing and consuming information on social media platforms Several features of the messages  such as number and frequency of tweets  distribution throughout the day or during the night hours  and their seasonal character  can be used for the detection and monitoring of mental disorders  such as depression This knowledge can help health care professionals and health institutions and services in the decision-making processes to ensure better management of patients suffering from depression.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://www.jmir.org/2019/6/e14199/,22,3,objectives,"Leis, Ronzano, Mayer, Furlong, Sanz",5,0,0,0,2019,There are many studies that have used data mining and machine learning techniques on social media platforms to automatically identify people with mental health problems  such as depression  posttraumatic stress disorder  schizophrenia  or eating disorders  usually focusing the studies on messages written in English As far as we know  on social media  there are no studies about mental disorders that analyze messages written in Spanish. Taking into account that Spanish speaking countries  such as Spain and Mexico  are among the 10 most active Twitter users in the world  with more than 6 million and 7 million users  respectively we focused our research on the expression of depression in Spanish language tweets. The aim of this study was to identify the linguistic features of tweets written in Spanish and the behavioral patterns of the corresponding Twitter users that could suggest signs of depression.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://www.jmir.org/2019/6/e14199/,22,4,study steps,"Leis, Ronzano, Mayer, Furlong, Sanz",5,0,0,0,2019,This study was designed and developed in 2 steps  with the aim of analyzing the linguistic patterns and behavioral features of Twitter users suffering from depression in comparison with the general population of Twitter users. The study was focused on tweets written in Spanish. In the first step  the selection of users and the compilation of tweets were performed. Given the design and purpose of the study  we decided to use the Twitter Application Programming Interface (API) Using this API  3 datasets of tweets were created: The depressive users dataset was made up of the timeline of 90 users who publicly mentioned on their Twitter profile that they suffer from depression. The control dataset was made up of the timeline of 450 randomly selected Twitter users. the depressive tweets dataset was constituted by a manual selection of tweets from the depressive users dataset  which specifically included expressions indicative of depression. In the second step  comparison and analysis of the 3 datasets of tweets (control  depressive users  and depressive tweets datasets) were carried out to spot their distinguishing features. In the rest of this section  we will describe the methodology in detail. The flow diagram of the study is depicted in figure 1,0,0,0,0,2,0,0,0,1,0,,0,0,0,0,2,0,,0
https://www.jmir.org/2019/6/e14199/,22,5,data collection and user selection,"Leis, Ronzano, Mayer, Furlong, Sanz",5,0,0,0,2019,The selection of the tweets and their users was based on the filtered real-time streaming support provided by the Twitter API. In the first step  we selected the users who showed potential signs of depression on Twitter on the basis of the 20 most frequent words in Spanish expressed by patients suffering from depression in clinical settings. These words were jointly identified and selected by a psychologist and a family physician with clinical experience and were based on the definition and general features of depression according to the Diagnostic and Statistical Manual of Mental Disorders The list of words used and their English translations are shown in textbox 1 During June 2018  1 470 000 tweets  including 1 or more occurrences of the words listed in Textbox 1  were collected. From this collection of tweets and to select the users who publicly stated in the textual description associated to their profile that they suffered from depression  all the profile descriptions  including 1 or more occurrences of the word “depr” and all the possible derivations related to the word depression in Spanish  such as “depre ” “depresión ” “depresivo ” “depresiva ” “deprimido ” and “deprimida ” were considered. From the 720 users who included 1 or more of these words in their description profile  90 users who stated they suffered from depression or were receiving treatment for depression were selected for the analysis. This selection was performed by a psychologist  verifying that the statements were related to real expressions of depression  excluding quotes  jokes  or fake ones. For each of these depressed Twitter users  we collected all the most recent tweets from their timeline  up to a maximum of about 3200 tweets. Thus  a total of 189 669 tweets were collected  a figure that was reduced to 140 946 after discarding the retweets. These 140 946 tweets constituted the depressive users dataset. Examples of sentences appearing in the user profiles that were used for selecting the depressive users are: “Paciente psiquiátrico con depresión crónica” (Psychiatric patient with chronic depression; example of a profile sentence that indicates depression). “Colecciono errores traducidos a tweets depresivos y a uno que otro impulso de amor” (I gather errors translated into depressing tweets and into one or another love impulse; example of a profile sentence that does not indicate depression) Once the users with profile sentences indicating depression had been retrieved  their Twitter timelines were collected. Only those users having in their timeline at least 10 tweets that suggested signs of depression were retained for further analyses. For each user  the selection of these tweets was performed by manually inspecting the tweets of the user’s complete timeline in reverse temporal order  starting from the most recent one to the oldest tweet of the timeline retrieved by means of the Twitter API . Finally  a total number of 1000 tweets issued by the 90 depressive users  suggesting signs of depression  were detected and used for the analysis. This set of tweets provided us with the depressive tweets dataset  which was used to analyze linguistic features of tweets showing signs of depression. It has to be mentioned that these 1000 tweets were not to be included in the depressive users dataset (see Figure 1). At the same time  more than 97 500 000 tweets were also collected in June 2018: such tweets were gathered by listening to the public Twitter stream during this time span by only considering tweets with Spanish textual contents (as detected by Twitter language identification support) Given that Twitter requires more restrictive filters than just the language of the tweets  we used a list of the most frequently used Spanish words (stopwords) to retrieve all tweets that included 1 or more of these words. The vast majority of Spanish tweets should match this criterion. A sample of 450 users who did not mention in their profile the word depression and its derivations were selected randomly from the 97 500 000 tweets. The complete timelines of these users were compiled (1 141 021 tweets)  which were reduced to 712 589 once retweets were removed. These 712 589 tweets constituted the control dataset. To identify the language of a tweet  we relied on the language automatically identified by Twitter for each tweet  selecting tweets in Spanish. It has to be noted that these data can contain some tweets from unidentified depressive users.,2,2,2,2,2,0,0,0,0,0,,0,0,1,0,2,0,,0
https://www.jmir.org/2019/6/e14199/,22,6,data analysis,"Leis, Ronzano, Mayer, Furlong, Sanz",5,0,0,0,2019,A comparison of the 3 datasets was performed to determine the existence of differential linguistic and behavioral features. The different features that were analyzed are shown in table 1 The textual content of each tweet was analyzed by means of the following sequence of steps: Tokenization performed by means of a custom Twitter tokenizer included in the Natural Language Toolkit Part-of-Speech (POS) tagging performed by means of the Freeling Natural Language Processing tool in order to analyse the usage patterns of grammatical categories (eg  adjectives  nouns  or pronouns) in the text of tweets Identification of negations performed by relying on a custom list of Spanish negation expressions  such as nada (nothing)  nadie (nobody)  no (no)  nunca (never)  and alike. Identification of occurrences of positive and negative words inside the text of each tweet by means of 2 Spanish polarity lexicons: the Spanish Sentiment Lexicon and the Spanish SentiCon Lexicon We exploited 2 lexicons to consider and compare 2 approaches of modeling polarity in Spanish texts  thus reducing any language modeling bias that the use of a single language resource could introduce identification of words and expressions associated to the basic emotions by using the Spanish Emotion Lexicon Such emotions are alegría (happiness)  enojo (anger)  miedo (fear)  repulsión (disgust)  sorpresa (surprise)  and tristeza (sadness). All the tools and aforementioned resources are publicly available. The statistical analyses were carried out with the R version 3.4.3 (R Development Core Team) and SPSS Statistics version 23.0 (IBM)  applying the relevant test for each type of comparison to be carried out.,0,0,0,0,0,2,0,0,0,0,,2,0,0,0,2,0,,0
https://www.jmir.org/2019/6/e14199/,22,7,ethical approval,"Leis, Ronzano, Mayer, Furlong, Sanz",5,0,0,0,2019,The protocol used in this study was approved by the Ethics Committee of Parc Salut Mar (approval number 2017/7234/1).,0,0,0,0,0,0,0,0,0,2,,0,0,0,0,0,0,,1
https://www.jmir.org/2019/6/e14199/,22,8,distribution over time,"Leis, Ronzano, Mayer, Furlong, Sanz",5,0,0,0,2019,Regarding the distribution of tweets over time  the number of tweets per hour and throughout the week of control and depressive users datasets were compared. The tweet hours were adjusted by the user’s time zone. As shown in Figure 2  the depressive users are less active in generating tweets than the control ones  reaching both groups the same activity level between 23:00 and 6:00. The comparison of the temporal distributions of tweets between both datasets was carried out by means of a repeated measures analysis of variance (Greenhouse-Geisser F=6.605; P<.001). As shown in Figure 3  the activity throughout the week of the depressive users dataset presented more regular activity than the control dataset  whose users’ activity showed a sharp drop during the weekend. The differences between these datasets were statistically significant (Greenhouse-Geisser F=4.153; P=.008).,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://www.jmir.org/2019/6/e14199/,22,9,part of speech,"Leis, Ronzano, Mayer, Furlong, Sanz",5,0,0,0,2019,As for the analysis of POS corresponding to the number of words by grammatical categories in each tweet  we compared the 3 datasets of tweets: the control  depressive users  and depressive tweets datasets. As previously stated  the tweets of the depressive tweets dataset were removed from the depressive users dataset. The frequencies of words in each group are shown in Table 2. The number of nouns used in the control group almost doubles that of the depressive users dataset. By contrast  verbs are more frequently used in the depressive users dataset than in the control dataset. There were statistically significant differences between the control and the depressive users datasets (χ27=1 242 600; P<.001)  between the control and the depressive tweets datasets (χ27=2 105.7; P<.001)  and between the depressive users and the depressive tweets datasets (χ27=15 888; P<.001). In relation to the different types of pronouns in the control dataset  we detected 396 181 personal pronouns (51.38%; 396 181/770 955)  the first-person singular (38.37%; 152 013/396 181) being the most used. A similar profile was observed in the depressive users dataset  where 124 614 pronouns were found (55.16%; 124 614/225 913)  the first-person singular remaining the most used (57.59%; 71 768/124 614). In the depressive tweets dataset  865 personal pronouns (53.16%; 865/1 627) were identified  and the first-person singular pronoun was by far the most used (80.00%; 692/865). The frequencies of personal pronouns in the different datasets are shown in Figure 4. There were statistically significant differences between the control and the depressive users datasets (χ25=15 912; P<.001)  between the control and the depressive tweets datasets (χ25=638.7; P<.001)  and between the depressive users and the depressive tweets datasets (χ25=183.9; P<.001). In relation to the number of characters per tweet  the mean of characters per tweet in the control and depressive users datasets was 83.48 (SD 40.57) and 65.76 (SD 36.99) characters  respectively  with statistically significant differences between them (t213770=161.6; P<.001). On the other hand  the mean in the depressive tweets dataset was 67.51 (SD 38.28)  which was not statistically significant and different in comparison with the depressive dataset (t1012.3=1.45; P=.15). The 200 most frequent words that appeared in the control and depressive users datasets are depicted in the 2 word clouds shown in Multimedia Appendix 1. The 10 most frequent words that appeared in the control dataset were the following: hoy (today)  día (day)  ver (to see)  quiero (I want)  gracias (thank you)  mejor (better)  siempre (always)  vida (life)  ahora (now)  and YouTube. In the depressive users dataset  the 10 most frequent words were the following: quiero (I want)  vida (life)  siempre (always)  siento (I feel)  nadie (nobody)  mierda (shit)  never (nunca)  and día (day). It should be noted that in the depressive tweets dataset  although there are several words in common with the depressive users dataset  we can find additional words that are not present in the other datasets  such as vacío/a (empty)  matar (to kill)  desaparecer (to disappear)  suicidar (commit suicide)  muerta (dead)  desastre (disaster)  inútil (useless )  deprimida (depressed as state in women)  depresiva (depressed as a condition in women)  and insomnio (insomnia). The word cloud of the depressive tweets dataset is shown in Multimedia Appendix 2. In relation to the use of links  hashtags  and mentions in tweets  the frequency of them in the control and depressive users datasets were 35.32% (251 728/712 584)  13.13% (93 575/712 588)  and 44.00% (313 574/712 577) and 18.07% (25 475/140 946)  1.44% (2030/140 946)  and 9.27% (13 060/140 942)  respectively. The number of tweets  including emojis  were 13.61% (97 038/712 589) in the control dataset and 5.72% (8069/140 947) in the depressive users dataset.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://www.jmir.org/2019/6/e14199/,22,10,emotion analysis,"Leis, Ronzano, Mayer, Furlong, Sanz",5,0,0,0,2019,Regarding the distribution of emotions  in the control dataset and in the depressive users dataset  the most frequent emotion was happiness (53.30%; 203 029/380 874 and 41.60%; 40 535/97 425) followed by sadness  which was more frequent in the depressive users dataset (17.59%; 67 033/380 874 and 25.49%; 24 834/97 425). In the depressive tweets dataset  the most frequent emotion was sadness (34.00%; 303/891). There were statistically significant differences between the control and the depressive users datasets (χ25=6838.2; P<.001)  between the control and the depressive tweets datasets (χ25=296.8; P<.001)  and between the depressive users and the depressive tweets datasets (χ25=65.6; P<.001). The frequencies of the different emotions are shown in Figure 5.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,2,0,,0
https://www.jmir.org/2019/6/e14199/,22,11,negation words,"Leis, Ronzano, Mayer, Furlong, Sanz",5,0,0,0,2019,Regarding the use of negation words  they were detected in 21.74% (154 953/712 588) of the tweets in the control dataset  in 34.15% (48 137/140 946) of the depressive users dataset  and in 45.50% (455/1000) of the depressive tweets dataset. The mean of negation words was 0.28 (SD 0.59) in the control dataset  it was 0.49 (SD 0.82) in the depressive users dataset  and it was 0.67 (SD 0.91) in the depressive tweets dataset. There were statistically significant differences between the control and the depressive users datasets (Mann-Whitney U=4.3657e+10; P<.001)  between the control and the depressive tweets datasets (Mann-Whitney U=266 990 000; P<.001)  and between the depressive users and the depressive tweets datasets (Mann-Whitney U=62 002 000; P<.001).,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,2,0,,0
https://www.jmir.org/2019/6/e14199/,22,12,polarity analysis,"Leis, Ronzano, Mayer, Furlong, Sanz",5,0,0,0,2019,In relation to the polarity of tweets  2 analyses were performed using 2 Spanish sentiment lexicons: the Senti Lexicon (including positive and negative categories) and the SentiCo Polarity (including positive  moderate positive  moderate negative  and negative categories). According to the Senti Lexicon  the analysis of tweets showed that the control dataset shows polarity in 33.47% (245 367/733 029) of the tweets  being positive in 56 54% (138 726/245 367) of them. In contrast  the depressive users dataset shows polarity in 41 31% (61 132/147 996) of the tweets  being positive in 46.14% (28 205/61 132) of them. Finally  the depressive tweets dataset shows polarity in 58.90% (589/1000) of the tweets  with positive polarity in 34.97% (206/589) of them. There were statistically significant differences between the control and the depressive users datasets (χ21=2134; P<.001)  between the control and the depressive tweets datasets (χ21=110.3; P<.001)  and between the depressive users and the depressive tweets datasets (χ21=28.8; P<.001). When using the SentiCo Polarity tool  the control dataset presented 20.97% (152 228/725 717) of tweets with polarity  29.32% (42 820/146 033) in the depressive users and 33.34% (348/1 044) in the depressive tweets dataset. The distributions of polarities are shown in Figure 6. There were statistically significant differences between the control and the depressive users datasets (χ23=8820.8  P<.001)  between the control and the depressive tweets datasets (χ23=308.8; P<.001)  and between the depressive users and the depressive tweets datasets (χ23=52.4; P<.001).,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,2,0,,0
https://www.jmir.org/2019/6/e14199/,22,13,principal findings,"Leis, Ronzano, Mayer, Furlong, Sanz",5,0,0,0,2019,The diagnosis of depression is complex because of the heterogeneous nature of this disease and the diverse manifestation of the symptoms among individuals  which result in a great number of depressive disorder cases that are undetected and untreated  making the prevention  diagnosis  and treatment of the depressive disorders a complicated task For these reasons and taking into account that people diagnosed with depression are increasing worldwide  new strategies for detecting and monitoring this disease would be very useful. In this study  we analyzed the behavioral and linguistic patterns of tweets in Spanish that suggest signs of depression. The results contribute to the growing body of scientific literature that analyzes the messages posted on social media using languages other than English. We have introduced a new approach that comprises analyzing the timelines of self-qualified depressed users  as well as their tweets related to depression  which are manually selected. Our results show that the tweets of depressive users have different features in comparison with those of a control dataset  even when their tweets that are not related to depression are considered (depressive users dataset). In addition  the differences with the control dataset become more evident when we consider the manual selection of tweets related to depression (depressive tweets dataset).,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,2,0,,0
https://www.jmir.org/2019/6/e14199/,22,14,different distributions of tweets over time,"Leis, Ronzano, Mayer, Furlong, Sanz",5,0,0,0,2019,As for the distribution of tweets over time  the users of the depressive dataset  although being less active in using Twitter  used to be more active during the night than the users of the control dataset. This can be explained as a result of insomnia  one of the most frequent symptoms of depression. This finding is consistent with previous studies carried out with English speakers  which demonstrated that individuals with depression are more active during the night Moreover  the daily mood changes  such as the morning and evening worsening that are typical in several forms of depression  could explain the lower activity of the depressive users In relation to the distribution of tweets throughout the week  the users of the depressive dataset showed a more regular activity throughout the week  tending to be more active on Saturdays  Sundays  and Mondays than those of the control dataset  whose activity showed a drop during the weekend. This trend may be related to the lowered social activity of the people suffering from depressive disorders  having a reduced participation in social leisure activities during the weekend and spending more time at home  sharing their feelings and thoughts on social media platforms,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://www.jmir.org/2019/6/e14199/,22,15,different style of writing,"Leis, Ronzano, Mayer, Furlong, Sanz",5,0,0,0,2019,The analysis of POS and the number of words by grammatical categories show that  generally  the users of the depressive dataset used more verbs  adverbs  and pronouns but less nouns than the control dataset. The same features are also present in the depressive tweets dataset. These findings suggest that the language of people suffering from depression is characterized by a different style of writing that some authors describe as poorly structured  indicating less interest in what surrounds them  people  objects  or things They focus on talking about actions  and this is correlated with sensitive disclosure. Consistent with many previous studies the use of first-person singular pronoun is more frequent among the users of the depressive dataset  with respect to those of the control dataset  and this difference increases in the depressive tweets dataset. The increased use of this pronoun demonstrates the attention to self-focus that is associated with the negative emotional states of depression and the reduced attentional resources  highlighting the psychological distancing to connect with others This social isolation may also explain that the first- and second-person plural pronouns are the least used. Language can be used as a measure of different individual features  on the basis of the fact that people’s word choice is stable over time and consistent across topics or context. For this reason  the language style appears to be a useful predictor of some mental health conditions  such as depression In addition  the number of characters written in the depressive users and depressive tweets datasets was smaller than the number of characters written in the control dataset  and this might be related to reduced interest and poorer language. According to the most frequent words that appeared in the depressive users and depressive tweets datasets  there are specific words that are linked to clinical symptoms and the way that depressive patients word their mood  such as words that may be related to suicide ideation. Consequently  they can be used as a signal to detect potentially depressed users on Twitter Similarly  we observed the frequent use of adjectives in feminine form in the depressive tweets dataset  which would suggest that a high proportion of the depressive users are women  a fact that is in agreement with clinical and epidemiological evidences,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://www.jmir.org/2019/6/e14199/,22,16,predominant emotions,"Leis, Ronzano, Mayer, Furlong, Sanz",5,0,0,0,2019,Emotions are one of the key aspects that characterize many mental health conditions and  particularly  when people are suffering from depression. An analysis of the 6 emotions that are commonly considered (happiness  sadness  surprise  anger  fear  and disgust) was performed to determine the existence of differences among the datasets. Happiness is the most frequent emotion in the control and depressive users datasets  although an important reduction was observed in the depressive tweets dataset. The surprise emotion is less frequent in depressive users and  specially  in the depressed tweets datasets than the control dataset  and this fact can be related to the depressive mood  in which there is a decrease in interest in almost everything. Fear does not seem to be a differential emotion in the groups of tweets analyzed in this study. Regarding negative emotions  we observed an increase in the frequency of words related to the sadness emotion in the depressive tweets dataset  doubling that of the control dataset. This feature had also been observed in other studies Moreover  anger is more frequent in the depressive user and depressive tweets datasets than in the control dataset. Although Twitter is used many times for expressing anger about personal or political aspects  this emotion is particularly frequent in patients suffering from depression  who tend to feel irritable  wronged  or angry at the world At the same time  disgust  an emotion that is known to be associated with the depressive disorders was found to be more frequent in the depressive users and depressive tweets datasets.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://www.jmir.org/2019/6/e14199/,22,17,negative focused emotion language,"Leis, Ronzano, Mayer, Furlong, Sanz",5,0,0,0,2019,In our analysis  the presence of negation words is more frequent in the depressed users (34.15%; 48 137/140 946) and depressive tweets (45.50%; 455/1000) datasets than in the control dataset (21.74%; 154 953/712 588)  indicating that there is an increased use of negatively focused emotion language  which is typical in depressive patients and feelings,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://www.jmir.org/2019/6/e14199/,22,18,negative polarity,"Leis, Ronzano, Mayer, Furlong, Sanz",5,0,0,0,2019,The classification of tweets  on the basis of the emotional positivity or negativity of their words  is another analysis that has been carried out. In this study  we used 2 types of polarity lexicons  the Senti Lexicon (SentiLex) and the Sentico Polarity (SentiCo). In both cases  the negative polarity was higher in the depressive users and depressive tweets datasets  even tripling the negativeness of the control dataset when using the Sentico Polarity lexicon. These findings are consistent with other studies  indicating that people suffering from depression tend to focus more on negative aspects of their life and thus their tweets contain much more negative emotional words compared with the control dataset . In addition  the self-focus state that characterizes depression is associated with negative emotions,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://www.jmir.org/2019/6/e14199/,22,19,limitations and future directions,"Leis, Ronzano, Mayer, Furlong, Sanz",5,0,0,0,2019,This study presents some limitations that have to be pointed out. On the one hand  the tweets of the depressive datasets come only from Twitter users who speak publicly about feelings and emotions that can be related with depression. This is an indirect and inaccurate way of detecting users suffering from depressive disorders. Without clinically assessing these people  there is no way to verify if the diagnosis is genuine or if they suffer from another mental disorder. On the other hand  it is possible that Twitter users self-disclose their mental health using words or expressions not included in the list of keywords used in this study for streaming tweets about depression In this respect  it is possible that a wider list could have yielded a greater coverage Privacy policies of social media restrict the access to users who did not grant access to their profile  and this may have generated biases in the composition of the depressive users and the depressive tweets datasets. In addition  tweets may incorporate biases because of the self-management and anonymity of the Web-based identities Moreover  Twitter users may be not be representative of the general population  and some studies have shown that they are often urban people with high levels of education More information about the socioeconomic and demographic details of Twitter users is needed The control dataset was a randomly selected sample of Twitter users  and it is consequently representative of the users of this social media. However  there is a possibility that users in this group may also have depression or other mental illness even though they did not mention this in their profile description. There is also the possibility that the users included in the control group are fake accounts. Only original tweets were analyzed  and perhaps retweets  which are not included in our linguistic study  reflect users’ emotions that can be related to depression status Finally  depression is a very complex mental disorder  and our study only provides a general observation of this disorder. Additional research might be carried out to examine specific depression types and determine if there are social media features that can contribute to classifying users or tweets to the different diagnosis of depression Similarly  in future works  we plan to study the linguistic features and the behavioral patterns of depression in different linguistics contexts. The possible relationship between depression and seasonality could be of interest for future studies in the context of monitoring Twitter activity,0,0,0,0,0,0,0,0,0,0,,2,0,0,0,0,0,,0
https://www.jmir.org/2019/6/e14199/,22,20,conclusions,"Leis, Ronzano, Mayer, Furlong, Sanz",5,0,0,0,2019,The prevalence of common mental disorders worldwide  such as depression  requires the ability of health care systems to provide adequate diagnosis  monitoring  and treatment. The wide popularity of social media platforms introduces new opportunities for the screening of depression. The introduction of new methods of analysis for the automatic detection of signals of depression on social media platforms  such as Twitter or Facebook  has the potential of being used as a complementary tool for the assessment of these patients  assisting health care professionals in the detection and monitoring of mental health disorders. Although the analysis of tweets as a way to determine the existence of depression cannot be used as a replacement for diagnosis  it has the potential as a screening tool for depressive disorders  with a lower cost than other traditional procedures. In addition  it can be helpful to health professionals for managing and monitoring patients more efficiently. Similarly  it can be useful for particular patients  as they feel more comfortable disclosing their symptoms on Twitter than in clinical settings. In this study  we have shown that several behavioral and linguistic features of the tweets in Spanish can be used as a complementary tool to detect signals of depression of their authors  corroborating and extending the findings obtained by studies carried out on English tweets. As we described in this study  signs of depression of Twitter users are not exclusively spotted by identifying and analyzing tweets that explicitly mention expressions related to depression. Moreover  Twitter users who are potentially suffering from depression globally modify the core traits of their language  independently from the fact that the tweets are related or not related to the expression of depression. On the basis of these changes  these users can be monitored and supported. The results of this paper  jointly with other studies on the matter  support the potential of social media as an important instrument for extending and enhancing mental health services available to people with mental disorders. By means of interdisciplinary collaborations  it is possible to develop digital apps and services providing personalized alerts and psychosocial support in the mental health domain.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://aclanthology.org/W18-0608.pdf,23,1,abstract,"Loveys, Torrez, Fine, Moriarty, Coppersmith",5,0,0,1,2018,Depression is a global mental health condition that affects all cultures. Despite this  the way depression is expressed varies by culture. Uptake of machine learning technology for diagnosing mental health conditions means that increasingly more depression classifiers are created from online language data. Yet  culture is rarely considered as a factor affecting online language in this literature. This study explores cultural differences in online language data of users with depression. Written language data from 1 593 users with self-reported depression from the online peer support community 7 Cups of Tea was analyzed using the Linguistic Inquiry and Word Count (LIWC)  topic modeling  data visualization  and other techniques. We compared the language of users identifying as White  Black or African American  Hispanic or Latino  and Asian or Pacific Islander. Exploratory analyses revealed crosscultural differences in depression expression in online language data  particularly in relation to emotion expression  cognition  and functioning. The results have important implications for avoiding depression misclassification from machine-driven assessments when used in a clinical setting  and for avoiding inadvertent cultural biases in this line of research more broadly,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://aclanthology.org/W18-0608.pdf,23,2,introduction,"Loveys, Torrez, Fine, Moriarty, Coppersmith",5,0,0,1,2018,Depression is a common mental health condition that affects more than 300 million people globally (World Health Organization  2017). A major contributor to the overall global burden of disease  Major Depression was indicated as the second leading cause of years lived with disability in 2013 (Vos et al.  2015). While effective treatments for depression exist  less than half of those affected by the condition will receive treatment (World Health Organization  2017). Barriers to appropriate treatment include social stigma associated with mental illness  a lack of resources or trained healthcare providers  and inaccurate assessments (World Health Organization  2017). One cause of inaccurate assessment is the use of culturally-inappropriate or -insensitive diagnostic tools; that is  administering an assessment in a cultural context that differs from that in which it was developed  without adaptation or validation (Ng et al.  2016). Inaccurate assessments increase risk of depression misdiagnosis  resulting in patients receiving either incorrect treatment or no treatment at all; both of which may be dangerous outcomes for the patient,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://aclanthology.org/W18-0608.pdf,23,3,cross cultural differences in depression experience and expression,"Loveys, Torrez, Fine, Moriarty, Coppersmith",5,0,0,1,2018,According to some evolutionary psychological approaches to depression  depression is a breakdown in an evolved and adaptive response to experiencing scarcity and loss  particularly in relation to goal attainment  social relationships or status (Nesse and Ellsworth  2009; Kirmayer et al.  2001); thus  depression is likely to constitute part of the human condition  in some sense independent of culture. Across cultural contexts  depression onset is reliably related to vulnerability factors such as lack of social support  stress  unemployment and poverty  a demanding climate  family history of depression  adverse childhood experiences  and a high level of trait neuroticism (Chentsova-Dutton and Tsai  2009; Kirmayer et al.  2001; Sullivan et al.  2000; Chapman et al.  2004). While depression affects humans crossculturally  cultural context nevertheless impacts the way depression is experienced and expressed  and plays a role in shaping a community’s general beliefs about mental health and illness  and how treatment is approached (Chentsova-Dutton and Tsai  2009; Ng et al.  2016).,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://aclanthology.org/W18-0608.pdf,23,4,depression expression,"Loveys, Torrez, Fine, Moriarty, Coppersmith",5,0,0,1,2018,Chentsova-Dutton and Tsai (2009) suggest that cultural scripts about normative and deviant behavior impact expression of depression across cultural contexts. Deviant scripts  specifically those that pertain to depression and the expression of distress  vary amongst cultures. This means that depression symptoms attended to and reported cross-culturally may vary (due to differences in what ‘healthy’ and ‘depressed’ functioning mean in context  as well as cultural differences in what are socially acceptable symptoms to report (Kirmayer et al.  2001)). A large body of literature suggests that cultural differences in depression symptom reporting are reliably observed. However  how these differences in symptom reporting are manifested varies between studies. This is the result of variation in measurement methods (e.g.  use of closed versus open-ended self-report questions to evaluate symptoms)  degree of acculturation  and other socio-demographic factors at play  such as the socioeconomic status or education level of participants. Some previous work has found cultural variation in somatic versus psychological symptom reporting for individuals with depression. Due to a prevailing (implicit or explicit) belief in mindbody dualism (Ayalon and Young  2003) in western cultures  there is a tendency in western cultures to ‘psychologize‘ the symptoms of depression  focusing on reporting psychological symptoms (e.g.  low mood  cognitive symptoms such as thoughts of hopelessness or excessive guilt) while discussing depression spontaneously (Ryder et al.  2008). Cultures that have traditionally viewed physical and mental health as an interlinked concept  by contrast  might be more likely to spontaneously report somatic symptoms to indicate psychological distress (Ryder et al.  2008)  particularly in contexts where mental illness is heavily stigmatized and thus  reporting of somatic symptoms is more socially acceptable (Kirmayer et al.  2001)  or somatic symptoms are more heavily embedded at the forefront of the culture’s ‘script’ for depression (Chentsova-Dutton and Tsai  2009). Somatization tendencies in depression symptom reporting have been observed especially in Asian and Middle Eastern cultures (Chan et al.  2004; Ayalon and Young  2003). However  methodology for assessing depression symptoms can impact the degree of somatization observed between cultures (Chan et al.  2004). For example  Ryder et al. (2008) observed that Chinese individuals with depression were more likely to self-report somatic symptoms spontaneously in response to an open-ended question about depression symptoms. When asked closedended questions about depression symptoms in a structured interview  the rate at which Chinese individuals reported experiencing psychological symptoms (e.g.  low mood) increased. Other literature has highlighted how cultural variation in emotion expression norms and ideals impacts how individuals with depression might express or regulate their low mood. One study in particular compared European American and Asian American individuals with depression to non-depressed controls on type and degree of emotion expression following exposure to a sad film (Chentsova-Dutton et al.  2007). Differences in cultural norms pertaining to emotion expression and regulation meant that participants with depression either expressed or regulated sadness in response to the film dependent on culture. In both cases  emotion expression or regulation was opposite to the non-depressed cultural norm. Thus  whether low mood is more likely to be expressed or regulated by individuals with depression varies by culture. Membership in an individualist or collectivist culture may also have implications for how depression symptoms are reported. Individualism and collectivism can influence the perceived causes of mental health diagnoses  the way conditions are conceptualized  and what is viewed to be an appropriate treatment response (Hall et al.  1999). Members of individualist western cultures tend to view depression as a mental health challenge experienced by the individual  caused by factors related to the individual specifically  and appropriately treated at the individual level. Conversely  collectivist cultures are more likely to conceptualize depression as a family  community  or tribal problem best treated with group involvement and consideration of social factors  with social factors a key contributor to the cause of illness. This may have implications for the ways in which individuals understand and thus talk about their depression.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://aclanthology.org/W18-0608.pdf,23,5,beliefs about mental illness,"Loveys, Torrez, Fine, Moriarty, Coppersmith",5,0,0,1,2018,Beliefs about the social acceptability and causes of mental illness can vary across cultural groups and impact how these groups talk about depression  whether members of the group are likely to seek help  and whether depression symptoms are considered to be a medical problem requiring treatment at all (Patel et al.  2016; Aggarwal et al.  2014; Saraceno et al.  2007). In some contexts  depression symptoms are viewed as a normal response to the conditions of human life (ChentsovaDutton and Tsai  2009)  or are perceived to be a ‘western’ problem (Patel et al.  2016). Eastern Europeans tend to view mild depression symptoms and negative emotion as part of normal functioning (Jurcik et al.  2013; Turvey et al.  2012). Given the clear evidence for cross-cultural differences in depression experience  expression  and beliefs about mental illness in the clinical literature discussed above  it follows that the ways in which people discuss their depression symptoms online might also vary according to culture.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://aclanthology.org/W18-0608.pdf,23,6,language markers of depression online,"Loveys, Torrez, Fine, Moriarty, Coppersmith",5,0,0,1,2018,Many studies have found linguistic predictors of depression in social media and online data more generally. In comparison to healthy controls  depressed individuals tend to write online with greater self-focus (Coppersmith et al.  2014; Preot¸iuc-Pietro et al.  2015)  tentativeness (Coppersmith et al.  2015)  general negativity (De Choudhury et al.  2013)  sadness (De Choudhury et al.  2013; Schwartz et al.  2014; Preot¸iucPietro et al.  2015)  anxiety (Coppersmith et al.  2014)  anger (Coppersmith et al.  2014)  interpersonal hostility (Preot¸iuc-Pietro et al.  2015)  swearing (Resnik et al.  2015a)  and are more likely to display evidence of anhedonia (Preot¸iucPietro et al.  2015)  social problems (Schwartz et al.  2014; Resnik et al.  2015a b)  health and sleep issues (Schwartz et al.  2014; Resnik et al.  2015b)  inactivity (Coppersmith et al.  2015)  death interest (Coppersmith et al.  2015; Preot¸iuc-Pietro et al.  2015)  perceived hopelessness (Schwartz et al.  2014)  and problems in key life domains such as work or school (De Choudhury et al.  2013; Resnik et al.  2015b). Depressed individuals are less likely to discuss leisure (Coppersmith et al.  2015)  self-care or exercise (Resnik et al.  2015b)  are less likely to provide evidence of engagement in social activities (Resnik et al.  2015a)  and are less likely to exhibit positivity in their online language (Resnik et al.  2015a; Reece et al.  2017). In the studies cited above  individuals were indicated to have depression based on self-reported diagnosis or electronic medical records  and language samples were taken from a diverse set of social media sites and forums  such as Facebook  Twitter  and Reddit. These findings suggest that individuals with depression have quantifiable differences in their use of language online  compared to the general population. However—and crucial to the goals of the current study—it is important to note that most research in this area has been either based on data taken from predominantly Caucasian western populations  or the cultural composition of the samples were simply not reported or analyzed. Thus  a major question in this literature is whether linguistic correlates of depression from internet data hold across different cultural groups.,0,0,0,0,1,0,0,0,0,0,,0,0,0,0,1,0,,0
https://aclanthology.org/W18-0608.pdf,23,7,cultural differences in online language markers of depression,"Loveys, Torrez, Fine, Moriarty, Coppersmith",5,0,0,1,2018,in internet-derived linguistic markers of depression to date. De Choudhury et al. (2017) analyzed Tweets of users who self-reported a diagnosis of depression  ‘mental illness’  or experiencing suicidal ideation in aggregate. Comparisons were made between ‘Western’ (United States  United Kingdom) and ‘Non-Western’ (South Africa  India) groups with the Linguistic Inquiry and Word Count (LIWC2015) software and topic modeling. ‘Non-western’ cultural groups were more likely to inhibit expression of their mental illness experience online  which manifested in multiple ways: 1) Firstly  ‘non-western’ individuals with depression expressed higher positive affect and lower negative affect  anger  anxiety  and sadness in comparison to ‘western’ cultural groups. 2) Secondly  individuals from ‘non-western’ cultural groups displayed lower cognitive impairment  as evidenced through greater mentions of cognitive processes (e.g. cause  know  ought)  certainty terms (e.g. always  never)  discrepancies (e.g. should  would)  and perceptions (e.g. look  heard  feeling) in comparison to ‘western’ cultural groups. 3) Additionally  ‘western’ groups were more likely to discuss functioning  such as social concerns  health  body  and biology  than ‘nonwestern’ groups. ‘Non-western’ groups were less likely to discuss ’taboo’ topics such as religion  death  and sexuality. Topic modeling further revealed cultural differences. ‘Western’ cultures were more likely to discuss social isolation  death and self-destruction  whereas ‘non-western’ cultures were more likely to discuss shame from experiencing a mental illness  and make confessions related to their mental health struggles. These findings suggest that ‘non-western’ cultural groups tend to inhibit expression of mental illness in online language. In contrast  ‘western’ groups let the cognitive  emotional  and social experiences of their mental illness be more clearly evident in online language. However  given the nascence of this research  further research is needed to replicate these findings as well as to examine language differences amongst more diverse cultural groups.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,1,0,,0
https://aclanthology.org/W18-0608.pdf,23,8,the present study,"Loveys, Torrez, Fine, Moriarty, Coppersmith",5,0,0,1,2018,In the current study  we present an exploratory analysis of differences in the linguistic expression of depression across cultural groups within the United States. Specifically  we explore how the language of White  Asian or Pacific Islander  Black or African American  and Hispanic or Latino individuals with depression compared while discussing their mental health on an online mental health support forum.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,1,0,,0
https://aclanthology.org/W18-0608.pdf,23,9,data collection,"Loveys, Torrez, Fine, Moriarty, Coppersmith",5,0,0,1,2018,Data was collected from 7 Cups of Tea  an anonymous online  chat-based peer support community for emotional distress1 . Users agree at signup that their data may be used for the purposes of research. All the data used for the current study was anonymous and securely stored. This research was performed in line with the ethical and privacy protocols outlined in detail in (Benton et al.  2017). Data from 7 Cups takes the form of written dialogue between users of the service and volunteers who are trained as “active listeners”. A fragment of an exchange between the user of the service (U) and the volunteer (V) might go as follows: For the analyses reported in this paper  we used only text generated by users of the service  not the volunteers providing peer support. Users who reported depression as their primary concern at sign up were eligible for inclusion in analyses. Our original sample was comprised of 23 048 conversations involving 1 937 unique users. Users were excluded from the sample if they did not indicate their culture  or if they selected ‘Other’. This resulted in the exclusion of 199 and 130 users  respectively. The original sample also included users identifying as Native American or American Indian. This group was excluded from analyses since the majority of the data among these users was not English. This resulted in the removal of 15 users  leaving a total sample size of 1 593.,2,2,2,2,0,0,0,0,0,0,,0,2,2,0,0,0,,0
https://aclanthology.org/W18-0608.pdf,23,10,measures,"Loveys, Torrez, Fine, Moriarty, Coppersmith",5,0,0,1,2018,Users of the service completed a questionnaire at sign-up in which they provided information about their demographic characteristics and mental health. Demographic characteristics assessed included age  gender  and ethnicity. Ethnicity response categories were White  Asian or Pacific Islander  Black or African American  Latino or Hispanic  Native American or American Indian  or Other. Users could only select one ethnic group category. Users also select the primary reason for using 7 Cups  and the users above all indicated a primary purpose of “Depression”.,0,0,0,0,0,0,0,2,0,0,,0,0,0,0,0,0,,0
https://aclanthology.org/W18-0608.pdf,23,11,results,"Loveys, Torrez, Fine, Moriarty, Coppersmith",5,0,0,1,2018,We report descriptive statistics of the sample  LIWC analyses  and the results of a topic modeling analysis.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://aclanthology.org/W18-0608.pdf,23,12,descriptive statistics,"Loveys, Torrez, Fine, Moriarty, Coppersmith",5,0,0,1,2018,Data was anonymous and users were analyzed in aggregate by cultural group. No personallyidentifiable information was available. We report descriptive statistics to give a sense of the overall composition of the sample. Table 1 outlines demographic characteristics and mental health status of participants. Overall  participants were predominantly female (67.3 percent)  white (68.6 percent)  young adults (m = 21.4  SD = 7.6)  who were somewhat distressed at sign-up (7/10). No statistically significant or meaningful differences in age  gender  or sign-up distress level were found between cultural groups and thus  these characteristics were not controlled for in subsequent analyses.,0,0,0,0,0,0,0,0,0,0,,0,2,0,0,0,0,,0
https://aclanthology.org/W18-0608.pdf,23,13,liwc analyses,"Loveys, Torrez, Fine, Moriarty, Coppersmith",5,0,0,1,2018,Next  exploratory analyses were conducted with the Linguistic Inquiry and Word Count software (LIWC2015) (Pennebaker et al.  2015)  which is a psychometrically-validated program that evaluates the percentage of total words in a document that relate to different psychological constructs (e.g.  “emotion”  “cognition”) or life domains (e.g.  “health”  “social”). LIWC has been used in prior research evaluating social media language patterns of diverse samples with depression (Coppersmith et al.  2015; De Choudhury et al.  2017). LIWC analyses in the present study compared language of White  Asian or Pacific Islander  Black or African American  and Hispanic or Latino users with depression. Language analyses with LIWC were exploratory in nature and thus we compared cultural groups on the degree to which they expressed content about a wide range of relevant topics  including emotion  cognitive impairment  social functioning  health  and taboo topics. Given the large amount of language comparisons made between cultural groups  we draw the reader’s attention to several interesting findings in light of existing cross-cultural depression literature (see Figure 1). Note also that due to the exploratory nature of the current study  we do not conduct or report statistical tests over the LIWC results. In the absence of a specific hypothesis about the distribution over LIWC scores  conditioned on ethnic group and LIWC category  statistical tests such as ANOVA would be misleading at best. We hope that the current exploratory analyses will guide future hypothesis-driven work. First  cultural differences in degree and type of emotion expression were observed. Here  emotion is captured by the LIWC category “tone”  which reprsents the ratio of positive to negative emotion expression. Asian or Pacific Islander users showed more inhibition of negative emotion  whereas White and Black or African American users expressed more negativity (in other words  exhibited less regulation of their negative emotional state). Hispanic or Latino users expressed a large amount of both positive and negative emotion compared to other groups. Second  cultural differences in cognitive categories were observed  whereby cognitive effects of depression were less evident in language of Asian or Pacific Islander users. Third  discussions of functioning were impacted by culture. White users appeared less social  and were more likely to report on health and death or self-destruction compared to other groups. Asian or Pacific Islander users were less open to discussing health or death  though social terms were more present. Black or African American users discussed social terms to a high degree  and were comparatively less likely to discuss death  but were more willing to talk about health compared to other groups. Opposite to Black or African American users  Hispanic or Latino users with depression had low mentions of social terms and were less willing to make disclosures about death or self-destruction  religion  or health. Our findings suggest that different cultural groups may be more or less willing to spontaneously discuss particular topics relevant to mental health online. This may have implications when looking to detect individuals with depression from online data  particularly when the sample population is culturally diverse.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,1,0,,0
https://aclanthology.org/W18-0608.pdf,23,14,topic modeling,"Loveys, Torrez, Fine, Moriarty, Coppersmith",5,0,0,1,2018,Users’ messages were analyzed with topic modeling to provide qualitative assessments of the emergent topics or themes that users wished to discuss with volunteers on the platform. While topic modeling may miss some of the fine-grained insights into users’ concerns that a human observer could provide  because it is an unsupervised  datadriven approach to analyzing linguistic data  it offers the intriguing possibility of discovering patterns in users’ preoccupations that a human observer would be less likely to identify. Topics were obtained by running Latent Dirichlet Allocation (LDA) over each cultural group’s messages  i.e. one topic model was created per cultural group and an individual document in each corpus was a single user message. The data was pre-processed by removing chat-specific stopwords  words with very high frequency (occurring in more than 75% of the documents) and words that occur fewer than five times. We then used Gensim’s implementation of multi-core LDA with the default hyper-parameter settings and three topics. Analysis of the terms that were assigned to top ics per cultural group revealed that among the top three topics for each cultural group  there was little overlap in terms. Term overlap was measured using the Jaccard similarity coefficient and is shown in Fig. 2. The similarity coeffecient can be interpreted as the percent overlap of the set of terms in each topic. A similarity coefficient of 1 would indicate that all terms assigned to two different topics were exactly the same. Values higher than 0.3  indicating approximately a third of terms were shared in common between two topics  occurred only four times out of fifty-four topic comparisons. Most coefficients are closer to 0.1 and there are many topics with no term overlap. Further work involving analysis of term overlap among members of the same cultural group and computing the difference in topic distribution between groups (by comparing to a single overall topic model) would further illuminate what topical diffences there are between cultural groups. Further analysis of the specific terms assigned to each topic is captured in Table 2  which shows the top five terms associated with each topic across cultural groups. These sorts of visualizations often resist neat  intuitive explanations. The collection of terms in each topic do not seem to form cohesive topics (e.g.  emotions  relationships  etc.)  and specific terms (e.g.  ‘work’  ‘friend’  and ‘need’) appear across multiple topics  both within a single cultural group and across cultural groups. Topics discussed by all groups may be relevant to individuals with depression cross-culturally; for example  analyses revealed all cultural groups made disclosures about the topic  ‘friend’  which suggests loneliness or ‘need of a friend’ is a concern for individuals with depression that cuts across culture. However  the collection of terms in each topic does vary across cultural groups  indicating that there are differences in the themes discussed by users belonging to different cultures. Further work is likely to involve mapping the original chat messages to the topics they are most likely to belong to in order to extract human-interpretable descriptions of the different topics.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,1,0,,0
https://aclanthology.org/W18-0608.pdf,23,15,discussion,"Loveys, Torrez, Fine, Moriarty, Coppersmith",5,0,0,1,2018,Our overall conclusion is consistent with existing cross-cultural depression research (De Choudhury et al.  2017)  namely: there are cross-cultural differences in online language of individuals with depression. Our results highlight the importance of creating culturally-adapted depression classifiers as automated assessments become increasingly commonplace in the treatment and identification of mental health issue  and suggest a role for research of this kind in developing culturally sensitive clinical instruments for measuring depression. Our study included a broad range of cultural groups analyzed relative to previous work. Moreover  our use of both closed- and open-vocabulary analyses allowed for both theory-informed and data-driven analyses of language of many diverse cultural groups with depression  which similarly complements the currently existing body of literature on this subject. A few caveats are worth noting. First  the data reported here was taken from a peer support community specifically for providing emotional support. It is therefore unclear whether and to what extent our findings generalize to other online spaces such as social media  where discussions about mental health are not explicitly encouraged  and where anonymity is not guaranteed. Some previous work has identified cross-cultural differences in language about mental health on social media  albeit for different cultural groups to the present study (De Choudhury et al.  2017). This suggests that our results are unlikely to reflect idiosyncratic properties of the platform  though a rigorous examination of this question must be left to future work. Second  data about participants’ country of residence and extent of acculturation were missing. Thus  the extent to which users were acculturated to western beliefs about mental illness or held traditional mental health beliefs of their culture is somewhat unclear. There is evidence to suggest acculturated individuals report symptoms differently compared to individuals more entrenched in the health beliefs that prevail in their culture (Jang et al.  2005). For example  it was not clear from the available data whether an individual identifying as ‘white’ was a white American or  for instance  a white German living in the United States. This is relevant since it is reasonable to suppose that white Americans and white Germans are not identical to each other in the way they think and talk about mental health. Similarly  it is plausible that a third-generation Korean American  on the one hand  and a Korean citizen living in the United States  on the other  would both identify as “Asian American”  though it would be odd to classify these individuals as having the same culture for the purposes of the current analysis. The relative frequency of these types of observations is unknown. A third limitation of this study was the labels used to define groups in our dataset  which include a mix of ethnic and racial groups. These labels were determined by the peer support community. While our cultural group labels were imperfect  we were still able to observe meaningful differences between groups  as well as to conduct a more fine-grained cultural analysis comparative to prior literature in this area  which compared ‘Western’ to ‘non-Western’ cultures (De Choudhury et al.  2017). This paper adds to a small but growing literature examining cross-cultural differences in the way symptoms of depression are expressed in online language data. Our findings have important implications for designing automated depression assessments with online data  and suggest that making good predictions about mental health on the basis of language data will require taking cultural/ethnic identity into account. Should machine-driven depression assessments be deployed in a clinical setting  culturally sensitive classifiers may be necessary to avoid misdiagnosis  a key barrier to receiving effective treatment for depression (World Health Organization  2017). In future and ongoing work  we plan to extend these analyses to mental health conditions apart from depression  or to focus on depression subtypes  and to deepen this approach by using our exploratory analyses as a springboard for hypothesis-driven work oriented towards informing mental health-related interventions and mental health policy.,0,0,0,0,0,0,2,0,0,0,,2,2,0,0,0,0,,0
https://aclanthology.org/W18-0608.pdf,23,16,conclusion,"Loveys, Torrez, Fine, Moriarty, Coppersmith",5,0,0,1,2018,In conclusion  findings from this exploratory study suggest there are cultural differences in online language of individuals with depression. Differences found in the degree to which culturally-diverse individuals with depression express particular topics relevant to mental health online suggest careful attention is required to the cultural contexts in which language classifiers for depression are deployed. Appropriate adaptations  such as depression classifiers made for the cultural group of in terest  may be necessary to avoid misclassification and thus  inappropriate treatment responses. Moreover  these findings suggest a path forward for empirically-driven assessment and creation of cultural sensitivity best practices for online therapy and peer support  based on the concerns and experiences of the people seeking help.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/pdf/10.1145/3359169,24,1,introduction,"Pendse, Niederhoffer, Sharma",3,0,0,0,2019,Common mental health disorders such as depression and anxiety are becoming a growing cause of the disease burden globally [96]. Across the Global North and South  however  access to mental healthcare is limited [69]. In the context of reduced access to care [70] and widely prevalent stigma against accessing mental health services [93]  online mental health forums have become one avenue by which people experiencing mental distress can both express how they are feeling in a safe environment [58] and look for support online [63]. In particular  online health forums have become a place where people from particularly stigmatized communities  such as homeless members of the LGBT community [26  102] or racial minorities [46] can find support from other peers who have lived experience with similar forms of distress. As Johnsen et al [49] note  online forums where people can openly discuss their mental health are uniquely safe environments for people with concealable and stigmatized identities  an environment that these individuals may not be able to find anywhere else outside of the Internet. Being an act of courage and vulnerability  sharing stigmatized experiences online can often be difficult  particularly when an experience of distress deviates from majority norms on what symptoms of an illness should look like [40]. These deviations in illness experience are highly tied to identity. Attributes of identity such as gender [38]  race [101]  and class [57] can all have significant impact on how people express their mental health to others  with differences particularly seen when comparing along cultural lines [77]. For instance  when presenting to a clinician  individuals in Asian countries often express depression and other mental health disorders in terms of somatic symptoms  such as stomachaches or headaches [52  77]  different from common expressions of mental illness in Western cultures. Due to this diverse variation in symptom presentation  it is important for support platforms to be inclusive of these culturally-influenced framings of distress to be successful [71]. Moreover  without consideration of a shared cultural understanding between supporters and people seeking help  support programs can be ineffective [90  97]. However  even as use of online mental health forums increases globally  little has been done to examine cultural differences between how people from underrepresented countries and people from majority countries use online mental health forums. Neglecting to understand cultural differences in how people use online mental health forums and accommodate these differences could make individuals that are already members of minority groups feel further otherized on these platforms. Further  as these platforms employ data-driven technologies to classify posts and users on different attributes [17  32]  cross-cultural differences can result in systematically differential rates of utility for people from different cultures [75]. Given this gap in the literature on cultural differences in online mental health forum use  in this paper  we provide a cross-cultural analysis of international mental health support communities and study how previously reported differences in mental health expression translate to online forums. We begin with an analysis of people’s use of Talklife 1 a global support platform for mental health. Guided by past research on cultural differences in expression of mental health [35  62  77  80] in India  Malaysia and Philippines  as well as their popularity on Talklife  we present a comparative analysis of people’s use of the platform from these countries versus others  following past research [31  33  52] in assuming geographical location as a proxy for cultural identity. To test whether our findings generalize to other online support communities  we repeat our analysis on a separate platform  7Cups.2 While both are focused on mental health support  Talklife and 7Cups differ in the way their community is structured: Talklife is structured as a social platform around a user and their posts  while 7Cups is structured around subcommunities on topical themes such as depression and anxiety. These variations provide a robust testbed for testing differences in forum use due to culture. We characterize cultural differences along three dimensions of analysis based on past research on cross-cultural differences in mental health [42  64]: identity-based differences  linguistic differences  and behavior-based differences. These dimensions form the basis for our three research questions respectively: How does self-expression of cultural identity vary between people from different countries  and how does it evolve over time? (2) How does the use of clinical mental health language vary between people from different countries  and how does it evolve over time? (3) How do patterns of support-giving vary between people from different countries  and how does they evolve over time? On Talklife  we find that individuals from underrepresented countries tend to talk about their country of origin more than the majority sample  use clinical language less overall when describing their mental health than people from the majority sample  as predicted by past work. However  we also find a wide variation in clinical language use across posts  and find that clinical language use stays constant over time  contrary to theories that predict an increase due to exposure to international users and terminology [52  86]. In terms of support behavior  people are more likely to support people from the same national background  even though the Talklife platform shows all posts in reverse chronological order. Further  all of these findings replicate on 7Cups  though to differing degrees  likely due to the structural differences in the two forums. These cultural differences also have an impact on the success of the platform in helping individuals. We find that both the amount of clinical language used as well as the amount of cultural homophily has a strong relation to whether an individual from the minority sample is able to seek help effectively. Based on these  we suggest implications of cross-cultural differences for understanding use and designing interventions on such online platforms.,0,0,0,0,0,0,2,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/pdf/10.1145/3359169,24,2,background and related work,"Pendse, Niederhoffer, Sharma",3,0,0,0,2019,Culture is an fundamental part of how people participate in supportive conversations that ease mental distress  and how they frame that distress online. In this context  significant work has been done both to understand cultural differences in self-expression and self-disclosure practices  as well as in how people create and sustain their identity in online forums.,0,0,0,0,0,0,2,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/pdf/10.1145/3359169,24,3,past work in mental health and hci,"Pendse, Niederhoffer, Sharma",3,0,0,0,2019,There is a growing body of work in HCI and CSCW at the intersection of mental health and general online communities. Such work can be divided into two main categories—understanding and predicting the expression of mental health online (such as on social media) [10  12  29–31]  and creating interventions that improve the well-being of those with mental health issues [67  88]. However  as Feuston et al. [36] note  most work concerned with analyzing online data is done with the aim of algorithmic prediction  and can often ignore finer nuances of the experience of expressing mental health online. Relatedly  the issues of cultural identity have only sparsely been addressed in CSCW [31  103]  and have primarily been studied with users from Western populations. Through a deeper examination of the intersections between cultural identity and the expression of mental distress and support on online mental health forums  this work intends to fill that gap.,0,0,0,0,0,0,2,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/pdf/10.1145/3359169,24,4,crosscultural differences in expression of mental health,"Pendse, Niederhoffer, Sharma",3,0,0,0,2019,The individual experience of mental health is strongly influenced by a complex interplay of diverse cultural factors  with these cultural factors having a notable impact on how mental illness is expressed  including in symptoms [77]  willingness and method of self-disclosure [14]  and both internalized and external stigma [11]. In the case of India [77]  Malaysia [62  80]  and the Philippines [35]  significant research has shown that expressions of distress among people experiencing mental illness tend to be mainly physical and somatic  and that individuals tend to use stress-related language rather than clinical language to describe their experience of mental distress. At the same time  terms used to describe mental illness can also be appropriated and “glocalized” [81]  such as in Kerala  India where the English word “depression” is used to describe a wide spectrum of illness experiences [53]. These culturally bound expressions of mental distress  often called idioms of distress [64]  are often used as insight into the different explanatory models that people have about their illness. As detailed by Jorm et al. [50  51]  public knowledge and discourse about what mental health is (as well as awareness and education campaigns) can also have an impact on how people conceptualize their mental health  or what Jorm dubs “mental health literacy”. In many cases  this can cause individuals to see their symptoms through a medical or clinical lens. However  globally  awareness of mental health issues is not high [70]  and as a result  we use clinical language (as we define it in section 3.1.2) as an approximation signifier of whether people conceptualize their distress through more clinical or medical framings.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/pdf/10.1145/3359169,24,5,expression of mental health in general online communities,"Pendse, Niederhoffer, Sharma",3,0,0,0,2019,Additionally  in online forums  there are significant identity-based differences in the medium and method by which people choose to self-disclose and discuss mental illness. De Choudhury et al. [31] investigated how gender and culture affect discussions of mental illness online  and found cultural differences in how people from different countries discuss their mental health on Twitter  such as people from India and South Africa are less likely than people in the United States or United Kingdom to express negative emotions when discussing their mental health on Twitter. Similarly  some work has been done understanding what culturally specific mental health forums look like  such as Zhang et al.’s work analyzing online support groups for depression in China [103]. Zhang et al. find that both Chinese cultural values and stigma play a role in how individuals experiencing depression in China express distress and seek help. The context in which online discussions about mental health happen  such as after trauma [11] or abuse [10] can also affect the method by which disclosure of mental health information happens  such as whether disclosures are direct or indirect [11]. Moreover  these practices and behaviors around self-disclosure of sensitive information can be culturally bound to the specific subcultures within an online mental health community. This has been found by Chancellor et al [21  23] in the case of specific subcultural norms around the expression of disordered eating practices in eating disorder communities online  and Pater et al. [72] specifically among male members of eating disorder communities online. Similarly  Horne et al. [45] find the existence of certain informal norms in forums centered around suicide that dictate which posts are accepted as authentic [45] by the community. Looking more closely at design  O’Leary et al. [67] look at what factors are necessary to design an effective mental health intervention that allows for mental health expression  including an analysis of the role identity and demographic play  eventually extending that work to design an online mental health improvement tool [68].,0,0,0,0,0,0,2,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/pdf/10.1145/3359169,24,6,cultural differences in online mental health communitites,"Pendse, Niederhoffer, Sharma",3,0,0,0,2019,The above discussion illustrates the prevalence of cultural differences in online self-expression  but how they affect help-seeking and supportive conversations on online forums specific to mental health topics is less clear. Past work on support forums has mainly been limited to language-based differences between individual groups speaking different languages  such as Ramirez-Esparza et al.’s [78] findings on differences between Spanish-speaking and English-speaking users on a bilingual depression forum. Other work  such as Loveys et al.’s [56] work examining differences in mental health expression based on race within American users of 7Cups  also focuses mainly on linguistic attributes and does not examine behavioral differences or dynamics of help-seeking and support-giving. Additionally  it only looks at cultural differences within the United States and not internationally. Similar to our work  Pruksachatkun et al. [75] find that cultural background has an impact on the predictability of whether someone will feel better on Talklife  but do not deeply analyze the differences between users from different cultures that may be causing this difference in prediction accuracy.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,2,0,,0
https://dl.acm.org/doi/pdf/10.1145/3359169,24,7,data and methods of analysis,"Pendse, Niederhoffer, Sharma",3,0,0,0,2019,To examine whether cultural differences exist between users of online mental health support communities  we begin by doing an extensive analysis on data from use of Talklife by people from different countries. We then do a focused analysis on data from 7Cups to see if the cultural differences in expression of mental health on Talklife generalize to a different online support platform.,2,2,2,1,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/pdf/10.1145/3359169,24,8,method of analysis,"Pendse, Niederhoffer, Sharma",3,0,0,0,2019,"3.1.1 Selection Criteria and Data Scope. To understand the impact of cultural differences on how individuals use online mental health platforms  we begin our analysis by creating a dataset of users from different national communities on Talklife  a support platform with over half a million users [91]. For this analysis  due to the fact that most research in CSCW on mental health online has been done either agnostic of cultural context [12  34] or in a Western context [60  67  88]  we choose to focus on users from non-Western countries  following Zhang et al. [103]. As researchers located in the Global South and with lived experience interacting with the health system and diverse explanatory models [52] of mental illness  we believe that moving the focus of CSCW and CSCWadjacent mental health research away from the West is crucial to better meet the needs of people often underserved by the medical system [70]. To create these subgroups of users  we choose the three non-Western countries with the highest user populations on Talklife  or India  Malaysia  and the Philippines. Guided by the rich amount of literature on the unique nuances to mental health expression for each country [35  62  77  80]  we examine the national identity  linguistic  and behavior-based differences of use between each user subgroup. In particular  this research notes that as a result of cultural norms around the sharing of distress and alternative conceptualizations of mental illness  in India  Malaysia  and the Philippines  symptoms are often expressed in somatic and religious terms  as opposed to traditionally clinical or psychiatric terms. We choose to analyze each subgroup at the national level for both theoretical and practical reasons. On a theoretical level  in past work in the medical anthropology of mental health  national identity has commonly been used for a approximate level of analysis for cultural identity [31  33  52]. Additionally  on a more practical level  each user’s country was determined using their IP address by Talklife and shared with us in an user-anonymized dataset. Inferring a more precise location could potentially compromise user anonymity  as discussed in past work [47]  and did not seem to have any more significant value for our analysis of cultural differences than analysis at the national level. We analyze data from 10 532 Indian users  3370 Malaysian users  and 3370 Filipino users  as shown in Table 2. Collectively  we refer to these countries as the minority sample. As a comparison set  we construct a random sample of all threads on Talklife and refer to it as the majority sample. Due to the relative prevalence of users from Western English-speaking countries in Talklife  most of the threads in the majority sample include posts from countries such as the USA  UK and Canada. Indians are the largest non-Western minority subgroup on Talklife. Data was sampled from May 2012 to June 2018. Following this cross-national analysis  to see if our broader results on Talklife generalize to a differently structured online mental health community  we picked the largest Western country (the United States) and the largest non-Western country (India) represented on 7Cups  a similar support platform with more than 15 000 users actively using the platform each week [7]. Using 7Cups data  we repeat our analysis  testing for the same cultural differences we found in our Talklife sample. For this analysis  we were provided a sample of data on activity from 6055 Indian users  and 18581 American users  as shown in Table 2. Unlike our sample of Talklife users  this dataset is not a random sample. There is an upsampling of Indian users to ensure that we have data from a sufficient number of Indians in the dataset. Like on Talklife  Indians are the largest non-Western minority subgroup on 7Cups. We focus on Indian users due to a lack of sufficient data on users from Malaysia or the Philippines. Data was sampled from March 2014 - August 2018. 3.1.2 Defining Cultural Identity and Use of Clinical Language. In this work  we examine the relationship between cultural identity and use of online mental health support forums. To do so  we leverage Tomlinson’s definition of cultural identity as “self and communal definitions based around specific  usually politically inflected  differentiations: gender  sexuality  class  religion  race and ethnicity  nationality"" [94]  particularly looking at the aspect that of modern cultural identity that runs along national lines  as delineated by Hall et al. [41]. As a diverse and amorphous form of identity  cultural identity can often intersect and interact with other forms of identity  including religious or ethnic identity. However  in the absence of direct information about religious or ethnic identity  based on the data available  we use national identity as a proxy for cultural identity. Additionally  following Schlesinger et al’s [83] call for more intersectional analyses and methods within HCI  we also include analyses of adjacent and intersecting identities when relevant  including religious identity. To analyze clinical language  we use a broader definition of clinical language than just specific medical diagnoses. Following methods used in past work to analyze antidepressant related language [30]  we create a dataset of clinical mental health language  including unigrams  bigrams and trigrams from a list of mental disorders as defined by the International Classification of Diseases (ICD-10) and Diagnostic and Statistical Manual of Mental Disorders (DSM-5) [100]. We also included all unigrams from the MacMillan Dictionary list of words used to describe illnesses and diseases  both specifically for mental illness and general illness [1–3]. As a result  we include unigrams like “night"" (from night terrors) or sleep (from “sleep disorder"") as these are often correlated with specific symptoms of mental illness or distress  such as sleep issues or being awake at night [30]. This included any clinically common abbreviations for mental disorders  such as OCD for “obsessive compulsive disorder"" or BPD for “borderline personality disorder."" Shorthand for disorders commonly used by online communities  such as “pro-ana” (as used in pro-eating disorder communities) [22] were not included due to the difficulty in finding an exhaustive list of these terms across disorders. We choose to use terms from and associated with DSM and ICD categorized disorders as a result of the common usage of these frameworks globally [99]. Throughout our analysis of these varied factors  we use µ to represent means  and σ to represent standard deviations. 3.1.3 Constraints  Limitations and Tradeoffs. Cultural identity can exist at many different and intersecting levels  including subcultures and subcommunities within the larger umbrella of a cultural identity. As a result  for the purpose of this analysis  we had to adopt some constraints in order to do a meaningful and specific analysis. One large limiting constraint that we chose for this study is to use national identity at the state level as a proxy for cultural identity. Though a major and formative part of modern cultural identity  as argued by both Hall [41] and Tomlinson [94]  each country we analyze is incredibly diverse  with many individual cultural identities that both intersect and diverge from a greater national identity [54  64  89]. A more rich analysis of these other forms of cultural identity is beyond the scope of this work  but could lead to richer conclusions about the nature of cultural identity in online mental health support communities  particularly with regard to cultural differences between users with the same national identity. Additionally  to stay consistent between analyses  as a result of a lack of data on users from Malaysia and the Philippines  we only analyze users in India on 7Cups  and extend these findings to the experience of being part of a minority group on an online mental health forum. We draw validity for these exploratory findings from similar consistent patterns we observe between Indian  Malaysian  and Filipino users  but a deeper analysis with a larger dataset is likely necessary to determine when and for which minority communities these conclusions do not hold true. Additionally  while we construct clinical language through use of the commonly used DSM and ICD  both frameworks of illness categorization have significant limitations  particularly in the countries we have selected. For example  there are both mental health disorders that are culturebound [74]  as well as mental health language that is used in different ways within the specific countries we analyze  such as depression often being an umbrella term for all mental illnesses [53]. Additionally  it is clear that online support communities often develop their own cultural norms and language around mental health [21  72]  and a deeper understanding of how this plays out on Talklife and 7Cups is neither the focus nor within the scope of this work. In this work  we intentionally use standard clinical and medical terms for mental health disorders in our analysis of clinical language. As detailed in past anthropological research [52]  it is theorized that the use of medical and clinical language is representative of a medicalized explanatory model of illness  and we frame use of this language across cultures as a approximate signifier of a greater awareness of the presence of a mental disorder  as opposed to conceptualizing distress as “stress""  “tension"" or “depression"" [25  53  98]. For our analysis  we strictly analyzed posts that were in the Latin alphabet  with almost all posts on both Talklife and 7Cups being in English. However  as both Malay [8] and Tagalog [82] are most commonly written in the Latin script  and since it is common for users from India speakers to use romanized versions of Indian languages online [79]  it is possible that a small minority of posts in our analysis were text in a different language. However  as confirmed by only seeing English words used in our analysis of the top n-grams among each user subgroup  it is clear that English is the predominant language on both platforms. Though beyond the immediate scope of this work  a greater analysis of non-English code-switching on these platforms could lead to a deeper understanding of the impact of interactions on expression between users with the same national identity but different language preferences.",0,0,0,0,2,2,2,0,0,0,,2,2,0,0,2,0,,0
https://dl.acm.org/doi/pdf/10.1145/3359169,24,9,platform context,"Pendse, Niederhoffer, Sharma",3,0,0,0,2019,"3.2.1 Talklife. Talklife [5] is a global mental health support platform founded in 2012 [6] and designed as a safe space for people to find support and openly discuss their mental health when going through emotional or mental distress [61]. Defining Posts and Replies. Unlike forums based on self-diagnosis of illness (such as depression or anxiety-focused forums on Reddit)  Talklife does not contain any explicit illness-based categories. Instead  a user expresses their mental health as “posts” to which others can reply with messages of empathy  questions  and support. We refer to a post and its replies collectively as a “thread ” calling the initial post an “index post”  and the user who contributes the first post in a thread as the “original poster” or simply OP. Additionally  we call posts that are responses to an original post as “support responses”  following norms of behavior on Talklife in which posters will make an initial post to seek help  and responses from the community are messages of support  both in the form of empathy and further probing questions about the user’s situation [75]. To confirm that support responses are usually posts that support the original poster  we do an analysis of the top 10 n-grams in a representative sample of posts  finding that across populations  support language is used  as seen in Table 1. Finally  we use the term “posts"" when describing any analysis we do on posts that might be an index post by an OP or be a support response to an index post. At any time  the user can also browse a global feed of recent posts on Talklife and choose which ones to reply to. Within a user’s feed  these posts are ordered strictly based on the time in which they are posted by other users. Additionally  there is a norm on Talklife of using constructed usernames and pseudonyms when making an account—users usually do not use their actual names and maintain some level of anonymity when using the website. Posts within threads on Talklife tend to be short and conversational  with an average of 36 posts per user (σ = 248) and an average length of 14 words (σ = 21)  as seen in Table 2. Figure 1a shows the distribution of usage activity by comparing number of posts per user across different countries on Talklife. All countries show a similar trend in that most users are not heavy users  and only post a few times  as the vast majority of users contribute fewer than 10 posts. Additionally  as described in past work [75]  the vast majority of conversations on Talklife tend to be in English. 3.2.2 7Cups. Similar to Talklife  7Cups [4] is another global online mental health support platform  founded in 2013 [87]. 7Cups shares the main characteristics of mental health expression and support-giving with Talklife. Typically  a user (original poster) contributes an index post and then other people can reply to it  forming a chain of posts which we refer to a thread. However  7Cups is similar to typical web forums like Reddit where the platform is divided into sub-communities based on different mental health-related attributes  including specific diagnoses (such as an “Anxiety Support”  “Depression Q&A”  or “Eating Disorder Support” group). Additionally  some volunteers on 7Cups are trained by the website to be listeners who offer emotional support  and are then directed to users who may be in distress and want emotional support [15]. Unlike on Talklife  posts within threads on 7Cups tend to be longer and go into greater detail than posts on Talklife  with an average of 7 posts per user (σ = 80) and an average length of 64 words (σ = 121). These statistics can be seen in Table 2. Figure 1b shows the distribution of usage activity by comparing number of posts per user across different countries on 7Cups. We find a similar  but starker difference in heavy and light users of 7Cups: compared to Talklife  a bigger fraction of users contribute less than 10 posts. Similar to Talklife  the vast majority of posts on 7Cups are in English. In the next two sections  we present a cross-sectional and longitudinal analysis of activity on Talklife  followed by a comparative analysis of 7Cups in Section 6.",0,0,0,0,0,0,0,0,0,0,,0,2,0,0,0,0,,0
https://dl.acm.org/doi/pdf/10.1145/3359169,24,10,privacy ethics and disclosure,"Pendse, Niederhoffer, Sharma",3,0,0,0,2019,All data analyzed in this study was sourced (with license and consent) from the Talklife and 7Cups platforms. Additionally  to maintain user anonymity  all personally identifiable information was removed from the dataset before any findings were reported. Additionally  all work was approved by our institution’s Institutional Review Board.,0,0,0,0,0,0,0,2,0,2,,0,2,0,0,0,0,,2
https://dl.acm.org/doi/pdf/10.1145/3359169,24,11,cross sectional analysis,"Pendse, Niederhoffer, Sharma",3,0,0,0,2019,We first present a cross-sectional analysis of differences in identity-based behavior (RQ1)  use of mental health language (RQ2)  and patterns of support practices (RQ3) between people from minority and majority countries.,0,0,0,0,0,0,2,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/pdf/10.1145/3359169,24,12,identity based expression of mental health,"Pendse, Niederhoffer, Sharma",3,0,0,0,2019,As Talklife is a quasi-anonymous forum  there is no requirement to explicitly state one’s location when beginning a new thread or post—users are as anonymous as they choose to be. With this norm of anonymity as context  we find that individuals from India and Malaysia tend to mention their country of origin in their first post (regardless of whether index post or support response) at a much higher rate than the majority of people on Talklife. While individuals in the majority sample mention their country of origin .4 percent of the time in their first post on average (µ = .4  σ = .644)  Indian and Malaysian users mention their country of origin 1.2 percent of the time in their first post (µ = 1.2  σ = .108). The above differences are significant at a level of p = 2.59e − 19. Filipinos mention their country less often than the rest of Talklife (µ = .3  σ = .055)  however this difference is not significant. Given the norm of anonymity on Talklife  the fact that people from India and Malaysia mention their country of origin in their first post at a higher rate than the majority sample suggests that cultural or national identity may potentially be one important part of how people from minority countries represent their identity to the Talklife community. This finding extends past research in medical anthropology and psychiatry to online mental health communities  showing that people’s country of origin is intimately connected to how they frame mental health to outsiders [20  85]. Following this past work  we conjecture that the use of identifiers of country of origin may be a signal by the individual to others on the forum that they want to be supported by people from their cultural or national background. However  our finding with Filipinos also shows that this observation is not universal and needs to be understood within the context of each country.,0,0,0,0,0,0,2,0,0,0,,0,2,0,0,0,0,,0
https://dl.acm.org/doi/pdf/10.1145/3359169,24,13,linguistic differences in mental health expression and support,"Pendse, Niederhoffer, Sharma",3,0,0,0,2019,"4.2.1 General linguistic differences. To better understand the content-based differences between posts from individuals in the minority sample  we compare the top unigrams  bigrams  and trigrams from each minority country to those from the majority sample. We filter out any words that were only one character  as well as the Natural Language Toolkit’s built in stopwords [16]. The top 5 n-grams unigrams  bigrams and trigrams can be seen in Table 3. N-grams that have been redacted are the usernames of Talklife users that were discussed in plain text by others on the website. We can make a few observations from the relative frequency of n-grams. First  English is the most commonly used language within each community on Talklife  as words from languages native to each country were not the majority of any of the n-grams. Second  top-5 n-grams from the majority sample are not common to the top-5 n-grams in any of the minority sample  suggesting a difference in how people express mental health. Even when people are talking about the same theme  such as getting support  they tend to use different phrases: the majority sample uses “need someone to talk to”  while Indians prefer “like to talk to friends”. Third  while much of the language most often expressed by users in the majority sample is support language expressing individual distress (such as “i’m” or “feel”)  we find that people from the minority sample are more likely to talk about themselves in relation to other people  as illustrated by the bolded n-grams in Table 3. For example  individuals from the minority sample use the term “us” at a higher amount than those in the majority sample. Further  individuals from India often talk about wanting or needing friends  whereas individuals from Malaysia and Filipino refer to loneliness (using the word “alone”) more often than people in the majority sample. This use of terms related to interpersonal connections to express mental distress is seen in India [73]  Malaysia [92]  and the Philippines [35] when individuals experiencing distress are asked to describe how they are feeling. Our finding extends this work in the context of online mental health support communities. Finally  references to religion (such as “god” or “pray find peace”) are more common in posts from Malaysia and the Philippines. This follows past research on the expression of mental health in both Malaysia [65] and the Philippines [55  84] showing that religion is often used as a foundation for how people from these cultural backgrounds express mental health concerns and for how people support one another. The higher presence of references to religion among users from Malaysia and the Philippines might also suggest that index posts that discuss religion but do not specifically discuss distress might actually be one culturally-sanctioned method of signposting a state of mental distress  as seen in past research in offline contexts [24  55]. 4.2.2 Differences in Clinical Language Use. Based on past research showing that individuals from minority countries often express mental distress in non-clinical terms [52  53  64]  we examine the use of clinical language on online mental health support communities. For this analysis of clinical language around mental distress from different countries  we use data from the top 25% of users based on number of posts on Talklife to analyze a greater number of posts. As Table 4 shows  the amount of clinical language consistently differs between the majority sample and minority samples. We find that clinical language is used less frequently in posts from the minority sample than in the majority sample. For Indians  Malaysians and Filipinos  14.2-17.5% of index posts have clinical language respectively  compared to 22.2% for the majority sample. We see a similar but smaller difference for support responses: 6.6-7.9% of support responses tend to have clinical language for the minority sample compared to 8.6% for the majority sample.These differences are significant with a p < .01 unless otherwise indicated.3 For all countries  though  we find consistently that support responses have less amounts of clinical language than index posts. However  if we look at the frequency of clinical language within posts  the difference between the minority sample and the majority sample is small. Combined with the evidence that the fraction of posts with at least one use of clinical language is substantially lower for the minority sample  this implies that there must be higher use of clinical language terms per post for each post from the minority sample that does contain clinical language. This implication is verified by the last two rows of Table 4: when we restrict our analysis to only those posts that do contain clinical language  posts from the minority sample use more clinical mental health language in comparison to the majority sample for both index posts and support responses  with a statistically significant difference at p = .01. Thus  while a smaller fraction of posts in the minority sample use clinical language overall  there is a big variation in its use. Posts that do use clinical language tend to use many more terms per post than those from majority countries  suggesting that people who do use clinical language from minority countries are participating in a standard and globalized language around describing clinical mental health. Table 5 confirms the above result when we look at the top unigrams  bigrams and trigrams of clinical language used by people from different countries. The percentages for each term or n-gram correspond to the fraction of occurrences of the n-gram relative to all clinical n-grams. We find that there is not much variation between the clinical language terms that are used across the different countries. The disorders that are talked about most explicitly (such as “borderline personality disorder ” “personality disorder ” or “social anxiety”) are used at relatively consistent rates between both the minority sample and the majority sample. As posited above  it is possible that this consistency in types of clinical language hints at some form of standardization in how those users who use clinical language use it  a globalized online clinical language around mental health and a potential difference between past work on expressions of distress in offline settings. That said  it should be noted that some of the most popular n-grams  especially unigrams  such as “sleep” and “night” can function as false positives  as not every mention of sleep or night on Talklife is a reference to a mental health issue (such as “sleep paralysis” or “night terrors”). These terms may also be used in different (and culturally bound) ways. More specific terms (bigrams such as ""anxiety disorder"" and trigrams such as ""borderline personality disorder"" are used rarely. Finally  we also report potential cross-cultural differences when individuals first begin to use clinical language on Talklife. On average  Indians tend to use clinical language on the 6th post (mean = 6.3  σ = 12.12)  whereas Malaysians  Filipinos  and the population from the majority sample tend to use clinical language on the 4th post (mean = 4.31  4.51  4.13  σ = 5.7  6.8  6.6). Interpretation of these results will require more work investigations the reasons why Indians use clinical language later in a thread. On balance  these results shows that individuals from the minority sample use lesser amounts of clinical language overall  but also show more variation in use than people from the majority sample  and add nuanced perspective to past work [52  53  64] showing that individuals from these minority countries are often less likely to use clinical language when conceptualizing and describing their experience of mental distress.",0,0,0,0,0,0,0,0,0,0,,0,0,0,0,2,0,,0
https://dl.acm.org/doi/pdf/10.1145/3359169,24,14,support behavior and cultural homophily,"Pendse, Niederhoffer, Sharma",3,0,0,0,2019,Beyond language expression  there are also differences in help-seeking versus support-giving behavior among people from different countries. Indian Talklife users tend to begin their usage of Talklife starting fewer threads than the majority population  with 45 percent of first posts from Indian users being index posts (σ = .497)  whereas 56 percent of Talklife users from the majority population tend to begin their time on Talklife with a new thread. In contrast with Indian users  Malaysian and Filipino users follow the trend of the majority population  with 68 percent of Malaysian users starting a thread as their first post (σ = .465)  and 60 percent of Filipino users starting a thread as their first post (σ = .490). Individuals from the minority sample are more likely to participate in threads with people from their own country  demonstrating patterns of homophily [59]. On average  a thread started by a user from a minority country has 42-59% of users from the same country as the original poster. To put these percentages in perspective  we compare them relative to the overall fraction of Indians  Malaysians or Filipinos users on Talklife. Without homophily  we would expect the fraction of Indians in a thread started by an Indian user to be roughly the same as the overall fraction (baseline) of Indians on the platform. We find  however  that the fraction of Indians on a thread started by an Indian is 26 (σ = 34) times more than the baseline fraction. Similarly  Malaysian users were 64.58 (σ = 110) times more likely  and Filipino users were 16.33 (σ = 32) times more likely than the baseline to participate in threads started by people from their own country. Given that Talklife is a forum in which users are as anonymous as they choose to be  and that the majority language used in the forum is English  this result is particularly interesting  as it shows that people tend to seek out others from their own background online when providing social support  even when this functionality is not explicitly enabled by the platform. This observation is strengthened when we look at the first respondents to threads started on Talklife. For threads initiated by users from minority countries  we find that the first respondents are also more likely to be users from the same country as the original poster among Indians  Malaysians  and Filipinos. Compared to a baseline of selecting posts at random  the effect is 9.94 (σ = 4.74) times more likely for Indians  3.36 (σ = 6.52) times for Malaysians  and times 12.43 (σ = 9.9) for Filipinos. However  it is possible that some of this cultural homophily might be the result of users from the same country being online at similar times due to a shared time zone. To factor in the impact of shared time zones on our observation of cultural homophily  we additionally compare the number of users from a minority country in a thread relative to the overall fraction of users from the same minority country active on Talklife during the hour that the index post was posted. Thus  the baseline now depends on the time at which an index post was posted: under no cultural homophily  we expect that the fraction of respondents from a minority country in a thread started during a particular hour will be equal to the overall fraction of users active at that hour from that country. We find that the fraction of Indians on a thread started by an Indian is 7.56 (σ = 16.8) times more than the baseline fraction  even when factoring in the effect of shared time zones. Similarly  with this consideration  Malaysian users were 406.89 (σ = 1650) times more likely  and Filipino users were 187.51 (σ = 842.97) times more likely than the baseline to participate in threads started by people from their own country. Note that our time-dependent analysis assumes that the bulk of responses to an index post happen within the same hour. When we only consider the first response to a thread started by a minority user  compared to baseline levels of Talklife usage  the effect is similar: Indian users are 3.03 (σ = 6.17) times more likely to be the first respondents for a thread started by an Indian  and the corresponding ratio is 160.82 (σ = 623.53) and 87.58 (σ = 483.65) times for Malaysians and Filipinos respectively. The substantially higher ratios for the Malaysian and Filipino subset are the result of fewer Malaysian and Filipino users in comparison to the Indian subset  who have a less consistent use of Talklife throughout the day in comparison to the Indian subset. Longer conversations among these minority users that happen at low activity hours pull up the overall homophily ratio. When separating out our analysis between high activity hours (for Malaysia and the Philippines) and low activity hours  we find that during high activity hours  the likelihood is 4.01 (σ = 6.18) and 1.35 (σ = 2.36) times more than the baseline respectively  and during low activity hours  the likelihood is 487.28 (σ = 842.97) and 223.91 (σ = 1796.90) times more than the baseline respectively. Overall  these results provide strong evidence that even when factoring in the potential impact of shared time zones  there is still substantial cultural homophily in where individuals from minority countries find support.,0,0,0,0,0,0,0,0,0,0,,0,2,0,0,0,0,,0
https://dl.acm.org/doi/pdf/10.1145/3359169,24,15,longitudinal analysis,"Pendse, Niederhoffer, Sharma",3,0,0,0,2019,Having analyzed cross-sectional differences across countries  we now turn to studying the evolution of these differences over time  in terms of cultural identity  clinical language and supportive behavior on Talklife.,0,0,0,0,0,0,2,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/pdf/10.1145/3359169,24,16,identity based expression of mental health,"Pendse, Niederhoffer, Sharma",3,0,0,0,2019,We found that while there were some small fluctuations in identity-based attributes  such as the amount in which an a user mentions their own country over time  we found that none of these fluctuations were different to any level of statistical significance.,0,0,0,0,0,0,2,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/pdf/10.1145/3359169,24,17,linguistic differences in mental health expression and support,"Pendse, Niederhoffer, Sharma",3,0,0,0,2019,Next  we study the changes in use of clinical language as a user participates more on Talklife. Based on past work [52  53] in medical anthropology showing the influence of environment on changing explanatory models of mental illness  we should expect an upward trend for minority countries in use of clinical language. For example  in research done on the the cultural dimensions of the Chinese experience of depression  Chinese individuals experiencing depression were found to express their symptoms using more Western idioms of distress after more exposure to psychiatry and clinical models of depression [52]. Past research has also shown that mental health forums are a ripe place for linguistic accommodation [86]. To investigate this question  one may look at the average use of clinical language as we go from post 1 to post k for users from each country. However  as we increase k  we may not be comparing posts from the same people  due to survivorship bias where the people that make up the earlier posts have dropped out and thus we compare a different set of people at different k. To counter such survivorship bias  we instead created a subsample of the top n% of users and looked at their first k posts  such that each of them had at least k posts. Figure 2b shows the use of clinical language over time for the top n=10% users and their first k=40 posts. Contrary to past work  we find that while clinical language does tend to slightly fluctuate over time  overall clinical language used by people from the minority sample does not fluctuate to any statistically significant level  or follow any particular changing trend over time. This result stays consistent irrespective of our choice for n  including for n=50%. To verify if these results could be due to lack of exposure to international users  we also constructed a subset of users from minority countries who contribute majority of their posts in international threads  or threads in which the majority of users are not from the same country as those from the minority country. We still find the same pattern across the three countries. Thus  our findings challenge theories from past work that suggest that individuals use more clinical language with exposure  at least for online support interactions.,0,0,0,0,0,0,0,0,0,0,,2,0,0,0,0,0,,0
https://dl.acm.org/doi/pdf/10.1145/3359169,24,18,support behavior and cultural homophily,"Pendse, Niederhoffer, Sharma",3,0,0,0,2019,When examining support practices  however  we do find differences over time. Talklife users tend to transition from starting threads to supporting other people on the website  regardless of national identity. As in the above analysis  we compute the ratio of index posts to support responses over time for the first k=40 posts by the top n=10 percent of users on the website  as shown in Figure 2a. We find that for Indians  while at first post  35 percent of posts (σ = 0.48) are index posts and 66 percent are support responses  by post 40  only 6 percent of posts (σ = 0.23) are index posts  whereas 94 percent of posts are support responses. This same pattern follows for Malaysians and Filipinos  with the percentage of index posts dropping from 44 (σ = 0.50  0.50) to 6 and 7 percent respectively (σ = 0.23  0.25) between the first and fortieth post for both populations. This same trend can be seen for the majority sample  with a trend from 43 percent questions to 8 percent index posts by the fortieth post. To see if this result could simply be explained by self-selection bias in that people who ask more questions are also more likely to drop out of the platform early  we also compare the index post-support response ratios of people with fewer total number of posts. For any m > 1  where m is the total number of posts by a user  we find that users have statistically significant reductions in the number of questions being asked  and trajectories that always tend towards higher eventual levels of support. This finding suggests that regardless of cultural background  as people engage more with the Talklife support community  they end up becoming peer supporters. While there may be initial cultural differences in whether individuals seek help or support more when they use the website  these cultural differences tend to be assimilated into the general community norms of helpseeking and supporting behavior on the website. Finally  for cultural homophily effects in thread participation  we find that they remain relatively stable over time; the changes over time were not significantly different.,0,0,0,0,0,0,2,0,0,0,,2,0,0,0,0,0,,0
https://dl.acm.org/doi/pdf/10.1145/3359169,24,19,cross platform validation,"Pendse, Niederhoffer, Sharma",3,0,0,0,2019,In our analysis of cultural differences in how users of Talklife use the forum  we find that users from the minority countries are more likely to mention their country when discussing their mental health  use lesser amounts of clinical language  and tend to support people that have a similar national background. We now check if these differences are generalizable to other global mental health platforms or are specific to Talklife. To see if results from Talklife hold across platforms  we do a focused analysis of cultural differences between users from one minority country (India) and one majority country (the United States) on 7Cups. We pick these two countries as they are the countries in the Eastern and Western hemispheres that have the highest user populations on Talklife and 7Cups  as we detail in Table 2. Since the dataset provided to us from 7Cups was much smaller than that for Talklife  other Eastern countries did not have enough coverage. Additionally  to adapt our analysis to the platform-specific usage norms of 7Cups  we only do a cross-sectional analysis. This is due to the fact that most 7Cups users only post once (as detailed in Table 2) and seek longer pieces of advice than the back-and-forth conversational support practices of Talklife. We analyze 7Cups data along the same dimensions of analysis as we do Talklife—identity-based differences  linguistic differences  and behavioral-based differences. We find that Indians are more likely to mention their country than Americans in their posts  that Indians do use lower amounts of clinical language specifically in their support posts  and that Indians tend to support other Indians on 7Cups  extending some of our findings on cultural differences from our analysis of Talklife.,0,0,0,0,0,0,2,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/pdf/10.1145/3359169,24,20,identity based expression of mental health,"Pendse, Niederhoffer, Sharma",3,0,0,0,2019,When analyzing how often people from India and the United States mention their country in their posts  we find that individuals from India mention their country more often than Americans when describing their mental distress on the forum  with Indians mentioning their country .3 percent of the time (σ = .06) and Americans mentioning their country .2 percent of the time (σ = .05). This is significant at p = .01  and extends our finding that national identity is tied to how people frame themselves on Talklife to 7Cups.,0,0,0,0,0,0,2,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/pdf/10.1145/3359169,24,21,linguistic differences in mental health expression and support,"Pendse, Niederhoffer, Sharma",3,0,0,0,2019,6.2.1 General Linguistic Differences. When examining the most common unigrams used by Indians and Americans on 7Cups  like on Talklife  we find that users in India are more likely to talk about their distress in relation to others  with “person”  “friends”  and “alone” all being top n-grams used. However  when examining bigrams and trigrams  we find that Indians use more clinical language (such as “panic attack” or “social anxiety”) than Americans  which is a different user behavior than what we found on Talklife. This may be the case due to the fact that 7Cups organizes categories around forum around specific diagnoses  and as a result  people who are more aware of their mental health disorder or diagnosis are more likely to seek out and use 7Cups. That is  we posit that a user specifically browsing through and posting to a forum on “social anxiety” or “depression” is more likely to be aware of these clinical terms and to use them in their posts. 6.2.2 Differences in Clinical Language Use. To further tease out this 7Cups-specific difference in clinical language use  we now look at clinical language specifically. Examining the top n-grams for clinical language used in Table 7  we find that the types of clinical language that is used is generally the same between Indians and Americans  similar to our findings on Talklife. Note  however  that 7Cups users use an order of magnitude higher percentage of trigrams that are specific to mental health disorders  compared to Talklife users. On prevalence of clinical language use on 7Cups  we find that Indians and Americans use clinical language in their questions at approximately the same amount of frequency  at 75.3% and 76.7% of questions asked containing clinical language respectively (Table 8). Unlike Talklife  there is no statistically significant difference between the level of clinical language used in questions. As 7Cups is a forum organized around types of disorders and diagnoses  this is not unexpected—it is more of a norm on 7Cups for people to ask longer and more specific questions about their mental health than it is on Talklife. Still  our finding of individuals from minority countries using less clinical language does hold for answers  as only 25.7% of answers from users from India have clinical language  whereas 31.0% of answers from users from the United States have clinical language. In addition  when we look at the fraction of clinical language terms in a post  we find that Indians use higher levels of clinical language in questions  but lower levels of clinical language in answers  both when considering all posts and when considering only posts with clinical language. All comparisons made above are statistical significant are significant at a level of p < .0001. Overall  thus  we find that 7Cups exhibits fewer differences in clinical language use between users from majority and minority countries  suggesting the possibility that forum design is associated with the expression of mental health by people from different countries. However  we cannot say whether forum design causes a change in expression  since the findings could easily be explained by self-selection of users into different platforms: one can argue that users from minority countries who are not familiar with clinical terms are less likely to find a specific subforum on 7cups and consequently less likely to contribute. At the same time  on both Talklife and 7Cups  we do find a lower use of clinical language for answers from minority countries. We also find that when we consider only the posts that do contains some clinical language  there are no significant differences in either the frequency of use or actual n-grams used by people from minority and majority countries,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/pdf/10.1145/3359169,24,22,support practices and cultural homophily,"Pendse, Niederhoffer, Sharma",3,0,0,0,2019,When analyzing whether there is cultural homophily in who Indians choose to support on 7Cups  we take a similar approach as our analysis on Talklife  scaling the average percentage of incidence of people from the country of origin of an OP on a thread. However  as the 7Cups dataset is one that upsamples Indian users  the value of homophily we get is an lower-bound on how likely it is for an Indian supporter to respond to a thread started by an Indian. With these constraints  we still find that there is an effect of homophily  as Indian users are at least 3 times more likely (µ = 3.4  σ = 3.9) to respond on a thread started by an Indian on 7Cups. This extends our finding from our analysis of Talklife that people want to support people from their national background to 7Cups.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/pdf/10.1145/3359169,24,23,impact on forum success,"Pendse, Niederhoffer, Sharma",3,0,0,0,2019,Through our analysis on two different online platforms  we find that there are significant differences in how people from minority countries talk about their mental health on online support platforms. We now show that these cultural differences make a difference with regards to whether an individual from a minority community successfully feels better through use of online mental health support platforms.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/pdf/10.1145/3359169,24,24,characterizing success moment of change,"Pendse, Niederhoffer, Sharma",3,0,0,0,2019,"Though rich with data  online mental health forums are characterized by an absence of the kind of post-conversation survey that signifies whether a user has had a positive experience that has been analyzed in past work  such as in Althoff et al. [9]. to determine whether a conversation has successfully helped an individual on an online mental health forum  we follow Pruksachatkun et al. [75] in defining a successful thread as a thread with a “moment of change."" A thread with a moment of change is a thread in which the OP has a positive change in sentiment towards a topic that they were previously feeling some distress towards. To construct a dataset of these threads  following Pruksachatkun et al.  first we identify threads in which the original poster (OP) initially described being in pain or distress through using words identified as trigger words by Talklife (such as “cut"" or “kill"")  and then later expressed that they were feeling better or that the other comments on the thread changed their mind about that topic of distress. Second  we look for a moment of change in these threads through the use of regular expression search for common phrases that signify a reduction in distress  as done in [75] (e.g. “thanks  I feel better now"" or “you are right""). The list includes an extensive set of 10 phrases (and subsequent variants) and is designed to minimize the number of false positives for detecting a moment of change. As noted by Pruksachatkun et al.  this method of deriving a ground truth has been validated via showing consistency between the method and manual annotation by crowdworkers on the relative sentiment between beginning and ending posts by a user. That said  these phrases do not account for the many different ways in which users might express a moment of change. Overall  regular expressions provide an approximate way of detecting specific kinds of moments of change that can be applied to large forum datasets. Using this filtering procedure  we find 295 threads started by Indian users that have a moment of change. The rate at which moments of change occur is approximately the same across the Malaysian  Indian  and Filipino subgroups. However  since Indian users are the largest non-Western minority community on Talklife  Filipino and Malaysian users have less than a third of such threads—less than 100 threads each with moments of change. We thus restrict our analysis to threads started by Indians with moments of change and compare it to 25 537 threads started by Indians that do not have moments of change from Talklife. For comparison  we find 6396 threads started by users who are not Indian with moments of change  and 14 604 threads started by users who are not Indian without moments of change. Through use of these two datasets  we look to see whether there are differences between Indian users who have a moment of change and Indian users who do not have a moment of change based on the cultural differences we identified and validated in the last two sections. Given the likelihood of cultural differences in how people express that they are feeling better  there are some limitations to this approach. As Pruksachatkun et al. [75] discuss  the relevance of a phrase  “I feel better now” as a marker of a moment of change may vary across cultures and thus is a very approximate metric for identifying whether a thread might have a moment of change. More analysis is necessary to have a deeper understanding of these cultural differences in expression of a moment of change  which we leave for future work.",0,0,0,0,0,0,0,0,0,0,,2,0,0,0,0,0,,0
https://dl.acm.org/doi/pdf/10.1145/3359169,24,25,impact of cultural differences on forum success,"Pendse, Niederhoffer, Sharma",3,0,0,0,2019,Below  we show how cross-cultural differences are associated with an individual being effectively helped on an online mental health support forum. In particular  we find that people who use lower clinical language and who are helped by people from a similar cultural background receive more effective support. 7.2.1 Identity. When looking at whether there is a difference in how often country of origin is mentioned between threads started by Indians with moments of change and threads started by Indians without moments of change  we find no statistically significant differences. Threads with moments of change are made up of an average of .1% (σ = .007) mentions of India  whereas threads without moments of change are made up of an average of .1% (σ = .006)  showing no real difference. 7.2.2 Language. Interestingly  we find that there is less clinical language from in threads by Indian users that have a moment of change. We find that threads by Indian users with a moment of change have an average of .6% (σ = .007) clinical mental health language  whereas threads by Indian users without a moment of change have an average of .9% (σ = .02) clinical mental health language. When only looking at posts by the OP  these averages are more stark  with .5% (σ = .009) for threads from Indians with moments of change and 1% (σ = .04) for threads from Indians without moments of change respectively. One way to interpret this result is that threads on less severe mental health issues are more likely to be have a moment of change  and those on severe mental health issues are less likely so. To verify  we look into the specific n-grams that were found in both types of threads: whether a lower overall clinical language also corresponds to less severe symptoms or disorders. First  it is important to note that disorders with severe symptoms more often have a formal diagnosis in India [53]  and it is also the case that undiagnosed mental distress that is less severe is often couched in language that is less clinical (such as descriptions that use words like “stress” or “tension”) [25]. To confirm that the relation we find between lower clinical language and a greater rate of success on threads was not due simply to less severe issues being discussed in threads that have moments of change  we did a search on the clinical n-grams found in both types of threads to see if there was any significant difference in the types of symptoms or diagnoses being described (such as “bipolar” or “schizophrenia” as opposed to “depression” and “anxiety”). Though this analysis would likely be more accurate if done with human coders  we use this n-gram based approach as an approximate measure to check for any substantial differences in severity of issues discussed. As seen in Table 9 for unigrams  for example  we find no significant differences in the specific diagnoses being discussed. This suggests that lower clinical language in threads with a moment of change is not simply due to difference in severity of mental health issues  but rather a complex linguistic phenomenon that merits further investigation. However  in light of this result  we find that a lower amount of clinical language in threads without moments of change follows general patterns on Talklife for users who aren’t Indian. We find that threads by users who aren’t Indian with moments of change have an average of .9% (σ = .012) clinical language  whereas threads without moments of change have an average of 1% clinical language (σ = .017). In comparing the two results  we find that Indians have a greater difference in clinical language than users who aren’t Indian  when comparing those who do end up successfully finding help on a thread versus those who do not successfully find help. All comparisons above are with statistical significance of p = .004 or below. This difference between Indian and non-Indian users demonstrates that the language and framing of mental distress that may lead to a moment of change may not be the same across cultures  and is something we address further in Section 8. 7.2.3 Homophily. Looking at threads by Indians with moments of change and threads by Indians without  we find a strong association between the amount of cultural homophily and ending success of a thread. The fraction of Indians on a thread started by an Indian in which there is a moment of change is 10.5 (σ = 5.21) times more than baseline  whereas a thread without a moment of change is only 6.9 (σ = 5.87) times more than baseline  with a statistical significant difference at the level of p = 5.24x10−25. This finding indicates that individuals in distress find greater success when being supported by people from their cultural background on online mental health support platforms  suggesting a substantial effect of culture on eventual success in providing effective support.,0,0,0,0,0,0,2,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/pdf/10.1145/3359169,24,26,discussion,"Pendse, Niederhoffer, Sharma",3,0,0,0,2019,Through our analysis of Talklife and 7Cups  we provide one of the first studies of cultural differences on online mental health forums. We then validate that these differences exist across forums  and indicate the implications of these differences on how and whether individuals from minority communities find support. In this section  we discuss some of the broader implications of these differences and potential reasons for they were observed  as well as make design recommendations for the impact these differences have for designing more inclusive mental health spaces and algorithms.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/pdf/10.1145/3359169,24,27,implications of cultural differences,"Pendse, Niederhoffer, Sharma",3,0,0,0,2019,8.1.1 The Importance of Cultural Differences Online. In this work  we find that there are specific cultural differences in how people from different minority communities use and express their mental health in online mental health support communities. More specifically  we find that individuals from underrepresented countries are more likely than the majority to mention their country of origin  are more likely than the majority to use lower amounts of clinical language  and prefer to be supported and be supported by individuals from the same national background. We also find that these cultural differences have significant impact on whether individuals are able to have interactions in which they have some positive improvement in their well-being  particularly with regards to being supported by people from the same community  and a lower level of clinical language being used. We also find that over time  while some of these factors do not change (such as use of clinical language or expression of national identity)  support behaviors do change  with the vast majority of users transitioning from expressing distress in index posts to primarily supporting others in subsequent posts. Past work shows how stressful it can be to navigate a platform that is unwelcoming or unfamiliar to an individual [37  48]. In showing that cultural differences exist in how minority users use online mental health support communities  as well as the importance of creating spaces in which participants have a shared cultural identity  we show that it is crucial to be considerate of minority groups when designing online platforms  particularly online mental health support communities. This is particularly the case on online mental health support communities  where individuals volunteer moments of vulnerability and distress in the hopes of gaining helpful support from the community  and exclusion or bullying could have serious consequences [44]. As a result  we urge community moderators and other stakeholders of online mental health communities to be more considerate of the cultural background of members when making design and organizational decisions for each platform. We detail specific methods for what this consideration could look like in Section 8.2. 8.1.2 Potential Reasons for Cultural Differences. As significant work from medical anthropology has shown  culture is a foundation for how people conceptualize their mental health  and how they express and frame their mental distress when describing it to others. In this work  we show that cultural differences broadly extend to online contexts  such as mental health support forums. Even when these forums are semi-anonymous  the language that people use  who they choose to support  and how they frame themselves when using the forum are all influenced by culture. The cause for the cultural differences we observe is not clear—based on past work  we speculate that there may be several different potential causes for these differences. One theory for why people from minority countries may use less clinical language might be the influence of stigma—namely  that people use less clinical language due to the culturally-bound implications of thinking of themselves as ill  as detailed by Raguram et al [76] in the case of psychiatric patients in South India. Though the online mental health forums we analyzed were pseudo-anonymous  the impact of stigma still may be present in terms of how people conceptualize their distress and choose to portray it online  even in an anonymous and less stigmatized environment. As seen in the higher incidence of clinical language used on 7Cups  it may also be the case that the type of forum and the way that it is framed influences how people express distress on the website. Websites organized around diagnoses may prove a destigmatized environment for people to be more specific and clinical about what they are experiencing. Additionally  it may also be the case that people from places with lower awareness of mental health and mental illness may learn about these terms via participation on these forums  although we did not find evidence for it in our longitudinal analysis on Talklife in Section 5. To clearly isolate what might cause these cultural differences necessitates more qualitative work to complement our findings. It is also true that the cultural differences we find do not exist in vacuums [27]  and may intersect with one another. For example  we find that both reduced clinical mental health language and cultural homophily relate to whether a minority user successfully finds help on Talklife. However  this higher incidence of lower clinical language may be a result of threads with moments of change having higher levels of people from the same cultural background as the original poster  particularly considering that Indians have lower incidences of clinical mental health language when seeking help or supporting others overall.,0,0,0,0,0,0,2,0,0,0,,0,2,0,0,0,0,,0
https://dl.acm.org/doi/pdf/10.1145/3359169,24,28,design recommendations,"Pendse, Niederhoffer, Sharma",3,0,0,0,2019,Just as it does when people support each other in real life  our work shows that culture matters when people support each other online  and that cultural differences are particularly important when making sure that people in minority groups feel included in an online mental health space. Feeling isolated and alone is a common symptom of mental illness [18  28  39]  and design choices that cause minority groups to feel otherized could only serve to exacerbate symptoms. It is not enough to simply design for the majority  and this is particularly the case for spaces meant to help improve the mental health of a vulnerable population  as minorities are particularly vulnerable for mental health issues [19]. With this in mind  we make recommendations for the future design of more inclusive online mental health spaces  and algorithms that make use of mental health data. 8.2.1 Peer Matching and Community Building. In this work  we find that people prefer to support and seek help from people with their cultural background  and that support from people who have the same background is particularly helpful in successfully helping people feel better. When designing inclusive mental health spaces  one potential design consideration is deciding how users of the website are recommended (or matched) to threads. It may be beneficial for users in distress to have more users recommended to their thread from their cultural background or who use the same kinds of clinical language to describe how they are feeling in other posts. Matching based on cultural or linguistic-based attributes would lead well from work that has been done in offline contexts  such as racial [95] or gender [13  43] based preferences when looking for effective counseling. Intelligent routing systems that take into account identity-based factors could lead to minority users of online mental health spaces feeling more included. 8.2.2 Building Predictive Models. Our work also shows the necessity of considering culture when building predictive models. Predictive models often use probabilities derived from majority data to make inferences about the future. However  in this work  we show that when analyzing along cultural dimensions  there are significant differences with regards to how people in minority communities express their mental health. It may be the case that an algorithm may make accurate predictions for the majority of a population  but consistently fail for some small minority due to a minority specific confounding factor  such as culture. For example  in our work  we find that individuals from minority communities use lower levels of clinical language when expressing that they feel better or have had a change of perspective. However  if an algorithm meant to predict when someone felt better used strictly medical language as a feature  this algorithm could potentially do poorly on minority users given that it does not account for this cultural diversity in expression of mental health. Given the importance to culture when expressing mental health  and the global need to help those most isolated and vulnerable to mental illness  we make the recommendation that it is important to consider culture when both designing any kind of machine learning algorithm on mental health data  and understanding kinds of user subgroups those algorithms do not work on.,0,0,0,0,0,0,2,0,0,0,,0,0,0,0,0,0,,0
https://dl.acm.org/doi/pdf/10.1145/3359169,24,29,limitations and future work,"Pendse, Niederhoffer, Sharma",3,0,0,0,2019,"There are some limitations to our work. One strong limitation is that we follow past work in assuming geographical location as a proxy for identity. However  all three of the countries we chose [77  80  84  92] have rich subcultures and communities norms around mental health that are not reflected by a national identity. Future work could make deeper analyses into these subcultures  and investigate the interaction between subcommunity norms and cultural norms. Additionally  while users from the minority sample use clinical language at a lower rate and use similar words  it may be the case that the way that people use these words are different. For example  as Lang [53] notes  the word “depression"" is used in India to describe a wide variety of mental disorders. A deeper qualitative investigation of how clinical language is used would extend our findings. Similarly  when looking at changes in clinical language over time  we find that clinical language stays constant over time. The question arises of whether this may be the result of an insufficient amount of time between posts  as some amount of reflection might be necessary to see some form of detectable change in language or explanatory model of illness. While this may be the case  past research in medical anthropology demonstrates that explanatory models of illness and fluid and continuous [66]  in which an individual’s mental model changes immediately with new information and not necessarily with specific time for reflection. Rather  it is likely that this consistency in the level of use of clinical language is the result of the cultural homophily we find and a subsequent lack of exposure to substantially different expressions of mental health. Additionally  when deciding whether a conversation was successful at helping an individual in distress  we used the “moment of change"" metric  looking to see whether an original poster said they were feeling better or experienced a change in perspective after having expressed distress. Given the consistency in culture-based patterns of usage between Indian  Malaysian  and Filipino users  we suggest that our findings on this Indian subgroup might hold for other minority subgroups. However  as we see in this work  there is a significant culturally-bound impact on how people express distress  and thus may be a culturally-bound impact on how people express when they are feeling better that a simple regular expression would not be able to detect. A more diverse and localized variety of terms  derived from qualitative fieldwork with a minority group  could result in a richer analysis  and a greater understanding of the cultural differences between different minority subgroups. Our findings are also limited to users of online mental health forums — these findings cannot be extended to general cultural differences when discussing mental health anonymously  such as on non-mental health focused forums or on social media  which may have different community-based or cultural norms on use.",0,0,0,0,0,0,2,0,0,0,,2,2,0,0,0,0,,0
https://dl.acm.org/doi/pdf/10.1145/3359169,24,30,conclusion,"Pendse, Niederhoffer, Sharma",3,0,0,0,2019,In this work  we show that there are cultural differences with regards to how people use online mental health support forums  that these differences extend to two different online mental health support forums  and that these differences matter with regards to how people from minority countries find support that successfully makes them feel better. Even with the norm of anonymity on these forums  we find that people still incorporate aspects of their cultural identity into how they use the forum  including mentioning their country of origin  using lower rates of clinical language  and supporting people from their country of origin. We also find that these factors  particularly the cultural homophily of a thread  are strongly related to whether the conversation will make the original poster feel better by the end of the conversation. Overall  these differences show the importance of considering culture  with a specific focus on cultural differences among users from minority groups  when designing inclusive online mental health spaces.,0,0,0,0,0,0,2,0,0,0,,0,2,0,0,0,0,,0
https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-018-2197-z,25,1,abstract,"Grant, Kucher, Leon, Gemmell, Raicu, Fodeh",6,0,0,0,2018,Suicide is an alarming public health problem accounting for a considerable number of deaths each year worldwide. Many more individuals contemplate suicide. Understanding the attributes  characteristics  and exposures correlated with suicide remains an urgent and significant problem. As social networking sites have become more common  users have adopted these sites to talk about intensely personal topics  among them their thoughts about suicide. Such data has previously been evaluated by analyzing the language features of social media posts and using factors derived by domain experts to identify at-risk users. In this work  we automatically extract informal latent recurring topics of suicidal ideation found in social media posts. Our evaluation demonstrates that we are able to automatically reproduce many of the expertly determined risk factors for suicide. Moreover  we identify many informal latent topics related to suicide ideation such as concerns over health  work  self-image  and financial issues. These informal topics topics can be more specific or more general. Some of our topics express meaningful ideas not contained in the risk factors and some risk factors do not have complimentary latent topics. In short  our analysis of the latent topics extracted from social media containing suicidal ideations suggests that users of these systems express ideas that are complementary to the topics defined by experts but differ in their scope  focus  and precision of language.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-018-2197-z,25,2,background,"Grant, Kucher, Leon, Gemmell, Raicu, Fodeh",6,0,0,0,2018,Suicide  the act of causing one’s own death  is the tenth leading cause of mortality in the United States and is estimated to cost 44.6 billion dollars per year. This understates the severity of the problem  as for every attempted suicide  there are nearly 10 times as many people who contemplate suicide [1]. Suicidal ideation includes a wide range of thoughts from momentary consideration to extensive planning or incomplete attempts. The scope and impact of this mental health issue make understanding it a public health priority. When discussing their ideations  many individuals often reference common symptoms: feeling helpless  feeling alone  excessive fatigue  low self-esteem  the feeling that one’s mind is racing  or excessive focus on dormant goals [2]. Understanding the common themes in suicidal ideation can help us understand the patterns behind suicidal thoughts  ultimately leading to treatment and prevention. Clinical research toward understanding suicide has identified several risk factors. Mental disorders such as depression  schizophrenia  alcoholism  and substance abuse all play a contributing role. Additionally  the emotional stress caused by bullying  interpersonal relationships  and finances are also important factors [3]. However  these descriptions of suicidal ideation often capture a clinical viewpoint. With the rise in sophistication and acceptance of online social networks  individuals contemplating suicide have increasingly expressed their suicidal ideation in online forums  tweets  and other online media. The result is a vast collaborative description of the thoughts and motivations associated with suicide. In this paper  we leverage advanced topic modeling techniques to extract informal latent topics from this data. Topic modeling is a machine learning approach for eliciting abstract topics from a collection of documents. This approach can be leveraged to discover common themes present in online posts such as depression  drug use  or violence. The idea of “depression” might be captured by a collection of related words such as “pain”  “feelings”  “fear”  “stress”  and “suffering”. In this paper  we perform topic modeling on over 130 000 submissions to r/SuicideWatch  an online forum described as a place of support for those suffering suicidal thoughts. We begin by learning semantic embeddings for words in the posts via a shallow  two-layer neural network. Then we cluster the words into topics producing informally generated latent topics. Finally  we evaluate these informal topics by comparing them to suicidal risk factors and common themes identified by mental health professionals [3]. Our experimental results reveal that we are able to automatically generate quality embeddings for words and corresponding topic models. Many of these topic models correspond to risk factors that domain experts have previously proposed. In some cases  our topic models were more specific  focusing on a narrow interpretation of the risk factor. In other cases  our topics were more broad  encompassing multiple risk factors at once. This suggests that the topics extracted from social media posts created by those experiencing suicidal ideation may have a different focus and specificity than those generated by mental health professionals.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-018-2197-z,25,3,related work,"Grant, Kucher, Leon, Gemmell, Raicu, Fodeh",6,0,0,0,2018,Researchers have previously attempted to use the massive amount of data generated through social media to characterize the mental health of users [4  5]  leading to the development of computational tools [6]. Attempts have been made to predict depression  identify suicidal Twitter posts  and analyze the effect of suicide in the media on suicidal ideation in social platforms [7–9]. Risk factors of suicide [10  11] identified by domain experts are often leveraged in such studies. A common tool used to analyze social media posts is the Linguistic Inquiry and Word Count (LIWC) [6]. Progress has been made using this tool to analyze text related to suicide and depression  often in social media posts [4  7  8  12–14]. An early study used the LIWC on Twitter to analyze the impact of depression on social media activity [7]. Twitter data has been used to analyze suicidal ideation [5  9  15]. In one study  tweets were filtered by using specific search terms which were associated with 12 suicide risk factors [5]. The twelve risk factors include bullying  depressive feelings  depression symptoms  drug abuse  family violence/discord  gun ownership  impulsively  prior suicide attempts  psychological disorders  self-harm  suicide around the individual  and suicide ideation [10  11]. We also evaluate the twelve risk factors identified in these studies. These researchers found that the volume of suicide-related tweets correlated to suicide rates by U.S. state  showing that Twitter data could be indicative of a population’s mental health. One study used human “coders” to label tweets according to their level of concern with respect to suicide. Language models were then used to predict the appropriate concern for new tweets [9]. Another study analyzed the content of Twitter users prior to their public declaration of a suicide attempt and found that there may be indications of suicidal ideation based on posts leading up to a suicide attempt [15]. There have also been studies which focused on the social media platform Reddit  specifically the subreddit called r/SuicideWatch. One study analyzed changes in suicide content in the wake of celebrity suicides by measuring post volume and modeling topics in the text [8]. Another study observed the propensity of users discussing mental health issues to transition into discussing suicidal [13]. The language that people use in Reddit has been shown to differ between subreddits focused on different mental health concerns [14]. In this work  we leverage computationally generated language models to explore suicidal ideation. Examples of language models include simple bag-of-word models [16] and extend to more robust models such as probabilistic latent semantic analysis [17]  latent dirichlet allocation [18]  and Word2Vec [19  20]. Such language models have been used to explore numerous topics such as comparing topics in data [21]  recommendation systems [22]  and different languages [23]. We focus on the Word2Vec language model developed by Mikolov et al. [19  20]. Our work extends upon these previous efforts in the following ways. Rather than using pre-defined risk factors or labeled data to identify at-risk users  we automatically discover topics from the users’ posts by leveraging Word2Vec language models. We compare the latent topics identified in posts to risk factors proposed by domain experts. In this section  we provide a detailed description of our procedure including how we represent words with Word2Vec models and then use k-means clustering to produce topics in text data. See Fig. 1 We represent words as a vector of real numbers [24]. More formally  each word We represent words as a vector of real numbers [24]. More formally  each word where ϕ(i1) through ϕ(i n ) represent the weight of the ith word in the vector space. ​​We can think of these word representations as populating a high dimensional space where the relative locations contain semantic information. For example  in previous work  the relationship of a country to its capital city has been represented by their relative position in the vector space [19  20]. There are several methods for learning these weights; we leverage Word2Vec.,0,0,0,0,0,0,0,0,1,0,,0,0,0,0,0,0,,0
https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-018-2197-z,25,4,methods,"Grant, Kucher, Leon, Gemmell, Raicu, Fodeh",6,0,0,0,2018,"In this section, we provide a detailed description of our procedure including how we represent words with Word2Vec models and then use k-means clustering to produce topics in text data. See Fig. 1. We represent words as a vector of real numbers [24]. More formally, each word 𝑤⃗  is represented as: 𝑤⃗ =⟨𝜙(𝑖1),𝜙(𝑖2)...𝜙(𝑖𝑛)⟩ where ϕ(i1) through ϕ(i n ) represent the weight of the ith word in the vector space. We can think of these word representations as populating a high dimensional space where the relative locations contain semantic information. For example, in previous work, the relationship of a country to its capital city has been represented by their relative position in the vector space [19, 20]. There are several methods for learning these weights; we leverage Word2Vec. Many topic modeling algorithms exist  including latent semantic indexing  latent Dirichlet allocation  and non-negative matrix factorization. In this work  we turn our attention to Word2Vec  which has been argued to have many advantages over these earlier algorithms [19  20]. Word2Vec describes two implementations of a shallow neural network  the continuous bag of words (CBOW) model and the skip-gram model. We focus on the skip-gram model in this work  which learns vector representations of words by predicting neighboring words in a text. See Fig. 2. Common words such as “the” add little meaning to the model and add computational time. Instead of using these words  the model often skips over them and goes to the next word when training. Word2Vec does this by using sub-sampling  a probabilistic approach with the most common words having the greatest chance of being ignored  and the least common words having the least chance of being ignored. In contrast to many other neural network models  the skip-gram model includes only a single hidden layer  dramatically reducing both training time and complexity [19  20]. Learning the word representation is achieved by performing back-propagation on our training examples. Instead of updating each of the many neurons used in the neural network  negative sampling [20] updates a small  specified amount of neurons. Since one of the most computationally expensive parts of training a neural network is the act of updating the weights  this technique greatly reduces the training time. Finally  the softmax function normalizes the output of the neural network  so that sum of all outputs is equal to 1. Word2Vec capitalizes on the fact that similar words should have similar probabilities of appearing in the same context. Therefore the vector representations of similar words are “close” in vector space  often capturing rich semantic characteristics. It has been previously shown that Word2Vec performs accurately on tasks involving word similarity  analogy discovery  and text completion [20].",0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,1,,0
https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-018-2197-z,25,5,clustering,"Grant, Kucher, Leon, Gemmell, Raicu, Fodeh",6,0,0,0,2018,The word representations are useful in their own right  often containing rich semantic information. However  using these representations as input into other algorithms  such as clustering  can produce meaningful collections of related words. Clustering is a technique wherein items are grouped together based on their similarity. Items in a cluster are “near” one another and “distant” from items in other clusters. In this work  we rely on Euclidean distance because we are interested in the relative positions of the representations in the vector space. We leverage the k-means clustering algorithm [25] to produce clusters of words. We choose k-means clustering due to its simplicity and ability to create localized  spherical clusters. K-means begins with cluster centers at k random locations in the vector space. The algorithm assigns every item  in this case words to the nearest cluster. For each cluster  the mean of all items is calculated  and the cluster center is moved to that point. The process is repeated until there are no new assignments. Clusters of words can be viewed as topics. The meaning of a topic is captured by the words in the cluster. For example  a topic containing the words “join”  “sports”  “team”  “joined”  “practice”  and “won” describes the topic of playing team sports. Thus  we can identify latent topics in a corpus of text by analyzing the clusters of words generated.,0,0,0,0,2,0,0,0,0,0,,0,0,0,0,0,0,,0
https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-018-2197-z,25,6,suicidewatch,"Grant, Kucher, Leon, Gemmell, Raicu, Fodeh",6,0,0,0,2018,In this section  we first present the data gathered and used in our analysis. Researchers interested in the code and the data are invited to contact the authors. Reddit is a website which enables users to aggregate  rate and discuss news  entertainment  politics and many other topics. According to Alexa  it is the 8th most popular website in the world. It was estimated by the Pew research center that 6% of online adults use Reddit [26]. The site is organized into a collection of “subreddits”  each focused on a particular topic and administered by a collection of moderators. The subreddit  r/SuicideWatch  is a forum in which online users are encouraged to post their thoughts regarding suicide. At the time of our data collection  it had over 58 000 subscribers. Sometimes users express a preoccupation with the thought of suicide. Other times users discuss immediate plans to take their own life. These posts often contain a description of their mental state including depression  reaction to stress  their feelings of being alone and having a low self-esteem. While most online sources of data are notoriously noisy  this particular subreddit is remarkably clean. Given the serious nature of the subreddit  individuals are less likely to post harassing comments or off-topic remarks. When users post such comments  the moderators of the subreddit quickly remove them. We collected all posts from its inception in 2008 to 2016. Each post is often commented on by other individuals. In this work  we focused on the original post as it most often represents the suicidal ideation of a user and comments often represent emotional support from other users. We cleaned this data. First  we removed empty posts in which the content had been deleted. Second  we removed links  and replaced them with the word “link”. Third  we concatenated the text of the post to the title  as many users begin their post in the title and continue in the body of the post. Finally  we removed punctuation and other special characters. After cleaning this data  we had 131 728 posts with 27 978 246 words  of which 84 607 words were unique  posted by 63 252 unique users.,2,2,2,2,0,2,0,0,0,0,,0,0,2,0,0,0,,0
https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-018-2197-z,25,7,results,"Grant, Kucher, Leon, Gemmell, Raicu, Fodeh",6,0,0,0,2018,In this section  we evaluate the models built upon the r/SuicideWatch data. We begin by exploring individual words to subjectively assess whether or not the word representations are effectively capturing semantic information. We analyze the clusters to assess their ability to express latent topics in the data. We then evaluate the clusters by comparing them to the risk factors previously defined by domain experts.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-018-2197-z,25,8,experimental parameters,"Grant, Kucher, Leon, Gemmell, Raicu, Fodeh",6,0,0,0,2018,Once we obtained the data we began by creating vector representations using the Word2Vec from the gensim module for python [27]. Each post was processed using the window size of 5  common in the literature  which looks at the previous and next 5 words  along with the current word  looking at a total of 11 words at once. Negative sampling was set to 20 from the default of 5 on the recommendation of the authors of the Word2Vec model  based on the size of our data [19  20]. After extensive evaluation  we chose to represent words with a vector of 300 features using the skip-gram model and hierarchical softmax. This value seemed to provide rich semantic descriptions while minimizing computational overhead. In order to preserve the meaning of phrases  we turned common phrases into single tokens  called n-grams. This allows phrases such as “new york” to be separate from “new” and “york” alone  which have very different meanings. This resulted in an increase in the size of vocabulary to 97 368 unique words and phrases. To avoid noise in the data  we set the minimum count for a word to be included as 10 occurrences. This removed noise in terms of misspelled words and unrecognized characters among other things. After filtering our vocabulary  we preserved 99.41% of all of the words in our data  which decreased our vocabulary to its final size of 28 663 unique words. Next  we clustered the vector representations of words by using k-means clustering implemented by scikit-learn [28]. An important input to the algorithm is the selection of k. After extensive evaluation  we chose a value of 100 because it offered a sufficient number of clusters to capture the topics of the posts without being too large to manually evaluate. Regarding an error as the distance of each vector to its cluster center  we calculated the sum of the squared errors (SSE) for clusterings of size 5 through 400. The knee of the SSE curve was approximately 100 clusters. To evaluate the clusters  we took the ten most common words from each cluster and attempted to assign the clusters to one of the twelve risk factors previously identified by experts in suicide ideation.,0,0,0,0,0,0,0,0,0,0,,0,0,2,0,2,0,,0
https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-018-2197-z,25,9,analysis of word representations,"Grant, Kucher, Leon, Gemmell, Raicu, Fodeh",6,0,0,0,2018,To visually inspect the effectiveness of our word representations  we first subjectively evaluated the representations of “heartbreak”  “pills”  and “knife”. Table 1 contains these query words along with the most similar words from the corpus. For example  the most similar token to “knife” is “kitchen knife”. In all three cases  the related words share meaningful semantic information. In the case of “knife”  the related words are synonyms. In the case of “pills”  the related words are specific types of pills such as “painkillers” or “tylenol”. In the case of “heartbreak”  the word representations appear to capture this emotional concept. Now  after looking at semantic similarity  we attempted to see if our word vectors could be used for analogical reasoning in the same way they were used in [20]. Since words are represented as vectors  it is possible to add and subtract them from each other. We first consider the vector resulting from “[father] - [man] + [woman]”. We found that the vector representation most similar to the vector created by the preceding arithmetic is the vector representation of “mother”. In addition to capturing general semantic meanings  our model also captured semantic information relevant to suicidal ideation. For example  when considering vector representations  we found that “[abusive] - [physical] + [words]” is most similar to “emotionally abusive”  and “[suicide] + [self]” is most similar to “killing myself”. This indicated to us that the word embeddings have captured semantic information relevant to the topic of suicidal ideation. ​​Finally  we observed that our model captured subtle distinctions between some similar words. This is demonstrated by the relation “[family] - [love] + [obligation]” being most similar to the word “relatives”. This example shows that even though “family” and “relatives” have many similar semantic components  our model is able to capture subtle distinctions in their meaning. As the previous examples matched our intuitions  we believed that our model has effectively extracted significant semantic information from the corpus and is suitable for clustering to extract latent topics.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,2,0,,0
https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-018-2197-z,25,10,analysis of informal topics,"Grant, Kucher, Leon, Gemmell, Raicu, Fodeh",6,0,0,0,2018,To evaluate the clusters  we visually inspected the most common words in 100 clusters to see if they are related. For example one cluster contains the following terms: “since”  “past”  “suicidal”  “havent”  “times”  “attempt’  “attempted suicide”  “suicide attempt”  “almost killed”  and “failed attempt”. The words in this cluster discuss suicide attempts. Note that there are n-grams appearing in our clusters  indicating that the words constituting the phrases “attempted suicide”  “suicide attempt”  and “almost killed” were often used together in their respective phrases. These words when clustered together appear to capture the topic of past suicide attempts. In another example  a cluster contains “physically”  “emotionally”  “bullied”  “treated”  “mentally”  “raped”  “ignored”  “rejected”  “abused”  and “abandoned”. These terms are mostly verbs describing some sort of abuse  both mental and physical. We observe that users often use these words when talking about physical abuse. Finally  one cluster contains the terms “school”  “college”  “failed”  “class”  “university”  “grades”  “classes”  “failing”  “degree” and “major”. These terms are all used to describe education  especially higher education. While this cluster does not represent a risk factor for suicide  it does indicate that people often talk about college in the context of suicide  perhaps as a stressor that can lead to suicidal tendencies. Some clusters capture concrete topics such as those containing “drugs” or “guns”. Still  others capture emotional topics such as those containing “anxiety” or “sadness”. Some clusters appear immediately relevant to the study of suicide such as those containing “cut” or “pain”  while others represent cohesive clusters but do not clearly represent topics related to suicide such as those containing “clothes” or “week”. While it is not possible to present all clusters here  a curated selection can be found in Table 2.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,2,0,,0
https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-018-2197-z,25,11,comparison of risk factors,"Grant, Kucher, Leon, Gemmell, Raicu, Fodeh",6,0,0,0,2018,In our previous section  we showed how the clusters we found extract meaningful topics from the r/SuicideWatch data. In this section  we compare these informally extracted topics to risk factors proposed by domain experts. In this work  we draw from the risk factors used in Jashinsky et al.  where Twitter data was analyzed according to risk factors identified by the National Institute of Mental Health and by Lewinsohn et al. [5  10  11]. The twelve risk factors can be seen in the left-hand column of Table 2. While analyzing our clusters  we identified many topics that matched very closely with the proposed risk factors. For example  the notion of “Suicide Ideation” is captured by several clusters. For convenience  we have labeled the columns “Cluster 1” through “Cluster 5”  but there is no natural order to the clusters. On the row labeled “Suicide Ideation” we find that the first cluster expresses thoughts about committing suicide. The second  third  and fourth clusters discuss methods of committing suicide  and the fifth cluster portrays the user’s thoughts about planning suicide. Additionally  the risk factor “Self-Harm” also aligns well with our informal latent topics. Cluster one captures the notion of cutting oneself while cluster two focuses more clearly on damage to body parts such as “body”  “blood”  “burn” and many other words describing harm to one’s body. These topics both fit within the risk factor “Self-Harm”  showing agreement between our automatically generated topics and expert opinions. The informal topics captured by these clusters seem to embody the notion on suicidal ideation and suggest that our topics agree with the proposed risk factor. We found that some clusters were not squarely matched with risk factors. For example  we assigned the cluster containing “mom”  “dad”  “kill herself” and “kill himself” to the risk factor “Suicide Around Individual”. This cluster also includes “friend”  “dog”  “gf”  “boyfriend” and a long list of other types of individuals in the user’s life. This informal latent topic seems to capture not only the occurrence of suicide but also examples of strong personal relationships  the loss of which could be particularly traumatic. Thus  this cluster relates to both of the risk factors “Family Violence and Discord” and “Suicide Around Individual”. In fact  there were conceptual overlaps in many clusters  especially those pertaining to depression  suicide ideation  psychological disorders  and self-harm. In addition to finding more general topics  in some cases  the informal latent topics are more specific than the expertly derived risk factors. A good example of this is the risk factor “Drug Abuse” and the related informal topics. The first cluster represents the notion of “pills” and “sleeping pills”. The second cluster represents the notion of “medication” and “meds”. The third cluster represents the notion of “alcohol”  ‘drinking”  and recreational drugs such as “weed”. All of these clusters fit well under the heading of “Drug Abuse”  but vary significantly in their focus. The nuances in the discussions of drug abuse in online social media appear to result in topics capturing differing dimensions of this risk factor. We also occasionally didn’t find clusters associated with risk factors. Despite “Gun Ownership” previously being identified as an important risk factor [11]  we were unable to find a cluster which explicitly represented the idea of owning a gun. We did find the word  “gun”  in our clusters as well as many related words such as “shoot”. However  these words are clustered with terms related to suicidal thoughts rather than ownership. This example highlights one of the main differences between the expertly derived risk factors and the informal latent topics extracted from social media. While it may be true that those who have access to a gun are at greater risk to commit suicide  it does not appear that those who express suicide ideation online reference their ownership of a gun with as much clarity as they discuss other topics. Some clusters were particularly difficult to classify. The clusters corresponding to “Depressive Feelings” and “Depressive Symptoms” were difficult to differentiate. The Anxiety and Depression Association of America lists symptoms of depression as  among other things  irritability  insomnia  fatigue  difficulty making decisions  persistent physical symptoms  and feelings of hopelessness  worthlessness and helplessness. Many users discuss their depression  not as a dichotomy between feelings and symptoms  but instead use the words more casually. When assigning clusters to risk factors  we attempt to make a distinction between feelings and symptoms. Symptoms can be identified as physical ailments or development of disorders and conditions such as anxiety  sadness  and stress. Feelings tend to be a more nuanced description of one’s self and experience. One cluster contains the words “no”  “any”  “real”  “future”  “real”  “experience”  “motivation”  “social”  “dreams”  “purpose”  and “plans”. We classified this cluster as depressive feelings because the words seem to indicate a lack of purpose and a sense of uselessness. On the other hand  the cluster which contains “these”  “thoughts”  “feelings”  “suicidal thoughts”  “emotions’ and “panic attacks” is more focused on symptoms of depression that one may face. Regardless of whether or not a word is labeled as a symptom or as a feeling  our informal latent topics often capture very specific depressive language. One cluster contains “depressed”  “angry” and “upset” capturing common emotional keywords. Another cluster contains “chest”  “stomach” and “heavy” describing the physical reaction to stress. A third contains “into”  “fall” and “down” using the familiar imagery of downward movement when describing depression. Indeed  we found a total of nineteen clusters relevant to depressive feelings and symptoms  a few of which are presented in Table 2. The diversity and specificity of our informal latent topics seem to capture subtle differences in how users discuss suicide in online posts. We found other clusters which we could not label according to the twelve suicide risk factors  and which we accordingly labeled “Other Important”. These clusters were identified as possible contributors to suicide ideation  and contain information which we determined may be valuable to identify and assess suicide risk in social media posts. For example  one cluster includes “stupid”  “failure” and “selfish”. Authors of the posts often use these words to describe their self-image. Another cluster includes “died”  “cancer”  and “disease” presenting the occurrence of a serious medical condition in the user’s life. In all  we identified 22 important clusters that did not fit well into the 12 previously proposed risk factors  many representing stressors that might lead to suicidal ideation. Other topics include poor performance in school  trouble with money  and disgust with one’s physical appearance. The complexity of natural language often made it challenging to categorize the informal topics. For example  positive words are sometimes used to express negative feelings. A cluster containing mainly positive tokens such as “nice”  “beautiful”  “perfect”  “strong”  and “smart” may be referencing legitimately positive characteristics. On the other hand  a user might be posting about how good the life of other people seems to be while their life is lacking. Examples of these sentiments from posts are  “My family acts so perfect and seems so perfect from the outside” and “Why is everyone else so beautiful?”. Finally  many of the latent topics did not seem immediately relevant to suicidal ideation but were often present in the online posts. Five of these clusters are shown in the last row of Table 2. For example  one cluster represents the notion of food while another represents clothes.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,2,0,,0
https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-018-2197-z,25,12,discussion,"Grant, Kucher, Leon, Gemmell, Raicu, Fodeh",6,0,0,0,2018,To evaluate our models  we first subjectively evaluated the latent topics represented by clusters of words. We then compared these topics to risk factors generated by domain experts. Our in-depth analysis revealed several key findings. First  we found that the topics discovered by our analysis had a large scope. Topics ranged from crying to clothing to the calendar. This illustrates that our model was able to identify different latent topics within the corpus and separate them into meaningful clusters. It also shows that there are topics that people discuss which are not directly related to suicide  as not every word is on the topic of suicide. When comparing our automatically generated topics to previously identified risk factors  we found that there were some differences in the focus of the topics compared to that of the risk factors. In the case of “Drug Abuse”  people tended to discuss recreational drugs  specifically alcohol  separately from medications and pills. This difference in focus shows how the public view of these two topics may fit under the umbrella term provided by experts  but differ enough to be separate topics. On the other hand  in the case of “Family Violence and Discord” and “Suicide Around Individual”  the topics generated by our model seemed to indicate a broader topic  rather than topics as specific as these risk factors. A result of collecting data from public users with presumably no professional medical experience is the difference in precision of language between users and medical professionals. An indicator of this difference is in discussing depression. While professionals made a difference between “Depressive Feelings” and “Depressive Symptoms”  the topics identified from users’ posts overlapped these ideas. This may be partly due to the fact that depressive feelings are a symptom of depression  but also to a lack in precise use of language to describe specific experiences and symptoms. Our contribution to this field is the discovery of latent topics within textual data known to contain suicidal ideation. A common method for identifying suicidal ideation in social media is to use a filter designed by medical professionals to extract data. Such a technique may impose a structure on the data by medical professionals that does not reflect the actual language used by those experiencing suicidal ideation. Our method uses topic modeling to uncover informal  latent topics directly from social media posts  which captures the ideas deemed important by those who are sharing their experiences with an online community. This information will inform the medical community which informal topics are important to monitor in informal contexts  such as social media  to effectively identify suicidal ideation.,0,0,0,0,0,0,0,1,1,0,,0,0,0,0,0,0,,0
https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-018-2197-z,25,13,conclusion,"Grant, Kucher, Leon, Gemmell, Raicu, Fodeh",6,0,0,0,2018,In this work  we automatically extracted informal latent topics from online social media expressing suicidal ideations. We first subjectively evaluated the latent topics and then exhaustively compared them to risk factors proposed by domain experts. In general  we found that our informal topics are similar to the expert’s risk factors; however  our topics differ in several important ways. Our topics can be more specific or more general. Some of our topics express meaningful ideas not contained in the risk factors and some risk factors do not have complimentary latent topics. In short  our analysis of the latent topics extracted from social media containing suicidal ideations suggests that users of these systems express ideas that are complementary to the topics defined by experts but differ in their scope  focus  and precision of their language. This effort opens up many possibilities for future work. First  we will build models leveraging the informal topics to predict the urgency of the posts. Second  we plan to compare these results to other topic modeling algorithms such as latent Dirichlet analysis and latent semantic analysis. Finally  we will extend our analysis to other mental health issues such as post-traumatic stress disorder and depression.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,1
https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-018-2197-z,25,14,availability of data and materials,"Grant, Kucher, Leon, Gemmell, Raicu, Fodeh",6,0,0,0,2018,Interested researchers and students are invited to contact the authors for access to the datasets and code examples.,0,0,0,0,0,2,0,0,0,0,,0,0,0,0,0,0,,0
https://ieeexplore.ieee.org/abstract/document/8609647,26,1,introduction,"Fraga, da Silva, Murai",3,0,0,0,2018,According to the World Health Organization (WHO)  the number of people across the globe who suffer from mental illnesses has been growing: 322 million people worldwide are suffering from depression symptoms – most of them are women1 –  an increase of 18% between 2005 and 2015. These numbers push for efficient public health policies. For instance  the Mental Health Gap Action Programme (mhGAP) aims at scaling up services for mental  neurological and substance use disorders for countries worldwide. The programme posits that with proper care  psychosocial assistance and medication  tens of millions could be treated for depression  schizophrenia  and epilepsy  prevented from suicide and begin to lead normal lives – even where resources are scarce. This epidemic scenario urges for more research and resources on understanding and helping the individuals affected by these disorders. One possible way of achieving this goal is the use of Online Social Networks (OSNs). Initially designed for promoting friendship  OSNs started to connect people willing to share experiences related to health problems  such as obesity [1] and depression [2]  [3]. Our work presents an in-depth analysis of online communities focused on the discussion of mental health disorders. We hope that the insights unveiled by our analyses will help on building successful online intervention strategies to support people in crisis and assist their counselors. Hence  online interventions can complement face-to-face interventions  as the individuals become part of a community of people with that share similar problems and anxieties  and end up receiving support from peers along with that provided by trained medical personnel. Our study is based on Reddit  a social media in which members share their experiences on a wide range of subjects. The possibility of writing anonymously encourages Reddit users to discuss sensitive topics and share thoughts and feelings that are still a taboo in our society. Reddit is organized into communities  dubbed subreddits. As of December 2017  it was formed by 1 204 126 communities2  and 900 million comments3. At least 25 sub-categories on Reddit are related to mental health disorders [4]  [5]. Aiming to better understand how people interact and share their experiences in this type of community  the main contributions of this paper are: • A characterization of user interactions and activity levels in subreddits Depression (/r/depression)  SuicideWatch (/r/suicide) – henceforth referred as “Suicide” –  Anxiety (/r/anxiety) and Bipolar (/r/bipolar). These are the largest subreddits in number of active users  focused on discussing the respective mental disorders. • An in-depth content analysis of the post and comments made by the members of those communities. First  we investigate the shape and size of the discussion threads. Second  we extract the descriptors of the post and comments  which reveal the main discussed topics on these communities.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://ieeexplore.ieee.org/abstract/document/8609647,26,2,related work,"Fraga, da Silva, Murai",3,0,0,0,2018,In the last few years  the combination of face-to-face interventions and remote counseling has been widely used for assisting people with mental health disorders. Technology advances have changed the way people communicate (cell phones  social media and Internet)  thus opening new avenues for offering support. In this new context  some studies in the literature focus on understanding the effectiveness of the new technologies on helping people with mental illnesses symptoms. For instance  authors in [6] present a large-scale  quantitative study on the discourse of text-message-based counseling conversations and identify conversation strategies that are associated with better conversation outcomes. Some recent works consider the problem of detecting depression and suicide ideation in social networks [7]–[10]. These papers seek to understand the behavior of users in these social networks  aiming to propose policies for reducing the number of people affected by mental disorders issues. A number of works in the literature have investigated online communities related to mental health and support in mental illness subreddits. Authors in [11] study the writing structure of individuals on subreddits dealing with depression  bipolar disorder  and schizophrenia. The analysis shows that users suffering from these diseases have difficulty on expressing their ideas. Studies lead by the authors in [12] present a framework for supporting users in the Twitter and Reddit communities. They propose methods to identify a user’s risk of self-harm  which is related to users suffering from depression. The language used in Reddit communities specifically related to mental health has been investigated to define linguistic characteristics that could be helpful for designing further applications to identify people that need urgent attention [4]. Users’ self-disclosure in Reddit mental illness communities has been characterized in order to develop language models that describe social support  which are observed to bear emotional  informational  instrumental  and prescriptive information [2]. In the same direction  authors in [3] automatically identify helpful comments in online posts appearing in suicide watch forums with the SuicideWatch subreddit as the use-case. Similar to these papers  we perform a characterization of users in mental health-related subreddits through interactions and analysis of the content in posts and comments. However  unlike the previously mentioned studies  we present an indepth comparison across different subreddits. Furthermore  we analyze the shared content through a recent and more effective topic modeling approach proposed in [13]  which better uncovers the main topics discussed by members of these communities.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://ieeexplore.ieee.org/abstract/document/8609647,26,3,methodology,"Fraga, da Silva, Murai",3,0,0,0,2018,In this section we describe our dataset (Section III-A)  the model used to study users’ interactions and the definition of discussion trees (Section III-B)  and text descriptors detected by the Relationship Modeling Network (Section III-C).,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://ieeexplore.ieee.org/abstract/document/8609647,26,4,dataset,"Fraga, da Silva, Murai",3,0,0,0,2018,Reddit is a multilingual Online Social Network founded in 2005 and organized in subcommunities by areas of interest called subreddits. We obtained data from the Reddit’s data repository4  focusing on four subreddits where people discuss issues related to mental heath disorders: Depression (/r/depression)  SuicideWatch (/r/SuicideWatch)  Anxiety (/r/anxiety) and Bipolar (/r/bipolar). Our dataset is comprised of user activities (posts and comments) that took place between 2011 and 2017. Here  we focus on data from January 2017 to December 2017. In total  we obtained 261 511 posts and 1 256 669 comments from 184 708 unique users. Table I shows the total number of users  posts and comments per subreddit. The total number of comments in each community is at least 4.2 times larger than the number of posts  which suggests a supportive behavior among users.,2,2,2,2,0,1,0,0,0,0,,0,0,0,0,0,0,,0
https://ieeexplore.ieee.org/abstract/document/8609647,26,5,user interactonis model and discussion trees,"Fraga, da Silva, Murai",3,0,0,0,2018,We model user interactions as a weighted directed graph G  where each node represents a user  each directed edge (u  v) indicates that user u commented on one or more posts made by user v  and the edge weight is equal to the number of u’s comments summed over all v’s posts. Given a user u in G  the in-degree of u is the number of users who interact with user u  i.e.  the number of unique users who commented on posts from u. The out-degree of u  in turn  is the number of interactions u had within the community  i.e.  the number of posts u commented on. Social support is the process of interactions in relationships which improves coping  esteem  belonging  and competence through actual or perceived exchanges of psychosocial resources (adapted from [14]). In addition to measuring support from the interaction graph  we also measure it from discussion trees. In a discussion tree  the root node corresponds to a post  while other nodes correspond to comments made either in response to the post or to other comments in the same thread. Analyzing tree sizes (depth) and shapes (width) can shed some light on the topics that most attract user attention as well as pinpoint key members in each community. For instance  a highly emotional comment can trigger more reactions than the original post  i.e.  the majority of the discussion might be concentrated in one of the branches.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://ieeexplore.ieee.org/abstract/document/8609647,26,6,relationship modeling network rmn,"Fraga, da Silva, Murai",3,0,0,0,2018,RMN is a recursive neural network designed to model relationships between pairs of entities from text [13]. Each relationship is represented at a given point in time as a vector of weights over K descriptors. Entities that form a relationship do not need to be of the same class: for example  in this work  we model the relationships between a user and the community in which she interacts. Each post or comment corresponds to a different instant. Words are represented as embeddings of dimension P  that is  each word w of a vocabulary V is a vector in RP . Users and communities are represented by embeddings of dimension U and C  respectively. As in previous works  we generated embeddings using GloVe [15]. The descriptors obtained from RMN are vectors in RP   allowing us to find the closest words to each descriptor. The post’s (or comment’s) representation is denoted by vpost ∈ RP . This vector is the average of the word embeddings contained in the post. The representations of users and communities are denoted by vuser and vcomm. For each post or comment  RMN takes as input a vector v ∈ RP +U+C obtained by concatenating vpost  vuser and vcomm. These vectors are combined through the weights of the neural network to obtain a representation dt ∈ RK of the relationship between the user and the community at that particular time. RMN uses a smoothing parameter α ∈ (0  1) to avoid abrupt changes in the representation of the same relation in consecutive instants  dt and dt−1. The descriptor array R ∈ RK×P is used for attempting to reconstruct the post vpost  by making rt = Rdt. RMN parameters (weights and matrix of descriptors) are trained in order to maximize an objective function that aims to approximate rt and vpost while retaining some distance between rt and other randomly sampled posts. See [13] for more details on RMN. The input data for RMN was preprocessed as follows. First  we removed all posts and comments marked as [deleted] or [removed]  standard stop-words (using the NLTK library)  punctuation and accents. Second  for each subreddit  we removed posts and comments from users who performed less than 50 activities (posts or comments)  following the methodology presented in [16]. Finally  we selected the words that appear at least once in each of the four subreddits analyzed  seeking to find similarities in the way people express themselves when discussing mental health disorders. The final subset of data analyzed by RMN is composed of 18 020 unique words  25 101 posts and 401 428 comments.,0,0,0,0,2,0,0,0,0,0,,0,0,0,0,0,0,,0
https://ieeexplore.ieee.org/abstract/document/8609647,26,7,user activities and interactions,"Fraga, da Silva, Murai",3,0,0,0,2018,We begin our analysis with some statistics regarding number of user activities (posts and comments). Figure 1 depicts the number of posts and comments per month on each subreddit over eight years (2011 to 2017). Note that the number of posts and comments has grown substantially in all subreddits: Bipolar is the subreddit with largest increase on the number of posts (202.9 times)  followed by Anxiety (47.6 times)  Depression (17 times) and Suicide (7 times). In terms of number of comments  the subreddit with the largest increase is Bipolar (297.2 times)  followed by Anxiety (27.3 times)  Depression (11.8 times) and Suicide (4.4 times). Now we turn our attention to the in- and out-degree distributions of each interaction graph. Figure 2 shows the Complementary Cumulative Distribution Function (CCDF) of the in- and out-degrees. Both are heavy-tailed distributions for all subreddits. The largest in-degree and out-degree values are similar  except for Depression  where the largest in-degree is roughly two times larger than the out-degree. A close inspection of the in-degree data reveals that 63.8% (Depression)  71.5% (Suicide)  66.2% (Anxiety) and 75.2% (Bipolar) of users received at least one comment in their posts. Both in- and out-degrees are measures of interaction diversity. However  we are also interested in their intensity. In this case  we analyzed the graph edge weights  which represent the number of comments made/received by an user. About 84.7% of members interact each other just once on Depression subreddit  73.6% on Suicide  85.7% on Anxiety and 81.5% on Bipolar. A non-negligible number of members interact with each other from 2 to 20 times: 15.2% on Depression  26.3% on Suicide  14.3% on Anxiety and 18.5% on Bipolar. We compute several structural metrics from the interaction graph of each subreddit. More specifically  we compute the network diameter (largest distance between nodes)  transitivity (fraction of all possible triangles that exist in G)  besides statistics – mean  median  coefficient of variation (CV) – of node metrics [17]  namely eccentricity  closeness centrality  clustering coefficient and number of triangles. Table II shows the results obtained for each metric. Overall  we observe that interaction patterns are very similar across subreddits. Low transitivity combined with high diameter and node eccentricity corroborate the fact that user interactions are based mostly on the content of the posts and comments  regardless of the users who generate them. This behavior is encouraged by reddit  which does not have any content filter based on friendships and forbids “voting rings” in its term of services. We conjecture that this is also the reason for low values of closeness centrality. We also observe a large CV in the distribution of number of triangles  indicating that this number varies substantially across different nodes. This is a direct consequence of the large variation in the number of activities performed by different users. In sum  our analysis show that in these four subreddits related to mental health discussion  the interaction model is centered around the content of posts and comments  rather than users. On a positive note  this model helps new users to start their participation in the network  as it prevents the formation of tightly-knit groups.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://ieeexplore.ieee.org/abstract/document/8609647,26,8,discussion patterns,"Fraga, da Silva, Murai",3,0,0,0,2018,Here we analyze users interactions based on how discussions are structured and what are their topics. First  we model user interactions as discussion trees and analyze the shape and size of these trees. Second  we perform an in-depth analysis of posts and comments  extracting descriptors that uncover the main topics discussed on these communities.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://ieeexplore.ieee.org/abstract/document/8609647,26,9,discussion trees,"Fraga, da Silva, Murai",3,0,0,0,2018,For each subreddit  we extracted discussion trees from posts and comments. In a discussion tree  the root node corresponds to a post  while other nodes correspond to comments made either in response to the post or to other comments in that thread. Figure 3 shows the histogram of tree size per subreddit. The maximum tree depth in Depression is 142  much larger than those for Suicide (93)  Bipolar (37)  and Anxiety (34). Discussion trees with more than five levels are most frequent in Suicide (12.09%)  followed by Bipolar (7.68%)  Depression (5.32%) and Anxiety (3.73%). The deepest discussion trees tend to be related to emotional struggles and problems faced by users  and their request for support. For instance  the deepest discussion tree in Depression was triggered by an user who is a community moderator. We also analyzed the content present in the 12 deepest discussion trees: one from Depression  four from Suicide  four from Anxiety  and three from Bipolar. We conducted a small survey asking 12 participants what was the intent of the user who wrote the initial post. Each item in the survey presented the post content followed by four options – seeking help  offering help  neutral or other (to be filled out) –  from which exactly one had to be chosen. Table III shows two of the excerpts classified by the participants. The participants of survey were students in Computer Science and did not know or had contact with users who wrote the posts. The interrater agreement measured by the Fleiss’ Kappa statistic [18] was above 0.3178 for all items  which indicates fair agreement in the worst case. This survey reveals interesting insights on how members talk about their feelings and emotional wrestlings: 1) 7 out of 12 trees analyzed were triggered by a member seeking for help. 2) Deepest discussion trees are those in which people are more interested on trying to better understanding either how the person who started the thread is facing her problems in real life or her current feelings. 3) Members who start the discussion tree with a post with more than 50 words (11 out of 12) tend to attract comments that are long as well (mean number of words per comment ranging from 32.05 to 259.48). In the Suicide community  a tree triggered by a post with 160 words received 88 comments  with the mean words by comment equals to 69.61. This finding corroborates the results presented in [6].,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0
https://ieeexplore.ieee.org/abstract/document/8609647,26,10,descriptors analysis,"Fraga, da Silva, Murai",3,0,0,0,2018,We use the RMN to automatically extract textual descriptors from posts and comments. RMN parameters were set according to the values recommended in [13]. We use data from all subreddits combined as input since we want to obtain descriptors common to all communities. Table IV presents the terms (descriptors) associated with each topic found by RMN. It is interesting to note that the model captures several topics containing coherent descriptors (i.e.  topics whose main terms are clearly related)  showing that the four subreddits we analyzed share a common language  and that topics are closely related to mental health disorders (see  for example  Topic 3). The results presented below focus on the analysis of the descriptors that are shown in bold in Table IV. Table V lists possible interpretations of highlighted topics. Each user in a subreddit has a set of comments and posts called spans by RMN. For each span in the input  RMN outputs a probability vector over topics. To analyze the topics that compose a subreddit  we calculate the average of the vectors associated to the respective spans. This average is a vector that can be interpreted as the probability distribution over topics for a span selected uniformly at random from the dataset. For each subreddit  Figure 4 shows the average probability associated with each topic in Table V. Note that: 1) Topic 8  among the 10 we analyzed  is the most prominent in the four communities. This topic contains positive descriptors  indicating that users of the subreddits offer support and help to each other  mainly through words of optimism and perspective of a better future. 2) Topic 7  which includes terms of encouragement  is very likely in all subreddits. 3) Topic 3 is the second most frequent in the Depression and the Suicide communities. This topic has negative sentiment descriptors and refers to death  injury  guilt  etc. This is expected of communities focused on discussing problems related to depression and suicide. 4) Subreddits Depression and Suicide have very similar profiles with respect to these 10 topics. However  two topics distinguish them: (i) Topic 12 it is more likely to appear in Depression. This topic comprises descriptors related to a sense of worthlessness and waste  which indicates that people in the Depression community express more often a sense of worthlessness. 5) Topic 9  which refers to places and groups of relationships (friends  girlfriends  university  meetings)  occurs more frequently in the Depression and the Suicide communities  suggesting that the mental health problems of the users in these communities have been triggered by some friction in their relationships with peers. 6) Topic 10  concerns insecurity and adolescence issues  being a more frequent subject in the Suicide community and potentially reflecting problems of bullying commonly experienced in this stage of life. We also use RMN to analyze posts and comments made by users in order to define user profiles as probability distributions over topics. To do so  we calculate the average of the vectors associated with users’ spans. For each topic t in Table V  we discuss the profiles of the five most relevant users  i.e.  those whose probability masses associated with t are largest. 1) As previously described  Topic 1 is characterized by descriptors inclined towards social support. Out of the five most relevant users for this topic  four are part of subreddit Suicide and the other is part of subreddit Depression. At the same time  the probability masses of topics with more negative descriptors (3  10 and 12) is small for these users. These users tend to behave as counselors  offering help to other members of these communities. 2) Topic 3 has highly negative descriptors. Out of five most relevant users for this topic  three are in subreddit Suicide community and the other two in subreddit Depression. It is important to point out that  while these users have a more negative discourse  they have a relatively high probability mass associated with Topic 8  which reflects a discourse of optimism and possible improvement. This finding corroborates the analysis in [6]  where the authors conclude that people who go through periods of great hardship are more likely to think about the future and to be positive  when aided by others who offer help. 3) The spans of users most relevant to Topic 28 – linked to suggestions for activities to soften a negative emotional condition – also have large probabilities masses associated with Topics 7 and 8  suggesting a profile of users who seek to share experiences of possible improvements in mental health through activities and hobbies.,0,0,0,0,2,0,0,0,0,0,,0,0,0,0,0,0,,0
https://ieeexplore.ieee.org/abstract/document/8609647,26,11,conclusion,"Fraga, da Silva, Murai",3,0,0,0,2018,This work presented an analysis of four subreddits related to mental health disorders: Anxiety  Bipolar  Depression and Suicide. Through modeling users interactions as a graph we characterized these subreddits with respect to several network metrics  providing strong evidences that even in these subreddits the interactions are centered around content  rather than users. Next  we used discussion trees to characterize conversations that takes place inside these communities. In particular  we conducted a small survey to analyze the 12 deepest discussion trees. Supported by a fair interrater agreement  we concluded that most posts are requests for help  and that more often than not  multiple users offer help. Most of the deepest trees are long conversations between two users. Using the RMN model  we automatically detected topics that describe what is most frequently discussed in the four communities. As expected  topics related to suicidal ideation are more common in the Suicide and Depression communities. Moreover  we discovered that subreddits Depression and Suicide are similar for most topics. We also analyzed the most relevant users for each topic. For example  for topic 7  which is related to overcoming problems  the most relevant users are part of the Bipolar  Suicide or Anxiety communities.,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,,0